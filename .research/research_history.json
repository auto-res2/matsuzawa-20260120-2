{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data efficient image classification",
    "few-shot augmentation",
    "sample efficient regularization",
    "semi-supervised image augmentation",
    "mixup regularization images",
    "cutmix image classification",
    "autoaugment small datasets",
    "contrastive vision regularization",
    "consistency regularization images",
    "efficientnet low data"
  ],
  "research_study_list": [
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification",
      "full_text": "Contextual Squeeze-and-Excitation for Efﬁcient Few-Shot Image Classiﬁcation Massimiliano Patacchiola University of Cambridge mp2008@cam.ac.uk John Bronskill University of Cambridge jfb54@cam.ac.uk Aliaksandra Shysheya University of Cambridge as2975@cam.ac.uk Katja Hofmann Microsoft Research kahofman@microsoft.com Sebastian Nowozin∗ nowozin@gmail.com Richard E. Turner University of Cambridge ret26@cam.ac.uk Abstract Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personaliza- tion, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a speciﬁc user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classiﬁcation setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to signiﬁcantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a ﬁne-tuning routine to adapt a linear head, deﬁning a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading ﬁne-tuning methods with the beneﬁt of orders of magnitude lower adaptation cost. 1 Introduction In recent years, the growth of industrial applications based on recommendation systems (Bennett et al., 2007), speech recognition (Xiong et al., 2018), and personalization (Massiceti et al., 2021) has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a speciﬁc user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classiﬁcation task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user’s device (context). In these applications the goal is twofold: maximize the classiﬁcation accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources. Methods developed to face these challenges in the few-shot classiﬁcation setting can be grouped in two categories: meta-learning and ﬁne-tuning. Meta-learning is based on the idea of learning-how-to- learn by improving the algorithm itself (Schmidhuber, 1987; Hospedales et al., 2020). Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates (Chen et al., 2019; Triantaﬁllou et al., 2019; Tian et al., 2020; Kolesnikov et al., 2020; Dumoulin et al., 2021). ∗Work done while the author was at Microsoft Research – Cambridge (UK) 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.09843v3  [cs.CV]  11 Jan 2023We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. Figure 1 illustrates this comparison by showing on the vertical axis the average classiﬁcation accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, Dumoulin et al. 2021), and on the horizontal axis the adaptation cost measured as the number of multiply–accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, ﬁne-tuners achieve a higher classiﬁcation accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, Kolesnikov et al. 2020) and LITE (Bronskill et al., 2021), shows a substantial performance gap of 14% in favor of the ﬁne-tuner but at a much higher adaptation cost, with BiT requiring 526 ×1012 MACs and LITE only 0.2 ×1012 MACs. Figure 1: Accuracy and adaptation cost on VTAB for meta-learners (blue), ﬁne- tuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE nar- rows the gap with the leading ﬁne-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost. It is crucial to ﬁnd solutions that retain the best of both worlds: the accuracy of ﬁne-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of ﬁne-tuners is the need for multiple gradi- ent adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) signiﬁcantly speeds up ﬁne-tuning, but it harms perfor- mance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is there- fore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a ﬁne-tuning routine for optimizing the head. At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by Hu et al. (2018) to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive ﬁne-tuning routines for the linear head, similarly to methods like MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). Figure 1 shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, ﬁne-tuners such as MD-Transfer (Triantaﬁllou et al., 2019), and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classiﬁcation performance. Our contributions can be summarized as follows: 1. We introduce a new adapter calledContextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by Hu et al. (2018), that outperforms other adaptation mechanisms (e.g. the FiLM generators used in Bronskill et al. 2021) in terms of parameter efﬁciency (a 75% reduction in the number of adaptation parameters) and classiﬁcation accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1. 2. We use CaSE adaptive blocks in conjuction with a ﬁne-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classiﬁcation accuracy compared to the SOTA meta-learner (Bronskill et al., 2021) on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT (Kolesnikov et al., 2020) with the beneﬁt of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset (Massiceti et al., 2021), where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT). 1https://github.com/mpatacchiola/contextual-squeeze-and-excitation 22 Contextual Squeeze-and-Excitation (CaSE) Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us deﬁne a collection of meta-training tasks as D= {τ1,...,τ D}where τi = (Ci,Ti) represents a generic task composed of a context set Ci = {(x,y)1,..., (x,y)M}and a target set Ti = {(x,y)1,..., (x,y)D}of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task τ∗ = {C∗,x∗}the goal is to predict the true label y∗ of the unlabeled target point x∗ conditioned on the context set C∗. In ﬁne-tuning methods, we are given a neural network fθ(·), with parameters θ estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task τ∗ adaptation consists of minimizing the lossL(·) via gradient updates to ﬁnd the task-speciﬁc parameters θτ∗ ←G(ϵ,L,τ∗,fθ), where ϵis a learning rate, and G(·) is a functional representing an iterative routine that returns the adapted parameters θτ∗ (used for prediction). This procedure is particularly effective because it can exploit efﬁcient mini-batching, parallelization, and large pretrained models. In meta-learning methods training and evaluation are performed episodically (Vinyals et al., 2016), with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to deﬁne a hierarchy. The parameters are divided in two groups: φtask-common parameters shared across all tasks (top of the hierarchy), and ψτ task-speciﬁc parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way φand ψτ come into play is method dependent; they can be estimated via gradient updates (e.g. MAML, Finn et al. 2017), learned metrics (e.g. ProtoNets, Snell et al. 2017), or Bayesian methods (Gordon et al., 2018; Patacchiola et al., 2020; Sendera et al., 2021). Standard Squeeze-Excite (SE) We brieﬂy introduce standard SE (Hu et al., 2018), as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of Llayers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function gφ(·). The number of hidden units in the MLP is deﬁned by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B×C×H×W where Cis the number of channels, Hthe height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H1,..., HB}with Hi ∈RC×H×W. To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B×C×1 ×1; this can be interpreted as a set of vectors {h1,..., hB}with hi ∈RC. For each layer l, the set is passed to the associated MLP that will generate an individual scale vector γi ∈RC, where γ(l) 1 = g(l) φ ( h(l) 1 ) ··· γ(l) B = g(l) φ ( h(l) B ) . (1) An elementwise product is then performed between the scale vector and the original tensor ˆH(l) 1 = H(l) 1 ∗γ(l) 1 ··· ˆH(l) B = H(l) B ∗γ(l) B , (2) with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in Figure 2 (left). Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to deﬁne a new version of SE for task-based channel-wise feature adaptation. For a task τ = (C,T), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H1,..., HN}with Hi ∈RC×H×W. As in standard SE, we ﬁrst apply a spatial pooling to each tensor Hi which produces N vectors {h1,..., hN}of shape hi ∈RC. Then a context pooling is performed; this corresponds to an empirical mean over {h1,..., hN}(see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer γ(l) = g(l) φ ( ¯h(l) ) with ¯h(l) = 1 N ( h(l) 1 + ··· + h(l) N ) , (3) 3Figure 2: Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, ∗elementwise multiplication. which is then multiplied elementwise by the original tensors ˆH(l) 1 = H(l) 1 ∗γ(l) ··· ˆH(l) N = H(l) N ∗γ(l). (4) The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of Figure 2. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in Figure 2 (right), additional technical details are provided in Appendix A. Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, Perez et al. 2018). Those mechanisms perform adaptation using a separate convolutional set-encoder to produce an embedding of the context set. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM (Oreshkin et al., 2018), CNAPs (Requeima et al., 2019), SimpleCNAPs (Bateni et al., 2020), CA VIA (Zintgraf et al., 2019), and LITE (Bronskill et al., 2021). We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efﬁcient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method (LITE, Bronskill et al. 2021), showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters. 3 UpperCaSE: system description and optimization protocol We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate- Descent (CD). We call this protocolhybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a ﬁne-tuning routine to estimate the task-speciﬁc parameters (head). Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), deﬁned as bθ(·) where θare the pretrained parameters. CaSE blocks, 4parameterized by φ, are added to the model at speciﬁc locations to give bθ,φ(·) (see Appendix A for details about this step). We are interested in learning the CaSE parameters φkeeping constant the pretrained parameters θ(omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks τ = {C,T}∼D , where Dis the training set. The number of classes (way) is calculated from the context set and used to deﬁne a linear classiﬁcation head hψτ (·) parameterized by ψτ. The complete model is obtained by nesting the two functions as hψτ (bφ(·)). We indicate a forward pass through the body over the context inputs with the shorthand bφ(Cx) →{z1,..., zN}, where zn is the context embedding for the input xn. All the context embeddings and the associated labels are stored in M= {(zn,yn)}N n=1. Optimization challenges We have two sets of learnable parameters,φthe CaSE parameters, and ψτ the parameters of the linear head for the taskτ. While φis shared across all tasks (task-common), ψτ must be inferred on the task at hand (task-speciﬁc). In both cases, the objective is the minimization of a classiﬁcation loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient dL dφ = ∑ τ (∂Lτ ∂ψτ dψτ dφ + ∂Lτ ∂φ ) . (5) The ﬁrst term ∂Lτ/∂ψτ (sensitivity of the loss w.r.t. the head) and the direct gradient ∂L/∂φ (sensitivity of the loss w.r.t. the adaptation parameters with a ﬁxed head) can be obtained with auto-differentiation as usual. The second term dψτ/dφ(sensitivity of the head w.r.t. the adaptation parameters) is problematic because ψτ is obtained iteratively after a sequence of gradient updates. Backpropagating the gradients to φincludes a backpropagation through all the gradient steps per- formed to obtain the task-speciﬁc ψτ. Previous work has showed that this produces instability, vanishing gradients, and high memory consumption (Antoniou et al., 2018; Rajeswaran et al., 2019). Meta-training via Coordinate-Descent A potential solution to these issues is the use of implicit gradients (Chen et al., 2020; Rajeswaran et al., 2019; Chen et al., 2022). The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy’s implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). These methods share the idea of inner-loop-head/outer- loop-body meta-training, and they ﬁnd the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) (Wright, 2015). The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. φand ψτ as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ψτ ﬁrst, we reach a local minimum where ∂Lτ/∂ψτ ≈0. Therefore CD induces a direct optimization objective w.r.t.φ, with Equation (5) reducing to ∂Lτ/∂φ(no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details. In practice, at each training iteration we sample a task τ = (C,T) ∼D, perform a forward pass on the body (with CaSE in adaptive mode) to get bφ(Cx) →{z1,..., zN}. (6) The context embeddings are temporarily stored in a buffer with their associated labels M = {(zn,yn)}N n=1 to avoid expensive calls to bφ(·). We then set the head parameters to zero, and solve the ﬁrst minimization problem (inner-loop), obtaining the task-speciﬁc parameters ψτ via ψτ ←G ( α,M,L,hψτ ) (7) where αis a learning rate, and G(·) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation(7) only relies on the headhψτ (·) and not on the bodybφ(·), which is the primary source of memory savings and the crucial difference with common ﬁne-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efﬁcient mini-batching. 5We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t.φ(outer-loop) given support/target points and the task-speciﬁc parameters ψτ identiﬁed previously. The ﬁnal form of the equation depends on the optimizer, for a generic SGD the update is given by φ←φ−β∇φL ( Cy ∪Qy,hψτ ,bφ ) , (8) where βis a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B. Inference on unseen tasks After the training phase, we are given an unseen task τ∗ = (C∗,x∗) where x∗ is a single target input and y∗ the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation (6) and embeddings/labels stored in M, (ii) estimation of the task-speciﬁc parameters ψ∗ via iterative updates as in Equation (7), and (iii) inference of the target-point membership via a forward pass over body and head ˆy∗ = hψ∗ (bφ(x∗)) with CaSE in inference mode. 4 Related work Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details (Hospedales et al., 2020). LITE (Bronskill et al., 2021) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs (Bateni et al., 2020) that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classiﬁcation accuracy and parameter efﬁciency. Fine-tuning Chen et al. (2019) were the ﬁrst to expose the potential of simple ﬁne-tuning baselines for transfer learning. MD-Transfer has been proposed in Triantaﬁllou et al. (2019) as an effective ﬁne-tuning baseline for the MetaDataset benchmark. More recently Kolesnikov et al. (2020) have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classiﬁcation head, learning schedule, normalization layers, and batching. Fine-tuning only the last linear layer can be effective (Bauer et al., 2017; Tian et al., 2020). We compare against this baseline in Section 5.1, showing that adapting the body via CaSE signiﬁcantly boosts the performance. Overall, ﬁne-tuners have consistently outperformed meta-learners in terms of classiﬁcation accuracy, only under particular conditions (e.g. strong class-imbalance) the trend is reversed (Ochal et al., 2021a,b). Hybrids Hybrid methods are trained episodically like meta-learners but rely on ﬁne-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, Finn et al. 2017) ﬁnds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ (Antoniou et al., 2018), ProtoMAML (Triantaﬁllou et al., 2019), and Reptile (Nichol et al., 2018). Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs (Han et al., 2021). Adaptive components have been used in a variety of applications, such as neural compression (Veit and Belongie, 2018; Wu et al., 2018), generation of artistic styles (Dumoulin et al., 2016; Huang and Belongie, 2017), or routing (Guo et al., 2019). Residual adapters (Rebufﬁ et al., 2017, 2018) have been used in transfer learning (non few-shot) but they rely on ﬁne-tuning routines which are signiﬁcantly slow during adaptation. More recently, Li et al. (2022) have used serial and residual adapters in the few-shot setting, with the task-speciﬁc weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-speciﬁc weights in the body of the network which is costly. In Sun et al. (2019) the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via ﬁne-tuning. This method suffers of similar limitations, as the ﬁne-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. 6Figure 3: Left: CaSE vs Squeeze-and-Excitation (SE) (both methods use EfﬁcientNetB0, 84 × 84 inputs, Mahalanobis-distance head). CaSE outperforms SE in all conditions. Center: CaSE vs. FiLM generators (Bronskill et al., 2021) and a baseline with no body adaptation (all methods use EfﬁcientNetB0, 84 ×84 inputs, Mahalanobis-distance head). CaSE outperforms FiLM generators in all conditions. Right: boxplot of CaSE activations at different depth of an EfﬁcientNetB0 for 800 tasks sampled from the MDv2 test set (224 ×224 inputs, UpperCaSE). The modulation of CaSE is minimal at early stages for general-purpose ﬁlters and increases at deeper stages. 5 Experiments In this section we report on experiments on VTAB+MD (Dumoulin et al., 2021) and ORBIT (Massiceti et al., 2021). VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfﬁcientNetB0 from the ofﬁcial Torchvision repository; ResNet50x1-S released with BiT (Kolesnikov et al., 2020). We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of Bronskill et al. (2021) ( 10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10−3,10−5] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (conﬁdence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . 5.1 Analysis of CaSE blocks In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE (Hu et al., 2018) and CaSE on MDv2 and VTAB, conﬁrming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator (Bronskill et al., 2021), showing that CaSE is signiﬁcantly more efﬁcient using 75% fewer parameters while boosting the classiﬁcation accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis. Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD- v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in Bronskill et al. (2021), reporting results with a linear head in the appendix. We summarize the results in Figure 3 (left) and add a tabular breakdown in Appendix C.2. CaSE outperforms SE in all conditions, conﬁrming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks. Comparing adaptation mechanisms We perform a comparison on VTAB+MD of CaSE against FiLM generators (Bronskill et al., 2021), and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfﬁcientNetB0 backbone, and same training schedule. We show a summary of the results in 7Figure 3 (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneﬁcial. This is likely due to the strong domain shift introduced by some of the specialized datasets. Role of CaSE blocks To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfﬁcientNetB0 trained with UpperCaSE on224×224 images. In Figure 3 (right) we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose ﬁlters that are useful across all tasks. In deeper layers the variance increases, showing a task-speciﬁc modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4. 5.2 Performance evaluation of UpperCaSE In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA ﬁne-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB;2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset (Massiceti et al., 2021) for the cross-domain condition MDv2→ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars. Table 1: UpperCaSE outperforms ﬁne-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold. Cost↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. MACs all all natur. spec. struc. MD-Transfer ﬁne-tuning RN18 126 11.2 118.6 63.4 55.6 52.4 72.9 49.3 SUR ﬁne-tuning RN50 224 164.6 28.8 71.3 43.7 50.9 66.2 27.2 Big Transfer ﬁne-tuning RN50 224 23.5 526.3 73.3 65.4 69.4 81.0 54.5 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.2 76.1 58.4 69.1 80.3 39.4 Table 2: UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest num- ber of parameters per adaptive blocks . Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold. Adapt.↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. count all all natur. spec. struc. ProtoMAML hybrid RN18 126 11.2 n/a 64.2 45.0 45.7 70.7 31.5 CTX meta-learning RN34 224 21.3 n/a 71.6 50.5 61.1 67.3 34.0 ProtoNet meta-learning ENB0 224 4.0 n/a 72.7 46.1 60.9 64.2 25.9 LITE meta-learning ENB0 224 4.0 1.7 73.8 51.4 65.2 71.9 30.8 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.4 76.1 58.4 69.1 80.3 39.4 Comparison on VTAB+MDWe compare UpperCaSE against ﬁne-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results 8Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% conﬁdence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al. (2021): meta-train on MetaDataset and test on ORBIT, image-size 84 ×84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold. Cost Clean Video Evaluation (CLE-VE) Clutter Video Evaluation (CLU-VE) Method MACs ↓ frame acc.↑ FTR↓ video acc.↑ frame acc.↑ FTR↓ video acc.↑ ProtoNet 3.2 59.0 ±2.2 11.5 ±1.8 69.2 ±3.0 47.0 ±1.8 20.4 ±1.7 52.8 ±2.5 CNAPs 3.5 51.9 ±2.5 20.8 ±2.3 60.8 ±3.2 41.6 ±1.9 30.7 ±2.1 43.0 ±2.5 MAML 95.3 42.5 ±2.7 37.3 ±3.0 47.0 ±3.2 24.3 ±1.8 62.3 ±2.3 25.7 ±2.2 FineTuner 317.7 61.0±2.2 11.5 ±1.8 72.6 ±2.9 48.4 ±1.9 19.1 ±1.7 54.1 ±2.5 UpperCaSE 3.5 63.0±2.2 8.8 ±1.6 74.4 ±2.8 48.1 ±1.8 18.2 ±1.7 54.5 ±2.5 in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfﬁcientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see Table 1) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%). The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE (Bronskill et al., 2021) which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difﬁcult to compensate without ﬁne-tuning the body. It is not clear whether transfer learning is beneﬁcial on these datasets in the ﬁrst place. The results in terms of adaptation cost (see Table 1) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efﬁcient (0.2 ×1012 MACs) than all ﬁne-tuners, with BiT being the most expensive method overall (526.3 × 1012 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see Table 2) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfﬁcientNetB0) compared to LITE (Bronskill et al., 2021) which is based on FiLM generators. Comparison on ORBIT We compare UpperCaSE to other methods on ORBIT (Massiceti et al., 2021), a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classiﬁers are meta-trained on MetaDataset and tested on ORBIT. Results are reported in Table 3. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR −2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efﬁcient than FineTuner and close to the leading method (ProtoNet). 6 Conclusions We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and- Excitation (SE) block proposed by Hu et al. (2018). CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of ﬁne-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark. 9Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that ﬁne-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large. Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classiﬁ- cation settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efﬁciency of our method can reduce energy consumption and beneﬁt the environment. Certain applications require careful consideration to avoid biases that can harm speciﬁc groups of people (e.g. surveillance, legal decision-making). Acknowledgments and Disclosure of Funding Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos, Daniela Massiceti, and Shoaib Ahmed Siddiqui for providing suggestions and feedback on the preliminary version of the manuscript. References Antoniou, A., Edwards, H., and Storkey, A. (2018). How to train your maml. arXiv preprint arXiv:1810.09502. Bateni, P., Goyal, R., Masrani, V ., Wood, F., and Sigal, L. (2020). Improved few-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Bauer, M., Rojas-Carulla, M., ´Swi ˛ atkowski, J. B., Schölkopf, B., and Turner, R. E. (2017). Discrimi- native k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326. Bennett, J., Lanning, S., et al. (2007). The netﬂix prize. In Proceedings of KDD cup and workshop. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136. Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efﬁcient meta-learning with large images. Advances in Neural Information Processing Systems. Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. (2019). A closer look at few-shot classiﬁcation. arXiv preprint arXiv:1904.04232. Chen, Y ., Friesen, A. L., Behbahani, F., Doucet, A., Budden, D., Hoffman, M., and de Freitas, N. (2020). Modular meta-learning with shrinkage. Advances in Neural Information Processing Systems. Dumoulin, V ., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and Larochelle, H. (2021). Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark. arXiv preprint arXiv:2104.02638. Dumoulin, V ., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style.arXiv preprint arXiv:1610.07629. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W. (2018). Neural processes. arXiv preprint arXiv:1807.01622. 10Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. (2018). Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921. Guo, Y ., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. (2019). Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. (2020). Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439. Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Huang, X. and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In European conference on computer vision. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021a). Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021b). How sensitive are meta-learners to dataset imbalance? arXiv preprint arXiv:2104.05344. Oreshkin, B., Rodríguez López, P., and Lacoste, A. (2018). Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31. Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and Storkey, A. J. (2020). Bayesian meta- learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. (2019). Meta-learning with implicit gradients. Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2018). Efﬁcient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. 11Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. (2019). Fast and ﬂexible multi- task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems. Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München. Sendera, M., Tabor, J., Nowak, A., Bedychaj, A., Patacchiola, M., Trzcinski, T., Spurek, P., and Zieba, M. (2021). Non-gaussian gaussian processes for few-shot regression. Advances in Neural Information Processing Systems. Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems. Sun, Q., Liu, Y ., Chua, T.-S., and Schiele, B. (2019). Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. (2020). Rethinking few-shot image classiﬁcation: a good embedding is all you need? In European Conference on Computer Vision. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. Veit, A. and Belongie, S. (2018). Convolutional networks with adaptive inference graphs. In Proceedings of the European Conference on Computer Vision (ECCV). Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learning. Advances in Neural Information Processing Systems. Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Programming, 151. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L. S., Grauman, K., and Feris, R. (2018). Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. In IEEE international conference on acoustics, speech and signal processing (ICASSP). Zintgraf, L., Shiarli, K., Kurin, V ., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In International Conference on Machine Learning. 12A CaSE: additional details A.1 CaSE implementation Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer las showed in Equation (3), that is ¯h(l) ∈RC, subtracting the mean and dividing by the standard deviation. Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c= 2which extends the range to [0,2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 (Table 6). CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in Bronskill et al. (2021) for FiLM generators. In EfﬁcientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the ﬁrst layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfﬁcientNet): Conv2d→BatchNorm2d→SiLU→CaSE→SqueezeExcitation→Conv2d→BatchNorm2d This results in a total of 18 CaSE blocks for EfﬁcientNetB0. Increasing the number of blocks did not provide a signiﬁcant beneﬁt. In ResNet18 we place two CaSE blocks per each basic block as: Conv2d→BatchNorm2d→ReLU→CaSE→Conv2d→BatchNorm2d→ReLU→CaSE Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details. Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor. CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input sizeC(corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor rmin that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels. A.2 Context pooling In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2). Similarities with other methods Context pooling is a way to summarize a task with a permutation- invariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets (Snell et al., 2017) a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class. The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs (Bateni et al., 2020) a prototype is estimated as in ProtoNets but it is used to deﬁne a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family (Garnelo et al., 2018) also rely on similar permutation-invariant aggregations to deﬁne distributions over functions. 13Global vs. local context-pooling Comparing CaSE with the FiLM generators of Bronskill et al. (2021) it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of Bronskill et al. (2021) rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More speciﬁcally, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding. CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4). 14A.3 Pytorch code for CaSE Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . import torch from torch import nn class CaSE (nn. Module ): def __init__ (self , cin , reduction =32 , min_units =32 , standardize =True , out_mul =2.0, device =None , dtype = None ): \"\"\" Initialize a CaSE adaptive block . Parameters : cin ( int ): number of input channels . reduction ( int ): divider for computing number of hidden units . min_units ( int ): clip hidden units to this value (if lower ). standardize ( bool ): standardize the input for the MLP . out_mul ( float ): multiply the MLP output by this value . \"\"\" factory_kwargs = {’ device ’: device , ’dtype ’: dtype } super (CaSE , self ). __init__ () self . cin = cin self . standardize = standardize self . out_mul = out_mul hidden = max ( min_units , cin // reduction ) self . gamma_generator = nn. Sequential ( nn. Linear (cin , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , cin , bias =True , ** factory_kwargs ), nn. Sigmoid () ) self . reset_parameters () def reset_parameters ( self ): nn. init . zeros_ ( self . gamma_generator [4]. weight ) nn. init . zeros_ ( self . gamma_generator [4]. bias ) self . gamma = torch . tensor ([1.0]) def forward (self , x): if( self . training ): # adaptive mode self . gamma = torch . mean (x, dim =[2,3]) # spatial pooling self . gamma = torch . mean ( self . gamma , dim =[0])# context pooling if( self . standardize ): self . gamma = ( self . gamma - torch . mean ( self . gamma )) / \\ torch . sqrt ( torch . var ( self . gamma , unbiased = False ) + 1e-5) self . gamma = self . gamma . unsqueeze (0) self . gamma = self . gamma_generator ( self . gamma ) * self . out_mul self . gamma = self . gamma . reshape ([1,-1,1,1]) return self . gamma * x else : # inference mode self . gamma = self . gamma .to(x. device ) return self . gamma * x def extra_repr ( self ): return ’cin ={}’. format ( self . cin ) 15B UppereCaSE: additional details B.1 Algorithm of UpperCaSE Algorithm 1 UpperCaSE: training function for the few-shot classiﬁcation setting. Require: D= {τ1,...,τ D}training dataset Require: bφ() pretrained feature extractor (body) with CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α, β: step-size hyperparameters for the optimizer. 1: Set φto random values ⊿optional: set φto enforce identity in CaSE output 2: while not done do 3: Sample task τ = (C,T) ∼D 4: Forward pass over context set bφ(Cx) → z1,..., zN ⊿CaSE in adaptive mode 5: Store context embeddings and associated labels M= {(zn,yn)}N n=1 ⊿temporary memory buffer 6: Deﬁne a linear model for the head hψτ () and set ψτ to zero 7: for total inner-steps do ⊿loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B∼M 9: Update the head parameters ψτ ←step(α,L,B,hψτ ) 10: end for 11: Update the CaSE parameters φ←step(β,L,C,T,bφ,hψτ ) ⊿CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classiﬁcation setting. Require: τ∗ = (C∗,x∗) unseen test task with target input x∗ an context C∗. Require: bφ() pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α: step-size hyperparameter for the optimizer. 1: Forward pass over context set bφ(Cx ∗ ) → z1,..., zN ⊿CaSE in adaptive mode 2: Store context embeddings and associated labels M∗ = {(zn,yn)}N n=1 ⊿temporary memory buffer 3: Deﬁne a linear model for the head hψτ∗ () and set ψτ∗ to zero 4: for total inner-steps do ⊿loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B∗ ∼M∗ 6: Update the head parameters ψτ∗ ←step(α,L,B∗,hψτ∗ ) 7: end for 8: Return Prediction ˆy∗ = hψτ∗ (bφ(x∗)) ⊿CaSE in inference mode C Additional experimental details and results C.1 Additional details MACs counting MACs are proportional to the size of the task, size of the images, and number of classes. We can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ×224 ×3 generated via Gaussian noise (µ= 0,σ = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfﬁcientNetB0 backbone for the ﬁrst and a ResNet50-S for the second. For MD-Transfer we used the same parameters reported in Dumoulin et al. (2021) with images of size 126 ×126 ×3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone. VTAB+MD trainingWe follow the protocol reported in the original papers (Triantaﬁllou et al., 2019; Dumoulin et al., 2021) training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and conﬁdence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfﬁcientNetB0 from the ofﬁcial Torchvision repository 3, and a pretrained ResNet50-S 2https://github.com/microsoft/ORBIT-Dataset 3https://pytorch.org/vision 16from the BiT repository 4. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use the BiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the ofﬁcial Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). For the evaluation phase we followed the instructions reported in Massiceti et al. (2021). C.2 CaSE vs SE Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al., 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold. Model SE CaSE SE CaSE Contextual pooling No Yes No Yes Adaptation head MD MD Linear Linear Image size 84 84 224 224 MetaDataset (all) 67.8 69.6 74.6 76.2 VTAB (all) 43.6 45.3 56.6 58.2 VTAB (natural) 47.5 50.2 65.3 68.1 VTAB (specialized) 63.6 64.9 79.8 79.6 VTAB (structured) 30.6 31.8 38.6 40.1 C.3 CaSE vs other adapters Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al. (2021), and a baseline with no body adaptation. CaSE blocks are more efﬁcient in terms of adaptive and amortization parameters while providing higher classiﬁcation accuracy. All models have been trained and tested on 84 ×84 images, using a Mahalanobis distance head. Best results in bold. Adaptation type None FiLM CaSE64 CaSE32 CaSE16 Adaptive Params (M) n/a 0.02 0.01 0.01 0.01 Amortiz. Params (M) n/a 1.7 0.4 0.8 1.6 MetaDataset (all) 53.4 68.4 69.8 69.6 70.4 VTAB (all) 43.5 44.7 46.2 45.3 46.4 VTAB (natural) 45.4 49.5 52.1 50.2 52.6 VTAB (specialized) 69.4 63.8 66.3 64.9 65.5 VTAB (structured) 29.1 31.7 31.8 31.8 32.1 C.4 Ablation studies In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows: • Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in Table 6 show that the sigmoid with multiplier and the linear layer provide the best results. 4https://github.com/google-research/big_transfer 17• Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in Table 8 show that blocks with more units provide marginal gains or no gains at all. This is probably due to overﬁtting issues affecting the models with more units. • Ablation of the number of hidden layers of CaSE. The results reported in Table 7 show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overﬁtting issues affecting the models with more parameters. • Ablation of the activation function for the hidden layers. Results reported in Table 9 show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfﬁcientNet backbones). Table 6: Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfﬁcientNetB0 backbone with a linear head on images of size 224 ×224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the ﬁrst iterations). Best results in bold. Adaptive block SE SE CaSE CaSE CaSE Activation (output) Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2 MetaDataset (all) 74.2 74.6 75.8 74.9 76.2 VTAB (all) 56.8 56.6 58.4 56.8 58.2 VTAB (natural) 67.0 65.3 68.3 67.1 68.1 VTAB (specialized) 81.1 79.8 79.5 80.8 79.6 VTAB (structured) 36.9 38.6 40.3 37.1 40.1 Table 7: Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ×224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. # Hidden layers 1 2 3 4 Amortiz. Params (M) 0.420 0.426 0.432 0.438 MetaDataset (all) 76.0 76.1 75.5 75.2 VTAB (all) 58.2 58.4 58.2 58.0 VTAB (natural) 68.3 69.1 68.0 67.4 VTAB (specialized) 79.7 80.3 80.5 80.3 VTAB (structured) 40.0 39.4 39.7 39.7 18Table 8: Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is deﬁned by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Reduction factor 64 32 16 8 Clip factor 16 32 48 64 Amortiz. Params (M) 0.4 0.8 1.6 3.0 MetaDataset (all) 76.1 76.2 75.8 76.2 VTAB (all) 58.4 58.2 57.9 58.5 VTAB (natural) 69.1 68.1 67.9 68.3 VTAB (specialized) 80.3 79.6 79.4 79.0 VTAB (structured) 39.4 40.1 39.7 40.9 Table 9: Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortiza- tion parameters) and they have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Activation (hidden) SiLU ReLU Tanh MetaDataset (all) 76.1 75.8 74.8 VTAB (all) 58.4 57.8 48.2 VTAB (natural) 69.1 69.8 67.0 VTAB (specialized) 80.3 79.7 80.8 VTAB (structured) 39.4 39.4 36.4 19C.5 Role of CaSE blocks Figure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfﬁcientNetB0 (horizontal axis, with early stages on the left). The box encloses ﬁrst to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles. 20Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfﬁcientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the ﬁrst stages but diverge in the latest. The magnitude tends to increase with depth. 21C.6 UpperCaSE: results on VTAB+MD In this section we provide a full breakdown of the results for UpperCaSE vs. other methods on the VTAB+MD benchmark. Results for other methods are taken from Bronskill et al. (2021) and Dumoulin et al. (2021). UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfﬁcientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks. In Table 10 we report the results for UpperCaSE against ﬁne-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE). Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneﬁcial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and ﬁne-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set. In this case, ﬁne-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against ﬁne-tuning methods. Best result in bold. Model BiT MD-Transfer SUR UpperCaSE UpperCaSE Image Size 224 126 224 224 224 Network RN50-S RN18 RN50 ×7 ENB0 RN50-S Params (M) 23.5 11.2 164.5 4.0 23.5 Omniglot 68.0 ±4.5 82.0 ±1.3 92.8±0.5 90.7±0.4 89.1 ±0.5 Aircraft 77.4 ±3.5 76.8 ±1.2 84.4 ±0.6 89.4±0.4 87.5±0.4 Birds 90.8±1.5 61.2±1.3 75.8 ±1.0 90.4±0.4 89.6 ±0.4 DTD 85.0±2.5 66.0±1.1 74.3 ±0.7 83.4±0.4 84.8 ±0.5 QuickDraw 66.6 ±3.7 61.3 ±1.1 70.3 ±0.7 76.8±0.5 73.7±0.6 Fungi 59.4 ±4.2 35.5 ±1.1 81.7±0.6 59.3±0.8 56.8 ±0.8 Trafﬁc Sign 73.5 ±4.7 84.7±0.9 50.0±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 65.7±2.7 39.6±1.0 49.4 ±1.1 50.8 ±0.7 46.7 ±0.8 Caltech101 87.2 70.6 82.3 88.3 86.2 CIFAR100 54.4 31.3 33.7 52.7 47.0 Flowers102 83.3 66.1 55.7 85.3 83.0 Pets 87.9 49.1 76.3 89.9 89.3 Sun397 33.3 13.9 27.5 35.8 32.5 SVHN 70.4 83.2 18.7 62.7 59.8 EuroSAT 94.4 88.7 78.9 92.2 91.6 Resics45 76.1 63.7 62.4 75.5 74.4 Patch Camelyon 83.1 81.5 75.6 79.3 80.9 Retinopathy 70.2 57.6 27.9 74.3 73.7 CLEVR-count 74.0 40.3 30.0 40.3 42.0 CLEVR-dist 51.5 52.9 37.1 38.9 37.3 dSprites-loc 82.7 85.9 30.0 45.3 38.1 dSprites-ori 55.1 46.4 19.8 42.5 41.4 SmallNORB-azi 17.8 36.5 12.9 15.7 15.1 SmallNORB-elev 32.1 31.2 18.1 22.7 21.0 DMLab 43.2 37.9 33.3 38.7 36.1 KITTI-dist 79.9 58.7 52.3 71.0 69.6 MetaDataset (all) 73.3 63.4 71.0 76.1 74.9 VTAB (all) 65.4 55.6 42.9 58.4 56.6 VTAB (natural) 69.4 52.4 49.0 69.1 66.3 VTAB (specialized) 81.0 72.9 61.2 80.3 80.1 VTAB (structured) 54.5 49.4 29.2 39.4 37.6 22Table 11: Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold. Model ProtoNet ProtoMAML CTX LITE UpperCaSE UpperCaSE Image Size 224 126 224 224 224 224 Network ENB0 RN18 RN34 ENB0 ENB0 RN50-S Params (M) 4.0 11.2 21.3 4.0 4.0 23.5 Omniglot 88.3 ±0.8 90.2±0.7 84.6±0.9 86.5 ±0.8 90.7±0.4 89.1±0.5 Aircraft 85.0 ±0.7 82.1 ±0.6 85.3 ±0.8 83.6 ±0.7 89.4±0.4 87.5±0.4 Birds 90.2±0.5 73.4±0.9 72.9 ±1.1 88.6 ±0.7 90.4±0.4 89.6±0.4 DTD 81.4 ±0.6 66.3 ±0.8 77.3 ±0.7 84.1±0.7 83.4±0.4 84.8±0.5 QuickDraw 76.0±0.7 66.4±1.0 73.3 ±0.8 75.7±0.8 59.3±0.8 56.8 ±0.8 Fungi 57.4 ±1.1 46.3 ±1.1 48.0 ±1.2 56.9 ±1.2 59.3±0.8 56.8±0.8 Trafﬁc Sign 53.5 ±1.1 50.3 ±1.1 80.1±1.0 65.8±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 49.8±1.1 39.0±1.0 51.4±1.1 50.0 ±1.0 50.8 ±0.7 46.7±0.8 Caltech101 87.4 73.1 84.2 87.7 88.3 86.2 CIFAR100 43.1 29.7 37.5 48.8 52.7 47.0 Flowers102 78.2 60.2 81.8 83.5 85.3 83.0 Pets 88.6 56.6 70.9 89.3 89.9 89.3 Sun397 32.9 8.1 24.8 30.9 35.8 32.5 SVHN 35.2 46.8 67.2 51.0 62.7 59.8 EuroSAT 83.3 80.1 86.4 89.3 92.2 91.6 Resics45 68.8 53.5 67.7 76.4 75.5 74.4 Patch Camelyon 73.3 75.9 79.8 81.4 79.3 80.9 Retinopathy 31.3 73.2 35.5 40.3 74.3 73.7 CLEVR-count 27.2 32.7 27.9 31.4 40.3 42.0 CLEVR-dist 28.5 35.4 29.6 32.8 38.9 37.3 dSprites-loc 13.4 42.0 23.2 12.3 45.3 38.1 dSprites-ori 19.6 23.0 46.9 31.1 42.5 41.4 SmallNORB-azi 9.4 13.4 37.0 14.5 15.7 15.1 SmallNORB-elev 17.0 18.8 21.6 21.0 22.7 21.0 DMLab 35.8 32.5 31.9 39.4 38.7 36.1 KITTI-dist 56.5 54.4 54.3 63.9 71.0 69.6 MetaDataset (all) 72.7 64.2 71.6 73.9 76.1 74.9 VTAB (all) 46.1 45.0 50.5 51.4 58.4 56.6 VTAB (natural) 60.9 45.7 61.1 65.2 69.1 66.3 VTAB (specialized) 64.2 70.7 67.3 71.9 80.3 80.1 VTAB (structured) 25.9 31.5 34.1 30.8 39.4 37.6 23",
      "references": [
        "How to train your maml.",
        "Improved few-shot visual classification.",
        "Discriminative k-shot learning using probabilistic models.",
        "The netflix prize.",
        "Meta-learning with differentiable closed-form solvers.",
        "Memory efﬁcient meta-learning with large images.",
        "Meta-learning feature representations for adaptive gaussian processes via implicit differentiation.",
        "A closer look at few-shot classification.",
        "Modular meta-learning with shrinkage.",
        "Comparing transfer and meta learning approaches on a uniﬁed few-shot classification benchmark.",
        "A learned representation for artistic style.",
        "Model-agnostic meta-learning for fast adaptation of deep networks.",
        "Neural processes.",
        "Meta-learning probabilistic inference for prediction.",
        "Spottune: transfer learning through adaptive fine-tuning.",
        "Dynamic neural networks: A survey.",
        "Meta-learning in neural networks: A survey.",
        "Squeeze-and-excitation networks.",
        "Arbitrary style transfer in real-time with adaptive instance normalization.",
        "Big transfer (bit): General visual representation learning.",
        "Meta-learning with differentiable convex optimization.",
        "Cross-domain few-shot learning with task-specific adapters.",
        "Orbit: A real-world few-shot dataset for teachable object recognition.",
        "On first-order meta-learning algorithms.",
        "Few-shot learning with class imbalance.",
        "How sensitive are meta-learners to dataset imbalance?",
        "Tadam: Task dependent adaptive metric for improved few-shot learning.",
        "Bayesian meta-learning for the few-shot setting via deep kernels.",
        "Film: Visual reasoning with a general conditioning layer.",
        "Rapid learning or feature reuse? towards understanding the effectiveness of maml.",
        "Meta-learning with implicit gradients.",
        "Learning multiple visual domains with residual adapters.",
        "Efficient parametrization of multi-domain deep neural networks.",
        "Fast and flexible multi-task classification using conditional neural adaptive processes.",
        "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook.",
        "Non-gaussian gaussian processes for few-shot regression.",
        "Prototypical networks for few-shot learning.",
        "Meta-transfer learning for few-shot learning.",
        "Rethinking few-shot image classification: a good embedding is all you need?",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples.",
        "Convolutional networks with adaptive inference graphs.",
        "Matching networks for one shot learning.",
        "Coordinate descent algorithms.",
        "Blockdrop: Dynamic inference paths in residual networks.",
        "The microsoft 2017 conversational speech recognition system.",
        "Fast context adaptation via meta-learning."
      ],
      "meta_data": {
        "arxiv_id": "2206.09843v3",
        "authors": [
          "Massimiliano Patacchiola",
          "John Bronskill",
          "Aliaksandra Shysheya",
          "Katja Hofmann",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2022-06-20T15:25:08Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Contextual Squeeze-and-Excitation (CaSE), a meta-learned adapter that rapidly personalises a pretrained CNN for a new few-shot task using one forward pass over the context set; proposes UpperCaSE, a hybrid meta-learning/ fine-tuning framework that meta-trains CaSE blocks for the network body while fine-tuning only a linear head via a coordinate-descent routine; achieves new state-of-the-art accuracy–adaptation-cost trade-off on the 26-dataset VTAB+MetaDataset benchmark and on the ORBIT real-world personalisation benchmark, with 50-75% fewer adaptation parameters than FiLM approaches and orders-of-magnitude fewer multiply-accumulates than conventional fine-tuning.",
        "methodology": "1) Extend classical Squeeze-and-Excitation from instance-based to task-based adaptation by (a) performing spatial pooling per image, (b) averaging across the N context images (context pooling) to obtain a task representation, and (c) passing it through a small MLP to output a single scale vector per layer that modulates channel activations for all task samples; no shift parameters are used, halving memory.\n2) Insert CaSE blocks at selected layers of a pretrained backbone (EfficientNetB0 or ResNet50-S).\n3) Meta-training: episodic training with coordinate-descent—inner loop learns a task-specific linear classifier h_ψ via SGD on stored context embeddings, outer loop updates shared CaSE parameters φ by back-propagating through the frozen head.\n4) Inference: single forward pass of context (adaptive mode) to compute scales, optimise ψ for the head, then predict targets with CaSE in inference mode (scales frozen).\n5) Numerous design choices validated: reduction ratio, activation, placement, comparison to FiLM, SE.",
        "experimental_setup": "• Benchmarks: MetaDataset-v2 (8 image datasets) and VTAB (18 datasets) for few-shot classification; additional cross-domain evaluation on ORBIT (videos of objects recorded by blind users).\n• Tasks: Standard 5- or 10-shot, variable-way episodes; VTAB uses 1000 train/validation images per task; evaluation averages over 1 200 tasks per MD dataset.\n• Models: Pretrained EfficientNetB0 and ResNet50-S; images resized to 84×84 or 224×224.\n• Training: 10 000 meta-training tasks, Adam with learning rate 1e-3→1e-5 decay; linear head optimised for 500 SGD steps with batch 128; CaSE reduction r∈{64,32}.\n• Hardware: 3 workstations with Tesla V100 GPU, 6-core CPU, 110 GB RAM.\n• Metrics: Mean classification accuracy per dataset and aggregated splits; adaptation cost measured in multiply–accumulate operations (MACs) for a synthetic 100-way 10-shot task.\n• Baselines: Fine-tuners (BiT, MD-Transfer, SUR), meta-learners (ProtoNet, LITE, CTX), hybrids (ProtoMAML).\n• Ablations: SE vs CaSE; CaSE vs FiLM generators; number of hidden layers/units; activation functions; channel-wise activation analysis.",
        "limitations": "1) Requires hundreds of gradient steps to optimise the linear head at adaptation time; devices lacking efficient autodiff or with strict latency/power budgets may still be challenged.\n2) Performance lags behind full fine-tuning on structured VTAB tasks (e.g., localization and counting), suggesting limited capacity of scale-only channel modulation under large domain shift.\n3) Relies on large ImageNet-pretrained backbones; effectiveness with smaller or non-vision backbones not studied.\n4) Context pooling uses simple mean aggregation, which could under-represent complex task distributions.\n5) Study confined to classification; generalisation to detection, segmentation, or non-visual modalities unverified.",
        "future_research_directions": "• Develop head-free or closed-form adaptation to remove gradient steps, enabling on-device deployment.\n• Combine CaSE with spatial attention or residual adapters to better tackle structured/geometry-heavy tasks.\n• Investigate automatic adapter placement and dynamic reduction ratios to further cut parameters.\n• Explore richer context aggregation (e.g., attention, set transformers) and uncertainty-aware scaling.\n• Extend CaSE/UpperCaSE to continual, multimodal, or reinforcement-learning settings and to tasks beyond classification such as segmentation or retrieval.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
      "full_text": "Frozen Feature Augmentation for Few-Shot Image Classification Andreas B¨ar1 2 * Neil Houlsby1 Mostafa Dehghani1 Manoj Kumar1 † 1Google DeepMind 2Technische Universit¨at Braunschweig andreas.baer@tu-braunschweig.de {neilhoulsby, dehghani, mechcoder}@google Abstract Vision foundation models are currently one of the main driving forces in computer vision research. Simply training a linear classifier or a lightweight model on top of model outputs or so-called ‘frozen features’ leads to impressive performance on a number of tasks. Currently, frozen fea- tures are not modified during training of such lightweight models. On the other hand, when networks are trained di- rectly on images, data augmentation is a standard recipe that improves performance with no additional overhead. In this paper, we conduct an extensive pilot study that ex- plores applying data augmentations in the frozen feature space for few-shot image classification. We dub this type of augmentation ‘frozen feature augmentation (FroFA)’. Our study demonstrates that adopting deceptively simple point- wise FroFAs, such as brightness, can improve few-shot per- formance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction A prevalent trend now is to pretrain vision models on large datasets and adapt them downstream [5, 41, 56]. Notable, even training a simple linear layer or a light-weight model on top of vision transformer (ViT) outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream tasks [13, 19, 43]. However, there is still an interest in training ViTs to achieve good performance on ImageNet-sized [36, 52] or smaller [31, 34] datasets. In this setting, a crucial ingre- dient is data augmentation — a predefined set of simple, stochastic input transformations. Simple but effective ex- amples for image augmentations include random cropping which extracts a fixed-sized region from an image of ar- bitrary resolution, or pixel-wise modifications that change brightness, saturation, or contrast. These are complemented by more advanced augmentation strategies such as mixup [58] or RandAugment [10]. *Work conducted as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0.0 2.5 5.0 top-1 acc. (abs. gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Few-shot results averaged across eight test sets, in- cluding ILSVRC-2012 [14, 44]. We use cached features from an L/16 model [16] pretrained on JFT-3B [56] (left) or WebLI [5] following a sigmoid language-image pretraining (SigLIP) [57] (right). Our method, i.e., a multi-head attention pooling [30] head trained with weight decay (MAPwd) and frozen feature augmenta- tion (FroFA), shows significant gains across all shots with respect to a weight-decayed MAP, i.e., MAPwd, or an L2-regularized lin- ear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques in a data-constrained, few-shot frozen feature setting. In particular, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmenta- tions on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eighteen augmentations applied to frozen features extracted from vision transformers pretrained on JFT-3B [56], ImageNet-21k [14, 44], or WebLI [5]. We train a small lightweight multi-head attention pooling (MAP) [30, 56] head using these augmented inputs and evaluate its performance across eight downstream image classification datasets, where we on average achieve signif- icant gains (see Fig. 1). Our major insights are as follows: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ImageNet. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady im- 1 arXiv:2403.10519v2  [cs.CV]  26 Jul 2024provements on 1-, 5-, and 10-shot settings. 2. Unlike traditional image augmentations that apply a sin- gle randomly sampled value across the entire image, we introduce per-channel stochasticity by sampling inde- pendent random values for each channel. For example, on the 5-shot setting, we improve accuracy over a well- tuned MAP and linear probe baseline by 0.5% absolute and 0.8% absolute, respectively. 3. While FroFA provides modest but significant improve- ments on ImageNet, it excels on smaller transfer datasets. Across seven downstream datasets, FroFA out- performs the mean accuracy of the MAP baseline in the 5 shot setting by 3.2% absolute and the linear probe base- line by 4.2% absolute. 2. Related Works Transfer learning on few-shot data : State-of-the-art vi- sion models [5, 13, 16, 56] are typically pretrained on large-scale datasets, e.g., ImageNet-21k [14, 44] or ver- sions of JFT [21, 56], before transferred to other middle- scale to small-scale ones,e.g., CIFAR10 [1], ILSVRC-2012 [14, 44], or SUN397 [53, 54]. Depending on the model size, efficient transfer learning becomes a challenge. Many meth- ods have been proposed for large language models (LLMs), e.g., adapters [22], low-rank adaptation (LoRA) [23], or prompt tuning [32], of which some have been successfully adapted to computer vision [4, 17, 24, 59]. CLIP-Adapter [17] builds on the power of contrastive language-image pre- training (CLIP) [43] and combines it with adapters [22]. A follow-up work [59] proposes TiP-Adapter which uses a query-key cache model [18, 42] instead of a gradient de- scent approach. Inspired by the success of prompt tuning in LLMs [32], Jia et al. propose visual prompt tuning at the model input [24]. On the other hand, AdaptFormer [4] uses additional intermediate trainable layers to finetune a frozen vision transformer [16]. In contrast, we do not introduce additional prompts [24] or intermediate parameters [4, 17] that require backprop- agating through the network. Instead, we train a small network on top of frozen features coming from a vision transformer. This aligns with linear probing [43] which is typically used to transfer vision models to other tasks [13, 19, 56] — our objective. In addition, we focus our experiments around transfer learning on few-shot data [29, 51]. Although not surprising, few-shot results obtained by Dehghani et al . [13] clearly show significant gaps between linear probing and full fine- tuning. We take these results as an incentive to improve upon linear probing. Data augmentation: One go-to method to improve per- formance while training in a low-data regime is data aug- mentation [46]. Some prominent candidates in computer vision are AutoAugment [9], AugMix [20], RandAugment [9], and TrivialAugment [39]. These methods typically combine low-level image augmentations together to aug- ment the input. Although some works propose augmen- tations in feature space [15, 28, 33, 37, 50], a large-scale empirical study on frozen features of single-modal vision models does not exist. To this end, we investigate frozen feature augmentation (FroFA) by reformulating eighteen image augmentations. In particular, we consider a subset used in AutoAugment [9], inception crop [48], mixup [50, 58], and the recently introduced patch dropout [35]. 3. Framework Overview In this section, we give an overview of our framework. 3.1. Notation Let x ∈ IH×W×3 be an RGB image of height H, width W, and I = [0, 1]. A classification model processes x and outputs class scores y ∈ [0, 1]S for each class in a pre- defined set of classes S, with S = |S|. Let L and D be the number of intermediate layers and the number of fea- tures of a multi-layer classification model, respectively. We describe the intermediate feature representations of x as f = f(ℓ) = (f(ℓ) d ) ∈ RD, with layer index ℓ ∈ {1, ..., L} and feature index d ∈ {1, ..., D}. In the vision trans- former [26] architecture, f = f(ℓ) = (f(ℓ) n,c) ∈ RN×C is a two-dimensional entity, where N and C are the number of patches and number of per-patch channels, respectively. In addition, we introduce the patch index n ∈ {1, ..., N} and the per-patch channel index c ∈ {1, ..., C}. 3.2. Training on Cached Features We investigate pretrained vision transformers [26] with L transformer blocks (TBs) followed by a multi-head atten- tion pooling (MAP) [30] and a classification layer (CL). Fig. 2a presents a simplified illustration. For simplicity, we neglect all operations before the first transformer block (e.g., patchifying, positional embedding, etc.). To cache intermediate feature representations, we pro- cess each image x from an image dataset Dx through the network up until transformer blockL. Next, we store the re- sulting features f. After processing Dx we obtain a (frozen) feature dataset Df , with f ∈ Df (Fig. 2b). Finally, we train a lightweight model using the cached (frozen) features. Fig. 2c shows an example where a single MAP layer followed by a classification layer is trained using the feature dataset Df . Since our focus is fast training, we defer a detailed analysis on larger models to future work. 3.3. Frozen Feature Augmentation (FroFA) Data augmentation is a common tool to improve general- ization and is typically applied on the input, or in our case: 2(Frozen) Pretrained Model TB TB TB MAP CL (a) Step 1: Select a (frozen) pretrained model and a layer for caching. (Frozen) Pretrained Model image dataset (frozen) feature dataset TB TB TB (b) Step 2: Process an image dataset and cache the (frozen) features. Lightweight Model (frozen) feature dataset MAP CL frozen feature augmentation (FroFA)  (c) Step 3: Train on (augmented) frozen features. Figure 2. Pipeline for caching and training on (frozen) fea- tures. (2a): Given a (frozen) pretrained vision transformer, withL Transformer blocks (TBs), a multi-head attention pooling (MAP) layer, and a classification layer (CL), we select its L-th Trans- former block for caching. (2b): Next, we feed images x ∈ Dx to cache (frozen) features f ∈ Df . (2c): Finally, we use Df to train a lightweight model on top. We investigate frozen feature augmentation (FroFA) af ∈ Af in this scenario. images. A natural question arises: How to map such image augmentations to intermediate feature representations? Recall that the feature representation f = (fn,c) ∈ RN×C (layer index ℓ omitted) is two-dimensional. We first reshape it to a three-dimensional representation, i.e., f∗ = (f∗ n1,n2,c) ∈ R √ N× √ N×C. (1) We further define f∗ c = f∗ :,:,c ∈ R √ N× √ N×1 (2) as a two-dimensional representation of the c-th channel. Images and feature representations differ in two funda- mental aspects: channel dimensionality and value range. Before adapting image augmentations to the feature space, it is crucial to handle these differences. Channel dimensionality: RGB images have just three channels while intermediate representations possess an ar- bitrary number of channels. To address this, we ignore im- age augmentations that rely on three color channels, e.g., color jitter, and consider augmentations which can have an arbitrary number of channels instead, denoted asCa, cover- ing a majority of commonly applied image augmentations. Value range: RGB values lie within a specific range I, e.g., I = [0, 1] or I = {0, ...,255} ⊂N0, while in theory features have no such constraints. Assuming H = √ N and W = √ N, we define an image augmentation as ax : I √ N× √ N×Ca → I √ N× √ N×Ca , ax ∈ Ax, (3) where Ax is the set of image augmentations andCa = C is an arbitrary number of channels. To also address the value range mismatch, we introduce a deterministic feature-to- image mapping tf→x : R √ N× √ N×Ct → I √ N× √ N×Ct (4) that maps each element of f∗ (1) from R to I. In our exper- iments, we use xf = tf→x(f∗) = f∗ − fmin fmax − fmin , (5) where fmin and fmax are the minimum and maximum value of f∗, respectively, with elements of xf now in I = [0, 1]. We further define an image-to-feature mapping tf←x : I √ N× √ N×Ct → R √ N× √ N×Ct (6) that maps xf back to the original feature value range, with Ct = C by default. In this case, we simply invert (4) and use f∗ = tf←x(xf ) =xf · (fmax − fmin) +fmin. (7) Combining (3), (4), and (6), we obtain a generic (frozen) feature augmentation (FroFA) as a function composition af = tf←x ◦ ax ◦ tf→x. (8) We use three variations of af : 1. (Default) FroFA: We applyaf (8) once across the entire feature representation. We set Ca = Ct = C and com- pute fmin and fmax in (5), (7) across all elements of f∗. Further, as normally done in pixel space,ax (3) samples a random augmentation value and changes all elements of xf using the same value. For example, employing random contrast in a FroFA fashion scales each element of xf by the exact same randomly sampled factor. 2. Channel FroFA (cFroFA) : For each channel in the mapped features xf (5), ax (3) samples a random aug- mentation value per channel and applies that value to all elements in that channel. By using cFroFA for our ran- dom contrast example, we obtain C independently sam- pled scaling factors, one for each channel. 3. Channel2 FroFA (c2FroFA): In addition to applying augmentations per channel as done in cFroFA,tf→x (4) and tx←f (6) also operate per channel. In this case,fmin and fmax are the per-channel maximum and minimum, respectively. In contrast, FroFA and cFroFA use the maximum and minimum across the entire feature. We 3denote this variant as c 2FroFA since both the mappings (4), (6) and the augmentation (3) are applied on a per- channel basis. Although not adding additional stochas- ticity, we found that for random brightness this variant gives more stable results across a range of augmentation hyper parameters. While an element-wise FroFA might seem like a natural next step, our initial experiments lead to significantly worse results. We hypothesize that per-element augmentations might lead to substantial changes in the feature appearance. 4. Experimental Setup In this section, we introduce our experimental setup. 4.1. Network Architectures We employ the following pretrained vision transformers from prior work: Ti/16 [49], B/16 [16], and L/16 [16]. Fur- ther, we follow [56] and employ a lightweight multi-head attention pooling (MAP) layer [30] before the final classifi- cation layer on top of the frozen features (cf . Sec. 3.3). 4.2. Datasets Pretraining: We consider three datasets: JFT-3B, ImageNet-21k, and WebLI. First introduced by Hinton et al. [21], JFT is now a widely used proprietary, large-scale dataset [5, 7, 11, 16, 26, 27, 47, 56]. For our investigations we use the JFT-3B version following Zhai et al . [56]. It consists of nearly 3 billion multi-labeled images following a class-hierarchy of 29,593 labels. We further use ImageNet- 21k [14, 44] which consists of 14,197,122 (multi)-labeled images and 21,841 distinct labels. We equally split the first 51,200 images into a validation and test set and use the remaining 14,145,922 images for training. As a third dataset, we use WebLI [5] which is a recently introduced web-scale multilingual image-text dataset. Please refer to the Appendix, Sec. A3.1, for more details. Few-shot transfer : After pretraining we use eight datasets for few-shot transfer: ILSVRC-2012 [14, 44], CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. ILSVRC-2012, also known as ImageNet-1k, is a slimmed version of ImageNet-21k and contains 1,281,167 training images of 1,000 classes. We use it as our main few-shot benchmark throughout the paper. We randomly sample 1-shot, 5-shot, 10-shot, and 25-shot versions from the first 10% of the training set. We further create addi- tional disjoint sets by using the next four 10% fractions of the training set. In addition, we follow previous works [3] and create a ‘minival’ set using the last 1% (12,811 images) of the ILSVRC-2012 training set. The ‘minival’ set is used for hyper parameter tuning and design decisions while the official ILSVRC-2012 validation set is used as a test set. In summary, our setup consists of 1,000, 5,000, 10,000, or 25,000 training images, 12,811 validation images (‘mini- val’), and 50,000 test images (‘validation’). For the other seven datasets, we also select a training, validation, and test split and create few-shot versions. More details on how these splits are created can be found in the Appendix, Sec. A3.1. We follow a similar procedure as with ILSVRC-2012 and use 10% of the training images to cre- ate 1-shot, 5-shot, 10-shot, and 25-shot versions of each dataset. We further use each validation set for hyper pa- rameter tuning and report final results on the respective test set. 4.3. Data Augmentation We reuse the set of augmentations first defined in AutoAug- ment [9] and adopted in later works, such as RandAugment [10] and TrivialAugment [39]. In addition, we also consider a few other image augmentations [35, 48, 58]. We select five geometric augmentations, i.e., rotate, shear-x, shear-y, translate-x, and translate-y; four crop & drop augmenta- tions, i.e., crop, resized crop, inception crop [48], and patch dropout [35]; seven stylistic augmentations, i.e., brightness, contrast, equalize, invert, posterize, sharpness, and solarize; and two other augmentations, i.e., JPEG and mixup [58]. In total, we end up with eighteen distinct augmentations . Note that all data augmentations incorporate random oper- ations, e.g., a random shift in x- and y-direction (translate- x and translate-y, respectively), a randomly selected set of patches (patch dropout), a random additive value to each feature (brightness), or a random mix of two features and their respective classes (mixup). Please refer to the Ap- pendix, Sec. A3.2, for more details. We focus on the following set of experiments: 1. We investigate FroFA for all eighteen augmentations. 2. For our top-performing FroFAs, namely, brightness, contrast, and posterize, we incorporate additional stochasticity using cFroFA and c 2FroFA variants ( cf . Sec. 3.3). 3. We investigate a sequential protocol where two of the best three (c/c 2)FroFAs are arranged sequentially, namely, brightness c 2FroFA, contrast FroFA, and pos- terize cFroFA. We test all six possible combinations. 4. Finally, we also apply variations of RandAugment [10] and TrivialAugment [39] directly on top of cached frozen features. More details and results can be found in the Appendix, Secs. A3.2 and A4, respectively. 4.4. Training & Evaluation Details We describe some base settings for pretraining, few- shot learning, and evaluation. Please refer to Appendix, Sec. A3.3 for more training details. Pretraining: We use the Big Vision code base for https://github.com/google-research/big_vision 4pretraining. We take the Ti/16, B/16, and L/16 models pre- trained on JFT-3B from Zhai et al. [56]. In addition, we pretrain Ti/16, B/16 and L/16 on ImageNet-21k following the settings of Steiner et al. [46]. To further explore trans- fer capabilities we also use an L/16 model with sigmoid language-image pretraining (SigLIP) [57] on WebLI [5]. Few-shot learning: We use the Scenic code base [12] for few-shot learning. We train the lightweight MAP-based head by sweeping across five batch sizes (32, 64, 128, 256, and 512), four learning rates (0.01, 0.03, 0.06, and 0.1), and five training step sizes (1,000; 2000; 4,000; 8,000; and 16,000), yielding 100 configurations for each shot. We use the respective validation set for early stopping and to find the best sweep setting. Our cached-feature setup fits on a single-host TPUv2 platform where our experiments run in the order of minutes. Evaluation: We report the top-1 accuracy across all our few-shot datasets. On ILSVRC-2012, we tune few-shot models exclusively on our validation set (our ILSVRC-2012 ‘minival’, cf . Sec. 4.2) and report results on our test set (of- ficial ILSVRC-2012 ‘validation’ set, cf . Sec. 4.2). 4.5. Baseline Models We establish two baselines: MAP and linear probe. MAP: We first cache theN×C-shaped (frozen) features from the last transformer block. Afterwards, we train a lightweight MAP head from scratch using the cached fea- tures followed by the final classification layer ( cf . Fig. 2). For simplicity, the MAP head follows the same architectural design as the underlying pretrained model. In some exper- iments, we additionally apply weight decay (wd), denoted as MAPwd. We sweep across [ADD V ALUES] and use the respective validation set for early stopping and to find the best sweep setting. Linear probe: We use cached1×C-shaped outputs from the pretrained MAP head to solve an L2-regularized regres- sion problem with a closed-form solution [56]. We sweep the L2 decay factor using exponents of 2 ranging from -20 up to 10. This setting is our auxiliary baseline. 5. Finding the Optimal FroFA Setup We focus our first investigations on an L/16 model pre- trained on JFT-3B, i.e., our largest model and largest im- age classification pretraining dataset, followed by few-shot learning on subsets of ILSVRC-2012 training set, i.e., our largest few-shot dataset. We will refer to this setup as our L/16 JFT-3B base setup. 5.1. Baseline Performance We first report the baseline performance in Tab. 1. We ob- serve a large gap between MAP and linear probe in the 1- https://github.com/google-research/scenic Method 1-shot 5-shot 10-shot 25-shot MAP 57.9 78.8 80.9 83.2 Linear probe 66.5 79.6 81.5 82.4 Table 1. Average top-1 accuracy for baseline settings on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5) and follow the respective baseline setting ( cf . Sec. 4.5). The best setting for each baseline is found using our ILSVRC- 2012 validation set. Further, each shot is sampled five times. The best result per shot is boldfaced. shot setting (-8.6% absolute) which significantly decreases in the 5-, 10-, and 25-shot settings to -0.8%, -0.6%, and +0.8% absolute, respectively. In the following, our main point of comparison is the MAP baseline. This might be counter-intuitive since the performance is worse than linear probe in most cases. How- ever, the higher input dimensionality in the MAP-based set- ting (cf . Sec. 4.5) gives us the option to reshape the input to three dimensions ( cf . Sec. 3.3) which opens up more room and variety for frozen feature augmentations (Fro- FAs). Later in Sec. 6.4, we compare the performance of our best augmentations to the linear probe baseline. 5.2. Default FroFA As a next step, we investigate the effect of adding a single FroFA to the MAP baseline setting. We first focus on the default FroFA formulation which uses a single randomly sampled value per input ( cf . Sec. 3.3). Results are shown in Tab. 2 where we report gains with respect to the MAP baseline using eighteen distinct FroFAs categorized into ge- ometric, crop & drop, stylistic, and other. Geometric: Interestingly, all geometric augmentations consistently lead to worse performance across all settings. Crop & drop: A simple crop or a resized crop yield a significant performance boost in the 1-shot setting of +3.0% and +1.9% absolute, respectively. Further, patch dropout provides modest gains in the 1-shot regime. Dropping patches is related to training efficiency, so we investigate this further. Fig. 3a shows the top-1 accuracy on 1- and 25- shot as a function of number of patches. More results can be found in Appendix, Sec. A4.1. Similar to observations by Liu et al. [35] we can randomly drop a large fraction of patches (>50%) without loosing performance. A key dif- ference is that Liu et al. only investigated the effect in the image space, while we provide evidence that patch dropout also transfers to the feature space. Finally, inception crop does not improve performance. Stylistic: The largest gains can be observed when em- ploying a stylistic FroFA, in particular brightness, contrast, and posterize. We identified brightness as the best perform- ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on 5-shot, and up to 0.6% on 10-shot. 5Geometric Crop & drop Stylistic Other Shots MAP rotate shear-x shear-y translate-x translate-y crop res. crop incept. crop patch drop. brightness contrast equalize invert posterize sharpness* solarize* JPEG* mixup 1 57.9 −1.3 −0.6 −0.8 −1.2 −1.4 +3.0 +1.9 +0.0 +0.4 +4.8 +2.8 +1.0 +2.7 +3.7 −0.1 +1.0 −0.1 −1.4 5 78.8 −0.3 −0.2 −0.2 −0.3 −0.3 +0.0 −0.2 +0.0 +0.0 +1.1 +0.8 +0.5 −0.3 +0.8 +0.1 −0.1 −0.3 −0.3 10 80.9 −0.2 −0.1 −0.1 −0.2 −0.2 +0.0 −0.2 +0.0 +0.0 +0.6 +0.6 +0.4 +0.0 +0.6 +0.1 +0.0 −0.1 +0.2 25 83.2 −0.2 −0.1 −0.2 −0.1 −0.2 +0.0 −0.1 −0.1 +0.0 +0.1 +0.1 +0.0 −0.2 +0.0 +0.0 +0.0 +0.0 +0.1 Table 2. (Average) top-1 accuracy for default FroFA on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). In total, we investigate eighteen FroFAs, categorized intogeometric, crop & drop, stylistic, and other. We sweep across a base sweep ( cf . Sec. 4.4) and the respective augmentation sweep (cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 validation set. Each shot is sampled five times, except for JPEG, sharpness, and solarize (marked with ‘*’). We highlight deterioration by shades of red and improvement by shades of green . Best three FroFAs are boldfaced. 1 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA (a) Patch dropout FroFA 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot + brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA Figure 3. Average top-1 accuracy for FroFA variantson our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each FroFA operation point (cf . Appendix, Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. Brightness Contrast Posterize Shots MAP c c 2 c c 1 57.9 +4.8 +5.9 +6.1 +2.8 +2.5 +3.7 +5.9 5 78.8 +1.1 +1.5 +1.6 +0.8 +0.0 +0.8 +0.8 10 80.9 +0.6 +1.1 +0.9 +0.6 +0.0 +0.6 +0.5 25 83.2 +0.1 +0.4 +0.3 +0.1 −0.1 +0.0 +0.0 Table 3. Average top-1 accuracy for a selection of default ( ) and channel (c/c 2) FroFA on our ILSVRC-2012 test set. Ab- solute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep (cf . Sec. 4.4) and the respective augmentation sweep ( cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 val- idation set. Each shot is sampled five times. The best results per shot and FroFA are boldfaced (multiple ones if close, i.e., ±0.2). Other: Neither JPEG nor mixup yield performance gains but rather more or less worsen the performance. 5.3. Channel FroFA Next, we investigate channel FroFA (cFroFA) for bright- ness, contrast, and posterize. Results are shown in Tab. 3, where we report absolute gains with respect to the MAP baseline. First, contrast cFroFA worsens performance across all shots. Second, posterize cFroFA improves perfor- mance on 1-shot from +3.7% to +5.9% while maintaining performance on all other shots. Lastly, brightness cFroFA significantly improves performance across all shots, i.e., from +4.8% to +5.9% on 1-shot, from +1.1% to +1.5% on 5-shot, from +0.6% to +1.1% on 10-shot, and from +0.1% to +0.4% on 25-shot. Giving the strong improvements for brightness cFroFA, we further test brightness c 2FroFA (see Tab. 3). On a first look, both variants perform equally well. In Fig. 3b, we further report the top-1 accuracy on 1-shot and 25-shot as a function of the brightness augmentation level. Results across other shots are similar and can be found in Appendix, Sec. A4.1. We clearly observe that brightness cFroFA is much more sensitive to the brightness level than brightness c2FroFA. Aross all shots, brightness cFroFA only works well for small brightness levels (0.1 to 0.5), while the c2FroFA variant performs better than the MAP baseline across the board. We attribute the better sensitivity prop- 6erties of brightness c2FroFA to the channel-wise mappings (5), (7) since this is the only change between cFroFA and c2FroFA. We did not a observe similar effect when switch- ing from cFroFA posterize to c2FroFA posterize. 5.4. Sequential FroFA Finally, out of our best three augmentations, i.e., bright- ness c 2FroFA (B-c 2), contrast FroFA (C), and posterize cFroFA (P-c), we combine two of them sequentially. We end up with a total of six combinations. Tab. 4 compares the performance of these six combinations against our prior best (B-c 2). On 1-shot, (B-c 2→P-c) significantly outper- forms (B-c2), improving absolute gains from 6.1% to 7.7%, while maintaining performance on other shots. We con- clude that advanced FroFA protocols may further improve performance. As an initial investigation, we applied varia- tions of RandAugment and TrivialAugment using our best three FroFAs ( cf . Tab. 3), however, with limited success. We include results in the Appendix, Sec. A4.2, and leave a deeper investigation to future works. 6. FroFA on More Datasets and Architectures How well does our best non-sequential augmentation strat- egy (brightness c 2FroFA) transfer across multiple dataset and architectures settings? In Secs. 6.1 to 6.3, we report results on seven other downstream few-shot datasets, two additional architectures, and two additional pretraining se- tups, respectively. This time, however, we also incorpo- rate weight decay in all MAP-based models . Further, in Secs. 6.2 and 6.3, we solely focus on the improvements over the MAP baseline and include a discussion on the improve- ments over the linear probe baseline in Secs. 6.1 and 6.4. 6.1. Transfer to Other Downstream Datasets In Tab. 5, we report results on seven additional transfer datasets, i.e., CIFAR10, CIFAR100, DMLab, DTD, Re- sisc45, SUN397, and SVHN. We compare the weight- decayed MAP and L2-regularized linear probe baseline to our approach, i.e., weight-decayed MAP combined with brightness c2FroFA (MAPwd + FroFA). We observe that across almost all shots and transfer datasets, MAP wd + FroFA shows the best results. Moreover, MAP wd + FroFA outperforms L2-regularized linear probe with only one exception, i.e., SUN397 (1-shot). With respect to the mean across all seven datasets, MAP wd + FroFA is signifi- cantly better than MAPwd, with improvements ranging from +4.4% absolute on 1-shot to +1.0% absolute on 25-shot. Fig. 1, left, displays the absolute accuracy gains averaged across all eight transfer datasets, including ILSVRC-2012. As before, our approach, i.e., MAPwd + FroFA, yields the best results across all shots. We further observe that the gains decrease with higher shots which aligns with our pre- vious observations. Shots MAP B-c 2 B-c2→C C→ B-c2 B-c2→P-c P-c→ B-c2 C→P-c P-c→C 1 57.9 +6.1 +4.0 +2.7 +7.7 +5.2 +5.0 +3.1 5 78.8 +1.6 +1.5 +0.2 +1.5 +0.4 +1.3 +0.0 10 80.9 +0.9 +1.2 +0.1 +1.0 +0.1 +0.9 +0.3 25 83.2 +0.3 +0.4 −0.7 +0.2 −0.5 +0.2 −0.4 Table 4. Average top-1 accuracy for a sequential FroFA pro- tocol on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We combine the best settings of brightness c 2FroFA (B- c2), contrast FroFA (C), and posterize cFroFA (P-c) sequentially (two at a time, order indicated by ‘ ↑’). We sweep across a base sweep (cf . Sec. 4.4) to first find the best setting on our ILSVRC- 2012 validation set. Each shot is sampled five times. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Trans. dataset Method 1-shot 5-shot 10-shot 25-shot CIFAR10 MAPwd 85.1 96.7 97.1 97.5 Linear probe 80.9 94.1 96.7 97.3 MAPwd + FroFA 93.8 97.6 97.8 97.8 CIFAR100 MAPwd 63.1 82.7 85.5 86.8 Linear probe 58.4 80.9 83.8 85.1 MAPwd + FroFA 67.8 84.0 86.2 87.1 DMLab MAPwd 24.4 30.3 30.2 36.5 Linear probe 24.0 26.3 25.6 30.9 MAPwd + FroFA 27.1 29.4 30.3 36.8 DTD MAPwd 49.2 68.2 74.1 80.8 Linear probe 46.9 65.9 71.3 77.3 MAPwd + FroFA 53.5 70.7 76.1 82.2 Resisc45 MAPwd 63.2 86.9 89.8 90.7 Linear probe 67.1 85.6 88.2 91.0 MAPwd + FroFA 67.6 87.2 89.7 91.5 SUN397 MAPwd 51.3 73.5 77.7 80.3 Linear probe 56.7 70.9 75.6 78.6 MAPwd + FroFA 56.2 75.9 78.9 81.2 SVHN MAPwd 20.7 23.9 30.2 47.4 Linear probe 11.8 15.0 18.7 21.5 MAPwd + FroFA 21.8 31.0 43.5 50.3 Mean MAPwd 51.0 66.0 69.2 74.3 Linear probe 49.1 62.7 65.7 68.8 MAPwd + FroFA 55.4 68.0 71.8 75.3 Table 5. Top-1 accuracy of our best FroFA for additional transfer datasets using a JFT-3B L/16 model. Results are re- ported on the respective test set ( cf . Sec. A3.1). We compare results to a weight-decayed MAP baseline, i.e., MAP wd, and an L2-regularized linear probe. Depending on the setting, we sweep across a base,cf . Sec. 4.4, a weight decay or L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on the respective validation set. Per shot and dataset, the best result is boldfaced while the second-best result is underlined (multiple ones if close, i.e., ±0.2). 7Ti/16 B/16 L/16 model −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model −0.5 0.0 0.5 5-shot Ti/16 B/16 L/16 model 0.00 0.25 0.50 0.75 1.00 1.25 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 5 25-shot MAPwd linear probe (a) JFT-3B Ti/16 B/16 L/16 model −20 −15 −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 5-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 25-shot (b) ImageNet21k Figure 4. Average top-1 accuracy of brightness c2FroFA for JFT-3B (a) and ImageNet-21k (b) models on our ILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline are reported. Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. 6.2. Transfer to Other Architectures We employ brightness c2FroFA on two other JFT-pretrained models, namely Ti/16 and B/16. In Fig. 4a, we report im- provements in top-1 accuracy with respect to the weight- decayed MAP baseline. Across all shots and model archi- tectures, incorporating FroFA either maintains or improves performance, except for B/16, 25-shot. Given that larger models tend to be more prone to overfitting in the 1-shot setting, we observe increasing improvements from FroFA when scaling the architecture. With a higher number of shots, the observed improvements over the baseline model become smaller. We attribute this to the strong baseline per- formance leaving lesser headroom for improvements. We refer to the Appendix, Sec. A4.3, for the exact values. 6.3. Transfer to Other Pretraining Setups ImageNet-21k: In Fig. 4b, we report improvements in top- 1 accuracy with respect to the weight-decayed MAP base- line for ImageNet-21k-pretrained Ti/16, B/16, and L/16. Consistent with our JFT-3B observations, across all shots and model architectures, incorporating FroFA either main- tains or improves performance. The improvements dimin- ish as the number of shots increases. This trend is likely due to the higher baseline accuracies at higher shot counts. We again refer to the Appendix, Sec. A4.3, for the exact values. WebLI and SigLIP : We also tested an L/16 model with sigmoid language-image pretraining (SigLIP), follow- ing [57]. We report the absolute accuracy gains averaged across eight datasets. The results are shown in Fig. 1, right. From the results we can conclude that our FroFA setting also transfers to language-image pretrained models further emphasizing its generalizability. 6.4. Linear Probe Comparison on ILSVRC-2012 We will now look at Figs. 4a and 4b, but discuss gains with respect to the L2-regularized linear probe baseline. We start with models pretrained on JFT-3B (cf . Fig. 4a). On 1-shot, we observe that we lack behind linear probe but can close the gap by scaling up the model size. On 5- to 25-shot, with the exception of Ti/16 on 5-shot, brightness c 2FroFA significantly outperforms the linear probe baseline. On ImageNet-21k (cf . Fig. 4b), we observe even larger gaps to linear probe on 1-shot (up to -20% absolute). How- ever, similar to results on JFT-3B, performance on 5- to 25-shot improves significantly over linear probe or at worst stays the same. 7. Conclusions We investigated eighteen frozen feature augmentations (FroFAs) along three axes: model size, pretraining and transfer few-shot dataset. We show that a training with Fro- FAs, in particular stylistic ones, gives large improvements upon a representative baseline across all shots. In addition, per-channel variants further improve performance, e.g., by 1.6% absolute in the ILSVRC-2012 5-shot setting. Finally, we were able to show that our results transfer. Averaged results across seven downstream tasks show that using a variant of brightness FroFA improves by 4.4% absolute upon the same representative baseline in the 1-shot setting. 8References [1] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009. 2, 4, 12 [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit- twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass- abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv, 1612.03801:1–11, 2016. 4, 12 [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet- ter Plain ViT Baselines for ImageNet-1k.arXiv, 2205.01580: 1–3, 2022. 4 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recogni- tion. In Proc. of NeurIPS, pages 16664–16678, New Orleans, LA, USA, 2022. 2 [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad- bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A Jointly-Scaled Multilingual Language- Image Model. InProc. of ICLR, pages 1–33, Kigali, Rwanda, 2023. 1, 2, 4, 5, 12 [6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens- ing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE, 105(10):1865–1883, 2017. 4, 12 [7] Franc ¸ois Chollet. Xception: Deep Learning With Depthwise Separable Convolutions. In Proc. of CVPR , pages 1063– 6919, Honolulu, HI, USA, 2017. 4 [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. of CVPR, pages 3606–3613, Columbus, OH, USA, 2014. 4, 12 [9] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va- sudevan, and Quoc V . Le. AutoAugment: Learning Aug- mentation Strategies From Data. In Proc. of CVPR , pages 113–123, Long Beach, CA, USA, 2019. 2, 4 [10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandAugment: Practical Automated Data Augmenta- tion with a Reduced Search Space. In Proc. of NeurIPS , pages 18613–18624, virtual, 2020. 1, 4, 13 [11] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan. CoAtNet: Marrying Convolution and Attention for All Data Sizes. In Proc. of NeurIPS, pages 3965–3977, virtual, 2021. 4 [12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A JAX Library for Computer Vision Research and Beyond. In Proc. of CVPR, pages 21393–21398, New Orleans, LA, USA, 2022. 5 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan- nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku- mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar- avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh- nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Proc. of ICML , pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 12 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, 2009. 1, 2, 4, 12 [15] Terrance DeVries and Graham W. Taylor. Dataset Augmen- tation in Feature Space. In Proc. of ICLR - Workshops, pages 1–12, Toulon, France, 2017. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proc. of ICLR, pages 1–21, virtual, 2021. 1, 2, 4 [17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP- Adapter: Better Vision-Language Models with Feature Adapters. Int. J. Comput. Vis., pages 1–15, 2023. 2 [18] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im- proving Neural Language Models with a Continuous Cache. In Proc. of ICLR, pages 1–9, Toulon, France, 2017. 2 [19] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-Efficient Model Adaptation for Vision Transformers. In Proc. of AAAI, pages 817–825, Washington, DC, USA, 2023. 1, 2 [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. In Proc. of ICLR, pages 1–15, Virtual, 2020. 2 [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling Knowledge in a Neural Network. In proc. of NIPS - Work- shops, pages 1–9, Montr´eal, QC, Canada, 2014. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2, 4 [22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proc. of ICML , pages 2790–2799, Long Beach, CA, USA, 2019. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR, pages 1–13, virtual, 2022. 2 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- 9sual Prompt Tuning. In Proc. of ECCV, pages 709–727, Tel Aviv, Israel, 2022. 2 [25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, pages 1–15, San Diego, CA, USA, 2015. 13 [26] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General Visual Representation Learning. In Proc. of ECCV, pages 491–507, virtual, 2020. 2, 4 [27] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Mod- els. arXiv, 2112.13492:1–32, 2023. 4 [28] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl- liam Campbell. A Closer Look At Feature Space Data Aug- mentation For Few-Shot Intent Classification. In Proc. of EMNLP - Workshops, pages 1–10, Hong Kong, China, 2019. 2 [29] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The Omniglot Challenge: a 3-year Progress Re- port. Curr. Opin. Behav. Sci., 29:97–104, 2019. 2 [30] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In Proc. of ICML , pages 3744–3753, Long Beach, CA, USA, 2019. 1, 2, 4 [31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision Transformer for Small-size Datasets. arXiv, 2112.13492:1–11, 2021. 1 [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proc. of EMNLP, pages 3045–3059, virtual, 2021. 2 [33] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In Proc. of ICPR , pages 728–733, Beijing, China, 2018. 2 [34] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient Training of Visual Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, 2021. 1 [35] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az- izpour, and Kevin Smith. PatchDropout: Economizing Vi- sion Transformers Using Patch Dropout. In Proc. of WACV, pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 5 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV, pages 10012–10022, virtual, 2021. 1 [37] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson. Learning Multimodal Data Augmentation in Feature Space. In Proc. of ICLR, pages 1–15, Kigali, Rwanda, 2023. 2 [38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proc. of ICLR, pages 1–18, New Orleans, LA, USA, 2019. 13 [39] Samuel G. M ¨uller and Frank Hutter. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proc. of ICCV, pages 774–782, virtual, 2021. 2, 4, 13 [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat- ural Images with Unsupervised Feature Learning. In proc. of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 4, 12 [41] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah- moud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je- gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi- otr Bojanowski. Dinov2: Learning Robust Visual Features Without Supervision. arXiv, 2304.07193:1–31, 2023. 1 [42] Emin Orhan. A Simple Cache Model for Image Recognition. In Proc. of NeurIPS, pages 10128–10137, Montr´eal, Canada, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, pages 8748–8763, virtual, 2021. 1, 2 [44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. Int. J. Comput. Vis., 115(3):211–252, 2015. 1, 2, 4, 12 [45] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proc. of ICML, pages 4596–4604, Stockholm, Sweden, 2018. 13 [46] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. Trans. Mach. Learn. Res., pages 1–16, 2022. 2, 5, 13 [47] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proc. of ICCV , pages 843–852, Venice, Italy, 2017. 4 [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Ar- chitecture for Computer Vision. In Proc. of CVPR , pages 2818–2826, Las Vegas, NV , USA, 2016. 2, 4 [49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training Data-Efficient Image Transformers & Distillation Through Attention. In Proc. of ICML , pages 10347–10357, virtual, 2021. 4 [50] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na- jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben- gio. Manifold Mixup: Better Representations by Interpo- lating Hidden States. In Proc. of ICML, pages 6438–6447, Long Beach, CA, USA, 2019. 2 10[51] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Proc. of NIPS , pages 3637–3645, Barcelona, Spain, 2016. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2 [52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. In Proc. of ICCV , pages 548–558, virtual, 2021. 1 [53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In Proc. of CVPR, pages 3485–3492, San Francisco, CA, USA, 2010. 2, 4, 12 [54] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN Database: Exploring a Large Collection of Scene Categories. Int. J. Comput. Vis., 119(1): 3–22, 2016. 2, 4, 12 [55] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andr´e Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A Large-scale Study of Representation Learn- ing with the Visual Task Adaptation Benchmark. arXiv, 1910.04867:1–33, 2020. 4, 12 [56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling Vision Transformers. In Proc. of CVPR, pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4, 5, 12, 13 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre- Training. In Proc. of ICCV , pages 11975–11986, Paris, France, 2023. 1, 5, 8, 13 [58] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Mini- mization. In Proc. of ICLR , pages 1–13, Vancouver, BC, Canada, 2018. 1, 2, 4 [59] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun- chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip- Adapter: Training-Free Adaption of CLIP for Few-Shot Classification. In Proc. of ECCV, pages 493–510, Tel Aviv, Israel, 2022. 2 11Frozen Feature Augmentation for Few-Shot Image Classification Supplementary Material A1. Introduction We give additional details and results to complement the main paper. All included citations refer to the main paper’s references. A2. Brightness We provide the code snippet for brightness c2 FroFa def transform_aug_reverse( x, augment, aug_min_val=0, aug_max_val=1.0, x_min_val=None, x_max_val=None, clip=True): \"\"\"Transform to (low, high)-space, perform augmentation, transform back.\"\"\" l = x_min_val if x_min_val is None: l = tf.reduce_min(x) h = x_max_val if x_max_val is None: h = tf.reduce_max(x) # [l, h] --> [0, 1] x = (x - l) / (h - l + 1e-8) # [0, 1] --> [low, high] x = x * (aug_max_val - aug_min_val) x = x + aug_min_val x = tf.cast(augment(x), tf.float32) if clip: tf.clip_by_value(x, aug_min_val, aug_max_val) # [low, high] --> [0, 1] x = (x - aug_min_val) x = x / (aug_max_val - aug_min_val) x = x * (h - l + 1e-8) + l # [0, 1] --> [l, h] return x def get_random_brightness(max_delta=0.1, clip=False): # A random value in [-max_delta, +max_delta] # is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. def _random_brightness(image): return tf.image.random_brightness( image, max_delta) def tar(x): return transform_aug_reverse( x, augment=_random_brightness, aug_min_val=0, aug_max_val=1.0, clip=clip) return tar def get_random_brightness_per_channel_v2( max_delta=0.1, clip=True): \"\"\"Applies channel-wise random brightness transformations.\"\"\" # A random value in [-max_delta, +max_delta] is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. random_brightness = get_random_brightness( max_delta, clip) def _random_brightness_pc(x): x = tf.expand_dims(x, axis=2) # (H, W, 1, C) x = tf.unstack(x, axis=-1) # C x (H, W, 1) x = [random_brightness( {\"image\": x_i})[\"image\"] for x_i in x] return tf.concat(x, axis=-1) return _random_brightness_pc A3. Detailed Experimental Setup In the following, we provide additional details to our exper- imental setup. A3.1. Datasets In this section, we focus on details regarding our pretraining and few-shot datasets. Pretraining: As stated in the main paper, Sec. 4.2, we pretrain our models by either using JFT-3B [56], ImageNet- 21k [14, 44], or WebLI [5]. In JFT-3B, the images are annotated with noisy labels by using a semi-automated pipeline. We follow common practice [13, 56] and ignore the hierarchical aspect of the labels. ImageNet-21k is a superset of the well known ILSVRC-2012 dataset, also known as “ImageNet-1k” or just “ImageNet”. WebLI is a recently introduced image- and-language dataset. It contains 10 billion images and tens of billions image-text pairs with over 100 languages. Few-shot transfer: As stated in the main paper, Sec. 4.2, our experiments concentrate around few-shot transfer on ILSVRC-2012 [14, 44]. We also provide results on CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. When official test and validation splits are available, we use them for eval- uation across all datasets. In general, we use the versions in TensorFlow Datasets. CIFAR10 contains 60,000 images of 10 equally dis- tributed classes split into 50,000 training images and 10,000 test images. We further split the official training dataset into 45,000 training images and 5,000 validation images. CIFAR100 is a superset of CIFAR10 with 100 equally distributed classes and 60,000 images. Similar to CIFAR10, we use 45,000 images for training, 5,000 images for valida- tion and 10,000 images for test. DMLab consists of frames collected from the DeepMind Lab environment. Each frame is annotated with one out https://www.tensorflow.org/datasets 12of six classes. We use 65,550 images for training, 22,628 images for validation, and 22,735 for test. DTD is a collection of 5,640 textural images categorized into 47 distinct classes. Each of the three splits, i.e., train- ing, validation, and test, has exactly 1,880 images. Resisc45 is a benchmark with 31,500 images for image scene classification in remote sensing scenarios. In total, 47 different catogries for scenes are defined. We use the first 23,000 images for training, the subsequent 2,000 images for validation and the last 6,300 images for test. SUN397 is a 397-category database of 108,753 images for scene understanding. We use 76,128 images for training, 10,875 images for validation, and 21,750 images for test. SVHN is a Google Street View dataset with a large col- lection of house number images. In total, 10 distinct classes exist. We use the cropped version with 73,257 images for training and 26,032 images for test. Further, we create a val- idation subset by only using the first 70,000 out of 73,257 training images for actual training and the remaining 3,257 images for validation. A3.2. Data Augmentation In this section, we provide additional details on the used data augmentation techniques and protocols. (c/c2)FroFA: In Tab. 6, we give detailed descriptions of each FroFA, cFroFA, and c 2FroFA setting. We mostly build upon an AutoAugment implementation from Big Vision. To keep it simple, we use v or v1, v2 as sweep parameter(s) for all augmentations. By default, we first re- shape the two-dimensional features f to three-dimensional features f∗ (1) of shape √ N × √ N × C, with N = 196 and C ∈ {192, 768, 1024} in all our experiments. Note that the value of C depends on the architecture. We further want to point out, while some augmentations heavily rely on the three-dimensional representation, e.g., all geometric ones, some others are also transferable to a two-dimensional rep- resentation, e.g., brightness or contrast. As pointed out in the main paper, Tab. 3, brightness c2FroFA, contrast FroFA, and posterize cFroFA are our best FroFAs. For all three, we list the best sweep settings in Tab. 7. Advanced protocols: As mentioned in the main paper, Sec. 4.3, besides our fixed sequential protocol ( cf . Tab. 4) we also tested variations of RandAugment [10] and Triv- ialAugment [39]. In all protocols, we sample from the best settings of brightness c2FroFA, contrast FroFA, and poster- ize cFroFA. In particular, we use v = 1.0 for brightness c2FroFA, v = 6.0 for contrast FroFA, and v1 = 1, v2 = 8 for posterize cFroFA ( cf . Tab. 6). We re-use the abbrevi- ations from Tab. 4 in the following, i.e., B-c 2, C, and P- c, respectively. For the RandAugment and TrivialAugment https://github.com/google- research/big_vision/ blob/main/big_vision/pp/autoaugment.py variations, we uniformly sample from either the best three FroFAs, i.e., Atop3 = {B-c2, C, P-c}, or the best two Fro- FAs, i.e., Atop2 = A3 \\ {C}. Further, our RandAugment variation randomly constructs a sequence of augmentations by uniformly sampling the integer sequence length from 1 to |A|, with A ∈ {Atop2, Atop3} depending on whether Atop2 or Atop3 is used. A3.3. Training Details Pretraining: In the JFT-3B setup, we use pretrained mod- els from Zhai et al. [56]. The models are pretrained using a sigmoid cross-entropy loss. The weights are optimized by Adafactor [45] in half-precision mode, β1 = 0.9, and β2 = 0.999. Further, (decoupled) weight decay [38] is applied with 3.0 on the head and 0.03 for the rest of the network weights. The learning rate is adapted by a recip- rocal square-root schedule for 4,000,000 steps with a lin- ear warm-up phase of 10,000 steps and a linear cool-down phase of 50,000 steps. The starting learning rate is 0.01 for Ti/16 and L/16 and 0.03 for B/16. The images are prepro- cessed by an224×224 inception crop and a random horizon- tal flip. We set the batch size to 4,096. To stabilize training, a global norm clipping of 1.0 is used. In the ImageNet-21k setup, we follow settings from Steiner et al. [46] and use a sigmoid cross-entropy loss for multi-label pretraining. We use the Adam optimizer [25] in half-precision mode and set β1 = 0.9 and β2 = 0.999. Fur- ther, we apply (decoupled) weight decay with either 0.03 for Ti/16 or 0.1 for B/16 and L/16. We adapt the learning rate using a cosine schedule for roughly 930,000 steps (300 epochs) with a linear warm-up phase of 10,000 steps. We set the starting learning rate to 0.001 for all models. During preprocessing, we crop the images to 224×224 following an inception-style crop and a random horizontal flip. While we don’t use any additional augmentation for Ti/16, we fol- low suggestions by Steiner et al. [46] and use the ‘light1’ and ‘medium2’ augmentation settings for B/16 and L/16, respectively. Finally, we use a batch size of 4,096 and sta- bilize training by using a global norm clipping of 1.0. In the WebLI setup, we take an L/16 model from [57]. In particular, we use [ADD DETAILS]. Few-shot learning: We first cache each few-shot dataset by processing each of them through a pretrained model and store the extracted features (cf . Fig. 2). We resize each im- age to 224×224 before feeding it to the model. We follow up with a training where we mostly use trans- fer learning settings from Steiner et al. [46]. We use a sig- moid cross-entropy loss. This might be non-intuitive given that all of our few-shot datasets are not multi-labeled. How- ever, we didn’t really observe any performance drops com- pared to using the more common softmax cross-entropy loss, so we stick to the sigmoid cross-entropy loss. We use stochastic gradient descent with momentum of 0.9. Simi- 13Augmentation Description Geometric rotate We rotate each of the C feature channels fc (2) by z ∼ U(−v, v). We sweep across v ∈ {15, 30, 45, 60, 75, 90} representing the maximum positive and negative rotation angle in degrees. shear-{x,y} We (horizontally/vertically) shear each of the C feature channels fc (2) by z ∼ U(0, v). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7} representing the maximum level of horizontal or vertical shearing. translate-{x,y} We (horizontally/vertically) translate each of theC feature channels fc (2) by uniformly samplingz from {0, 1, ..., v}. We sweep across integer values 1 ≤ v ≤ 7 representing the maximum horizontal or vertical translation. Crop & drop crop We randomly crop each of the C feature channels fc (2) to v×v at the same spatial position. We sweep across integer values 1 ≤ v ≤ 13 representing the square crop size. resized crop We resize each of the C feature channels fc (2) to v × v and then randomly crop each to 14 × 14 at the same spatial position. We sweep across v ∈ {16, 18, 20, 22, 24, 26, 28, 35, 42} representing the resized squared spatial resolution. inception crop We apply an inception crop with probability v. We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. patch dropout We randomly keep v out of N patches of f having shape N × C. Note that the patch ordering is also randomized. We sweep across v ∈ {1, 2, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 116, 132, 148, 164, 180}. Stylistic brightness We randomly add a value z ∼ U(−v, v) to each of the C feature channels fc (2). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. We test this method using all FroFA variants. In the default FroFA and the cFroFA variants, the features are scaled by (5) taking the minimumfmin and maximum fmax across all chan- nels into account. In the c 2FroFA variant, each channel fc (2) is shifted individually and uses the channel minimum and maximum instead. Further, in the cFroFA and c2FroFA variants we sample C values of z, one for each channel. contrast We randomly scale each of the C feature channels fc (2) by z ∼ U( 1 v , v). We sweep across v ∈ {1.25, 1.5, 2, 3, 4, 5, 6, 7, 9, 10}. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sample C values of z, one for each channel. equalize We first map the features from value range R to the integer subset I = {0, 1, ...,195}, i.e., executing (5) followed up by a discretization step. We choose this value range as preliminary results mapping from R to the more commonly used I = {0, 1, ...,255} instead didn’t show any effects. We continue by equalizing 196 bins and then transforming the results back to the original space using (7). We apply equalize with probability v. In particular, we sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. invert We change the sign of features f∗ with probability v. We sweep acrossv ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. posterize We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. In other words, we use an 8-bit representation for features f∗. Posterize performs a quantization by a bit-wise left and right shift. We uniformly sample the shift value z between integer values v1 and v2. In our sweep, we test a subset of all possible combinations. In particular, we first set v2 = 8and reduce v1 from 7 to 1. We then fix v1 = 1and increase v2 from 2 to 7 again. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sampleC values of z, one for each channel. sharpness We first apply a two-dimensional convolution on f∗ (1) using a 3×3 smoothing filter. Next, we mix the original features with the resulting “smoothed” features using a randomly sampled blending factor z ∼ U(0, v). We sweep across v ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0}. solarize We do not map features from R to I = [0, 1], but stay in R. We compute the minimum fmin and maximum fmax across features f∗. We conditionally subtract all values smaller than0.5·fmin from fmin or larger than0.5·fmax from fmax. We apply this method with a probabilityv and sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Other JPEG We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. We then perform a JPEG compression of each channel by randomly sampling a JPEG quality z ∼ U(v1, v2). We sweep across combinations of v1 ∈ {10, 25, 50, 75} and v2 ∈ {25, 50, 75, 100}, with v2 > v1. mixup We do not map features from R to [0, 1], but stay in R. We mix two features f∗ i , f∗ j according to z ·f∗ i + (1−z) ·f∗ j by sampling a random value z ∼ B(α, α), with Beta distribution B(α, α) parameterized by α = v. The labels are mixed using the same procedure. We sweep across v ∈ {0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Table 6. Details on our used set of augmentations. For simplicity, instead of introducing a new hyper parameter for each data augmenta- tion, we re-use v as a sweep parameter that is set during a sweep and differs for each augmentation. If not stated otherwise, each method is only applied as default FroFA and we first map featuresf (two-dimensional representation) or f∗ (three-dimensional representation) from value range R to I = [0, 1] using (5). By default, we assume a three-dimensional representation f∗ although some augmentations would work also in the two-dimensional representation f, i.e., a reshaping is not necessary. lar to the pretraining setup, we also store the internal state in half-precision. We do not apply any weight decay. The learning rate is adapted following a cosine schedule with a linear warm-up phase of 500 steps. In addition, we stabilize 14FroFA Shots Base learning rate Batch size Training steps v or v1, v2 B-c2 1 0.01 512 4,000 1.0 10 0.01 64 16,000 1.0 15 0.01 256 8,000 0.9 25 0.01 512 8,000 0.8 C 1 0.01 32 16,000 6.0 10 0.01 128 8,000 6.0 15 0.01 512 2,000 6.0 25 0.01 256 4,000 7.0 P-c 1 0.01 512 8,000 1, 8 10 0.03 512 8,000 1, 8 15 0.03 512 16,000 1, 8 25 0.03 64 16,000 2, 8 Table 7. Our best sweep settings for our best three FroFAs , namely, brightness cFroFA (B-c 2), contrast (C), and posterize cFroFA (P-c). We list the shots, base learning rate, batch size, number of training steps, and the augmentation parameter, denoted as v or v1, v2 (see Tab. 6 for a detailed explanation ofv and v1, v2). The best sweep settings are found using our ILSVRC-2012 vali- dation set. RA∗ TA∗ Shots MAP B-c 2 Atop2 Atop3 Atop2 Atop3 1 58.4 +6.0 +3.9 +2.4 +4.8 +4.3 5 79.1 +1.5 +1.0 +0.4 +1.4 +1.2 10 80.7 +1.3 +1.0 +0.6 +1.4 +1.4 25 83.0 +0.6 +0.4 +0.0 +0.5 +0.4 Table 8. Top-1 accuracy for advanced FroFA protocols on our ILSVRC-2012 test set. Absolute gains to the MAP baseline (ref- erence run) are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We compare brightness c 2FroFA (B-c2) with our variations of RandAugment (RA∗) and TrivialAugment (TA∗), cf . Sec. A3.2. For the latter, we either use the top-2 ( Atop2) or top- 3 ( Atop3) augmentations. We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 val- idation set. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). training by using a global norm clipping of 1.0. Further, we sweep across batch size, learning rate and number of steps yielding 100 combinations (cf . Sec. 4.4) for each shot. A4. Additional Experimental Results In this section, we show additional experimental results. A4.1. Patch Dropout and Brightness In Fig. 3, we only report results for 1-shot and 25- shot settings using patch dropout FroFA and brightness (c/c2)FroFA. We extend this by also reporting results for 5-shot and 10-shot settings in Figs. 5 and 6. We observe the same effects in the other settings as well. A4.2. Advanced FroFA Protocols In Tab. 8, we report results for our RandAugment (RA ∗) and TrivialAugment (TA∗) variations. We did not average across five runs and thus only report absolute gains with re- spect to a reference run. Therefore, numbers which are also reported in the main paper, e.g., Tab. 4, are slightly differ- ent. All in all, we observe that both RA ∗ and TA∗ do not improve upon the best single augmentation, i.e., brightness c2FroFA (B-c2). We also observe that increasing the set of augmentations from Atop2 to Atop3 rather worsens the per- formance for both RA∗ and TA∗. A4.3. Detailed FroFA Transfer Results In Tab. 9, we report exact numbers for Fig. 4, i.e., Ti/16, B/16, and L/16 pretrained on either ImageNet-21k or JFT- 3B and subsequently finetuned on few-shotted ILSVRC- 2012 training sets. Numbers for the two baselines, i.e., MAP ( with weight decay) and linear probe, and our best method, i.e., MAP ( with weight decay) combined with brightness c2FroFA (MAP + FroFA), are reported. In addi- tion, we report numbers, where we use MAPwithout weight decay in Tab. 10. As before, we observe that our method performs worse on all 1-shot settings, but is on par or sig- nificantly better than MAP and/or linear probe on most 5- to 25-shot settings. 151 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 72 74 76 78 5-shot 1 50 100 150 number of patches 76 77 78 79 80 81 10-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA Figure 5. Average top-1 accuracy for patch dropout FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each number of patches (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 74 76 78 80 5-shot 0.1 0.3 0.5 0.7 0.9 brightness level 79 80 81 82 10-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot MAP + brightness cFroFA + brightness c2FroFA Figure 6. Top-1 accuracy for channel variants (c/c2) of brightness FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each brightness level (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAPwd 20.5 53.6 59.7 64.9 19.1 46.4 53.6 60.2 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAPwd + FroFA 20.6 54.5 60.1 65.2 19.6 47.2 53.6 60.3 B/16 MAPwd 30.5 71.7 75.3 78.0 51.3 74.8 77.5 79.8 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAPwd + FroFA 30.6 73.3 76.0 78.1 52.5 75.1 77.6 79.5 L/16 MAPwd 38.7 75.9 78.6 80.6 62.0 79.9 81.5 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAPwd + FroFA 39.3 78.0 80.0 81.0 63.7 80.4 82.0 83.6 Table 9. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline, as well as our best FroFA-based approach, i.e., weight-decayed MAP combined with brightness c 2FroFA (MAPwd + FroFA). Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAPwd + FroFA, is on par or significantly better than MAPwd and/or linear probe on most 5- to 25-shot settings. 16ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAP 20.4 53.2 59.5 64.7 17.9 45.5 53.5 60.1 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAP + FroFA 22.1 54.9 60.1 65.0 20.3 47.2 53.6 60.1 B/16 MAP 31.3 70.3 75.1 78.1 48.9 73.4 76.5 79.4 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAP + FroFA 30.6 73.4 76.3 78.3 52.4 75.2 77.8 79.9 L/16 MAP 38.8 74.9 78.5 80.7 57.9 78.8 80.9 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAP + FroFA 39.3 78.0 80.0 81.2 63.9 80.3 82.0 83.6 Table 10. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the MAP and L2-regularized linear probe baseline, as well as our best FroFA-based approach,i.e., MAP combined with brightness c2FroFA (MAP + FroFA). Depending on the setting, we sweep across a base,cf . Sec. 4.4, an L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAP + FroFA, is on par or significantly better than MAP and linear probe on most 5- to 25-shot settings. 17",
      "references": [
        "Learning Multiple Layers of Features from Tiny Images",
        "DeepMind Lab",
        "Better Plain ViT Baselines for ImageNet-1k",
        "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "Remote Sensing Image Scene Classification: Benchmark and State of the Art",
        "Xception: Deep Learning With Depthwise Separable Convolutions",
        "Describing Textures in the Wild",
        "AutoAugment: Learning Augmentation Strategies From Data",
        "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
        "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "Scenic: A JAX Library for Computer Vision Research and Beyond",
        "Scaling Vision Transformers to 22 Billion Parameters",
        "ImageNet: A Large-Scale Hierarchical Image Database",
        "Dataset Augmentation in Feature Space",
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
        "Improving Neural Language Models with a Continuous Cache",
        "Parameter-Efficient Model Adaptation for Vision Transformers",
        "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "Distilling Knowledge in a Neural Network",
        "Parameter-Efficient Transfer Learning for NLP",
        "LoRA: Low-Rank Adaptation of Large Language Models",
        "Visual Prompt Tuning",
        "Adam: A Method for Stochastic Optimization",
        "Big Transfer (BiT): General Visual Representation Learning",
        "Three Towers: Flexible Contrastive Learning with Pretrained Image Models",
        "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification",
        "The Omniglot Challenge: a 3-year Progress Report",
        "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
        "Vision Transformer for Small-size Datasets",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Data Augmentation via Latent Space Interpolation for Image Classification",
        "Efficient Training of Visual Transformers With Small Datasets",
        "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
        "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
        "Learning Multimodal Data Augmentation in Feature Space",
        "Decoupled Weight Decay Regularization",
        "TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation",
        "Reading Digits in Natural Images with Unsupervised Feature Learning",
        "Dinov2: Learning Robust Visual Features Without Supervision",
        "A Simple Cache Model for Image Recognition",
        "Learning Transferable Visual Models From Natural Language Supervision",
        "ImageNet Large Scale Visual Recognition Challenge",
        "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers",
        "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "Rethinking the Inception Architecture for Computer Vision",
        "Training Data-Efficient Image Transformers & Distillation Through Attention",
        "Manifold Mixup: Better Representations by Interpolating Hidden States",
        "Matching Networks for One Shot Learning",
        "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
        "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
        "SUN Database: Exploring a Large Collection of Scene Categories",
        "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
        "Scaling Vision Transformers",
        "Sigmoid Loss for Language Image Pre-Training",
        "Mixup: Beyond Empirical Risk Minimization",
        "Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification"
      ],
      "meta_data": {
        "arxiv_id": "2403.10519v2",
        "authors": [
          "Andreas Bär",
          "Neil Houlsby",
          "Mostafa Dehghani",
          "Manoj Kumar"
        ],
        "published_date": "2024-03-15T17:59:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Frozen Feature Augmentation (FroFA), a simple yet effective way to apply standard data-augmentation operations directly in the frozen feature space of pretrained vision transformers for few-shot image classification. Provides the first large-scale empirical study (18 augmentations, 3 model sizes, 3 pre-training datasets, 8 downstream tasks) showing that point-wise stylistic augmentations, especially brightness with channel-wise variants, consistently outperform strong linear-probe and MAP baselines, giving up to 7.7% absolute accuracy gains in 1-shot ImageNet and >4% mean gain across seven other datasets.",
        "methodology": "1) Cache frozen patch features (N×C) from the last transformer block of ViT Ti/16, B/16, or L/16 pretrained on JFT-3B, ImageNet-21k or WebLI (SigLIP).\n2) Reshape to H×W×C, min-max scale each channel or full tensor into [0,1] or [0,255].\n3) Adapt 18 common image augmentations into feature space via mapping f→x, apply op, then map back (x→f). Variants:\n   • FroFA – single random parameter per sample\n   • cFroFA – independent random parameter per channel\n   • c²FroFA – channel-wise scaling and random parameter\n4) Train a lightweight Multi-head Attention Pooling (MAP) head plus classifier (≈0.1–1 M params) on the augmented features; optimise with SGD, weight decay, cosine learning-rate schedule.\n5) Evaluate sequential augmentation pipelines and RandAugment/TrivialAugment variants.",
        "experimental_setup": "Pretraining corpora: JFT-3B (~3 B images/29 k labels), ImageNet-21k (14 M/21 k labels), WebLI (10 B image-text pairs) with SigLIP.\nDownstream benchmarks: ILSVRC-2012 (ImageNet-1k) split into 1/5/10/25-shot sets (five random seeds + ‘minival’ for tuning); plus CIFAR10, CIFAR100, DMLab-30k, DTD, RESISC45, SUN397, SVHN – all reduced to the same shot settings.\nBaselines: (i) L2-regularised linear probe on MAP output; (ii) MAP head trained without FroFA (with/without weight decay).\nHyper-parameter sweeps: 5 batch sizes (32–512), 4 lrs (0.01–0.1), 5 train steps (1 k–16 k) × augmentation-specific grids; early stop on validation; experiments run on single-host TPU-v2.\nMetric: Top-1 accuracy reported on official test/validation splits; results averaged over 5 shot samples where applicable.",
        "limitations": "• Gains diminish as shot count grows; on 1-shot FroFA can still trail linear probes for smaller models or non-stylistic ops.\n• Only ViT backbones and last-block features examined; effect on CNNs, intermediate layers, or other tasks (detection, segmentation) untested.\n• Geometric and complex augmentations hurt performance, indicating limited generality.\n• Requires min-max statistics per feature tensor/channel; may be unstable for out-of-range activations.\n• Relies on large, partly proprietary pretraining data (JFT-3B, WebLI) making exact reproduction difficult.\n• Computational analysis (storage of cached features, training speed vs full fine-tuning) is qualitative.",
        "future_research_directions": "1) Design learned or search-based feature-space augmentation policies to go beyond manual ops.\n2) Extend FroFA to other architectures (CNNs, hybrid models) and to intermediate layers or multi-layer augmentation.\n3) Apply the idea to non-classification tasks such as detection, segmentation, video, multimodal retrieval.\n4) Combine FroFA with parameter-efficient tuning (prompts, adapters, LoRA) for further gains in low-shot regimes.\n5) Study theoretical foundations: why stylistic per-channel perturbations help while geometric ones hurt; analyse robustness, calibration, and adversarial effects.\n6) Provide open benchmarks without proprietary data and investigate memory/compute trade-offs for large-scale deployment.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Interventional Few-Shot Learning",
      "full_text": "Interventional Few-Shot Learning Zhongqi Yue1,3, Hanwang Zhang1, Qianru Sun2, Xian-Sheng Hua3 1Nanyang Technological University, 2Singapore Management University, 3Damo Academy, Alibaba Group yuez0003@ntu.edu.sg, hanwangzhang@ntu.edu.sg, qianrusun@smu.edu.sg, xiansheng.hxs@alibaba-inc.com Abstract We uncover an ever-overlooked deﬁciency in the prevailing Few-Shot Learning (FSL) methods: the pre-trained knowledge is indeed a confounder that limits the performance. This ﬁnding is rooted from our causal assumption: a Structural Causal Model (SCM) for the causalities among the pre-trained knowledge, sample features, and labels. Thanks to it, we propose a novel FSL paradigm: Interventional Few- Shot Learning (IFSL). Speciﬁcally, we develop three effective IFSL algorithmic implementations based on the backdoor adjustment, which is essentially a causal intervention towards the SCM of many-shot learning: the upper-bound of FSL in a causal view. It is worth noting that the contribution of IFSL is orthogonal to existing ﬁne-tuning and meta-learning based FSL methods, hence IFSL can improve all of them, achieving a new 1-/5-shot state-of-the-art on miniImageNet, tieredImageNet, and cross-domain CUB. Code is released at https://github. com/yue-zhongqi/ifsl. 1 Introduction Few-Shot Learning (FSL) — the task of training a model using very few samples — is nothing short of a panacea for any scenario that requires fast model adaptation to new tasks [69], such as minimizing the need for expensive trials in reinforcement learning [ 31] and saving computation resource for light-weight neural networks [28, 26]. Although we knew that, more than a decade ago, the crux of FSL is to imitate the human ability of transferring prior knowledge to new tasks [ 19], not until the recent advances in pre-training techniques, had we yet reached a consensus on “what & how to transfer”: a powerful neural network Ω pre-trained on a large dataset D. In fact, the prior knowledge learned from pre-training prospers today’s deep learning era, e.g., D= ImageNet, Ω = ResNet in visual recognition [25, 24]; D= Wikipedia, Ω = BERT in natural language processing [65, 17]. Pre-Training D Meta-Learning {Fine-tune(Si,Qi)} Fine-Tuning (S,Q) Fine-Tuning (S,Q) or Ω Ω Ωφ Figure 1: The relationships among different FSL paradigms (color green and orange). Our goal is to remove the deﬁciency introduced by Pre-Training. In the context of pre-trained knowledge, we de- note the original FSL training set as support set Sand the test set as query set Q, where the classes in (S,Q) are unseen (or new) in D. Then, we can use Ω as a backbone (ﬁxed or partially trainable) for extracting sample rep- resentations x, and thus FSL can be achieved simply by ﬁne-tuning the target model on Sand test it on Q[13, 18]. However, the ﬁne-tuning only exploits the D’s knowledge on “what to transfer”, but neglects “how to transfer”. Fortu- nately, the latter can be addressed by applying a post-pre-training and pre-ﬁne-tuning strategy: meta-learning [55]. Different from ﬁne-tuning 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2009.13000v2  [cs.LG]  4 Dec 202050% Weak Ω Strong Ω 0.4 0.5 0.6 0.7 0.8 0.9 1 1-shot 5-shot 10-shot 1-shot 5-shot 10-shot 1-shot 5-shot 10-shot Chart Title ResNet10 WRN28-10 1 5 10 1 5 101 5 10 60% 70% 80% 90% (a)  (b) Accuracy S∼Q Average S̸∼Q Support Set “Lion” “African Hunting Dog” Query Set Classiﬁed as“Dog”(due to “yellow grass”) Classiﬁed as“Lion”(due to “green grass”) Figure 2: Quantitative and qualitative evidences of pre-trained knowledge misleading the ﬁne-tune FSL paradigm. (a) miniImageNet ﬁne-tuning accuracy on 1-/5-/10-shot FSL using weak and strong backbones: ResNet-10 and WRN-28-10. S∼Q (or S̸∼Q ) denotes the pre-trained classiﬁer scores of the query is similar (or dissimilar) to that of the support set. “Average” is the mean of both. The dissimilarity is measured using query hardness deﬁned in Section 5.1. (b) An example of 5-shot S̸∼Q . whose goal is the “model” trained onSand tested on Q, meta-learning aims to learn the “meta-model” — a learning behavior — trained on many learning episodes {(Si,Qi)}sampled from Dand tested on the target task (S,Q). In particular, the behavior can be parametrized by φusing model parameter generator [49, 21] or initialization [20]. After meta-learning, we denote Ωφ as the new model starting point for the subsequent ﬁne-tuning on target task (S,Q). Figure 1 illustrates the relationships among the above discussed FSL paradigms. It is arguably a common sense that the stronger the pre-trained Ω is, the better the downstream model will be. However, we surprisingly ﬁnd that this may not be always the case in FSL. As shown in Figure 2(a), we can see a paradox: though stronger Ω improves the performance on average, it indeed degrades that of samples in Qdissimilar to S. To illustrate the “dissimilar”, we show a 5-shot learning example in Figure 2(b), where the prior knowledge on “green grass” and “yellow grass” is misleading. For example, the “Lion” samples in Qhave “yellow grass”, hence they are misclassiﬁed as “Dog” whoseShas major “yellow grass”. If we use stronger Ω, the seen old knowledge (“grass” & “color”) will be more robust than the unseen new knowledge (“Lion” & “Dog”), and thus the old becomes even more misleading. We believe that such a paradox reveals an unknown systematic deﬁciency in FSL, which has been however hidden for years by our gold-standard “fair” accuracy, averaged over all the random (S,Q) test trials, regardless of the similarity between Sand Q(cf. Figure 2(a)). Though Figure 2 only illustrates the ﬁne-tune FSL paradigm, the deﬁciency is expected in the meta-learning paradigm, as ﬁne-tune is also used in each meta-train episode (Figure 1). We will analyze them thoroughly in Section 5. In this paper, we ﬁrst point out that the cause of the deﬁciency: pre-training can do evil in FSL, and then propose a novel FSL paradigm: Interventional Few-Shot Learning (IFSL), to counter the evil. Our theory is based on the assumption of the causalities among the pre-trained knowledge, few-shot samples, and class labels. Speciﬁcally, our contributions are summarized as follows. • We begin with a Structural Causal Model (SCM) assumption in Section 2.2, which shows that the pre-trained knowledge is essentially a confounder that causes spurious correlations between the sample features and class labels in support set. As an intuitive example in Figure 2(b), even though the “grass” feature is not the cause of the “Lion” label, the prior knowledge on “grass” still confounds the classiﬁer to learn a correlation between them. • In Section 2.3, we illustrate a causal justiﬁcation of why the proposed IFSL fundamentally works better: it is essentially a causal approximation to many-shot learning. This motivates us to develop three effective implementations of IFSL using the backdoor adjustment [46] in Section 3. • Thanks to the causal intervention, IFSL is naturally orthogonal to the downstream ﬁne-tuning and meta-learning based FSL methods [ 20, 66, 29]. In Section 5.2, IFSL improves all base- lines by a considerable margin, achieving new 1-/5-shot state-of-the-arts: 73.51%/83.21% on miniImageNet [66], 83.07%/88.69% on tieredImageNet [52], and 50.71%/64.43% on cross- domain CUB [70]. • We further diagnose the detailed performances of FSL methods across different similarities between Sand Q. We ﬁnd that IFSL outperforms all baselines in every inch. 22 Problem Formulations 2.1 Few-Shot Learning We are interested in a prototypical FSL: train aK-way classiﬁer on an N-shot support set S, where N is a small number of training samples per class (e.g., N=1 or 5); then test the classiﬁer on a query set Q. As illustrated in Figure 1, we have the following two paradigms to train the classiﬁer P(y|x; θ), predicting the class y∈{1,...,K }of a sample x: Fine-Tuning. We consider the prior knowledge as the sample feature representation x, encoded by the pre-trained network Ω on dataset D. In particular, we refer x to the output of the frozen sub-part of Ω and the rest trainable sub-part of Ω (if any) can be absorbed into θ. We train the classiﬁer P(y|x; θ) on the support set S, and then evaluate it on the query set Qin a standard supervised way. Meta-Learning. Yet, Ω only carries prior knowledge in a way of “representation”. If the datasetD can be re-organized as the training episodes {(Si,Qi)}, each of which can be treated as a “sandbox” that has the same N-shot-K-way setting as the target (S,Q). Then, we can model the “learning behavior” from Dparameterized as φ, which can be learned by the above ﬁne-tuning paradigm for each (Si,Qi). Formally, we denote Pφ(y|x; θ) as the enhanced classiﬁer equipped with the learned behavior. For example, φcan be the classiﬁer weight generator [21], distance kernel function in k- NN [66], or even θ’s initialization [20]. Considering Lφ(Si,Qi; θ) as the loss function of Pφ(y|x; θ) trained on Si and tested on Qi, we can have φ←arg min(φ,θ) Ei[Lφ(Si,Qi; θ)], and then we ﬁx the optimized φand ﬁne-tune for θon Sand test on Q. Please refer to Appendix 5 for the details of various ﬁne-tuning and meta-learning settings. 2.2 Structural Causal Model From the above discussion, we can see that (φ,θ) in meta-learning and θ in ﬁne-tuning are both dependent on the pre-training. Such “dependency” can be formalized with a Structural Causal Model (SCM) [46] proposed in Figure 3(a), where the nodes denote the abstract data variables and the directed edges denote the (functional) causality, e.g., X →Y denotes that X is the cause and Y is the effect. Now we introduce the graph and the rationale behind its construction at a high-level. Please see Section 3 for the detailed functional implementations. 𝐷 𝑋 𝐶 𝑌 (a) …… Channel 1 𝑐3 Image: School Bus Channel 2 Channel 3 (b) Class Manifold Unicycle Class Manifold Street Sign Class Manifold Ashcan 𝒄𝟑 = 𝟎.𝟏𝟎𝟑 𝒄𝟐 = 𝟎.𝟐𝟏𝟏 𝒄𝟏 = 𝟎.𝟑𝟕𝟔 Image: School Bus (c) Figure 3: (a) Causal Graph for FSL; (b) Feature-wise illustration of D →C: Feature channels of pre-trained network(e.g.1 . . .512 for ResNet-10). X →C: Per-channel response to an image (“school bus”) visualized by CAM[82]; (c) Class-wise illustration for D → C: features are clustered according to the pre-training semantic classes (colored t-SNE plot[ 39]). X →C: An image (“school bus”) can be represented in terms of the similarities among the base classes (“ashcan”, “unicycle”, “sign”). D →X. We denote X as the fea- ture representation and Das the pre- trained knowledge, e.g., the dataset D and its induced model Ω. This link as- sumes that the feature X is extracted by using Ω. D →C ←X. We denote Cas the transformed representation of X in the low-dimensional manifold, whose base is inherited from D. This as- sumption can be rationalized as fol- lows. 1) D → C: a set of data points are usually embedded in a low- dimensional manifold. This ﬁnding can be dated back to the long history of dimensionality reduction [63, 53]. Nowadays, there are theoretical [ 3, 10] and empirical [82, 76] evidences showing that disentangled semantic manifolds emerge during training deep networks. 2) X →C: features can be represented using (or projected onto) the manifold base linearly [ 64, 11] or non- linearly [8]. In particular, as later discussed in Section 3, we explicitly implement the base as feature dimensions (Figure 3(b)) and class-speciﬁc mean features (Figure 3(c)). X →Y ←C. We denote Y as the classiﬁcation effect (e.g., logits), which is determined by X via two ways: 1) the direct X →Y and 2) the mediation X →C →Y. In particular, the ﬁrst way can be removed if X can be fully represented by C(e.g., feature-wise adjustment in Section 3). The second way is inevitable even if the classiﬁer does not take C as an explicit input, because any X can be 3inherently represented by C. To illustrate, suppose that Xis a linear combination of two base vectors plus a noise residual: x = c1b1 + c2b2 + e, any classiﬁer f(x) = f(c1b1 + c2b2 + e) will implicitly exploit the C representation in terms of b1 and b2. In fact, this assumption also fundamentally validates unsupervised representation learning [7]. To see this, if C ̸→Y in Figure 3(a), uncovering the latent knowledge representation from P(Y|X) would be impossible, because the only path left that transfers knowledge fromDto Y: D→X →Y, is cut off by conditioning onX: D̸→X →Y. An ideal FSL model should capture the true causality between X and Y to generalize to unseen samples. For example, as illustrated in Figure 2(b), we expect that the “Lion” prediction is caused by the “lion” feature per se, but not the background “grass”. However, from the SCM in Figure 3(a), the conventional correlation P(Y|X) fails to do so, because the increased likelihood of Y given Xis not only due to “X causes Y” via X →Y and X →C →Y, but also the spurious correlation via 1) D→X, e.g., the “grass” knowledge generates the “grass” feature, and 2)D→C →Y, e.g., the “grass” knowledge generates the “grass” semantic, which provides useful context for “Lion” label. Therefore, to pursue the true causality between X and Y, we need to use the causal intervention P(Y|do(X)) [48] instead of the likelihood P(Y|X) for the FSL objective. 2.3 Causal Intervention via Backdoor Adjustment By now, an astute reader may notice that the causal graph in Figure 3(a) is also valid for Many-Shot Learning (MSL), i.e., conventional learning based on pre-training. Compared to FSL, the P(Y|X) estimation of MSL is much more robust. For example, onminiImageNet, a 5-way-550-shot ﬁne-tuned classiﬁer can achieve 95% accuracy, while a 5-way-5-shot one only obtains 79%. We used to blame FSL for insufﬁcient data by the law of large numbers in point estimation [16]. However, it does not answer why MSL converges to the true causal effects as the number of samples increases inﬁnitely. In other words, why P(Y|do(X)) ≈P(Y|X) in MSL while P(Y|do(X)) ̸≈P(Y|X) in FSL? To answer the question, we need to incorporate the endogenous feature sampling x ∼P(X|I) into the estimation of P(Y|X), where I denotes the sample ID. We have P(Y|X = xi) := Ex∼P(X|I)P(Y|X = x,I = i) = P(Y|I), i.e., we can use P(Y|I) to estimate P(Y|X). In Fig- ure 4(a), the causal relation between I and X is purely I →X, i.e., X →I does not exist, because tracing the X’s ID out of many-shot samples is like to ﬁnd a needle in a haystack, given the nature that a DNN feature is an abstract and diversity-reduced representation of many samples [23]. However, as shown in Figure 4(b), X →I persists in FSL, because it is much easier for a model to “guess” the correspondence, e.g., the 1-shot extreme case that has a trivial 1-to-1 mapping for X ↔I. Therefore, as we formally show in Appendix 1, the key causal difference between MSL and FSL is: MSL essentially makes I an instrumental variable [1] that achieves P(Y|X) := P(Y|I) ≈P(Y|do(X)). Intuitively, we can see that all the causalities between I and D in MSL are all blocked by col- liders1, making I and D independent. So, the feature X is essentially “intervened” by I, no longer dictated by D, e.g., neither “yellow grass” nor “green grass” dominates “Lion” in Fig- ure 2(b), mimicking the casual intervention by controlling the use of pre-trained knowledge. Many-shot 𝑋 𝑌𝐼 𝐷 ✘ Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 𝑋 𝑌 𝐶 𝑑1 ⋯𝑑𝑛 𝐷 (a) Many-shot 𝑋 𝑌𝐼 𝐷 ✘ Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 𝑋 𝑌 𝐶 𝑑1 ⋯𝑑𝑛 𝐷 (b) Many-shot 𝑋 𝑌𝐼 𝐷 ✘ ✘ Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 Few-shot 𝑋 𝑌𝐼 𝐷 𝐶 𝑋 𝑌 𝐶 𝑑1⋯𝑑𝑛 𝐷 (c) Figure 4: Causal graphs with sampling process. (a) Many-Shot Learning, where P(Y |X) ≈P(Y |do(X)); (b) Few-Shot Learn- ing where P(Y |X) ̸≈P(Y |do(X); (c) Interventional Few-Shot Learning where we directly model P(Y |do(X)). In this paper, we propose to use the backdoor adjustment [46] to achieve P(Y|do(X)) without the need for many-shot, which certainly under- mines the deﬁnition of FSL. The back- door adjustment assumes that we can observe and stratify the confounder, i.e., D= {d}, where each dis a strat- iﬁcation of the pre-trained knowledge. Formally, as shown in Appendix 2, the backdoor adjustment for the graph in Figure 3(a) is: P(Y|do(X = x)) = ∑ d P(Y|X = x,D = d,C = g(x,d)) P(D= d), (1) 1In causal graph, the junction A →B ←C is called a “collider”, makingA and C independent even though A and C are linked via B [46]. For example, A = “Quality”,C = “Luck”, andB = “Paper Acceptance”. 4where gis a function deﬁned later. However, it is not trivial to instantiate d, especially when Dis a 3rd-party delivered pre-trained network where the dataset is unobserved [22]. Next, we will offer three practical implementations of Eq. (1) for Interventional FSL. 3 Interventional Few-Shot Learning Our implementation idea is inspired from the two inherent properties of any pre-trained DNN. First, each feature dimension carries a semantic meaning, e.g., every channel in convolutional neural network is well-known to encode visual concepts [82, 76]. So, each feature dimension represents a piece of knowledge. Second, most prevailing pre-trained models use a classiﬁcation task as the objective, such as the 1,000-way classiﬁer of ResNet [ 25] and the token predictor of BERT [ 17]. Therefore, the classiﬁer can be considered as the distilled knowledge, which has been already widely adopted in literature [26]. Next, we will detail the proposed Interventional FSL (IFSL) by providing three different implementations2 for g(x,d), P(Y|X,D,C ), and P(D) in Eq. (1). In particular, the exact forms of P(Y|·) across different classiﬁers are given in Appendix 5. Feature-wise Adjustment. Suppose that Fis the index set of the feature dimensions of x, e.g., from the last-layer of the pre-trained network Ω. We divide Finto nequal-size disjoint subsets, e.g., the output feature dimension of ResNet-10 is 512, if n= 8, the i-th set will be a feature dimension index set of size 512/8 = 64, i.e., Fi = {64(i−1) + 1,..., 64i}. The stratum set of pre-trained knowledge is deﬁned as D:= {d1,...,d n}, where each di = Fi. (i) g(x,di) := {k|k ∈Fi ∩It}, where It is an index set whose corresponding absolute values in x are larger than the threshold t. The reason is simple: if a feature dimension is inactive in x, its corresponding adjustment can be omitted. We set t=1e-3 in this paper. (ii) P(Y|X,D,C ) = P(Y|[x]c), where c= g(x,di) is implemented as the index set deﬁned above, [x]c = {xk}k∈c is a feature selector which selects the dimensions of x according to the index set c. The classiﬁer takes the adjusted feature [x]c as input. Note that dis already absorbed in c, so [x]c is essentially a function of (X,D,C ). (iii) P(di) = 1/n, where we assume a uniform prior for the adjusted features. (iv) The overall feature-wise adjustment is: P(Y|do(X = x)) = 1 n n∑ i=1 P(Y|[x]c), where c= {k|k∈Fi ∩It}. (2) It is worth noting that the feature-wise adjustment is always applicable, as we can always have the feature representation x from the pre-trained network. Interestingly, our feature-wise adjustment sheds some light on the theoretical justiﬁcations for the multi-head trick in transformers [65]. We will explore this in future work. Class-wise Adjustment. Suppose that there are mpre-training classes, denoted asA= {a1,...a m}. In class-wise adjustment, each stratum of pre-trained knowledge is deﬁned as a pre-training class, i.e., D:= {d1,...,d m}and each di = ai. (i) g(x,di) := P(ai|x)¯ xi, where P(ai|x) is the pre-trained classiﬁer’s probability output that x belongs to class ai, and ¯ xi is the mean feature of pre-training samples from class ai. Note that unlike feature-wise adjustment where cis an index set, here c= g(x,di) is implemented as a real vector. (ii) P(Y|X,D,C ) = P(Y|x ⊕g(x,di)), where ⊕denotes vector concatenation. (iii) P(di) = 1/m, where we assume a uniform prior of each class. (iv) The overall class-wise adjustment is: P(Y|do(X = x)) = 1 m m∑ i=1 P(Y|x ⊕P(ai|x)¯ xi) ≈P(Y|x ⊕ 1 m m∑ i=1 P(ai|x)¯ xi) , (3) 2We assume that the combinations of the feature dimensions or classes are linear, otherwise the adjustment requires prohibitive O(2n) sampling. We will relax this assumption in future work. 5where we adopt the Normalized Weighted Geometric Mean (NWGM) [ 71, 72] approximation to move the outer sum ∑P into the inner P(∑). This greatly reduces the network forward-pass consumption as mis usually large in pre-training dataset. Please refer to Appendix 3 for the detailed derivation. Combined Adjustment. We can combine feature-wise and class-wise adjustment to make the stratiﬁcation in backdoor adjustment much more ﬁne-grained. Our combination is simple: applying feature-wise adjustment after class-wise adjustment. Thus, we have: P(Y|do(X = x)) ≈1 n n∑ i=1 P(Y|[x]c ⊕ 1 m m∑ j=1 [P(aj|x)¯ xj]c), where c= {k|k∈Fi ∩It}. (4) 4 Related Work Few-Shot Learning. FSL has a wide spectrum of methods, including ﬁne-tuning [13, 18], optimizing model initialization [20, 42], generating model parameters [54, 36], learning a feature space for a better separation of sample categories [66, 77], feature transfer [58, 43], and transductive learning that additionally uses query set data [ 18, 29, 27]. Thanks to them, the classiﬁcation accuracy has been drastically increased [29, 77, 73, 37]. However, accuracy as a single number cannot explain the paradoxical phenomenon in Figure 2. Our work offers an answer from a causal standpoint by showing that pre-training is a confounder. We not only further improve the accuracy of various FSL methods, but also explain the reason behind the improvements. In fact, the perspective offered by our work can beneﬁt all the tasks that involve pre-training—any downstream task can be seen as FSL compared to the large-scale pre-training data. Negative Transfer. The above phenomenon is also known as the negative transfer, where learning in source domain contributes negatively to the performance in target domain [44]. Many research works have being focused on when and how to conduct this transfer learning [30, 4, 81]. Yosinski et al. [74] split ImageNet according to man-made objects and natural objects as a test bed for feature transferability. They resemble the S̸∼Q settings used in Figure 2(a). Other work also revealed that using deeper backbone might lead to degraded performance when the domain gap between training and test is large [33]. Some similar ﬁndings are reported in the few-shot setting [ 50] and NLP tasks [62]. Unfortunately, they didn’t provide a theoretical explanation why it happens. Causal Inference. Our work aims to deal with the pre-training confounder in FSL based on causal inference [48]. Causal inference was recently introduced to machine learning [40, 9] and has been applied to various ﬁelds in computer vision. [ 72] proposes a retrospective for image captioning and other applications include image classiﬁcation [ 12, 38], imitation learning [ 15], long-tailed recognition [60] and semantic segmentation [78]. We are the ﬁrst to approach FSL from a causal perspective. We would like to highlight that data-augmentation based FSL can also be considered as approximated intervention. These methods learn to generate additional support samples with image deformation [14, 79] or generative models [2, 80]. This can be view as physical interventions on the image features. Regarding the causal relation between image X and label Y, some works adopted anti-causal learning [ 41], i.e., Y →X, where the assumption is that labels Y are disentangled enough to be treated as Independent Mechanism (IM) [ 45, 59], which generates observed images X through Y →X. However, our work targets at the more general case where labels can be entangled (e.g.“lion” and “dog” share the semantic “soft fur”) and the IM assumption may not hold. Therefore, we use causal prediction X →Y as it is essentially a reasoning process, where the IM is captured by D, which is engineered to be disentangled through CNN (e.g., the conv-operations are applied independently). In this way, Dgenerates visual features through D→X and emulates human’s naming process through D →Y (e.g., “fur”, “four-legged”→“meerkat”). In fact, the causal direction X →Y (NOT anti-causal Y →X) has been empirically justiﬁed in complex CV tasks [32, 67, 60, 61]. 5 Experiments 5.1 Datasets and Settings Datasets. We conducted experiments on benchmark datasets in FSL literature: 1)miniImageNet [66] containing 600 images per class over 100 classes. We followed the split proposed in [51]: 64/16/20 6classes for train/val/test. 2) tieredImageNet [52] is much larger compared to miniImageNet with 608 classes and each class around 1,300 samples. These classes were grouped into 34 higher-level concepts and then partitioned into 20/6/8 disjoint sets for train/val/test to achieve larger domain difference between training and testing. 3) Caltech-UCSD Birds-200-2011 ( CUB) [70] for cross- domain evaluation. It contains 200 classes and each class has around 60 samples. The models used for CUB test were trained on the miniImageNet. Training and evaluation settings on miniImageNet and tieredImageNet are included in Appendix 5. Implementation Details. We pre-trained the 10-layer ResNet (ResNet-10) [25] and the WideResNet (WRN-28-10) [75] as our backbones. Our proposed IFSL supports both ﬁne-tuning and meta-learning. For ﬁne-tuning, we applied average pooling on the last residual block and used the pooled features to train classiﬁers. For meta-learning, we deployed 5 representative methods that cover a large spectrum of meta-learning based FSL: 1) model initialization: MAML [20], 2) weight generator: LEO [54], transductive learning: SIB [29], 4) metric learning: MatchingNet (MN) [66], and 5) feature transfer: MTL [58]. For both ﬁne-tuning and meta-learning, our IFSL aims to the learn classiﬁer P(Y|do(X)) instead of the conventional P(Y|X). Detailed implementations are given in Appendix 5. Evaluation Metrics. Our evaluation is based on the following metrics: 1) Conventional accuracy (Acc) is the average classiﬁcation accuracy commonly used in FSL [20, 66, 58]. 2) Hardness-speciﬁc Acc. For each query, we deﬁne a hardness that measures its semantic dissimilarity to the support set, and accuracy is then computed at different levels of query hardness. Speciﬁcally, query hardness is computed by h = log ((1 −s)/s) and s = exp⟨r+,p+ c=gt⟩/∑ cexp⟨r+,p+ c ⟩, where ⟨·⟩is the cosine similarity, (·)+ represents the ReLU activation function, r denotes the Ω prediction logits of query, pc denotes the average prediction logits of class cin the support set and gtis the ground-truth of query. Using Hardness-speciﬁc Acc is similar to evaluating the hardness of FSL tasks [18], while ours is query-sample-speciﬁc and hence is more ﬁne-grained. Later, we will show its effectiveness to unveil the spurious effects in FSL. 3) Feature localization accuracy (CAM-Acc) quantiﬁes if a model “pays attention” to the actual object when making prediction. It is deﬁned as the percentage of pixels inside the object bounding box by using Grad-CAM [56] score larger than 0.9. Compared to Acc that shows if the prediction is correct, CAM-Acc reveals whether the prediction is based on the correct visual cues. Table 1: Acc (%) averaged over 2000 5-way FSL tasks before and after applying IFSL. We obtained the results by using ofﬁcial code and our backbones for a fair comparison across methods. We also implemented SIB in both transductive and inductive setting to facilitate fair comparison. For IFSL, we reported results of combined adjustment as it almost always outperformed feature-wise and class-wise adjustment. See Appendix 6 for Acc and 95% conﬁdence intervals on all 3 types of adjustment. ResNet-10 WRN-28-10 miniImageNet tieredImageNet miniImageNet tieredImageNetMethod 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot Linear 76.38 56.26 81.01 61.39 79.79 60.69 85.37 67.27 +IFSL+2.19 77.97+1.59 60.13+3.87 82.08+1.07 64.29+2.9 80.97+1.18 64.12+3.43 86.19+0.82 69.96+2.69 Cosine 76.68 56.40 81.13 62.08 79.72 60.83 85.41 67.30 +IFSL+1.77 77.63+0.95 59.84+3.44 81.75+0.62 64.47+2.39 80.74+1.02 63.76+2.93 86.13+0.72 69.36+2.06 k-NN 76.63 55.92 80.85 61.16 79.60 60.34 84.67 67.25 Fine-Tuning +IFSL+3.13 78.42+1.79 62.31+6.36 81.98+1.13 65.71+4.55 81.08+1.48 64.98+4.64 86.06+1.39 70.94+3.69 MAML [20] 70.85 56.59 74.02 59.17 73.92 58.02 77.20 61.40 +IFSL+5.55 76.37+5.52 59.36+2.77 81.04+7.02 63.88+4.71 79.25+5.33 62.84+4.82 85.10+7.90 67.70+6.30 LEO [54] 74.49 58.48 80.25 65.25 75.86 59.77 82.15 68.90 +IFSL+1.94 76.91+2.42 61.09+2.61 81.43+1.18 66.03+0.78 77.72+1.86 62.19+2.42 85.04+2.89 70.28+1.38 MTL [58] 75.65 58.49 81.14 64.29 77.30 62.99 83.23 70.08 +IFSL+2.02 78.03+2.38 61.17+2.68 82.35+1.21 65.72+1.43 80.20+2.9 64.40+1.41 86.02+2.79 71.45+1.37 MN [66] 75.21 61.05 79.92 66.01 77.15 63.45 82.43 70.38 +IFSL+1.34 76.73+1.52 62.64+1.59 80.79+0.87 67.30+1.29 78.55+1.40 64.89+1.44 84.03+1.60 71.41+1.03 SIB [29] (transductive) 78.88 67.10 85.09 77.64 81.73 71.31 88.19 81.97 +IFSL+1.15 80.32+1.44 68.85+1.75 85.43+0.34 78.03+0.39 83.21+1.48 73.51+2.20 88.69+0.50 83.07+1.10 SIB [29] (inductive) 75.64 57.20 81.69 65.51 78.17 60.12 84.96 69.20 Meta-Learning +IFSL+2.05 77.68+2.04 60.33+3.13 82.75+1.06 67.34+1.83 80.05+1.88 63.14+3.02 86.14+1.18 71.45+2.25 20% 40% 60% 80% Accuracy Hardness 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (a) Fine-Tuning (Linear) 20% 40% 60% 80% Accuracy Hardness 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear sib (b) Meta-Learning (SIB) Figure 5: Accuracy across query hardness on 5-shot ﬁne- tuning and meta-learning. Ad- ditional results are shown in Appendix 6. 5.2 Results and Analysis Conventional Acc. 1) From Table 1, we observe that IFSL consistently improves ﬁne-tuning and meta-learning in all settings, which suggests that IFSL is agnostic to methods, datasets, and backbones. 2) In particular, the improvements are typically larger on 1-shot than 5-shot. For example, in ﬁne- 7Table 2: Comparison with state-of-the-arts of 5-way 1-/5- shot Acc (%) on miniImageNet and tieredImageNet. Method Backbone miniImageNet tieredImageNet 5-shot 1-shot 5-shot 1-shot Baseline++ [13] ResNet-10 75.90 53.97 - - IdeMe-Net† [14] ResNet-10 73.78 57.61 80.34 60.32 TRAML [34] ResNet-12 79.54 67.10 - - DeepEMD [77] ResNet-12 82.41 65.91 86.03 71.16 CTM [35] ResNet-18 80.51 64.12 84.28 68.41 FEAT [73] WRN-28-10 81.80 66.69 84.38 70.41 Tran. Baseline [18] WRN-28-10 78.40 65.73 85.50 73.34 wDAE-GNN [21] WRN-28-10 78.85 62.96 83.09 68.18 SIB† [29] WRN-28-10 81.73 71.31 88.19 81.97 SIB+IFSL (ours) WRN-28-10 83.21 73.51 88.69 83.07 †Using our pre-trained backbone. Table 3: Results of cross-domain eval- uation: miniImageNet →CUB. The whole report is in Appendix 6. Backbone Method 5-shot 1-shot Linear 58.84 42.25 +IFSL 60.65 45.14 SIB 60.60 45.87 ResNet-10 +IFSL 62.07 47.07 Linear 62.12 42.89 +IFSL 64.15 45.64 SIB 62.59 49.16 WRN-28-10 +IFSL 64.43 50.71 “mixing bowl”“crate” “triﬂe” “ant”“electric guitar” Query Linear +IFSL MAML +IFSL CAM-Acc 5-shot 1-shotLinearMAML LinearMAML29.0229.43 25.2227.39+IFSL29.8530.06 +IFSL26.6728.42 Figure 6: Some miniImageNet visualizations of Grad-Cam [56] activation of query images and the CAM-Acc (%) table of using linear classiﬁer and MAML. Categories with red text represent failed cases. The complete results on CAM-Acc are shown in Appendix 6, where IFSL achieves similar or better results in all settings. tuning, the average performance gain is 1.15% on 5-shot and 3.58% on 1-shot. The results support our analysis in Section 2.3 that FSL models are more prone to bias in lower-shot settings. 3) Regarding the average improvements on ﬁne-tuning vs. meta-learning (e.g.k-NN and MN), we observe that IFSL improves more on ﬁne-tuning in most cases. We conjecture that this is because meta-learning is an implicit form of intervention, where randomly sampled meta-training episodes effectively stratify the pre-trained knowledge. This suggests that meta-learning is fundamentally superior over ﬁne-tuning due to increased robustness against confounders. We will investigate this potential theory in future work. 4) Additionally we see that the improvements on miniImageNet are usually larger than that on tieredImageNet. A possible reason is the much larger training set for tieredImageNet: it substantially increases the breadth of the pre-trained knowledge and the resulting models explain query samples much better. 5) According to Table 1 and Table 2, it is clear that our k-NN+IFSL outperforms IdeMe-Net [14] using the same pre-trained ResNet-10. This shows that using data augmentation — a method of physical data intervention as in IdeMe-Net [ 14] is inferior to our causal intervention in IFSL. 6) Overall, our IFSL achieves the new state-of-the-art on both datasets. Note that IFSL is ﬂexible to be plugged into different baselines. Hardness-speciﬁc Acc. 1) Figure 5(a) shows the plot of Hardness-speciﬁc Acc of ﬁne-tuning. We notice that when query becomes harder, ResNet-10 (blue curves) becomes superior to WRN-28-10 (red curves). This tendency is consistent with Figure 2(a) illustrating the effect of the confounding bias caused by pre-training. 2) Intriguingly, in Figure 5(b), we notice that this tendency is reversed for meta-learning, i.e., deeper backbone always performs better. The improved performance of deeper backbone on hard queries suggests that meta-learning should have some functions to remove the confounding bias. This evidence will inspire us to provide a causal view of meta-learning in future work. 3) Overall, Figure 5 shows that using IFSL futher improves ﬁne-tuning and meta-learning consistently across all hardness, validating the effectiveness of the proposed causal intervention. CAM-Acc & Visualization. In Figure 6, we compare +IFSL to baseline linear classiﬁer on the left and to baseline MAML [20] on the right, and summarize CAM-Acc results in the upper-right table. From the visualization, we see that using IFSL let the model pay more attention to the objects. However, notice that all models failed in the categories colored as red. A possible reason behind the failures is the extremely small size of the object — models have to resort to context for prediction. From the numbers, we can see our improvements for 1-shot are larger than that for 5-shot, consistent 8with our ﬁndings using other evaluation metrics. These results suggest that IFSL helps models use the correct visual semantics for prediction by removing the confounding bias. Cross-Domain Generalization Ability. In Table 3, we show the testing results on CUB using the models trained on the miniImageNet. The setting is challenging due to the big domain gap between the two datasets. We chose linear classiﬁer as it outperforms cosine and k-NN in cross-domain setting and compared with transductive method — SIB. The results clearly show that IFSL works well in this setting and brings consistent improvements, with the average 1.94% of Acc. In addition, we can see that applying IFSL brings larger improvements to the inductive linear classiﬁer than to the transductive SIB. It is possibly because transductive methods involve unlabeled query data and performs better than inductive methods with the additional information. Nonetheless we observe that IFSL can further improve SIB in cross-domain (Table 3) and single-domain (Table 1) generalization. 6 Conclusions We presented a novel casual framework: Interventional Few-Shot Learning (IFSL), to address an overlooked deﬁciency in recent FSL methods: the pre-training is a confounder hurting the performance. Speciﬁcally, we proposed a structural causal model of the causalities in the process of FSL and then developed three practical implementations based on the backdoor adjustment. To better illustrate the deﬁciency, we diagnosed the classiﬁcation accuracy comprehensively across query hardness, and showed that IFSL improves all the baselines across all the hardness. It is worth highlighting that the contribution of IFSL is not only about improving the performance of FSL, but also offering a causal explanation why IFSL works well: it is a causal approximation to many-shot learning. We believe that IFSL may shed light on exploring the new boundary of FSL, even though FSL is well-known to be ill-posed due to insufﬁcient data. To upgrade IFSL, we will seek other observational intervention algorithms for better performance, and devise counterfactual reasoning for more general few-shot settings such as domain transfer. 7 Acknowledgements The authors would like to thank all the anonymous reviewers for their constructive comments and suggestions. This research is partly supported by the Alibaba-NTU Singapore Joint Research Institute, Nanyang Technological University (NTU), Singapore; the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 and Tier 2 grant; and Alibaba Innovative Research (AIR) programme. We also want to thank Alibaba City Brain Group for the donations of GPUs. 8 Broader Impact The proposed method aims to improve the Few-Shot Learning task. Advancements in FSL helps the deployment of machine learning models in areas where labelled data is difﬁcult or expensive to obtain and it is closely related to social well-beings: few-shot drug discovery or medical imaging analysis in medical applications, cold-start item recommendation in e-commerce, few-shot reinforcement learning for industrial robots, etc.. Our method is based on causal inference and the analysis is rooted on causation rather than correlation. The marriage between causality and machine learning can produce more robust, transparent and explainable models, broadening the applicability of ML models and promoting fairness in artiﬁcial intelligence. References [1] Joshua D Angrist and Alan B Krueger. Instrumental variables and the search for identiﬁcation: From supply and demand to natural experiments. Journal of Economic Perspectives, 2001. 4 [2] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adver- sarial networks. In Proceedings of the International Conference on Learning Representations Workshops, 2018. 6 [3] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In Advances in Neural Information Processing Systems, 2019. 3 9[4] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Factors of transferability for a generic convnet representation. IEEE transactions on Pattern Analysis and Machine Intelligence, 2015. 6 [5] Pierre Baldi and Peter Sadowski. The dropout learning algorithm. Artiﬁcial intelligence, 2014. 16 [6] Alexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect compliance. Journal of the American Statistical Association, 1997. 15 [7] Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, 2012. 4 [8] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013. 3 [9] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In International Conference on Learning Representations , 2019. 6 [10] Michel Besserve, Rémy Sun, and Bernhard Schölkopf. Counterfactuals uncover the modular structure of deep generative models. arXiv preprint arXiv:1812.03253, 2018. 3 [11] Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM (JACM), 2011. 3 [12] Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. In Uncertainty in Artiﬁcial Intelligence, 2015. 6 [13] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. In International Conference on Learning Representations, 2019. 1, 6, 8, 18, 19 [14] Zitian Chen, Yanwei Fu, Yu-Xiong Wang, Lin Ma, Wei Liu, and Martial Hebert. Image deformation meta-networks for one-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 6, 8 [15] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems, 2019. 6 [16] F.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, and L.E. Meester. A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer Texts in Statistics. Springer, 2005. 4 [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. 1, 5 [18] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In International Conference on Learning Representations, 2020. 1, 6, 7, 8 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006. 1 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta- tion of deep networks. In International Conference on Machine Learning, 2017. 2, 3, 6, 7, 8, 17, 18, 20, 21, 22, 23 [21] Spyros Gidaris and Nikos Komodakis. Generating classiﬁcation weights with gnn denoising autoencoders for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2, 3, 8 [22] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. Detectron. https://github.com/facebookresearch/detectron, 2018. 5 [23] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. The MIT Press, 2016. 4 [24] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, 2017. 1 10[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 1, 5, 7, 17, 18 [26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In Advances in Neural Information Processing Systems Deep Learning Workshop, 2014. 1, 5 [27] Ruibing Hou, Hong Chang, MA Bingpeng, Shiguang Shan, and Xilin Chen. Cross attention network for few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. 6 [28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1 [29] Shell Xu Hu, Pablo Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil Lawrence, and Andreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients. In International Conference on Learning Representations, 2020. 2, 6, 7, 8, 19, 20, 21, 22, 23 [30] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016. 6 [31] Muhammad Abdullah Jamal and Guo-Jun Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 1 [32] Qi Jiaxin, Niu Yulei, Huang Jianqiang, and Zhang Hanwang. Two causal principles for improving visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 6 [33] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2019. 6 [34] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot learning with adaptive margin loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 8 [35] Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang. Finding task-relevant features for few-shot learning by category traversal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 8 [36] Huaiyu Li, Weiming Dong, Xing Mei, Chongyang Ma, Feiyue Huang, and Bao-Gang Hu. Lgm-net: Learning to generate matching networks for few-shot learning. In International Conference on Machine Learning, 2019. 6 [37] Yaoyao Liu, Bernt Schiele, and Qianru Sun. An ensemble of epoch-wise empirical bayes for few-shot learning. In European Conference on Computer Vision, 2020. 6 [38] David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Léon Bottou. Discovering causal signals in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 6 [39] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 2008. 3 [40] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. In Advances in Neural Information Processing Systems, 2018. 6 [41] Gong Mingming, Zhang Kun, Liu Tongliang, Tao Dacheng, Clark Glymour, and Bernhard Schölkopf. Domain adaptation with conditional transferable components. In International Conference on Machine Learning, 2016. 6 [42] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018. 6 [43] Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, 2018. 6 [44] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2009. 6 11[45] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Schölkopf. Learning independent causal mechanisms. In International Conference on Machine Learning, 2018. 6 [46] J. Pearl, M. Glymour, and N.P. Jewell. Causal Inference in Statistics: A Primer. Wiley, 2016. 2, 3, 4, 14 [47] Judea Pearl. Causal diagrams for empirical research. Biometrika, 1995. 15 [48] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2nd edition, 2009. 4, 6, 14 [49] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 2 [50] Tiago Ramalho, Thierry Sousbie, and Stefano Peluchetti. An empirical study of pretrained representations for few-shot classiﬁcation. arXiv preprint arXiv:1910.01319, 2019. 6 [51] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Interna- tional Conference on Learning Representations, 2017. 6 [52] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenen- baum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. In International Conference on Learning Representations, 2018. 2, 7 [53] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 2000. 3 [54] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. InInternational Conference on Learning Representations, 2019. 6, 7, 17, 19, 20, 21, 22, 23 [55] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016. 1 [56] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, 2017. 7, 8 [57] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017. 16, 18 [58] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 6, 7, 18, 19, 20, 21, 22, 23 [59] Raphael Suter, Ðor ¯de Miladinovi´c, Bernhard Schölkopf, and Stefan Bauer. Robustly disen- tangled causal mechanisms: Validating deep representations for interventional robustness. In International Conference on Machine Learning, 2019. 6 [60] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classiﬁcation by keeping the good and removing the bad momentum causal effect. In Advances in Neural Information Processing Systems, 2020. 6 [61] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 6 [62] Marc Tanti, Albert Gatt, and Kenneth P Camilleri. Transfer learning from language models to im- age caption generators: Better models may not transfer better. arXiv preprint arXiv:1901.01216, 2019. 6 [63] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 2000. 3 [64] Matthew Turk and Alex Pentland. Face recognition using eigenfaces. In Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1991. 3 12[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. 1, 5 [66] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, 2016. 2, 3, 6, 7, 19, 20, 21, 22, 23 [67] Tan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense r-cnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 6 [68] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens van der Maaten. Simpleshot: Re- visiting nearest-neighbor classiﬁcation for few-shot learning. arXiv preprint arXiv:1911.04623, 2019. 18 [69] Yaqing Wang and Quanming Yao. Few-shot learning: A survey. arXiv preprint arXiv:1904.05046, 2019. 1 [70] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical report, California Institute of Technology, 2010. 2, 7 [71] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, 2015. 6 [72] Xu Yang, Hanwang Zhang, and Jianfei Cai. Deconfounded image captioning: A causal retrospect. arXiv preprint arXiv:2003.03923, 2020. 6 [73] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 6, 8 [74] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, 2014. 6 [75] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference, 2016. 7, 17, 18 [76] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision, 2014. 3, 5 [77] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classiﬁ- cation with differentiable earth mover’s distance and structured classiﬁers. 2020. 6, 8 [78] Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian sheng Hua, and Qianru Sun. Causal inter- vention for weakly-supervised semantic segmentation. In Advances in Neural Information Processing Systems, 2020. 6 [79] Hongguang Zhang, Jing Zhang, and Piotr Koniusz. Few-shot learning via saliency-guided hallucination of samples. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 6 [80] Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua Bengio, and Yangqiu Song. Metagan: An adversarial approach to few-shot learning. In Advances in Neural Information Processing Systems, 2018. 6 [81] Youshan Zhang and Brian D Davison. Impact of imagenet model selection on domain adaptation. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision Workshops, 2020. 6 [82] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 3, 5 13Supplementary Material for Interventional Few-Shot Learning Zhongqi Yue1,3, Hanwang Zhang1, Qianru Sun2, Xian-Sheng Hua3 1Nanyang Technological University, 2Singapore Management University, 3Damo Academy, Alibaba Group yuez0003@ntu.edu.sg, hanwangzhang@ntu.edu.sg, qianrusun@smu.edu.sg, xiansheng.hxs@alibaba-inc.com This supplementary material is organized as follows: • Section A.1 details our analysis in Section 2.3 by showing many-shot learning converges to true causal effect through instrumental variable (IV); • Section A.2 gives the derivation for the backdoor adjustment formula in Eq. (1); • Section A.3 presents the detailed derivation for the NWGM approximation used in Eq. (3) and (4); • Section A.4 includes the algorithms for adding IFSL to ﬁne-tuning and meta-learning; • Section A.5 shows the implementation details for pre-training (Section A.5.1), ﬁne-tuning (Section A.5.2) and meta-learning (Section A.5.3); • Section A.6 includes additional experimental results on Conventional Acc (Section A.6.1), Hardness-Speciﬁc Acc (Section A.6.2), CAM-Acc (Section A.6.3) and cross-domain evalu- ation (Section A.6.4). A.1 Instrumental Variable In this section, we will show that in our causal graph for many-shot learning, the sampling ID I is essentially an instrumental variable for X →Y that achieves P(Y|I) ≈P(Y|do(X)). Before introducing instrumental variable, we ﬁrst formally deﬁne d-separation [46], which gives a criterion to study the dependencies between nodes (data variables) in any structural causal model. d-separation. A set of nodes Zblocks a path pif and only if 1) pcontains a chain A→B →Cor a fork A←B →Cand the middle node Bis in Z; 2) pcontains a collider A→B ←Csuch that the middle node Band its descendants are not in Z. If conditioning on Zblocks every path between X and Y, we say X and Y are d-separated conditional on Z, i.e., X and Y are independent given Z (X ⊥ ⊥Y|Z). Instrumental Variable. For a structual causal model G, a variable Z is an instrumental variable (IV) to X →Y by satisfying the graphical criteria [48]: 1) (Z ⊥ ⊥Y)GX ; 2) (Z ̸⊥ ⊥X)G, where GX is the manipulated graph where all incoming arrows to node X are deleted. For the SCM of many-shot learning in Figure 4(a), it is easy to see that I satisﬁes both criteria and therefore it is an IV for X →Y. However, in the few-shot SCM in Figure 4(b), the paths I ←X ←D →C →Y and I ←X →C →Y are not blocked in GX, which means the ﬁrst criterion is not met (I ̸⊥ ⊥Y)GX and I is not an instrumental variable in the few-shot learning case. Instrumental variable can help ﬁnd the true causal effect even in the presence of confounder. This is due to the collider junction that makes the IV and confounder independent ( I ⊥ ⊥D in Figure 4(a)). To see this, we will ﬁrst consider a simpliﬁed case of Figure 4(a) where each causal link represents a linear relationship and we aim to ﬁnd the true causal effect from X →Y through linear regression. Without loss of generality, let I,X,Y take the value of real number. Denote rIX,rXY , 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.and rIY as the slope of regression line between I and X, X and Y, I and Y respectively. Notice that rXY is spurious as it is contaminated by the backdoor path X ←D→C →Y. However, since the path I →X ←D →C →Y is blocked due to collider at X, rIY is free from confounding bias. Therefore rIY /rIX gives the true causal effect from X →Y. Similarly, in the classiﬁcation case of many-shot learning, a classiﬁer is trained to maximize the conditional probability on the IV P(Y|I). As the ID-sample matching I →X is deterministic, the classiﬁer eventually learns to predict based on the true causal relationship X →Y. Yet in the complex case of image classiﬁcation, it is unreasonable to assume linear relationships between variables. In the nonlinear case, it is shown in [6] that observations on IV provide a bound for the true causal effect. This means that learning based on P(Y|I) provides an approximation to the true causal effect, i.e.P(Y|I) ≈P(Y|do(X)). A.2 Derivation of Backdoor Adjustment for the Proposed Causal Graph We will show the derivation of the backdoor adjustment for the causal graph in Figure 3(a) using the three rules of do-calculus [47]. For a causal directed acyclic graph G, let X,Y,Z and W be arbitrary disjoint sets of nodes. We use GX to denote the manipulated graph where all incoming arrows to node X are deleted. Similarly GX represents the graph where outgoing arrows from node X are deleted. We use lower case x,y,z and wfor speciﬁc values taken by each set of nodes: X = x,Y = y,Z = zand W = w. For any interventional distribution compatible with G, we have the following three rules: Rule 1 Insertion/deletion of observations: P(y|do(x),z,w ) = P(y|do(x),w),if(Y ⊥ ⊥Z|X,W )GX (A5) Rule 2 Action/observation exchange: P(y|do(x),do(z),w) = P(y|do(x),z,w ),if(Y ⊥ ⊥Z|X,W )GXZ (A6) Rule 3 Insertion/deletion of actions: P(y|do(x),do(z),w) = P(y|do(x),w),if(Y ⊥ ⊥Z|X,W )GXZ(W) , (A7) where Z(W) is the set of nodes in Zthat are not ancestors of any W-node in GX. In our causal graph, the desired interventional distribution P(Y|do(X = x)) can be derived by: P(Y|do(x)) = ∑ d P(Y|do(X = x),D = d)P(D= d|do(X = x)) (A8) = ∑ d P(Y|do(X = x),D = d)P(D= d) (A9) = ∑ d P(Y|X = x,D = d)P(D= d) (A10) = ∑ d ∑ c P(Y|X = x,D = d,C = c)P(C = c|X = x,D = d)P(D= d) (A11) = ∑ d P(Y|X = x,D = d,C = g(x,d))P(D= d), (A12) where Eq. (A8) and Eq. (A11) follow the law of total probability; Eq.(A9) uses Rule 3 given D⊥ ⊥X in GX; Eq. (A10) uses Rule 2 to change the intervention term to observation as (Y ⊥ ⊥X|D) in GX; Eq. (A12) is because in our causal graph, C takes a deterministic value given by function g(x,d). This reduces summation over all values of Cin Eq. (A11) to a single probability measure in Eq. (A12). A.3 Derivation of NWGM Approximation We will show the derivation of NWGM approximation used in Eq. (3) and (4). In a K-way FSL problem, let f(·) be a classiﬁer function that calculates logits for K classes and σbe the softmax 15function over K classes. The approximation effectively moves the outer expectation inside the classiﬁer function: E[σ(f(·))] ≈σ(f(E[·])). We will ﬁrst show the derivation for moving the expectation inside softmax function, i.e., E[σ(f(·))] ≈ σ(E[f(·)]). Without loss of generality, the backdoor adjustment formula in Eq. (3) and Eq. (4) can be written as: P(Y = y|do(X = x)) = ∑ d∈D σ(fy(x ⊕c))P(d), (A13) where Drepresents the set of stratiﬁcations, fy is the classiﬁer logit for class y, c = g(x,d) is the feature concatenated to x in Eq. (3) and (4) and P(d) is the prior for each stratiﬁcaction. It is shown in [5] that Eq. (A13) can be approximated by the Normalized Weighted Geometric Mean (NWGM) as: ∑ d∈D σ(fy(x ⊕c))P(d) ≈NWGMd∈D(σ(fy(x ⊕c))) (A14) = ∏ d[exp(fy(x ⊕c))]P(d) ∑K i=1 ∏ d[exp(fi(x ⊕c))]P(d) (A15) = exp(∑ dfy(x ⊕c)P(d)) ∑K i=1 exp(∑ dfi(x ⊕c)P(d)) (A16) = σ(Ed[fy(x ⊕c)]) , (A17) where Eq. (A14) follows [5], Eq. (A15) follows the deﬁnition of NWGM, Eq. (A16) is because exp(a)b = exp(ab). Next we will show the derivation for linear, cosine andk-NN classiﬁer to further move the expectation inside the classiﬁer function, i.e., σ(E[f(·)]) = σ(f(E[·])). For the linear classiﬁer, f(x ⊕c) = W1x + W2c, where W1,W2 ∈RK×N denote the learnable weight, N is the feature dimension, which is the same for x and c in Eq. (3) and (4). The bias term is dropped as it does not impact our analysis. Now the expectation can be further moved inside the classiﬁer function through: ∑ d f(x ⊕c))P(d) = ∑ d (W1x + W2c)P(d) (A18) = W1x + ∑ d W2cP(d) (A19) = f(x ⊕ ∑ d cP(d)), (A20) where Eq. (A19) is because the feature vector x is the same for all dand Ed[x] = x. For the cosine classiﬁer, f(x ⊕c) = (W1x + W2c)/∥x ⊕c∥∥W∥, where W ∈RK×2N is the concatenation of W1 and W2. In the special case where x and c are unit vector, ∥x ⊕c∥is √ 2 and the cosine classiﬁer function reduces to a linear combination of terms involving only x and only c. From there, the analysis for linear classiﬁer follows and we have σ(Ef(·)) = σ(f(E·)) for cosine classiﬁer. In the general case where x and c are not unit vector, moving the expectation inside cosine classiﬁer function is an approximation σ(E[f(·)]) ≈σ(f(E[·])). For the k-NN classiﬁer, our implementation calculates class centroids using the mean feature of the K support sets and then uses the nearest centroid for prediction ( 1-NN). Speciﬁcally, let x be a feature vector and x′be the ith class centroid, i ∈{1,...,K }. The logit for class iis given by fi(x) = −∥x −x′∥2. It is shown in [57] that k-NN classiﬁer that uses squared Euclidean distance to generate logits is equivalent to a linear classiﬁer with a particular parameterization. Therefore, our analysis on linear classiﬁer follows for k-NN. In summary, the derivation ofE[σ(f(·))] ≈σ(f(E[·])) is a two-stage process where we ﬁrst move the expectation inside the softmax function and then further move it inside the classiﬁer function. 16A.4 Algorithms for Fine-tuning and Meta-Learning with IFSL In this section, we will brieﬂy revisit the settings of ﬁne-tuning and meta-learning and introduce how to integrate IFSL into them. In ﬁne-tuning, the goal is to train a classiﬁer θ conditioned on the current support set S = {(xi,yi)}ns i=1, where xi is the feature generated by Ω for ith sample, yi is the ground-truth la- bel for ith sample and ns is the support set size. This is achieved by ﬁrst predicting the support label ˆyusing the classiﬁer P(y|x; θ). Then with the predicted label ˆyand ground-truth label y, one can calculate a loss L(ˆy,y) (usually cross-entropy loss) to update the classiﬁer parameter, e.g.through stochastic gradient descent. Adding IFSL to ﬁne-tuning is simple: 1) Pick an adjustment strategy introduced in Section 3. Each implementation deﬁnes the set of pre-trained knowledge stratiﬁcations D, function form of g(X,D), function form of P(Y|X,D,C ) and the prior P(D); 2) The classiﬁer prediction is now based on P(Y|do(X); θ). The process of ﬁne-tuning with IFSL is summarized in Algorithm 1. Note that for the non-parametric k-NN classiﬁer, the ﬁne-tuning process is not applicable. When adding IFSL to k-NN, each sample is represented by the adjusted feature instead of original feature x. Please refer to the classiﬁer inputs in Eq. (2), (3) and (4) for the exact form of adjusted feature. In meta-learning, the goal is to learn the additional “learning behavior” parameterized by φusing training episodes {(Si,Qi)}sampled from training dataset D. The classiﬁer in meta-learning makes predictions by additionally conditioning on the learning behavior, written as Pφ(y|x; θ). Within each episode, θ is ﬁrst ﬁne-tuned on the support set Si. Then the ﬁne-tuned model is tested on the query set Qi to obtain the loss Lφ(Si,Qi) (e.g.using cross-entropy loss). Finally the loss is used to update φusing an optimizer. It is also easy to integrate IFSL into meta-learning by only changing the classiﬁer from Pφ(y|x; θ) to Pφ(y|do(x); θ). The ﬂow of meta-learning with IFSL is presented in Algorithm 2. Firstly notice that the initialization of θin each task may depend on φor Si. For example, in MAML [ 20] φessentially deﬁnes an initialization of model parameters, and in LEO [ 54] the initial classiﬁer parameter is generated conditioned on φand Si. Secondly, although the ﬁne-tuning of θlargely follows Algorithm 1, some meta-learning methods additionally utilize meta-knowledge φ. For example, in SIB the gradients for updating θare predicted by φusing unlabelled query features instead of calculated from L(ˆy,y) as in Algorithm 1. Algorithm 1: Fine-tuning + IFSL Input : D, Support set S= {(xi,yi)}ns i=1 Output :Fine-tuned classiﬁer parameters θ Initialize θ; while not converged do for i= 1,...,n s do for d∈Ddo Calculate c= g(x,d); Obtain P(Y|xi,c,d ; θ),P(d) Prediction ˆyi = P(y|do(x); θ); Update θusing L(ˆyi,yi) return θ Algorithm 2: Meta-Learning + IFSL Input : D, training dataset D Output :Optimized meta-parameters φ Initialize φ; while not converged do Sample (Si,Qi) from D; Initialize classiﬁer θwith φ,Si; Fine-tune θusing Algorithm 1 conditioned on φ; Predict query based on Pφ(y|do(x); θ); Update φusing Lφ(Si,Qi; θ) return φ A.5 Implementation Details A.5.1 Pre-training Prior to ﬁne-tuning or meta-learning, we pre-trained a deep neural network (DNN) as feature extractor on the train split of a dataset. We use ResNet-10[25] or WRN-28-10[75] as feature extractor backbone. This section will present the architecture and exact training procedure for our backbones. Network Architecture. The architecture of our ResNet-10 and WRN-28-10 backbone is shown in Figure A1. Speciﬁcally, each convolutional layer is described as “ n×nconv, p”, where nis the kernel size and pis the number of output channels. Convolutional layers with “/2” have a stride of 2 and are used to perform downsampling. The solid curved lines represent identity shortcuts, and the 17dotted lines are projection shortcuts implemented by 1 ×1 convolutions. The batch normalization and ReLU layers are omitted in Figure A1 to highlight the key structure of the two backbones. Pre-training Procedure. The networks are trained from scratch with stochastic gradient descent in a fully-supervised manner, i.e., minimizing cross-entropy loss on the train split of a dataset. Speciﬁcally the training is conducted on 90 epochs with early stopping using validation accuracy. We used batch size of 256 and image size of 84 ×84. For data augmentation, a random patch is sampled from an image, resized to 84 ×84 and randomly ﬂipped along horizontal axis before used for training. The initial learning rate is set to 0.1 and it is scaled down by factor of 10 every 30 epochs. A.5.2 Fine-Tuning 3×3conv, 64 3×3conv, 64 3×3conv, 64 3×3conv, 128, /2 3×3conv, 128 3×3conv, 256, /2 3×3conv, 256 3×3conv, 512, /2 3×3conv, 512 image Avg Pool fc logits 3×3conv, 16 image ×3 Avg Pool fc logits 3×3conv, 160 3×3conv, 160 3×3conv, 160 3×3conv, 160 ×3 3×3conv, 320 3×3conv, 320, /2 3×3conv, 320 3×3conv, 320 ×3 3×3conv, 640 3×3conv, 640, /2 3×3conv, 640 3×3conv, 640 (a) 3×3conv, 64 3×3conv, 64 3×3conv, 64 3×3conv, 128, /2 3×3conv, 128 3×3conv, 256, /2 3×3conv, 256 3×3conv, 512, /2 3×3conv, 512 image Avg Pool fc logits 3×3conv, 16 image ×3 Avg Pool fc logits 3×3conv, 160 3×3conv, 160 3×3conv, 160 3×3conv, 160 ×3 3×3conv, 320, /2 3×3conv, 320 3×3conv, 320 3×3conv, 320 ×3 3×3conv, 640, /2 3×3conv, 640 3×3conv, 640 3×3conv, 640 (b) Figure A1: The architecture of our back- bones: (a) ResNet-10 [ 25]; (b) WRN-28- 10 [75]. We consider linear, cosine and k-NN classiﬁer for our ﬁne-tuning experiments. In a K-way FSL problem, the detailed implementations for the classiﬁer function f(x) are: Linear. f(x) = Wx + b, where x is the input feature, W ∈ RK×N is the learnable weight parameter, N is the feature dimension and b ∈RK is the learnable bias parameter. Cosine. f(x) = Wx/∥W∥∥x∥, where W ∈RK×N is the learnable weight parameter. We implemented cosine classiﬁer without using the bias term. k-NN. Our implementation of k-NN is similar to [57, 68]. For each of the K classes, we ﬁrst calculated the av- erage support set feature (centroid) denoted as xi,i ∈ {1,...,K }. The classiﬁer output for class iis then given by fi(x) = −∥x −xi∥2. Notice that the prediction given by this classiﬁer will be the nearest centroid. We froze the backbone and used the average pooling layer output of Ω to learn the classiﬁer. The output logits from classiﬁer functions are normalized using softmax to gen- erate probability output P(y|x). For linear and cosine classiﬁer, we followed [13] and trained the classiﬁer for 100 iteration with a batch size of 4. For ﬁne-tuning baseline, we set the learning rate as 1 ×10−2 and weight decay as 1 ×10−3. For IFSL, we set the learning rate as 5 ×10−3 and weight decay as 1 ×10−3. k-NN classiﬁer is non-parametric and can be initialized directly from support set. A.5.3 Meta-Learning MAML. MAML [20] aims to learn an initialization of network parameters such that it can be ﬁne-tuned within a few steps to solve a variety of few-shot classiﬁcation tasks. When using pre- trained network with MAML, it has been shown that learning initialization of the backbone can lead to unsatisfactory performance [13, 58]. Therefore in our experiment, we froze the backbone and appended a 2-layer MLP with ReLU activation in between the hidden layers and a linear classiﬁer after the average pooling layer ofΩ. The hidden dimension of the layers in MLP is the same as output dimension of Ω (512 for ResNet-10 and 640 for WRN-28-10). The initialization of MLP and the linear classiﬁer is meta-learnt using MAML. For hyper-parameters, we set the inner loop learning rate α= 0.01, the outer loop learning rate β = 0.01 and the number of adaptation steps as 20. For IFSL, we adopted the same hyper-parameter setting and set n=8 for feature-wise and combined adjustment. Implementation-wise, we adopted the released code1 from [13] and performed experiments on MAML without using ﬁrst-order approximation. Following the implementation in [13], the model was trained on 10,000 randomly sampled tasks with model selection using validation accuracy. We used 2,000 randomly sampled tasks for validation and testing. 1https://github.com/wyharveychen/CloserLookFewShot 18MTL. MTL [58] learns scaling and shifting parameters at each convolutional layer of the backbone. We used the MTL implementation released by the author2 which adopts linear classiﬁer. We integrated our ResNet-10 and WRN-28-10 backbones into the released code. The learning rate for scaling and shifting weights φSS and initial classiﬁer parameters was set to 1 ×10−4 uniformly. We set the inner loop learning rate for classiﬁer as 1 ×10−2 and the inner loop update step as 100. For IFSL, we adopted the same hyper-parameter setting and set n=8 for feature-wise and combined adjustment. We trained the MTL model on 10,000 randomly sampled tasks with model selection using validation accuracy and used 2,000 randomly sampled tasks for validation and testing. We used 3 RTX 2080 Ti for MTL experiments on WRN-28-10 backbone. LEO. LEO [54] learns to generate classiﬁer parameters conditioned on support set and the generated parameters are further ﬁne-tuned within each FSL task. Our experiments were conducted on the released code of LEO 3 using linear classiﬁer. Following author’s implementation, we saved the center cropped features from our pre-trained backbones and used the saved features to train LEO. For baseline, we used the hyper-parameter settings released by the author. For IFSL, we set n=8 for feature-wise and combined adjustment and halved the outer loop learning rate compared to baseline. The model was trained up to 100,000 randomly sampled tasks from training split with early stopping using validation accuracy. We used 2,000 randomly sampled tasks for validation and testing. Matching Net. Matching Net (MN) [ 66] is a metric-based method that learns a distance kernel function for k-NN. We used the Matching Net implementation in [13]. The implementation follows the setup in [66] and uses LSTM-based fully conditional embedding. We set the learning rate as 0.01 uniformly. For IFSL, we used n=16 for feature-wise and combined adjustment. The model was trained using 10,000 randomly sampled tasks with model selection using validation accuracy. We used 2,000 randomly sampled tasks for validation and testing. SIB. SIB [29] initializes classiﬁer from support set and generates gradients conditioned on unlabelled query set features to update classiﬁer parameters. We followed the SIB implementation released by the author4 which uses cosine classiﬁer. In the transductive setting, the query set size is set to 15. In the inductive setting, we used only 1 query sample randomly selected from the K classes in each episode. In terms of hyper-parameter settings, we took 3 synthetic gradient steps ( K = 3) for all our experiments. For baseline, the learning rate for SIB network and classiﬁer was set to 1 ×10−3 following author’s implementation. For IFSL, we set the learning rate to5 ×10−4 and used n=4 for feature-wise and combined adjustment. In both transductive and inductive settings, we meta-trained SIB using 50,000 randomly sampled tasks with model selection using validation accuracy. We used 2,000 randomly sampled tasks for validation and testing. A.6 Additional Results In this section, we include additional results on 1) Conventional Acc in Table A1 supplementary to Table 1; 2)Hardness-Speciﬁc Acc in Figure A2 forminiImageNet and Figure A3 fortieredImageNet, supplementary to Figure 5; 3) CAM-Acc in Table A2 supplementary to Figure 6; 4) Cross-Domain Evaluation in Table A3 supplementary to Table 3. 2https://github.com/yaoyao-liu/meta-transfer-learning 3https://github.com/deepmind/leo 4https://github.com/hushell/sib_meta_learn 19A.6.1 Conventional Acc Table A1: Supplementary to Table 1. Acc (%) and 95% conﬁdence intervals averaged over 2000 5- way FSL tasks before and after applying three proposed implementations of adjustment. Speciﬁcally, “ft” refers to feature-wise adjustment, “ cl” refers to class-wise adjustment and “ ft+cl” refers to combined adjustment. ResNet-10 WRN-28-10 miniImageNet tieredImageNet miniImageNet tieredImageNetMethod 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot Linear 76.38 ± 0.36 56.26 ± 0.47 81.01 ± 0.38 61.39 ± 0.47 79.79 ± 0.33 60.69 ± 0.45 85.37 ± 0.34 67.27 ± 0.49 ft 76.84 ± 0.36 57.37 ± 0.43 81.45 ± 0.38 61.88 ± 0.47 80.22 ± 0.31 60.84 ± 0.45 85.70 ± 0.33 67.94 ± 0.48 cl 77.23 ± 0.34 59.45 ± 0.45 81.33 ± 0.38 62.60 ± 0.48 80.27 ± 0.32 62.15 ± 0.44 85.54 ± 0.33 68.11 ± 0.48 ft+cl 77.97 ± 0.34 60.13 ± 0.45 82.08 ± 0.37 64.29 ± 0.48 80.97 ± 0.31 64.12 ± 0.44 86.19 ± 0.34 69.96 ± 0.46 Cosine 76.68 ± 0.36 56.40 ± 0.46 81.13 ± 0.39 62.08 ± 0.47 79.72 ± 0.33 60.83 ± 0.46 85.41 ± 0.34 67.30 ± 0.50 ft 76.83 ± 0.35 56.86 ± 0.44 81.34 ± 0.37 62.45 ± 0.47 79.80 ± 0.32 61.25 ± 0.44 85.74 ± 0.33 67.86 ± 0.46 cl 76.99 ± 0.35 57.65 ± 0.45 81.42 ± 0.38 63.37 ± 0.48 79.96 ± 0.32 62.04 ± 0.45 85.77 ± 0.33 68.45 ± 0.46 ft+cl 77.63 ± 0.34 59.84 ± 0.46 81.75 ± 0.38 64.47 ± 0.48 80.74 ± 0.32 63.76 ± 0.45 86.13 ± 0.33 69.36 ± 0.47 k-NN 76.63 ± 0.36 55.92 ± 0.46 80.85 ± 0.39 61.16 ± 0.48 79.60 ± 0.32 60.34 ± 0.45 84.67 ± 0.34 67.25 ± 0.52 ft 77.98 ± 0.34 60.71 ± 0.44 81.95 ± 0.36 65.66 ± 0.48 81.17 ± 0.31 64.87 ± 0.44 85.76 ± 0.34 71.00 ± 0.47 cl 78.36 ± 0.35 61.32 ± 0.45 81.93 ± 0.37 65.71 ± 0.48 80.61 ± 0.31 64.43 ± 0.45 85.90 ± 0.33 70.08 ± 0.48 Fine-Tuning ft+cl 78.42 ± 0.34 62.31 ± 0.44 81.98 ± 0.38 65.71 ± 0.47 81.08 ± 0.32 64.98 ± 0.43 86.06 ± 0.32 70.94 ± 0.49 MAML [20] 70.85 ± 0.38 56.59 ± 0.48 74.02 ± 0.41 59.17 ± 0.52 73.92 ± 0.36 58.02 ± 0.47 77.20 ± 0.38 61.40 ± 0.54 ft 73.84 ± 0.37 57.63 ± 0.47 80.19 ± 0.40 60.03 ± 0.51 78.82 ± 0.36 58.55 ± 0.48 84.74 ± 0.37 66.74 ± 0.52 cl 73.01 ± 0.36 56.69 ± 0.48 78.41 ± 0.40 61.16 ± 0.53 76.22 ± 0.35 58.32 ± 0.46 81.74 ± 0.38 63.61 ± 0.51 ft+cl 76.37 ± 0.37 59.36 ± 0.48 81.04 ± 0.39 63.88 ± 0.50 79.25 ± 0.34 62.84 ± 0.46 85.10 ± 0.39 67.70 ± 0.53 LEO [54] 74.49 ± 0.36 58.48 ± 0.48 80.25 ± 0.38 65.25 ± 0.51 75.86 ± 0.35 59.77 ± 0.47 82.15 ± 0.37 68.90 ± 0.49 ft 76.77 ± 0.35 60.52 ± 0.47 80.97 ± 0.36 65.44 ± 0.49 77.81 ± 0.34 61.81 ± 0.46 84.95 ± 0.36 69.59 ± 0.47 cl 74.66 ± 0.36 58.62 ± 0.46 80.74 ± 0.37 65.37 ± 0.50 76.13 ± 0.35 60.22 ± 0.47 82.31 ± 0.37 69.23 ± 0.48 ft+cl 71.91 ± 0.35 61.09 ± 0.47 81.43 ± 0.36 66.03 ± 0.48 77.72 ± 0.34 62.19 ± 0.45 85.04 ± 0.36 70.28 ± 0.47 MTL [58] 75.65 ± 0.35 58.49 ± 0.46 81.14 ± 0.36 64.29 ± 0.50 77.30 ± 0.34 62.99 ± 0.46 83.23 ± 0.37 70.08 ± 0.52 ft 77.17 ± 0.35 58.85 ± 0.44 82.01 ± 0.36 64.67 ± 0.47 79.40 ± 0.34 63.65 ± 0.45 84.76 ± 0.36 70.25 ± 0.49 cl 77.10 ± 0.34 58.86 ± 0.45 82.34 ± 0.36 66.70 ± 0.51 79.29 ± 0.35 63.14 ± 0.46 86.21 ± 0.37 70.16 ± 0.50 ft+cl 78.03 ± 0.33 61.17 ± 0.45 82.35 ± 0.35 65.72 ± 0.48 80.20 ± 0.33 64.40 ± 0.45 86.02 ± 0.35 71.45 ± 0.48 MN [66] 75.21 ± 0.35 61.05 ± 0.46 79.92 ± 0.37 66.01 ± 0.50 77.15 ± 0.36 63.45 ± 0.45 82.43 ± 0.37 70.38 ± 0.49 ft 75.52 ± 0.35 61.23 ± 0.45 80.18 ± 0.36 66.33 ± 0.49 77.80 ± 0.35 64.42 ± 0.46 83.82 ± 0.36 70.90 ± 0.50 cl 75.40 ± 0.34 61.14 ± 0.44 80.04 ± 0.35 66.26 ± 0.50 77.23 ± 0.35 64.21 ± 0.47 82.77 ± 0.35 70.61 ± 0.51 ft+cl 76.73 ± 0.34 62.64 ± 0.46 80.79 ± 0.35 67.30 ± 0.48 78.55 ± 0.36 64.89 ± 0.44 84.03 ± 0.36 71.41 ± 0.49 SIB [29] (transductive) 78.88 ± 0.35 67.10 ± 0.56 85.09 ± 0.35 77.64 ± 0.58 81.73 ± 0.34 71.31 ± 0.56 88.19 ± 0.34 81.97 ± 0.56 ft 79.58 ± 0.35 67.94 ± 0.55 85.12 ± 0.35 77.68 ± 0.57 82.00 ± 0.34 71.95 ± 0.56 88.20 ± 0.34 82.01 ± 0.56 cl 79.04 ± 0.33 67.77 ± 0.55 85.22 ± 0.35 77.72 ± 0.56 81.93 ± 0.35 71.66 ± 0.56 88.21 ± 0.33 82.01 ± 0.54 ft+cl 80.32 ± 0.35 68.85 ± 0.56 85.43 ± 0.35 78.03 ± 0.57 83.21 ± 0.33 73.51 ± 0.56 88.69 ± 0.33 83.07 ± 0.52 SIB [29] (inductive) 75.64 ± 0.36 57.20 ± 0.57 81.69 ± 0.34 65.51 ± 0.56 78.17 ± 0.35 60.12 ± 0.56 84.96 ± 0.36 69.20 ± 0.58 ft 76.23 ± 0.35 58.67 ± 0.56 82.04 ± 0.35 66.69 ± 0.57 79.34 ± 0.35 61.77 ± 0.56 85.24 ± 0.36 70.05 ± 0.57 cl 76.61 ± 0.35 58.12 ± 0.55 82.21 ± 0.35 66.28 ± 0.56 79.11 ± 0.35 61.25 ± 0.55 85.63 ± 0.34 69.90 ± 0.57 Meta-Learning ft+cl 77.68 ± 0.34 60.33 ± 0.54 82.75 ± 0.35 67.34 ± 0.55 80.05 ± 0.34 63.14 ± 0.54 86.14 ± 0.34 71.45 ± 0.55 20A.6.2 Hardness-Speciﬁc Acc 20% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (a) Linear 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (b) Cosine 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (c) k-NN 20% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (d) MAML [20] 20% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (e) LEO [54] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (f) MTL [58] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (g) Matching Net [66] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (h) SIB(transductive) [29] 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (i) SIB(inductive) [29] Figure A2: Supplementary to Figure 5. Hardness-speciﬁc Acc of 5-shot ﬁne-tuning and meta-learning on miniImageNet. 2120% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (a) Linear 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (b) Cosine 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (c) k-NN 20% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (d) MAML [20] 20% 40% 60% 80% Accuracy Hardness leo mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL leo tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness linear mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL linear tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness maml mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL maml tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (e) LEO [54] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (f) MTL [58] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL (g) Matching Net [66] 20% 40% 60% 80% Accuracy Hardness mn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness mtl mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL mtl tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness sib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL sib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (h) SIB(transductive) [29] 20% 40% 60% 80% Accuracy Hardness Cosine mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL Cosine tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness indsib mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL indsib tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL 20% 40% 60% 80% Accuracy Hardness knn mini 20% 40% 60% 80% Accuracy Hardness ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL knn tiered ResNet-10 Baseline ResNet-10 IFSL WRN-28-10 Baseline WRN-28-10 IFSL  (i) SIB(inductive) [29] Figure A3: Supplementary to Figure 5. Hardness-speciﬁc Acc of 5-shot ﬁne-tuning and meta-learning on tieredImageNet. A.6.3 CAM-Acc Table A2: Supplementary to Figure 6. CAM-Acc (%) on ﬁne-tuning and meta-learning. We used combined adjustment for IFSL. ResNet-10 WRN-28-10 miniImageNet tieredImageNet miniImageNet tieredImageNetMethod 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot Linear 29.02 ± 0.38 25.22 ± 0.38 31.62 ± 0.38 31.05 ± 0.39 25.99 ± 0.35 24.74 ± 0.34 30.17 ± 0.36 29.76 ± 0.37 +IFSL 29.85 ± 0.37 26.67 ± 0.38 31.75 ± 0.38 31.43 ± 0.37 26.02 ± 0.37 24.96 ± 0.36 32.57 ± 0.36 30.64 ± 0.38 Cosine 28.10 ± 0.37 27.12 ± 0.38 29.82 ± 0.37 28.54 ± 0.38 27.54 ± 0.38 25.73 ± 0.37 32.60 ± 0.35 31.21 ± 0.36 +IFSL 28.18 ± 0.37 27.26 ± 0.38 31.38 ± 0.37 28.70 ± 0.40 27.82 ± 0.38 25.85 ± 0.38 33.65 ± 0.37 31.66 ± 0.35 k-NN 27.96 ± 0.37 26.65 ± 0.37 32.25 ± 0.39 30.36 ± 0.39 24.15 ± 0.34 23.30 ± 0.33 23.91 ± 0.34 21.99 ± 0.33 Fine-Tuning +IFSL 28.15 ± 0.37 26.81 ± 0.37 32.75 ± 0.39 30.84 ± 0.39 25.23 ± 0.36 24.14 ± 0.35 28.04 ± 0.37 26.46 ± 0.37 MAML [20] 29.43 ± 0.37 27.39 ± 0.38 32.72 ± 0.40 32.14 ± 0.40 27.56 ± 0.36 26.46 ± 0.36 34.39 ± 0.40 31.07 ± 0.39 +IFSL 30.06 ± 0.38 28.42 ± 0.38 32.93 ± 0.40 32.24 ± 0.39 27.61 ± 0.36 26.91 ± 0.38 34.57 ± 0.41 31.22 ± 0.40 LEO [54] 30.24 ± 0.38 28.56 ± 0.37 31.64 ± 0.38 29.88 ± 0.37 29.15 ± 0.38 27.86 ± 0.38 31.27 ± 0.37 29.73 ± 0.38 +IFSL 30.67 ± 0.37 28.76 ± 0.37 32.01 ± 0.38 30.65 ± 0.37 29.20 ± 0.37 28.45 ± 0.38 31.98 ± 0.39 30.32 ± 0.38 MTL [58] 31.45 ± 0.39 30.13 ± 0.39 33.52 ± 0.39 33.11 ± 0.39 30.56 ± 0.39 29.78 ± 0.40 33.13 ± 0.39 32.35 ± 0.39 +IFSL 34.21 ± 0.39 31.59 ± 0.40 33.67 ± 0.38 33.50 ± 0.39 31.78 ± 0.39 30.12 ± 0.39 33.30 ± 0.39 32.64 ± 0.39 MN [66] 28.50 ± 0.38 28.42 ± 0.39 32.55 ± 0.40 31.88 ± 0.39 24.93 ± 0.38 25.34 ± 0.39 34.87 ± 0.37 29.10 ± 0.38 +IFSL 28.68 ± 0.38 28.77 ± 0.38 32.67 ± 0.40 32.10 ± 0.40 27.93 ± 0.37 25.81 ± 0.37 35.47 ± 0.41 30.71 ± 0.39 SIB [29] (transductive) 32.10 ± 0.39 31.19 ± 0.39 32.16 ± 0.39 30.49 ± 0.39 28.32 ± 0.37 26.76 ± 0.38 31.02 ± 0.36 28.43 ± 0.38 +IFSL 32.14 ± 0.39 31.34 ± 0.39 34.31 ± 0.40 32.59 ± 0.40 31.54 ± 0.38 29.82 ± 0.36 32.33 ± 0.37 30.26 ± 0.38 SIB [29] (inductive) 31.26 ± 0.38 30.56 ± 0.39 31.35 ± 0.40 30.48 ± 0.39 29.76 ± 0.38 28.02 ± 0.37 29.45 ± 0.39 27.98 ± 0.39 Meta-Learning +IFSL 31.46 ± 0.39 30.78 ± 0.40 31.56 ± 0.39 30.89 ± 0.40 30.23 ± 0.37 28.75 ± 0.39 30.07 ± 0.40 28.57 ± 0.39 22A.6.4 Cross-Domain Evaluation Table A3: Supplementary to Table 3. Acc (%) and 95% conﬁdence interval averaged over 2000 5-way FSL tasks on cross-domain evaluation. Speciﬁcally, “ft” refers to feature-wise adjustment, “cl” refers to class-wise adjustment and “ft+cl” refers to combined adjustment. ResNet-10 WRN-28-10 Method 5-shot 1-shot 5-shot 1-shot Linear 58.84 ±0.41 42.25 ±0.42 62.12 ±0.40 42.89 ±0.41 ft 60.12 ±0.39 42.30 ±0.41 63.13 ±0.39 43.39 ±0.40 cl 60.51 ±0.40 42.43 ±0.42 62.95 ±0.39 44.21 ±0.40 ft+cl 60.65 ±0.39 45.14 ±0.40 64.15 ±0.38 45.64 ±0.39 Cosine 58.30 ±0.39 40.47 ±0.40 60.21 ±0.39 42.12 ±0.39 ft 58.32 ±0.39 41.01 ±0.40 61.16 ±0.38 42.35 ±0.41 cl 58.68 ±0.39 40.67 ±0.41 61.87 ±0.40 43.23 ±0.40 ft+cl 60.23 ±0.38 42.78 ±0.40 62.49 ±0.38 45.12 ±0.39 k-NN 57.18 ±0.40 38.44 ±0.37 59.31 ±0.41 40.53 ±0.42 ft 59.44 ±0.39 43.49 ±0.40 62.48 ±0.39 45.68 ±0.43 cl 58.37 ±0.39 43.20 ±0.41 62.04 ±0.39 45.36 ±0.40 Fine-Tuning ft+cl 59.59 ±0.40 43.45 ±0.40 62.45 ±0.40 45.72 ±0.40 MAML [20] 51.09 ±0.43 37.20 ±0.46 55.04 ±0.42 39.06 ±0.47 ft 54.95 ±0.44 37.34 ±0.47 59.57 ±0.44 39.25 ±0.46 cl 53.62 ±0.43 38.13 ±0.47 56.80 ±0.45 40.32 ±0.48 ft+cl 56.71 ±0.46 40.36 ±0.46 60.89 ±0.45 42.16 ±0.47 LEO [54] 56.52 ±0.46 39.21 ±0.53 56.66 ±0.48 41.45 ±0.54 ft 56.77 ±0.48 39.72 ±0.54 62.95 ±0.47 45.46 ±0.55 cl 56.73 ±0.47 40.12 ±0.55 56.90 ±0.47 41.93 ±0.56 ft+cl 61.27 ±0.46 42.79 ±0.52 63.30 ±0.47 43.81 ±0.56 MTL [58] 56.61 ±0.42 41.56 ±0.43 56.89 ±0.41 43.15 ±0.44 ft 61.34 ±0.41 42.90 ±0.43 63.49 ±0.40 45.28 ±0.44 cl 60.62 ±0.41 42.87 ±0.42 62.94 ±0.40 45.57 ±0.43 ft+cl 62.39 ±0.40 44.51 ±0.43 65.00 ±0.40 46.67 ±0.43 MN [66] 53.39 ±0.46 40.34 ±0.56 53.08 ±0.45 42.04 ±0.57 ft 54.22 ±0.46 40.62 ±0.57 54.97 ±0.47 42.52 ±0.58 cl 53.72 ±0.47 40.42 ±0.56 53.43 ±0.45 42.19 ±0.56 ft+cl 56.03 ±0.45 41.68 ±0.54 58.69 ±0.44 43.58 ±0.56 SIB [29] (transductive) 60.60 ±0.46 45.87 ±0.55 62.60 ±0.49 49.16 ±0.58 ft 61.12 ±0.45 46.64 ±0.55 63.15 ±0.47 49.78 ±0.56 cl 60.70 ±0.46 46.14 ±0.56 63.02 ±0.48 49.43 ±0.57 ft+cl 62.07 ±0.44 47.07 ±0.53 64.07 ±0.49 50.71 ±0.54 SIB [29] (inductive) 59.06 ±0.42 41.48 ±0.43 59.94 ±0.42 43.27 ±0.44 ft 59.45 ±0.41 41.98 ±0.44 60.33 ±0.44 43.61 ±0.45 cl 59.32 ±0.42 41.67 ±0.43 60.46 ±0.43 43.52 ±0.45 Meta-Learning ft+cl 59.89 ±0.41 43.20 ±0.43 61.45 ±0.43 44.27 ±0.44 23",
      "references": [
        "Instrumental variables and the search for identification: From supply and demand to natural experiments",
        "Data augmentation generative adversarial networks",
        "Implicit regularization in deep matrix factorization",
        "Factors of transferability for a generic convnet representation",
        "The dropout learning algorithm",
        "Bounds on treatment effects from studies with imperfect compliance",
        "Deep learning of representations for unsupervised and transfer learning",
        "Representation learning: A review and new perspectives",
        "A meta-transfer objective for learning to disentangle causal mechanisms",
        "Counterfactuals uncover the modular structure of deep generative models",
        "Robust principal component analysis?",
        "Visual causal feature learning",
        "A closer look at few-shot classification",
        "Image deformation meta-networks for one-shot learning",
        "Causal confusion in imitation learning",
        "A Modern Introduction to Probability and Statistics: Understanding Why and How",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "A baseline for few-shot image classification",
        "One-shot learning of object categories",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Generating classification weights with gnn denoising autoencoders for few-shot learning",
        "Detectron",
        "Deep Learning",
        "Mask r-cnn",
        "Deep residual learning for image recognition",
        "Distilling the knowledge in a neural network",
        "Cross attention network for few-shot classification",
        "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
        "Empirical bayes transductive meta-learning with synthetic gradients",
        "What makes imagenet good for transfer learning?",
        "Task agnostic meta-learning for few-shot learning",
        "Two causal principles for improving visual dialog",
        "Do better imagenet models transfer better?",
        "Boosting few-shot learning with adaptive margin loss",
        "Finding task-relevant features for few-shot learning by category traversal",
        "Lgm-net: Learning to generate matching networks for few-shot learning",
        "An ensemble of epoch-wise empirical bayes for few-shot learning",
        "Discovering causal signals in images",
        "Visualizing data using t-sne",
        "Domain adaptation by using causal inference to predict invariant conditional distributions",
        "Domain adaptation with conditional transferable components",
        "Reptile: a scalable metalearning algorithm",
        "Tadam: Task dependent adaptive metric for improved few-shot learning",
        "A survey on transfer learning",
        "Learning independent causal mechanisms",
        "Causal Inference in Statistics: A Primer",
        "Causal diagrams for empirical research",
        "Causality: Models, Reasoning and Inference",
        "Few-shot image recognition by predicting parameters from activations",
        "An empirical study of pretrained representations for few-shot classification",
        "Optimization as a model for few-shot learning",
        "Meta-learning for semi-supervised few-shot classification",
        "Nonlinear dimensionality reduction by locally linear embedding",
        "Meta-learning with latent embedding optimization",
        "Meta-learning with memory-augmented neural networks",
        "Grad-cam: Visual explanations from deep networks via gradient-based localization",
        "Prototypical networks for few-shot learning",
        "Meta-transfer learning for few-shot learning",
        "Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness",
        "Long-tailed classification by keeping the good and removing the bad momentum causal effect",
        "Unbiased scene graph generation from biased training",
        "Transfer learning from language models to image caption generators: Better models may not transfer better",
        "A global geometric framework for nonlinear dimensionality reduction",
        "Face recognition using eigenfaces",
        "Attention is all you need",
        "Matching networks for one shot learning",
        "Visual commonsense r-cnn",
        "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning",
        "Few-shot learning: A survey",
        "Caltech-UCSD Birds 200",
        "Show, attend and tell: Neural image caption generation with visual attention",
        "Deconfounded image captioning: A causal retrospect",
        "Few-shot learning via embedding adaptation with set-to-set functions",
        "How transferable are features in deep neural networks?",
        "Wide residual networks",
        "Visualizing and understanding convolutional networks",
        "Deepemd: Few-shot image classification with differentiable earth mover’s distance and structured classifiers",
        "Causal intervention for weakly-supervised semantic segmentation",
        "Few-shot learning via saliency-guided hallucination of samples",
        "Metagan: An adversarial approach to few-shot learning",
        "Impact of imagenet model selection on domain adaptation",
        "Learning deep features for discriminative localization"
      ],
      "meta_data": {
        "arxiv_id": "2009.13000v2",
        "authors": [
          "Zhongqi Yue",
          "Hanwang Zhang",
          "Qianru Sun",
          "Xian-Sheng Hua"
        ],
        "published_date": "2020-09-28T01:16:54Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Interventional Few-Shot Learning (IFSL), a new paradigm that treats knowledge obtained from large-scale pre-training as a confounder that can harm few-shot generalisation. Builds a causal Structural Causal Model of pre-training, features and labels, shows why conventional fine-tuning/meta-learning estimate spurious correlations, and derives a back-door adjustment to approximate the causal effect P(Y|do(X)). Provides three practical interventions—feature-wise, class-wise, and combined adjustments—that are model-agnostic and can be plugged into existing fine-tuning or meta-learning pipelines, achieving state-of-the-art 1-/5-shot accuracy on miniImageNet, tieredImageNet and cross-domain CUB.",
        "methodology": "1. Formulate a SCM with nodes: pre-trained knowledge D, features X, latent code C, labels Y, and sample identity I. Show D is a confounder of X→Y in few-shot setting.\n2. Use causal calculus: back-door adjustment to estimate P(Y|do(X)) by stratifying over D.\n3. Propose three approximations for the stratification:\n   • Feature-wise adjustment: partition feature channels into groups, average predictions over subsets active in the input.\n   • Class-wise adjustment: weight the mean feature of each pre-training class by the classifier probability P(ai|x) and concatenate.\n   • Combined adjustment: apply feature-wise on top of class-wise.\n4. Plug adjusted features into any classifier; for meta-learning replace P(Y|X) with P(Y|do(X)).",
        "experimental_setup": "Backbones: ResNet-10 and WideResNet-28-10 pre-trained on training split of each dataset.\nDatasets: miniImageNet (64/16/20 split), tieredImageNet (20/6/8 super-classes), cross-domain evaluation on CUB.\nTasks: 5-way 1-shot and 5-shot classification; both fine-tuning (linear, cosine, k-NN) and meta-learning (MAML, LEO, MTL, MatchingNet, SIB transductive & inductive).\nMetrics: conventional accuracy over 2,000 tasks, hardness-specific accuracy measuring support/query dissimilarity, CAM-Acc using Grad-CAM to verify correct attention.\nImplementation: early stopping during pre-training, uniform hyper-parameters; IFSL uses n=8 feature groups, uniform priors; evaluation reports mean and 95% CI.",
        "limitations": "1. Back-door implementations rely on heuristics (feature grouping size, uniform priors) and linear combination assumptions; causal validity may be approximate.\n2. Requires access to internal feature representations and possibly pre-training class logits, limiting applicability to black-box models.\n3. Computational overhead from multiple forward passes for each stratum, though mitigated by NWGM.\n4. Experiments focus on vision datasets with classification tasks; generalisation to other modalities or structured outputs untested.\n5. Theoretical analysis assumes correct causal graph; unmeasured factors or mis-specification could diminish benefits.",
        "future_research_directions": "1. Develop more principled or adaptive stratification schemes, e.g., learnable partitions or continuous adjustments.\n2. Extend IFSL to other domains such as NLP, reinforcement learning, or multimodal tasks.\n3. Investigate counterfactual reasoning and other causal inference tools (e.g., front-door, instrumental variables) for few-shot settings.\n4. Explore integration with data-augmentation and generative models to combine physical and causal interventions.\n5. Analyse and reduce computational cost, enabling real-time or resource-constrained deployment.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Meta-Tuning Loss Functions and Data Augmentation for Few-Shot Object Detection",
      "full_text": "Meta-tuning Loss Functions and Data Augmentation for Few-shot Object Detection Berkan Demirel1,2 Orhun Bu˘gra Baran1 Ramazan Gokberk Cinbis1 1Middle East Technical University 2HA VELSAN Inc. bdemirel@havelsan.com.tr bugra@ceng.metu.edu.tr gcinbis@ceng.metu.edu.tr Abstract Few-shot object detection, the problem of modelling novel object detection categories with few training instances, is an emerging topic in the area of few-shot learning and ob- ject detection. Contemporary techniques can be divided into two groups: ﬁne-tuning based and meta-learning based approaches. While meta-learning approaches aim to learn dedicated meta-models for mapping samples to novel class models, ﬁne-tuning approaches tackle few-shot detection in a simpler manner, by adapting the detection model to novel classes through gradient based optimization. Despite their simplicity, ﬁne-tuning based approaches typically yield competitive detection results. Based on this observation, we focus on the role of loss functions and augmentations as the force driving the ﬁne-tuning process, and propose to tune their dynamics through meta-learning principles. The pro- posed training scheme, therefore, allows learning inductive biases that can boost few-shot detection, while keeping the advantages of ﬁne-tuning based approaches. In addition, the proposed approach yields interpretable loss functions, as opposed to highly parametric and complex few-shot meta- models. The experimental results highlight the merits of the proposed scheme, with signiﬁcant improvements over the strong ﬁne-tuning based few-shot detection baselines on benchmark Pascal VOC and MS-COCO datasets, in terms of both standard and generalized few-shot performance metrics. 1. Introduction Object detection is one of the computer vision problems that has greatly beneﬁted from the advances in supervised deep learning approaches. However, similar to the case in many other problems, state-of-the-art in object detection re- lies on the availability of large-scale fully-annotated datasets, which is particularly problematic due to the difﬁculty of collecting accurate bounding box annotations [19, 47]. This practical burden has lead to a great interest in the approaches Augmentations FSOD model Loss function Loss Query set mAP REINFORCE over the augmentation & loss functions Gradient descentFew-shot support  set Recall Precision Recall Proxy task Figure 1. The overall architecture of the meta-tuning approach. that can potentially reduce the annotation cost, such as weakly-supervised learning [30, 58], learning from point annotations [7], and mixed supervised learning [46]. A more recently emerging paradigm in this direction is few-shot ob- ject detection (FSOD). In the FSOD problem, the goal is to build detection models for the novel classes with few labeled training images by transferring knowledge from the base classes with a large set of training images. In the closely related Generalized-FSOD (G-FSOD) problem, the goal is to build few-shot detection models that perform well on both base and novel classes. FSOD methods can be categorized into meta-learning and ﬁne-tuning approaches. Although meta-learning based methods are predominantly used in the literature in FSOD research [8,23,32,37,53,76,77,80,82,84], several ﬁne-tuning based works have recently reported competitive results [6, 16, 33, 54, 62, 66, 73, 85]. The main premise of meta-learning approaches is to design and train dedicated meta-models that map given few train samples to novel class detection models, e.g. [74] or learn easy-to-adapt models [31] in a MAML [17] fashion. In contrast, however, ﬁne-tuning based methods tackle the problem as a typical transfer learning problem and apply the general purpose supervised training techniques, i.e. regularized loss minimization via gradient- based optimization, to adapt a pre-trained model to few-shot classes. It is also worth noting that the recent results on ﬁne- tuning based FSOD are aligned with related observations on few-shot classiﬁcation [9, 12, 64] and segmentation [4]. While some of the FSOD meta-learning approaches are at- tractive for being able to learn dedicated parametric training 1 arXiv:2304.12161v1  [cs.CV]  24 Apr 2023mechanisms, they also come with two important shortcom- ings: (i) the risk of overﬁtting to the base classes used for training the meta-model due to model complexity, and (ii) the difﬁculty of interpreting what is actually learned; both of which can be crucially important for real-world, in-the-wild utilization of a meta-learned model. From this point of view, the simplicity and generality of a ﬁne-tuning based FSOD ap- proach can be seen as major advantages. In fact, one can ﬁnd a large machine learning literature on the components (opti- mization techniques, loss functions, data augmentation, and architectures) of an FT approach, as opposed to the unique and typically unknown nature of a meta-learned inference model, especially when the model aims to replace standard training procedures for modeling the novel few-shot classes. While MAML [17] like meta-learning for quick adaptation is closer in nature to ﬁne-tuning based approaches, the van- ishing gradient problems and the overall complexity of the meta-learning task practically limits the approach to target only one or few model update steps, whereas an FT approach has no such computational difﬁculty. Perhaps the biggest advantage of a ﬁne-tuning based FSOD approach, however, can also be its biggest disad- vantage: its generality may lack the inductive biases needed for effective learning with few novel class samples while preserving the knowledge of base classes. To this end, such approaches focus on the design of ﬁne-tuning details, e.g. whether to freeze the representation parameters [66], use contrastive ﬁne-tuning losses [62], increase the novel class variances [85], introduce the using additional detection heads and branches [16, 73]. However, optimizing such details speciﬁcally for few-shot classes in a hand-crafted manner is clearly difﬁcult, and likely to be sub-optimal. To address this problem, we focus on applying meta- learning principles to tune the loss functions and augmen- tations to be used in the ﬁne-tuning stage for FSOD, which we call meta-tuning (Figure 1). More speciﬁcally, much like the meta-learning of a meta-model, we deﬁne an episodic training procedure that aims to progressively discover the optimal loss function and augmentation details for FSOD purposes in a data-driven manner. Using reinforcement learn- ing (RL) techniques, we aim to tune the loss function and augmentation details such that they maximize the expected detection quality of an FSOD model obtained by ﬁne-tuning to a set of novel classes. By deﬁning meta-tuning over well- designed loss terms and an augmentation list, we restrict the search process to effective function families, reducing the computational costs compared to AutoML methods that aim to discover loss terms from scratch for fully-supervised learn- ing [21, 43]. The resulting meta-tuned loss functions and augmentations, therefore, inject the learned FSOD-speciﬁc inductive biases into a ﬁne-tuning based approach. To explore the potential of the meta-tuning scheme for FSOD, we focus on the details of classiﬁcation loss func- tions, based on the observations that FSOD prediction mis- takes tend to be in classiﬁcation rather than localization details [62]. In particular, we ﬁrst focus on the softmax temperature parameter, for which we deﬁne two versions: (i) a simple constant temperature, and (ii) time (ﬁne-tuning iteration index) varying dynamic temperature, parameterized as an exponentiated polynomial. In all cases, the parameters learned via meta-tuning yield an interpretable loss function that has a negligible risk of over-ﬁtting to the base classes, in contrast to a complex meta-model. We also model augmen- tation magnitudes during meta-tuning for improving the data loading pipeline for few-shot learning purposes. Addition- ally, we incorporate a score scaling coefﬁcient for learning to balance base versus novel class scores. We provide an experimental analysis on the Pascal VOC [14] and MS-COCO [41] benchmarks for FSOD, using the state-of-the-art ﬁne-tuning based baselines MPSR [73] and DeFRCN [54]. Our experimental results show that the proposed meta-tuning approach provides signiﬁcant perfor- mance gains in both FSOD and Generalized FSOD settings, suggesting that meta-tuning loss functions and data augmen- tation can be a promising direction in FSOD research. 2. Related Work This section provides an overview of recent developments on few-shot image classiﬁcation, few-shot object detection, automated loss function and data augmentation discovery. Few-shot classiﬁcation. Most of the meta-learning ap- proaches for few-shot learning (FSL) of classiﬁcation mod- els can be grouped as adaptation-based and mapping-based approaches. Adaptation-based (also called gradient-based) approaches aim to learn model parameters that can easily be adapted to new unseen few-shot tasks within a few model up- date steps, e.g. [18, 39, 48, 49, 52, 55, 59]. Mapping-based ap- proaches (also called metric-based) aim to bypass a gradient- descent based adaptation step, and instead learn a data-to- classiﬁer mapping, e.g. [5, 45, 50, 60, 61, 63, 65, 78, 79, 81]. Some of the other notable approaches include learning to generate synthetic data for novel classes [24, 34, 69], us- ing better feature representations [1, 2, 20, 29, 42, 64, 68] or utilizing differentiable convex solvers [3, 35]. Importantly, several works highlight that a carefully trained representa- tion combined with simple ﬁne-tuning or even just shallow classiﬁers can yield competitive or better performance than meta-learning based approaches, e.g. [9, 12, 64]. Few-shot object detection. The FSOD approaches can be summarized as meta-learning and ﬁne-tuning (also called transfer-learning) based ones. Most meta-learning based FSOD approaches embrace formulations similar to those used in mapping-based meta-learning approaches for FSL, e.g. [8, 23, 32, 37, 53, 76, 77, 80, 82, 84]. Support feature aggregation is one of the main aspects where meta-learning- 2based methods differ from each other. Xiao and Marlet [76] use both the differences and the channel-wise multiplication of the features in addition to the combination of the features directly for support-query aggregation. Fan et al. [15] use attention blocks to make support and query features more distinguishable for base and novel object classes. Zhang et al. [82] use inter-class correlations to highlight important support features. Li et al. [37] propose to use specialized support and query features for classiﬁcation and localization. Recent efforts towards improving meta-learning based FSOD include complimentary techniques, mainly to im- prove loss functions, feature matching, and novel class sam- ple usage efﬁciency. [37] uses class margin loss, [27] uses margin-based ranking loss, [83] uses hybrid loss which con- sist of focal loss, adaptive margin loss and contrastive loss. Hu et al. [28] perform feature matching between query and support images to use the information from the support im- ages more effectively. Similarly, Han et al. [22] construct a matching network between query and support instances using heterogeneous graph convolutional networks. Li and Li [36] augment novel class samples via adding Gaussian noise. Yin et al. [80] decouple classiﬁcation task from local- ization by using the proposed class-conditional architecture. Fine-tuning-based methods typically freeze parts of a pre-trained detection network, add auxiliary detection heads, increase the novel class variances and then apply gradient descent based model update steps, unlike meta- learning-based methods that use complex episodic learn- ing [16, 33, 54, 62, 66, 72, 73]. Wanget al. [66] propose a Faster-RCNN [57] based approach, where the class-agnostic region proposal network (RPN) component is kept frozen during ﬁne-tuning. Sun et al. [62] use a similar approach and differently include FPN and RPN layers to the learnable pa- rameter set in the same architecture. These learnable layers allow using contrastive proposal encodings that facilitate the more accurate classiﬁcation of novel objects. Wu et al. [73] show that the scale distribution of support set tends to be imbalanced, and proposes a multi-scale positive sample re- ﬁnement (MPSR) branch as an addition to the main model. Fan et al. [16] propose Retentive R-CNN architecture to prevent forgetting during ﬁne-tuning for base classes. The obtained object proposals are fed into two ROI detectors responsible for base class and novel class instances. Qiao et al. [54] focus on decoupling network modules, and intro- duce a gradient decoupling layer and prototypical calibration block. Kaul et al. [33] extend the novel class annotations in the training set. In this context, the proposed method obtains object candidates from the base detector, and applies the box reﬁnement step. While our approach is based on ﬁne-tuning based FSOD, we embrace meta-learn principles to optimize the loss func- tion and augmentations to improve the ﬁne-tuning process for FSOD, without learning a complex and over-ﬁtting-prone meta-model. The resulting loss function and data augmenta- tions are then utilized within the ﬁne-tuning steps. Automated loss function discovery. Loss function discov- ery is an emerging AutoML topic towards improving the learning systems in a data-driven manner. Existing methods are mainly based on either (i) constructing the loss func- tion directly from the basic operators [21, 43, 56] or (ii) optimizing parameterized loss functions [38, 67]. For loss construction, [43] proposes a genetic algorithm that consists of loss function veriﬁcation and quality ﬁltering modules. In this approach, the predeﬁned proxy task eliminates divergent and poor candidate loss functions and survives the promising loss functions for other steps. [21] uses a genetic algorithm to select candidate loss functions from a tree of simple math- ematical operations, and the successful loss functions pass to other stages to mutate. [56] suggests a method to learn not only the loss function but also the whole machine learning al- gorithm from scratch. For loss optimization, [38] re-analyzes the existing loss functions and presents them in a combined formula. [67] observes that the search space used in [38] can be too complex, and propose to simplify the search space via heuristics. In contrast to these works targeting supervised training scenarios, we aim to adapt loss function learning principles to the FSOD problem. AutoML for data augmentation. A variety of auto- mated data augmentation techniques have recently been proposed [10,11,26,40]. Cubuk et al. [10] generate augmen- tation policies using reinforcement learning and a controller RNN. Ho et al. [26] propose a method that reduces the computational costs compared to [10] by using a population- based framework. Similarly, Lim et al. [40] propose a direct Bayesian method to reduce costs. Cubuk et al. [11] show that the optimal augmentation magnitudes tend to be similar across transformations, and the search process can greatly be simpliﬁed by using a shared value. We follow this suggestion and use a shared magnitude across the transforms in our for- mulation. In contrast to these works on supervised learning, however, we focus on learning detectors with few-samples. In summary, while loss function and augmentation dis- covery topics increasingly attract attention towards improv- ing supervised training pipelines, ours is the ﬁrst work on learning few-sample speciﬁc inductive biases for ﬁne-tuning based few-shot object detection based on meta-learning and AutoML principles, to the best of our knowledge. 3. Method This section provides a brief summary of the FSOD prob- lem deﬁnition and the baseline model we utilize. We then present our deﬁnition and instantiation of meta-tuning. Problem deﬁnition. We follow the FSOD setup of [32], where a relatively large set of training images for the set Cb of base classes is made available. Each training im- 3age corresponds to a tuple (x,y) consisting of image x and annotations y = {y0,...,y M}. Each object annotation yi = {ci,bi}contains a category label (ci) and a bounding box (bi = {xi,yi,wi,hi}). Once the FSOD model training is complete, the evaluation is carried out based on a limited number (k) of training images made available for the set Cn of distinct novel (i.e. few-shot) classes. Base model. We use the MPSR FSOD method [73] as the infrastructure for our loss function and data augmentation search methods. MPSR adapts the Faster-RCNN to be suit- able for ﬁne-tuning-based FSOD and uses an auxiliary multi- scale positive sample reﬁnement (MPSR) branch to handle the scale scarcity problems. This branch expands the scale space of positive samples without increasing improper nega- tive instances, unlike feature pyramid networks and image pyramids that do not change data distribution, hence the scale sparsity problem. In this context, objects in the im- ages are cropped and resized in multiple sizes to create scale pyramids. The MPSR uses two groups of loss functions for the region proposal network (RPN) and detection heads, and feeds differently scaled positive samples to these loss func- tions together with the main detection branch. Finally, we note that the proposed approach can in principle be applied to virtually any ﬁne-tuning based FSOD model. 3.1. Meta-tuning loss functions Our main goal is to improve few-shot detector ﬁne-tuning based on meta-learning principles. For meta-tuning the FSOD loss, we speciﬁcally focus on the classiﬁcation loss term, as the FSOD errors tend to be primarily caused by misclassiﬁcations [62]. The MPSR classiﬁcation loss term can be expressed as follows: ℓcls(x,y) =− 1 NROI NROI∑ i log ( ef(xi,yi) ∑ yef(xi,y) ) (1) where NROI is the number of ROIs (i.e. candidate regions) in an image, yi is the groundtruth class label for thei-th ROI, and f(xi,y) is the corresponding class y prediction score. To add more ﬂexibility into the loss function, we re-deﬁne it as a parametric function ℓcls(x,y; ρ), where ρrepresents the loss function parameters. First, we introduce a temperature scalar ρτ, i.e. ρ= (ρτ): ℓcls(x,y; ρ) =− 1 NROI NROI∑ i log ( ef(xi,yi)/ρτ ∑ y′ ef(xi,y′)/ρτ ) (2) Our motivation comes from the observations on the impor- tance of temperature scaling in log loss on various other problems, such as knowledge distillation [25], few-shot clas- siﬁcation [50, 79], and zero-shot learning [44]. While tem- perature is typically tuned in a manual manner, here we aim to meta-learn it speciﬁcally for ﬁne-tuning based FSOD purposes, giving a chance to observe the behavior of meta- tuning in a simple case. We also deﬁne a more sophisticated variant of the loss function by deﬁning the dynamic tempera- ture function fρ and novel class scaling α: ℓcls(x,y; ρ) = −1 NROI NROI∑ i log ( eα(yi)f(xi,yi)/fρ(t) ∑ y′ eα(y′)f(xi,y′)/fρ(t) ) (3) where fρ(t) = exp(ρat2 + ρbt+ ρc). Here, ρ= (ρa,ρb,ρc) is a 3-tuple of polynomial coefﬁcients, and t∈[0,1] is the normalized ﬁne-tuning iteration index. The temperature can increase or decrease over time, making the predicted class distributions smoother or sharper. α(y) is set to 1 for y∈Cb, and otherwise the novel class score scaling coefﬁcient ρα, as a way to learn base and novel score balancing. 3.2. Meta-tuning augmentations For meta-tuning augmentations, we focus on the photo- metric augmentations that are likely to be transferable from base to novel classes. In this context, we model the bright- ness, saturation, contrast, and hue transforms, with a shared magnitude parameter (ρaug), which is known to be effective for supervised training [11]. 3.3. Meta-tuning procedure In our work, we utilize a REINFORCE [71] based rein- forcement learning (RL) approach to search for the optimal loss function and augmentations, where we use the AutoML approach of Wang et al. [67] on loss function search for fully-supervised face recognition as our starting point. In order to meta-tune the loss function and augmentations to maximize FSOD generalization abilities, we generate proxy tasks over base class training data to imitate real FSOD tasks over the novel classes. For this purpose, we divide base classes into two subsets, proxy-base Cp-base and proxy-novel Cp-novel. We then construct three non-overlapping data set splits using the base class training set: (i) Dp-pretrain contain- ing Cp-base-only samples, used for training a temporary object detection model for meta-tuning purposes; (ii) Dp-support con- taining samples of Cp-base ∪Cp-novel classes to be used as ﬁne- tuning images during meta-tuning; (iii) Dp-query containing samples of Cp-base ∪Cp-novel classes to be used for evaluating the generalized FSOD performance of a ﬁne-tuned model during meta-tuning. We generate a series of FSOD proxy tasks for meta- tuning, similar to episodic meta-learning: at each proxy task T, we sample a few-shot training set from Dp-support. We also sample a loss function/augmentation magnitude pa- rameter combination ρ, where each ρj ∈ρis modeled in terms of a Gaussian distribution: ρj ∼N (µj,σ2). Using the loss function or augmentations corresponding to the sam- pled ρ, we ﬁne-tune the initial model on the support images using gradient-based optimization, and compute the mean 4Input Images Backbone + FPN ROI Pooling RPN Objectness Localization Feature  Extractor Classiﬁcation Localization Reﬁnement Branch Shared  Weights ROI Cls Loss Obj. Loss AP Regress. Loss N(ρ; μ, σ2) Sampling Update μ R Get reward Object Detection Figure 2. The meta-tuning approach. At each RL iteration over a proxy task, the distribution parameters modeling the loss function and augmentations are updated as a function of the obtained mAP scores, towards improved training with few-samples. average precision (mAP) scores on Dp-query. We get mul- tiple mAP scores by repeating this process multiple times over multiple proxy support samples. Meta-tuning is then carried over by updating µvalues via the REINFORCE rule after each episode, towards ﬁnding µvalues centered around well-performing ρcombinations. µ′ j ←µj + ηR(ρ)∇µlog (p(ρj; µj,σ)) (4) where p(ρ; µ,σ) is the Gaussian probability density function, ηis the RL learning rate. We apply the REINFORCE update rule using the ρwith the highest reward per episode. R(ρ) is the normalized re- ward function obtained by whitening the mAP scores. We empirically observe that normalization improves the results (Section 4) since without reward normalization, the RL up- dates are scaled with respect to the inherent difﬁculty of the proxy task, which greatly varies depending on the sampled support examples. Reward normalization approximately re- moves the average reward, enabling better performing ρ samples to inﬂuence based on their relative success. Finally, similar to [51], starting with σ= 0.1, we dimin- ish σover the RL iterations to progressively reduce explo- rations by sampling more conservatively, which improves converge. The ﬁnal scheme is illustrated in Figure 2. 4. Experiments Metrics. We use mAP to evaluate the base and novel class detection results separately. To evaluate the generalized FSOD performance, we use the Harmonic Mean (HM) met- ric to compute a balanced aggregation of base and novel class performance scores. Adapted from generalized zero- shot learning [75], HM is deﬁned as the harmonic mean of mAPbase and mAPnovel scores. Datasets. We use Pascal VOC [14] and MS COCO [41] with the same splits deﬁned in FSOD benchmarks [66, 73]. On Pascal VOC, three separate base/novel class splits exist, where each one consists of 15 base and 5 novel classes. In each split, we select 5 base classes to mimic novel classes during meta-tuning. On MS-COCO, we select 15 base classes to mimic novel classes in each proxy task, and evalu- ate the models for the 10-shot and 30-shot settings. Baselines. We primarily use the MPSR [73] and De- FRCN [54] as our baselines, which are among the best per- forming ﬁne-tuning based FSOD methods on Pascal VOC. For the DeFRCN experiments, we transfer the meta-tuned loss functions and augmentation magnitudes from MPSR to the DeFRCN method, which are both based on Faster-RCNN. We take the results for FRCN [77], Ret. R-CNN [16], Meta- RCNN [77], FSRW [32], MetaDet [70], FsDetView [76] and ONCE [53] from [16] for a fair comparison. For the MPSR, DeFRCN (seed is set to 0) and FSCE [62], we report the results we obtain experimentally. We take the results for TFA+Hal [85], CME [37], TIP [36], DCNet [28], QA- FewDet [22] FADI [6], LVC [33], KFSOD [84] and FCT [23] from the original papers. Finally, while it is difﬁcult to fairly compare ﬁne-tuning versus meta-learning based approaches, we provide a discussion in the supplementary material. Implementation details. We use 200 RL episodes for loss function meta-tuning, with REINFORCE learning rate set to 0.0005. The meta-tuning for augmentation parameter is carried out using the trained and frozen the loss function parameters. We keep the ﬁne-tuning implementation details of MPSR unchanged, which uses 4000 and 8000 gradient descent iterations for 10-shot and 30-shot experiments on MS-COCO, and 2000 iterations on Pascal VOC. We will publish the full source code upon publication; a preliminary version is provided as supplementary material. 4.1. Main results We ﬁrst compare the meta-tuning results against the cor- responding MPSR baseline in Table 1. In the table, Meta- 5Method/Shot Pascal VOC MS-COCO Novel Classes All Classes (HM) Novel Classes All Classes (HM) 1 2 3 5 10 1 2 3 5 10 10 30 10 30 MPSR [73] 33.1 37.2 44.3 47.1 52.1 43.1 47.4 54.5 57.2 60.8 9.1 13.7 11.5 15.0 MPSR+Meta-Static 33.4 39.4 45.1 47.3 52.6 43.7 50.4 55.4 57.5 61.4 10.1 14.8 12.7 16.4 MPSR+Meta-Dynamic 34.5 39.8 45.0 48.2 52.5 45.0 51.0 55.5 58.3 61.6 11.9 14.9 14.3 16.6 MPSR+Meta-ScaledDynamic 35.2 40.3 45.8 48.4 52.9 45.6 51.2 55.9 58.3 61.8 12.3 15.0 14.4 16.7 MPSR+Aug 34.6 38.6 46.0 48.3 52.7 45.1 49.5 56.2 58.4 62.0 9.9 14.9 12.5 16.3 MPSR+Meta-Static+Aug 35.3 39.1 46.1 48.4 52.7 45.9 49.9 56.2 58.3 61.8 10.2 15.2 12.8 16.7 MPSR+Meta-Dynamic+Aug 35.4 39.6 46.5 48.9 53.3 46.0 50.5 56.8 58.9 62.5 12.1 15.3 14.5 16.8 MPSR+Meta-ScaledDynamic+Aug 35.8 40.6 46.8 49.2 53.7 46.3 51.5 57.0 59.2 62.7 12.5 15.4 14.7 16.9 Table 1. FSOD (mAP) and G-FSOD (HM of the base and novel class mAPs) results on Pascal VOC and MS-COCO datasets for MPSR baseline method. HM stands for harmonic mean. Static, Meta-Dynamic, Meta-ScaledDynamic refer to meta- tuning a single temperature, dynamic temperature, and novel class scaled dynamic temperature functions, respectively. Similarly, Aug, Meta-Static+Aug, Meta-Dynamic+Aug, and Meta-ScaledDynamic+Aug refer to meta-tuning only aug- mentation, single temperature and augmentation, dynamic temperature and augmentation, and novel class scaled dy- namic temperature and augmentation functions, respec- tively. We observe that meta-tuning consistently improves the FSOD and G-FSOD results of the MPSR model. We also observe steady improvements gradually from the base- line to Meta-Static, to Meta-Dynamic, and ﬁnally to Meta- ScaledDynamic. In addition, the meta-tuned augmentation magnitude parameter also contributes positively to the few- shot object detection performance. The overall consistency of the improvements provides positive evidence for the value of loss and augmentation meta-tuning. Pascal VOC results. In Table 2, we report the Pascal VOC results for our MPSR and DeFRCN based Meta- ScaledDynamic+Aug approach and compare them against the state-of-the-art ﬁne-tuning based FSOD methods. While we present the scores averaged over the three splits in this table, additional per-split FSOD and G-FSOD results can be found in the supplementary material. The left side of Table 2 presents the FSOD results for the varying number of support images. We observe that DeFRCN combined with Meta-ScaledDynamic+Aug, i.e. meta-tuning of the score coefﬁcient, dynamic temperature and the augmentation pa- rameter, yields the best mAP scores in all k-shot settings among all methods. The right side of Table 2 presents the G-FSOD results on Pascal VOC. We observe that the best-performing Meta- ScaledDynamic+Aug method improves the HM scores fur- ther above the state-of-the-art in all k-shot settings. Overall, these results suggest that the proposed framework is an ef- fective way for meta-learning inductive biases to be used in ﬁne-tuning-based FSOD. Figure 3 presents visual detection examples without and with meta-tuned scaled dynamic temperature and augmenta- tions in the ﬁrst and second rows, respectively. We observe various improvements, such as reductions in false positives, improved recall, and more precise boxes, most likely due to the improved model ﬁtting in the low-data regime. MS-COCO results. In Table 3, we compare the MPSR and DeFRCN based Meta-ScaleDynamic+Aug results against other ﬁne-tuning based FSOD methods that report 10-shot and 30-shot results on the MS-COCO dataset. We observe that with meta-tuning, the FSOD scores of MPSR improve from 9.1 to 12.5 (10-shot mAP), and from 13.7 to 15.4 (30- shot mAP). We also observe that the scores of DeFRCN improve from 18.5 to 18.8 (10-shot mAP), and from 21.9 to 23.4 (30-shot mAP), obtaining the best and second best results against all other models. Similarly, in the case of G- FSOD, with meta-tuning, the 10-shot HM score of DeFRCN improves from 24.0 to 24.4, outperforming all other models. In addition, the 30-shot HM score of DeFRCN improves from 26.8 to 28.0, which is slightly below the 28.1 score of LVC-PL [33]. 4.2. Ablation studies Meta-tuning details. The proposed meta-tuning approach involves three important technical details: Proxy-novel im- itation, model re-initialization, and reward normalization. Proxy-novel imitation refers to reinforcement learning over the sampled proxy-novel tasks, instead of the whole train- ing set, to mimic the test-time FSOD challenges. Model re-initialization is the re-initialization of the base model for each task. Without re-initialization, not only the sampled loss/augmentation parameters and tasks but also the accumu- lated model updates undesirably affect the rewards. Reward normalization further reduces the effect of task difﬁculty variance by normalizing the rewards obtained within a single episode, allowing a more isolated assessment of the sampled loss functions and augmentations. We evaluate the contributions of these three important details in terms of G-FSOD HM scores using the 5-shot setting of Pascal VOC Split-1 with MPSR+Meta-Dynamic. The results averaged over 5 runs are given in Table 4. We observe that each component progressively improves the 6Method/Shot Novel Classes All Classes (HM) 1 2 3 5 10 1 2 3 5 10 FRCN [77] (ICCV’19) 16.1 20.6 28.8 33.4 36.5 25.9 31.7 40.0 44.3 46.7 TFA-fc [66] (ICML’20) 27.6 30.6 39.8 46.6 48.7 40.5 44.1 52.9 58.3 59.9 TFA-cos [66] (ICML’20) 31.4 32.6 40.5 46.8 48.3 44.6 46.0 53.5 58.4 59.6 FSCE [62] (CVPR’21) 29.2 36.3 42.5 47.1 52.2 41.8 48.8 54.2 57.7 61.0 Ret. R-CNN [16] (CVPR’21) 31.4 37.1 41.4 46.8 48.8 44.7 50.5 54.7 59.1 60.8 TFA+Hal [85] (CVPR’21) 32.9 35.5 40.4 46.3 48.1 - - - - - FADI [6] (NeurIPS’21) 42.2 46.5 47.9 52.4 56.9 - - - - - LVC [33] (CVPR’22) 30.9 35.4 43.6 51.1 54.1 - - - - - LVC-PL [33] (CVPR’22) 45.2 45.0 54.8 57.5 58.6 - - - - - MPSR [73] (ECCV’20) 33.1 37.2 44.3 47.1 52.1 43.1 47.4 54.5 57.2 60.8 DeFRCN [54] (ICCV’21) 46.5 52.6 55.9 60.0 60.8 57.6 62.5 64.7 67.6 67.8 MPSR+Meta-ScaledDynamic+Aug 35.8 40.6 46.8 49.2 53.7 46.3 51.5 57.0 59.2 62.7 DeFRCN+Meta-ScaledDynamic+Aug 49.2 54.0 57.2 61.3 61.8 59.8 63.7 65.9 68.6 68.7 Table 2. FSOD (mAP) and G-FSOD (HM of the base and novel class mAPs) results on Pascal VOC. The best and the second-best results are marked with red and blue. HM stands for harmonic mean. Split-1/5-shot Split-1/10-shot Split-2/5-shot Split-2/10-shot Split-3/5-shot Split-3/10-shot Figure 3. Qualitative results using MPSR without (ﬁrst row) and with (second row) meta-tuning, over multiple Pascal VOC splits. Base and novel class detections are shown with green and red boxes, respectively. (Best viewed in color.) Method/Shots Novel Classes All Classes (HM) 10-shot 30-shot 10-shot 30-shot FRCN [77] (ICCV’19) 9.2 12.5 12.8 15.6 FRCN-BCE [66] (ICML’20) 6.4 10.3 10.9 16.1 TFA-fc [66] (ICML’20) 10.0 13.4 15.4 19.4 TFA-cos [66] (ICML’20) 10.0 13.7 15.6 19.8 MPSR [73] (ECCV’20) 9.1 13.7 11.5 15.0 FSCE [62] (CVPR’21) 10.5 14.4 16.0 20.2 Ret. R-CNN [16] (CVPR’21) 10.5 13.8 16.6 20.4 FADI [6] (NeurIPS’21) 12.2 16.1 - - DeFRCN [54] (ICCV’21) 18.5 21.9 24.0 26.8 LVC [33] (CVPR’22) 12.1 17.8 17.8 22.8 LVC-PL [33] (CVPR’22) 17.8 24.5 22.8 28.1 MPSR+Meta-ScaledDynamic+Aug 12.5 15.4 14.7 16.9 DeFRCN+Meta-ScaledDynamic+Aug 18.8 23.4 24.4 28.0 Table 3. Comparison of Meta-ScaledDynamic results to the ﬁne- tuning based (G-)FSOD methods on the MS-COCO dataset. The best and the second-best results are marked with red and blue. HM scores, and the most signiﬁcant contribution is made by reward normalization, which improves from 62.1 to 63.3. We also observe that reward normalization considerably im- proves the overall experimental stability. To quantify this Proxy-novel imit. Model re-init. Reward norm. HM \u0017 \u0017 \u0017 61.5 \u0013 \u0017 \u0017 61.8 \u0013 \u0013 \u0017 62.1 \u0013 \u0013 \u0013 63.3 Table 4. Evaluation of meta-tuning details. Proxy-novel imitation is the imitation of novel classes using a subset of base classes. Model re-initialization is the re-initialization of the base model at each task. Reward normalization is within-episode normalization of the mAP scores during meta-tuning. observation, we estimate the 95% conﬁdence interval over the runs using CI = 1.96 s√n, where s, n, and 1.96 are the standard deviation, number of runs, and Z-value, respec- tively [66]. According to this estimator, the normalization step narrows the conﬁdence interval from ±0.75 to ±0.13, providing a clear improvement in reliability. Learned loss functions. In Figure 4, we plot the learned loss functions according to the µ values obtained at the end of the RL process. The upper plot shows the dynamic 70.0 0.2 0.4 0.6 0.8 1.0 time 0.75 0.80 0.85 0.90 0.95 1.00 1.05value split-1 split-2 split-3 0.0 0.2 0.4 0.6 0.8 1.0 time 0.7 0.8 0.9 1.0 1.1 1.2value split-1 (temp.) split-1 (scale) split-2 (temp.) split-2 (scale) split-3 (temp.) split-3 (scale) Figure 4. The dynamic temperature functions and score scaling co- efﬁcients learned by the meta-tuning process, using Meta-Dynamic (upper) and Meta-ScaledDynamic (lower) formulations. Results for each Pascal VOC split is shown with a separate curve. temperature functions learned over three different splits. We observe that temporally attenuated temperature values are preferred consistently, sharpening the predictions towards the end of the ﬁne-tuning process. The lower plot shows the learned dynamic temperature functions with novel class score scaling. The learned scaling coefﬁcients, i.e. µα of the learned ρα distribution, are shown as horizontal lines. We observe that similar dynamic temperature functions are learned, and µα values vary between 1.09 to 1.2, suggesting that the meta-tuning process learns to boost the novel class scores. The interpretability of these outcomes, we believe, highlights a signiﬁcant advantage of loss meta-tuning. In the context of interpretability, we observe that as the ﬁne- tuning process continues on the few-shot training set, the predictions are progressively made sharper, i.e. the loss becomes more sensitive to classiﬁcation errors and enforces towards making more conﬁdent correct predictions. This is in alignment with one of our original motivations for reducing the dominating classiﬁcation errors in G-FSOD, as the meta-tuning process automatically learns to enforce more accurate classiﬁcations, where the curve steepness and the numerical ranges are learned via RL. Learned augmentations. The learned photometric augmen- S/M TFA [66] TFA+Hal [85] TFA+Meta-ScaledDynamic+Aug 1 3.4 3.8 4.7 2 4.6 5.0 5.8 3 6.6 6.9 7.1 Table 5. Low-shot (1-shot, 2-shot and 3-shot) experiments on MS-COCO dataset with novel classes. tation magnitude values learned are 0.29, 0.24, 0.13, and 0.36 for Pascal VOC split-1, split-2, split-3, and MS-COCO datasets, respectively. We observe that the learned augmen- tation magnitudes positively contribute to the performance. According to the results in Table 1, the average Pascal VOC split-1/1-shot score increases from 33.1 to 34.6 with only augmentation steps. Very low-shot experiments. Finally, we evaluate the meta- tuning approach in low-shot many-class settings. [85] pro- poses TFA+Hal method that uses the TFA baseline and con- ducts 1-shot, 2-shot, and 3-shot FSOD on the MS-COCO dataset. As we already observe the positive effects of the loss terms and augmentation magnitudes obtained from the MPSR on the DeFRCN, we similarly apply the learned pa- rameters to the TFA baseline. The results are presented in Table 5. We observe that results are consistently improved using the meta-tuned functions on the TFA baseline. 5. Conclusion Fine-tuning based frameworks offer simple and reliable approaches to building detection models from few samples. However, a major limitation of the existing ﬁne-tuning-based FSOD models is their focus on the hand-crafting the de- sign of ﬁne-tuning details for few-shot training, which is inherently difﬁcult and likely to be sub-optimal. Towards addressing this limitation, we propose to meta-learn the ﬁne- tuning based learning dynamics as a way of introducing learned inductive biases for few-shot learning. The proposed tuning scheme uses meta-learning principles with reinforce- ment learning, and obtains interpretable loss functions and augmentation magnitudes for few-shot training. Our compre- hensive experimental results on Pascal VOC and MS COCO datasets show that the proposed meta-tuning approach consis- tently provides signiﬁcant performance improvements over the strong ﬁne-tuning based few-shot detection baselines in both FSOD and G-FSOD settings. While we restrict our experiments to loss and augmenta- tion functions, meta-tuning other learning components, e.g. initial model, and applications to other few-shot learning problems can be interesting future work directions. Acknowledgements. This work was supported in part by the TUBITAK Grant 119E597 and a Google Faculty Research Award. 8References [1] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, and Frank Wood. Enhancing few-shot image classiﬁcation with unlabelled examples. In Proceedings of the IEEE/CVF Win- ter Conference on Applications of Computer Vision (WACV), pages 2796–2805, January 2022. 2 [2] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved Few-Shot Visual Classiﬁcation. arXiv e-prints, page arXiv:1912.03432, Dec. 2019. 2 [3] Luca Bertinetto, Joao F. Henriques, Philip Torr, and An- drea Vedaldi. Meta-learning with differentiable closed-form solvers. In Proc. Int. Conf. Learn. Represent., 2019. 2 [4] Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo Piantanida, Ismail Ben Ayed, and Jose Dolz. Few-Shot Seg- mentation Without Meta-Learning: A Good Transductive Inference Is All You Need? In arXiv:2012.06166 [cs], 2021. 1 [5] Kaidi Cao, Maria Brbi´c, and Jure Leskovec. Concept learners for few-shot learning. In Proc. Int. Conf. Learn. Represent., 2021. 2 [6] Yuhang Cao, Jiaqi Wang, Ying Jin, Tong Wu, Kai Chen, Ziwei Liu, and Dahua Lin. Few-shot object detection via association and discrimination. Proc. Adv. Neural Inf. Process. Syst. , 34:16570–16581, 2021. 1, 5, 7, 13 [7] Liangyu Chen, Tong Yang, Xiangyu Zhang, Wei Zhang, and Jian Sun. Points as queries: Weakly semi-supervised object detection by points. Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 8819–8828, 2021. 1 [8] Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng Chang, Yu-Hsiang Lin, Jia-Fong Yeh, Wen-Chin Chen, and Winston Hsu. Dual-awareness attention for few-shot object detection. IEEE Transactions on Multimedia, 2021. 1, 2, 15 [9] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A Closer Look at Few-shot Classi- ﬁcation. In ICLR 2019, 2019. 1, 2 [10] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 3 [11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops, pages 702–703, 2020. 3, 4 [12] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A Baseline for Few-Shot Image Classiﬁ- cation. In ICLR, page 20, 2020. 1, 2 [13] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing- ming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proc. IEEE Int. Conf. on Computer Vision, pages 6569–6578, 2019. 15 [14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338, 2010. 2, 5 [15] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai. Few- shot object detection with attention-rpn and multi-relation detector. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 4013–4022, 2020. 3 [16] Zhibo Fan, Yuchen Ma, Zeming Li, and Jian Sun. Generalized few-shot object detection without forgetting. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 4527–4536, 2021. 1, 2, 3, 5, 7, 13, 14 [17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning , pages 1126–1135. PMLR, 2017. 1, 2 [18] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In Proc. Int. Conf. Mach. Learn. , volume 70, pages 1126– 1135, 2017. 2 [19] Golnaz Ghiasi, Yin Cui, A. Srinivas, Rui Qian, Tsung-Yi Lin, Ekin Dogus Cubuk, Quoc V . Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 2917–2927, 2021. 1 [20] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord. Boosting few-shot visual learning with self-supervision. In Proc. IEEE Int. Conf. on Computer Vision, 2019. 2 [21] Santiago Gonzalez and Risto Miikkulainen. Improved train- ing speed, accuracy, and data utilization through loss function optimization. In 2020 IEEE Congress on Evolutionary Com- putation (CEC), pages 1–8. IEEE, 2020. 2, 3 [22] Guangxing Han, Yicheng He, Shiyuan Huang, Jiawei Ma, and Shih-Fu Chang. Query adaptive few-shot object detection with heterogeneous graph convolutional networks. In Proc. IEEE Int. Conf. on Computer Vision, pages 3263–3272, 2021. 3, 5, 14, 15 [23] Guangxing Han, Jiawei Ma, Shiyuan Huang, Long Chen, and Shih-Fu Chang. Few-shot object detection with fully cross- transformer. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 5321–5330, 2022. 1, 2, 5, 14, 15 [24] Bharath Hariharan and Ross B. Girshick. Low-shot visual recognition by shrinking and hallucinating features. Proc. IEEE Int. Conf. on Computer Vision, pages 3037–3046, 2017. 2 [25] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 4 [26] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmen- tation policy schedules. In Proc. Int. Conf. Mach. Learn. , pages 2731–2741. PMLR, 2019. 3 [27] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng- Luh Liu. One-shot object detection with co-attention and co-excitation. arXiv preprint arXiv:1911.12529, 2019. 3 [28] Hanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, and Liwei Wang. Dense relation distillation with context-aware aggregation for few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 10185–10194, 2021. 3, 5, 14 [29] Shell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, and Timo- thy M. Hospedales. Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference. arXiv e-prints, page arXiv:2204.07305, Apr. 2022. 2 9[30] Zeyi Huang, Yang Zou, BVK Kumar, and Dong Huang. Com- prehensive attention self-distillation for weakly-supervised object detection. Proc. Adv. Neural Inf. Process. Syst., 33, 2020. 1 [31] Taewon Jeong and Heeyoung Kim. Ood-maml: Meta-learning for few-shot out-of-distribution detection and classiﬁcation. In Advances in Neural Information Processing Systems, vol- ume 33, pages 3907–3916, 2020. 1 [32] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proc. IEEE Int. Conf. on Computer Vision, pages 8420–8429, 2019. 1, 2, 3, 5, 14, 15 [33] Prannay Kaul, Weidi Xie, and Andrew Zisserman. Label, verify, correct: A simple few shot object detection method. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 14237– 14247, 2022. 1, 3, 5, 6, 7, 13 [34] Michalis Lazarou, Yannis Avrithis, and Tania Stathaki. Ten- sor feature hallucination for few-shot learning. ArXiv, abs/2106.05321, 2021. 2 [35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 10649–10657, 2019. 2 [36] Aoxue Li and Zhenguo Li. Transformation invariant few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 3094–3102, 2021. 3, 5, 14, 15 [37] Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, and Qixiang Ye. Beyond max-margin: Class margin equilibrium for few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 7363–7372, 2021. 1, 2, 3, 5, 14, 15 [38] Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli Ouyang. Am-lfs: Automl for loss function search. In Proc. IEEE Int. Conf. on Computer Vision, pages 8410–8419, 2019. 3 [39] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta- SGD: Learning to Learn Quickly for Few-Shot Learning. arXiv e-prints, page arXiv:1707.09835, July 2017. 2 [40] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Proc. Adv. Neural Inf. Process. Syst., 32, 2019. 3 [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proc. European Conf. on Computer Vision, pages 740–755. Springer, 2014. 2, 5 [42] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Ming- sheng Long, and Han Hu. Negative margin matters: Under- standing margin in few-shot classiﬁcation. arXiv preprint arXiv:2003.12060, 2020. 2 [43] Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xi- aodan Liang, Yong Jiang, and Zhenguo Li. Loss function discovery for object detection via convergence-simulation driven search. arXiv preprint arXiv:2102.04700, 2021. 2, 3 [44] Shichen Liu, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Generalized Zero-Shot Learning with Deep Calibra- tion Network. In NeurIPS, pages 2005–2015. 2018. 4 [45] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang, and Yi Yang. Learning to propagate la- bels: Transductive propagation network for few-shot learning. In Proc. Int. Conf. Learn. Represent., 2019. 2 [46] Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, and Liqing Zhang. Mixed supervised object detection by transferring mask prior and semantic similarity. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Proc. Adv. Neural Inf. Process. Syst., 2021. 1 [47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Proc. IEEE Int. Conf. on Computer Vision, 2021. 1 [48] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with conditionally shifted neurons. In ICML, 2018. 2 [49] Alex Nichol and John Schulman. Reptile: a scalable met- alearning algorithm. arXiv: Learning, 2018. 2 [50] Boris N. Oreshkin, Pau Rodriguez Lopez, and Alexandre La- coste. Tadam: Task dependent adaptive metric for improved few-shot learning. In NeurIPS, 2018. 2, 4 [51] Matteo Papini, Andrea Battistello, and Marcello Restelli. Bal- ancing learning speed and stability in policy gradient via adaptive exploration. In Proc. Int. Conf. on Artif. Intellig. and Stat., pages 1188–1199, 2020. 5 [52] Eunbyung Park and Junier B. Oliva. Meta-curvature. In NeurIPS, 2019. 2 [53] Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy M Hospedales, and Tao Xiang. Incremental few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 13846–13855, 2020. 1, 2, 5, 15 [54] Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang. Defrcn: Decoupled faster r-cnn for few-shot object detection. In Proc. IEEE Int. Conf. on Computer Vision, pages 8681–8690, 2021. 1, 2, 3, 5, 7, 13, 14 [55] Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with implicit gradients. In NeurIPS, 2019. 2 [56] Esteban Real, Chen Liang, David So, and Quoc Le. Automl- zero: Evolving machine learning algorithms from scratch. In Proc. Int. Conf. Mach. Learn., pages 8007–8019. PMLR, 2020. 3 [57] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Proc. Adv. Neural Inf. Process. Syst. , 28:91–99, 2015. 3 [58] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming- Yu Liu, Yong Jae Lee, Alexander G. Schwing, and Jan Kautz. Instance-aware, context-focused, and memory- efﬁcient weakly supervised object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2020. 1 [59] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In Proc. Int. Conf. Learn. Represent., 2019. 2 [60] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory- 10augmented neural networks. In Proc. Int. Conf. Mach. Learn., volume 48, pages 1842–1850, 2016. 2 [61] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Proc. Adv. Neural Inf. Process. Syst., volume 30, 2017. 2 [62] Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang. Fsce: Few-shot object detection via contrastive proposal en- coding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 7352–7362, 2021. 1, 2, 3, 4, 5, 7, 13, 14 [63] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 1199–1208, 2018. 2 [64] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen- baum, and Phillip Isola. Rethinking Few-Shot Image Clas- siﬁcation: a Good Embedding Is All You Need? In Proc. European Conf. on Computer Vision, 2020. 1, 2 [65] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Proc. Adv. Neural Inf. Process. Syst., volume 29, 2016. 2 [66] Xin Wang, Thomas E Huang, Trevor Darrell, Joseph E Gon- zalez, and Fisher Yu. Frustratingly simple few-shot object detection. arXiv preprint arXiv:2003.06957, 2020. 1, 2, 3, 5, 7, 8, 13, 14 [67] Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, and Tao Mei. Loss function search for face recognition. In Proc. Int. Conf. Mach. Learn., pages 10029–10038. PMLR, 2020. 3, 4 [68] Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, and Lau- rens van der Maaten. Simpleshot: Revisiting nearest- neighbor classiﬁcation for few-shot learning. arXiv preprint arXiv:1911.04623, 2019. 2 [69] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-Shot Learning from Imaginary Data. arXiv:1801.05401 [cs], Jan. 2018. 2 [70] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta- learning to detect rare objects. In Proc. IEEE Int. Conf. on Computer Vision, pages 9925–9934, 2019. 5, 14 [71] Ronald J Williams. Simple statistical gradient-following al- gorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. 4 [72] Aming Wu, Suqi Zhao, Cheng Deng, and Wei Liu. Gener- alized and discriminative few-shot object detection via svd- dictionary enhancement. Proc. Adv. Neural Inf. Process. Syst., 34:6353–6364, 2021. 3 [73] Jiaxi Wu, Songtao Liu, Di Huang, and Yunhong Wang. Multi- scale positive sample reﬁnement for few-shot object detection. In Proc. European Conf. on Computer Vision, pages 456–472. Springer, 2020. 1, 2, 3, 4, 5, 6, 7, 13, 14 [74] Xiongwei Wu, Doyen Sahoo, and Steven Hoi. Meta-rcnn: Meta learning for few-shot object detection. In Proceedings of the 28th ACM International Conference on Multimedia , pages 1679–1687, 2020. 1 [75] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 4582–4591, 2017. 5, 15 [76] Yang Xiao and Renaud Marlet. Few-shot object detection and viewpoint estimation for objects in the wild. In Proc. Eu- ropean Conf. on Computer Vision, pages 192–210. Springer, 2020. 1, 2, 3, 5, 14 [77] Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xi- aodan Liang, and Liang Lin. Meta r-cnn: Towards general solver for instance-level low-shot learning. In Proc. IEEE Int. Conf. on Computer Vision, pages 9577–9586, 2019. 1, 2, 5, 7, 13, 14, 15 [78] Huaxiu Yao, Linjun Zhang, and Chelsea Finn. Meta-learning with fewer tasks through task interpolation. In Proceeding of the 10th International Conference on Learning Representa- tions, 2022. 2 [79] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few- shot learning via embedding adaptation with set-to-set func- tions. Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 8805–8814, 2020. 2, 4 [80] Li Yin, Juan M Perez-Rua, and Kevin J Liang. Sylph: A hypernetwork framework for incremental few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 9035–9045, 2022. 1, 2, 3 [81] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classiﬁcation with differentiable earth mover’s distance and structured classiﬁers. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., June 2020. 2 [82] Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu, and Eric P Xing. Meta-detr: Image-level few-shot detection with inter-class correlation exploitation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 1, 2, 3, 14, 15 [83] Lu Zhang, Shuigeng Zhou, Jihong Guan, and Ji Zhang. Ac- curate few-shot object detection with support-query mutual guidance and hybrid loss. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 14424–14432, 2021. 3 [84] Shan Zhang, Lei Wang, Naila Murray, and Piotr Koniusz. Kernelized few-shot object detection with efﬁcient integral aggregation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 19207–19216, 2022. 1, 2, 5, 14 [85] Weilin Zhang and Yu-Xiong Wang. Hallucination improves few-shot object detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 13008–13017, 2021. 1, 2, 5, 7, 8, 13 11A. Appendix A.1. Proxy task class splits We use proxy tasks to apply the meta-tuning ideas, so we generate sub-splits in the base classes. In this context, we select some base classes to mimic novel classes to con- duct the proxy task. We summarize the list of proxy Pascal VOC classes on Table 6. The list of selected proxy novel classes for the MS-COCO dataset is as follows: { \"skis\", \"tennis racket\", \"scissors\", \"truck\", \"baseball bat\", \"hand- bag\", \"carrot\", \"mouse\", \"parking meter\", \"apple\", \"knife\", \"microwave\", \"\"refrigerator\", \"cake\", \"zebra\"}. A.2. Algorithm We summarize the main meta-tuning procedure in Algo- rithm 1. We can divide this algorithm into three parts: (i) model initialization and parameter sampling, (ii) instance sampling and mAP calculation, (iii) mAP normalization and RL steps. 1) Model initialization and parameter sampling. This al- gorithm ﬁrstly initializes the base proxy detection model weights for the proxy task and sample ρvalue from normal distributions. The base proxy detection model represents the object detection model trained using the Dp-pretrain dataset. 2) Instance sampling and mAP calculation.The proposed algorithm samples new instances from the proxy ﬁne-tuning dataset Dp-support, and calculates the mean average precision scores on proxy validation dataset Dp-query after a certain number of iterations. The algorithm repeats this process for N times. 3) mAP normalization and RL steps. The proposed al- gorithm normalizes the mAP scores, selects the maximum score as the reward value among the normalized APs, and applies a single REINFORCE step. A.3. Additional Experimental Results In this section, we share detailed experimental compari- son results for Pascal VOC and MS COCO datasets. Comparison to ﬁne-tuning based FSOD and G-FSOD methods on Pascal VOC. We ﬁrst present the detailed Pas- cal VOC comparisons for each split and shot with only novel classes in Table 7, and the detailed comparisons with all classes in Table 8. The experimental results show that the meta-tuning approach signiﬁcantly improves the strong ﬁne- tuning based few-shot detection baselines on the Pascal VOC benchmark. We provide complementary visual results of the MPSR+Meta-ScaledDynamic+Aug method using the Pascal VOC split-3/10-shot setting in Figure 5. We also present examples from the visual results of the DeFRCN+Meta- ScaledDynamic+Aug method using the Pascal VOC split- 2/10-shot setting in Figure 6. Algorithm 1 Meta-tuning Loss Function Learning Input: Pre-trained model minit, proxy ﬁne-tuning dataset Dp-support, proxy validation datasetDp-query, number ofrho trials N, maximum iteration number M iteration_index = 1 repeat Initialize minit and sample new ρ for rho_index= 1to N do Sample new ﬁne-tuning images from Dp-support Take minit, run all iter. using current samples Calculate mAP[rho_index] on Dp-query end for Normalize mAP scores Get max normalized AP as a reward Make a single REINFORCE step iteration_index += 1 until iteration_index = M Comparisons to meta-learning based FSOD and G- FSOD on Pascal VOC.We present the detailed Pascal VOC comparisons with meta-learning based methods in Table 9 and Table 10 for novel-only and all-classes settings, respec- tively. Since the most of the meta-learning methods do not share G-FSOD results, we are able to compare against a more limited number of meta-learning methods than FSOD. The experimental results (Table 9) show that our DeFRCN+Meta- ScaledDynamic+Aug method obtains the best results in all of the FSOD cases, except for the Split-2/1-shot setting. In the G-FSOD experiments (Table 10), it is observed that the proposed meta-tuning approach obtains the state-of-the-art results with a clear margin against existing meta-learning based methods. Comparisons to meta-learning based FSOD and G- FSOD on MS-COCO. We compare our results with meta- learning based methods on the MS-COCO dataset and share the obtained results in Table 11. In this table, we are able to report a rather limited number of meta-learning methods to compare the G-FSOD results since most meta-learning based methods do not share G-FSOD results on the MS- COCO dataset. In FSOD experiments, we also observe that our DeFRCN+Meta-ScaledDynamic+Aug method obtains higher results than several recently published meta-learning based methods. We additionally observe major improve- ments in terms of HM scores in the G-FSOD setting, similar to the improvements obtained on the Pascal VOC dataset. A.4. Implementation and runtime We run our MPSR and DeFRCN experiments on a server with 4 Nvidia Tesla V100 32GB GPUs. The base MPSR 12Proxy-base classes (Cp-base) Proxy-novel classes (Cp-novel) Split-1 Split-2 Split-3 Split-1 Split-2 Split-3 aeroplane bicycle aeroplane person motorbike horse bicycle bird bicycle pottedplant person person boat boat bird sheep sheep pottedplant bottle bus bottle train train train car car bus tvmonitor tvmonitor tvmonitor cat cat car chair chair chair diningtable diningtable cow dog dog diningtable horse pottedplant dog Table 6. Proxy task class splits for Pascal VOC. Split 1 Split 2 Split 3Method/Shot 1 2 3 5 10 1 2 3 5 10 1 2 3 5 10 FRCN [77] (ICCV’19) 15.2 20.3 29.0 25.5 28.7 13.4 20.6 28.6 32.4 38.8 19.6 20.8 28.7 42.2 42.1 TFA-fc [66] (ICML’20) 36.8 29.1 43.6 55.7 57.0 18.2 29.0 33.4 35.5 39.0 27.7 33.6 42.5 48.7 50.2 TFA-cos [66] (ICML’20) 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8 MPSR [73] (ECCV’20) 37.2 43.6 50.9 53.7 60.2 24.8 28.1 38.0 39.8 45.9 37.3 40.0 43.9 47.8 50.1 Ret. R-CNN [16] (CVPR’21) 42.4 45.8 45.9 53.7 56.1 21.7 27.8 35.2 37.0 40.3 30.2 37.6 43.0 49.7 50.1 TFA+H [85] (CVPR’21) 45.1 44.0 44.7 55.0 55.9 23.2 27.5 35.1 34.9 39.0 30.5 35.1 41.4 49.0 49.3 FSCE [62] (CVPR’21) 37.6 44.7 46.9 52.2 60.3 24.5 30.1 38.2 40.4 45.9 25.4 34.2 42.3 48.7 50.3 FADI [6] (NeurIPS’21) 50.3 54.8 54.2 59.3 63.2 30.6 35.0 40.3 42.8 48.0 45.7 49.7 49.1 55.0 59.6 LVC [33] (CVPR’22) 36.0 40.1 48.6 57.0 59.9 22.3 22.8 39.2 44.2 47.8 34.3 43.4 42.9 52.0 54.5 LVC-PL [33] (CVPR’22) 54.5 53.2 58.8 63.2 65.7 32.8 29.2 50.7 49.8 50.6 48.4 52.7 55.0 59.6 59.6 DeFRCN [54] (CVPR’21) 53.7 59.5 61.2 65.7 66.6 32.3 42.0 49.5 52.4 53.4 53.6 56.2 56.9 61.9 62.3 MPSR+Meta-Static 36.7 47.0 52.1 53.8 60.8 25.3 31.6 38.4 40.8 46.9 38.3 39.7 44.8 47.2 50.1 MPSR+Meta-Dynamic 40.4 47.5 51.9 54.9 60.5 25.6 31.7 38.5 40.6 46.7 37.6 40.2 44.7 49.1 50.3 MPSR+Meta-ScaledDynamic 41.5 47.9 52.7 55.4 60.9 25.7 32.2 38.9 40.8 46.8 38.5 40.9 45.9 49.0 51.0 MPSR+Aug 39.5 47.1 53.2 54.9 59.5 26.2 31.0 39.7 41.8 47.8 38.0 37.8 45.2 48.4 50.9 MPSR+Meta-Static+Aug 40.9 47.6 53.6 54.7 60.2 26.5 31.6 38.9 42.2 47.3 38.7 38.1 45.8 48.2 50.8 MPSR+Meta-Dynamic+Aug 41.0 47.5 53.8 55.2 60.2 26.4 32.2 39.8 42.7 48.5 38.9 39.1 46.0 48.8 51.3 MPSR+Meta-ScaledDynamic+Aug 41.8 48.7 54.2 55.7 61.1 26.5 32.7 40.0 42.5 48.7 39.0 40.4 46.2 49.6 51.2 DeFRCN+Meta-ScaledDynamic+Aug 58.4 62.4 63.2 67.6 67.7 34.0 43.1 51.0 53.6 54.0 55.1 56.6 57.3 62.6 63.7 Table 7. Comparison to ﬁne-tuning based FSOD methods on the Pascal VOC dataset, with only novel classes. The best and the second-best results are marked with red and blue. MPSR+Meta-Static, MPSR+Meta-Dynamic, and MPSR+Meta-ScaledDynamic represent meta-tuning results. model training to be used during ﬁne-tuning takes 0.25 days for Pascal VOC and 0.45 days for MS COCO datasets. Since the base models used for the proxy tasks contain fewer classes and demand fewer iterations, the training of the MPSR model takes 0.1 days in Pascal VOC and 0.6 days in MS COCO datasets for the proxy-base classes. RL training for meta-tuning using the ﬁnal setting takes 0.05 days for Pascal VOC splits and 0.5 days for the MS COCO dataset. Finally, we note that meta-tuning operations do not incur any overhead during the ﬁne-tuning for novel classes. 13Split-1 Split-2 Split-3Method/Shot 1 2 3 5 10 1 2 3 5 10 1 2 3 5 10 FRCN [77] (ICCV’19) 24.9 31.4 40.3 37.6 41.0 22.1 31.3 39.1 43.0 47.5 30.8 32.3 40.5 52.2 51.7 TFA-fc [66] (ICML’20) 50.4 42.6 56.2 65.4 66.1 29.7 42.4 47.0 49.0 52.1 41.3 47.4 55.6 60.6 61.6 TFA-cos [66] (ICML’20) 53.1 49.5 57.1 65.4 65.3 36.3 40.0 47.6 48.6 52.2 44.5 48.5 55.9 61.2 61.4 MPSR [73] (ECCV’20) 45.8 52.5 59.3 61.8 65.5 36.0 39.7 49.8 51.7 56.9 47.6 49.9 54.5 58.1 60.0 FSCE [62] (CVPR’21) 50.7 56.5 58.1 61.6 66.1 36.5 42.4 49.8 51.5 55.8 38.2 47.4 54.6 59.9 61.1 Ret. R-CNN [16] (CVPR’21) 55.6 58.5 58.6 64.5 66.2 34.3 41.5 49.2 51.0 54.0 44.1 51.6 56.4 61.9 62.2 DeFRCN [54] (CVPR’21) 63.3 67.3 68.1 71.1 71.2 45.9 54.7 60.3 62.8 63.1 63.7 65.4 65.5 68.8 69.2 MPSR+Meta-Static 45.7 56.4 60.3 62.1 66.1 36.7 43.7 50.3 52.7 57.9 48.6 51.2 55.5 57.8 60.1 MPSR+Meta-Dynamic 50.2 57.2 60.6 63.3 67.0 37.0 43.9 50.4 52.5 57.8 47.9 51.8 55.4 59.1 60.2 MPSR+Meta-ScaledDynamic 51.0 57.3 60.9 63.3 67.1 37.1 44.1 50.7 52.5 57.7 48.7 52.1 56.1 59.0 60.5 MPSR+Aug 49.9 56.2 61.5 63.0 66.5 37.4 43.0 51.4 53.6 58.6 48.1 49.3 55.7 58.7 60.8 MPSR+Meta-Static+Aug 51.3 56.9 62.0 62.8 66.9 37.7 43.5 50.7 53.7 58.1 48.6 49.5 55.9 58.5 60.3 MPSR+Meta-Dynamic+Aug 51.3 56.8 62.1 63.3 67.0 37.8 44.2 51.7 54.3 59.3 48.9 50.5 56.5 59.0 61.2 MPSR+Meta-ScaledDynamic+Aug 51.9 57.6 62.4 63.7 67.6 37.8 44.9 51.9 54.2 59.4 49.2 51.9 56.7 59.7 61.1 DeFRCN+Meta-ScaledDynamic+Aug 66.7 69.3 69.8 72.2 72.1 47.7 55.8 61.8 63.9 63.7 64.9 65.8 66.2 69.7 70.2 Table 8. Comparison to ﬁne-tuning based G-FSOD methods on the Pascal VOC dataset, with both base and novel classes. The best and the second-best results are marked with red and blue. The harmonic mean (HM) of the base and novel class mAPs is used for the calculation. Method/Shot Novel Set 1 Novel Set 2 Novel Set 3 1 2 3 5 10 1 2 3 5 10 1 2 3 5 10 ML M. R-CNN [77] (ICCV’19) 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1 M. R-CNN* [77] (ICCV’19) 16.8 20.1 20.3 38.2 43.7 7.7 12.0 14.9 21.9 31.1 9.2 13.9 26.2 29.2 36.2 FSRW [32] (ICCV’19) 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 39.2 19.2 21.7 25.7 40.6 41.3 MetaDet [70] (ICCV’19) 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 FsDet [76] (ECCV’20) 25.4 20.4 37.4 36.1 42.3 22.9 21.7 22.6 25.6 29.2 32.4 19.0 29.8 33.2 39.8 TIP [36] (CVPR’21) 27.7 36.5 43.3 50.2 59.6 22.7 30.1 33.8 40.9 46.9 21.7 30.6 38.1 44.5 50.9 DCNet [28] (CVPR’21) 33.9 37.4 43.7 51.1 59.6 23.2 24.8 30.6 36.7 46.6 32.3 34.9 39.7 42.6 50.7 CME [37] (CVPR’21) 41.5 47.5 50.4 58.2 60.9 27.2 30.2 41.4 42.5 46.8 34.3 39.6 45.1 48.3 51.5 QA-FewDet [22] (ICCV’21) 41.0 33.2 35.3 47.5 52.0 23.5 29.4 37.9 35.9 37.1 33.2 29.4 37.6 39.8 41.5 KFSOD [84] (CVPR’22) 44.6 - 54.4 60.9 65.8 37.8 - 43.1 48.1 50.4 34.8 - 44.1 52.7 53.9 FCT [23] (CVPR’22) 49.9 57.1 57.9 63.2 67.1 27.6 34.5 43.7 49.2 51.2 39.5 54.7 52.3 57.0 58.7 Meta-DETR [82] (TPAMI’22) 40.6 51.4 58.0 59.2 63.6 37.0 36.6 43.7 49.1 54.6 41.6 45.9 52.7 58.9 60.6 Ours DeFRCN+Meta-ScaledDynamic+Aug 58.4 62.4 63.2 67.6 67.7 34.0 43.1 51.0 53.6 54.0 55.1 56.6 57.3 62.6 63.7 Table 9. Comparison to meta-learning based FSOD methods on the Pascal VOC dataset, with only novel classes. The best and the second-best results are marked with red and blue. MPSR+Meta-Static, MPSR+Meta-Dynamic, and MPSR+Meta-ScaledDynamic represent meta-tuning results. ML represents the meta learning based methods. Split-1 Split-2 Split-3Method/Shot 1 2 3 5 10 1 2 3 5 10 1 2 3 5 10 M. R-CNN [77] (ICCV’19) 17.3 25.3 27.3 44.4 50.4 11.6 18.5 21.9 30.8 41.3 13.3 20.2 33.4 38.0 45.5 FSRW [32] (ICCV’19) 24.2 24.8 37.8 44.2 54.2 25.5 24.9 33.8 41.5 49.0 29.7 32.4 36.7 49.9 49.9ML FsDet [76] (ECCV’20) 31.1 28.4 39.1 43.5 49.5 29.3 30.5 30.7 34.4 39.8 35.2 26.9 35.6 41.8 47.8 Ours DeFRCN+Meta-ScaledDynamic+Aug 58.4 62.4 63.2 67.6 67.7 34.0 43.1 51.0 53.6 54.0 55.1 56.6 57.3 62.6 63.7 Table 10. Comparison to meta-learning based G-FSOD methods on the Pascal VOC dataset, with both base and novel classes. The best results are marked with red. The harmonic mean (HM) of the base and novel class mAPs is used for the calculation. 14Novel Classes All Classes (HM)Method/Shot 10-shot 30-shot 10-shot 30-shot ONCE [53] 1.2 - 2.2 - Meta R-CNN [77] 6.1 9.9 5.6 8.3 FSRW [32] 5.6 9.1 - - FsDetView [75] 7.6 12.0 6.9 10.5 TIP [36] 16.3 18.3 - - DCNET [13] 12.8 18.6 - - CME [37] 15.1 16.9 - - QA-FewDet [22] 10.2 11.5 - - FCT [23] 17.1 21.4 - - DAnA [8] 18.6 21.6 - - ML Meta-DETR [82] 19.0 22.2 - - Ours DeFRCN+Meta-ScaledDynamic+Aug 18.8 23.4 24.4 28.0 Table 11. FSOD and G-FSOD results on the MS COCO dataset with novel classes. The best results are marked with red. The harmonic mean (HM) of the base and novel class mAPs is used for the calculation. Figure 5. Randomly sampled MPSR+Meta-ScaledDynamic+Aug object detection results for the Pascal VOC dataset Split-3/10-shot experiment. Base class instance candidates are marked with green, and novel class instance candidates are marked with red color. (Best viewed in color.) 15Figure 6. Randomly sampled DeFRCN+Meta-ScaledDynamic+Aug object detection results for the Pascal VOC dataset Split-2/10-shot experiment. Base class instance candidates are marked with green, and novel class instance candidates are marked with red color. (Best viewed in color.) 16",
      "references": [
        "Enhancing few-shot image classification with unlabelled examples",
        "Improved Few-Shot Visual Classification",
        "Meta-learning with differentiable closed-form solvers",
        "Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?",
        "Concept learners for few-shot learning",
        "Few-shot object detection via association and discrimination",
        "Points as queries: Weakly semi-supervised object detection by points",
        "Dual-awareness attention for few-shot object detection",
        "A Baseline for Few-Shot Image Classification",
        "Autoaugment: Learning augmentation policies from data",
        "Randaugment: Practical automated data augmentation with a reduced search space",
        "A Closer Look at Few-Shot Classification",
        "Centernet: Keypoint triplets for object detection",
        "The pascal visual object classes (voc) challenge",
        "Few-shot object detection with attention-rpn and multi-relation detector",
        "Generalized few-shot object detection without forgetting",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Meta-SGD: Learning to Learn Quickly for Few-Shot Learning",
        "Simple copy-paste is a strong data augmentation method for instance segmentation",
        "Negative margin matters: Understanding margin in few-shot classification",
        "Improved training speed, accuracy, and data utilization through loss function optimization",
        "Query adaptive few-shot object detection with heterogeneous graph convolutional networks",
        "Few-shot object detection with fully cross-transformer",
        "Low-Shot Visual Recognition by Shrinking and Hallucinating Features",
        "Distilling the knowledge in a neural network",
        "Population based augmentation: Efficient learning of augmentation policy schedules",
        "One-shot object detection with co-attention and co-excitation",
        "Dense relation distillation with context-aware aggregation for few-shot object detection",
        "Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference",
        "Comprehensive attention self-distillation for weakly supervised object detection",
        "Ood-maml: Meta-learning for few-shot out-of-distribution detection and classification",
        "Few-shot object detection via feature reweighting",
        "Label, verify, correct: A simple few shot object detection method",
        "Tensor feature hallucination for few-shot learning",
        "Meta-learning with differentiable convex optimization",
        "Transformation invariant few-shot object detection",
        "Beyond max-margin: Class margin equilibrium for few-shot object detection",
        "Am-lfs: Automl for loss function search",
        "Meta-curvature",
        "Fast autoaugment",
        "Microsoft coco: Common objects in context",
        "Loss function discovery for object detection via convergence-simulation driven search",
        "Generalized Zero-Shot Learning with Deep Calibration Network",
        "Learning to propagate labels: Transductive propagation network for few-shot learning",
        "Mixed supervised object detection by transferring mask prior and semantic similarity",
        "Swin transformer: Hierarchical vision transformer using shifted windows",
        "Rapid adaptation with conditionally shifted neurons",
        "Reptile: a scalable meta-learning algorithm",
        "Tadam: Task dependent adaptive metric for improved few-shot learning",
        "Balancing learning speed and stability in policy gradient via adaptive exploration",
        "Incremental few-shot object detection",
        "Defrcn: Decoupled faster r-cnn for few-shot object detection",
        "Meta-learning with implicit gradients",
        "Automl-zero: Evolving machine learning algorithms from scratch",
        "Faster r-cnn: Towards real-time object detection with region proposal networks",
        "Instance-aware, context-focused, and memory-efficient weakly supervised object detection",
        "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning",
        "Prototypical networks for few-shot learning",
        "FSCE: Few-shot object detection via contrastive proposal encoding",
        "Learning to compare: Relation network for few-shot learning",
        "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
        "Matching networks for one shot learning",
        "Frustratingly simple few-shot object detection",
        "Loss function search for face recognition",
        "Low-Shot Learning from Imaginary Data",
        "Meta-learning to detect rare objects",
        "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "Generalized and discriminative few-shot object detection via svd-dictionary enhancement",
        "Multi-scale positive sample refinement for few-shot object detection",
        "Meta R-CNN: Towards general solver for instance-level low-shot learning",
        "Zero-shot learning-the good, the bad and the ugly",
        "Few-shot object detection and viewpoint estimation for objects in the wild",
        "DeepEMD: Few-shot image classification with differentiable earth mover’s distance and structured classifiers",
        "Few-shot learning via embedding adaptation with set-to-set functions",
        "Sylph: A hypernetwork framework for incremental few-shot object detection",
        "Meta-detr: Image-level few-shot detection with inter-class correlation exploitation",
        "Accurate few-shot object detection with support-query mutual guidance and hybrid loss",
        "Kernelized few-shot object detection with efficient integral aggregation",
        "Hallucination improves few-shot object detection"
      ],
      "meta_data": {
        "arxiv_id": "2304.12161v1",
        "authors": [
          "Berkan Demirel",
          "Orhun Buğra Baran",
          "Ramazan Gokberk Cinbis"
        ],
        "published_date": "2023-04-24T15:14:16Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces a meta-learning based tuning scheme that automatically learns task-specific loss functions (temperature-scaled cross-entropy with optional time-varying polynomial and novel-class score scaling) and photometric data-augmentation magnitudes for fine-tuning based few-shot object detection (FSOD). The learned inductive biases are interpretable, lightweight, and significantly improve both standard FSOD and generalized FSOD performance over strong fine-tuning baselines (MPSR, DeFRCN) on Pascal VOC and MS-COCO.",
        "methodology": "1. Start from a pre-trained detector (e.g., MPSR/Faster-RCNN).\n2. Define a parametric classification loss ℓ_cls(x,y;ρ) where ρ includes:\n   • Constant or dynamic (time-dependent) softmax temperature.\n   • Optional novel-class score scaling coefficient.\n3. Define a shared magnitude parameter for photometric augmentations (brightness, saturation, contrast, hue).\n4. Meta-training: episodic proxy tasks built from base classes (proxy-base & proxy-novel splits). For each episode:\n   • Sample loss/augmentation parameters from Gaussian priors.\n   • Fine-tune the detector on few proxy-novel shots; evaluate mAP on held-out proxy queries.\n   • Use REINFORCE to update the means of the Gaussian distributions toward parameter settings with higher reward (normalized mAP).\n   • Model is re-initialized each episode; reward normalization and σ annealing improve stability.\n5. The learned ρ and augmentation magnitude are fixed and applied during real few-shot fine-tuning on novel classes; no extra cost at deployment. The approach is also transferable to other fine-tuning detectors (e.g., DeFRCN).",
        "experimental_setup": "Datasets & splits:\n• Pascal VOC 2007+2012: 3 standard FSOD splits (15 base / 5 novel each). Evaluate 1,2,3,5,10-shot.\n• MS-COCO: 20 base / 20 novel split; evaluate 10-shot and 30-shot.\nProxy tasks: within base classes, further split into proxy-base (pre-train), proxy-novel (few-shot) to mimic test-time scenario.\nTraining details:\n• 200 RL episodes; Gaussian σ starts at 0.1 and is annealed.\n• Fine-tuning iterations per episode: 2 000 (VOC) or 4 000/8 000 (COCO) matching downstream setup.\n• Evaluate reward as mAP@0.5 on proxy query set; for G-FSOD report harmonic mean (HM) of base & novel mAP.\nBaselines compared: MPSR, DeFRCN, FSCE, Retentive R-CNN, LVC-PL, FADI, TFA(+Hallucination), and numerous meta-learning detectors.\nResults: Meta-tuning boosts MPSR by up to +2–3 mAP and DeFRCN by ~+1.5 mAP; achieves state-of-the-art HM on both datasets.",
        "limitations": "1. Search space limited to classification temperature, simple score scaling, and photometric augmentations; ignores localization losses, optimizer schedules, or geometric transforms.\n2. Requires additional compute for episodic RL (proxy model training and hundreds of episodes), increasing development cost.\n3. Proxy task design assumes availability of many base classes; effectiveness in extremely low-base regimes is untested.\n4. Gains depend on the underlying detector; some settings show modest improvements.\n5. Experiments focus on VOC and COCO; generalisation to other domains or real-world class imbalance not validated.",
        "future_research_directions": "• Extend meta-tuning to other learning components: localization/regression losses, optimizer hyper-parameters, learning rate schedules, or initial weight generators.\n• Incorporate richer augmentation search spaces including geometric transforms, copy-paste, and mixup strategies.\n• Explore differentiable or evolutionary search to reduce RL variance and computational overhead.\n• Apply the meta-tuning framework to other few-shot tasks such as segmentation, key-point detection, or open-vocabulary detection.\n• Study domain adaptation and cross-dataset transfer to test robustness of learned biases.\n• Theoretically analyse why temperature dynamics and score scaling help few-shot generalisation, guiding principled design of future loss functions.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration",
      "full_text": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Seungyul Han 1 Youngchul Sung1 Abstract In this paper, sample-aware policy entropy regu- larization is proposed to enhance the conventional policy entropy regularization for better explo- ration. Exploiting the sample distribution obtain- able from the replay buffer, the proposed sample- aware entropy regularization maximizes the en- tropy of the weighted sum of the policy action dis- tribution and the sample action distribution from the replay buffer for sample-efﬁcient exploration. A practical algorithm named diversity actor-critic (DAC) is developed by applying policy iteration to the objective function with the proposed sample- aware entropy regularization. Numerical results show that DAC signiﬁcantly outperforms existing recent algorithms for reinforcement learning. 1. Introduction Reinforcement learning (RL) aims to maximize the expected return under Markov decision process (MDP) (Sutton & Barto, 1998). When the given task is complex, e.g., the envi- ronment has high action-dimensions or sparse rewards, it is important to explore state-action pairs well for high perfor- mance (Agre & Rosenschein, 1996). For better exploration, recent RL considers various methods: maximizing the pol- icy entropy to take actions more uniformly (Ziebart et al., 2008; Fox et al., 2015; Haarnoja et al., 2017), maximizing diversity gain that yields intrinsic reward to explore rare states by counting the number of visiting states (Strehl & Littman, 2008; Lopes et al., 2012), maximizing information gain (Houthooft et al., 2016; Hong et al., 2018), maximiz- ing model prediction error (Achiam & Sastry, 2017; Pathak et al., 2017). In particular, based on policy iteration for soft Q-learning, Haarnoja et al. (2018a) extended maximum en- tropy RL and proposed an off-policy actor-critic algorithm, soft actor-critic (SAC), which has competitive performance 1Department of Electrical Engineering, Korea Advanced Insti- tute of Science and Technology, Daejeon, South Korea. Correspon- dence to: Youngchul Sung <ycsung@kaist.ac.kr>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). for challenging continuous control tasks. In this paper, we consider the problem of policy entropy regularization in off-policy learning and propose a general- ized approach to policy entropy regularization for sample- efﬁcient exploration. In off-policy learning, we store sam- ples in the replay buffer and reuse old samples to update the current policy (Mnih et al., 2015). Thus, the sample buffer has information about the old samples. However, the simple policy entropy regularization tries to maximize the entropy of the current policy irrespective of the distribution of the previous samples in the replay buffer. In order to exploit the sample information in the replay buffer and enhance per- formance, we propose sample-aware entropy regularization, which tries to maximize the entropy of the weighted sum of the current policy action distribution and the sample action distribution from the replay buffer. We develop a practical and efﬁcient algorithm for return maximization based on the proposed sample-aware entropy regularization without ex- plicitly computing the replay-buffer sample distribution, and demonstrate that the proposed algorithm yields signiﬁcant enhancement in exploration and ﬁnal performance on vari- ous difﬁcult environments such as tasks with sparse reward or high action dimensions. 2. Related Works Entropy regularization: Entropy regularized RL maxi- mizes the sum of the expected return and the policy action entropy. It encourages the agent to visit the action space uni- formly for each given state, and can provide more accurate model prediction (Ziebart, 2010). Entropy regularization is considered in various domains: inverse reinforcement learn- ing (Ziebart et al., 2008), stochastic optimal control prob- lems (Todorov, 2008; Toussaint, 2009; Rawlik et al., 2013), and off-policy reinforcement learning (Fox et al., 2015; Haarnoja et al., 2017). Nachum et al. (2017a) showed that there exists a connection between value-based and policy- based RL under entropy regularization. O’Donoghue et al. (2016) proposed an algorithm combining value-based and policy-based RL, and Schulman et al. (2017a) proved that they are equivalent. Hazan et al. (2019) maximized the en- tropy of state distribution induced by the current policy by using state mixture distribution for better pure exploration. arXiv:2006.01419v2  [cs.LG]  9 Jun 2021Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Diversity gain: Diversity gain is used to provide a guid- ance for exploration to the agent. To achieve diversity gain, many intrinsically-motivated approaches and intrinsic re- ward design methods have been considered, e.g., intrinsic reward based on curiosity (Chentanez et al., 2005; Baldas- sarre & Mirolli, 2013), model prediction error (Achiam & Sastry, 2017; Pathak et al., 2017; Burda et al., 2018), di- vergence/information gain (Houthooft et al., 2016; Hong et al., 2018), counting (Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2017; Martin et al., 2017), and uniﬁcation of them (Bellemare et al., 2016). Eysenbach et al. (2018) explicitly maximized diversity based on mutual information. Off-policy learning: Off-policy learning reuses samples generated from behaviour policies for policy update (Sutton & Barto, 1998; Degris et al., 2012), so it is sample-efﬁcient compared to on-policy learning. In order to reuse old sam- ples, a replay buffer that stores trajectories generated by previous policies is used for Q-learning (Mnih et al., 2015; Lillicrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018a). To further enhance both stability and sample ef- ﬁciency, several methods were considered, e.g., combin- ing on-policy and off-policy (Wang et al., 2016; Gu et al., 2016; 2017), and generalization from on-policy to off-policy (Nachum et al., 2017b; Han & Sung, 2019). 3. Background Setup: We assume a basic RL setup composed of an envi- ronment and an agent. The environment follows an inﬁnite horizon Markov decision process (S,A,P,γ,r ), where S is the state space, Ais the action space, P is the transition probability, γis the discount factor, and r: S×A→ R is the reward function. In this paper, we consider continuous state and action spaces. The agent has policy distribution π ∈Π : S×A→ [0,∞) which selects an action at for given state st at time step t, where Π is the policy space. Then, the agent receives reward rt := r(st,at) from the en- vironment and the state changes to st+1. Standard RL aims to maximize the discounted returnEs0∼p0,τ0∼π[∑∞ t=0 γtrt], where τt = (st,at,st+1,at+1 ···) is an episode trajectory. Soft Actor-Critic: Soft actor-critic (SAC) includes a policy entropy regularization term in the objective function with the aim of performing more diverse actions for each given state and visiting states with higher entropy for better exploration (Haarnoja et al., 2018a). The entropy-augmented policy objective function of SAC is given by JSAC(π) = Eτ0∼π [∞∑ t=0 γt(rt + βH(π(·|st))) ] , (1) where His the entropy function and β ∈ (0,∞) is the entropy coefﬁcient. SAC is a practical off-policy actor-critic algorithm based on soft policy iteration (SPI) that alternates soft policy evaluation to estimate the true soft Q-function and soft policy improvement to ﬁnd the optimal policy that maximizes (1). SPI theoretically guarantees convergence to the optimal policy that maximizes (1) for ﬁnite MDPs. 4. The Diversity Actor-Critic Algorithm 4.1. Motivation of Sample-Aware Entropy In order to guarantee the convergence of Q-learning, there is a key assumption: Each state-action pair must be vis- ited inﬁnitely often (Watkins & Dayan, 1992). Without proper exploration, policy can converge to local optima and task performance can be degraded severely (Plappert et al., 2017). Therefore, exploration for visiting diverse state-action pairs is important for RL. There has been exten- sive research for better exploration in RL. One important line of recent methods is to use intrinsic reward based on prediction model (Chentanez et al., 2005; Baldassarre & Mirolli, 2013; Achiam & Sastry, 2017; Pathak et al., 2017; Burda et al., 2018). In this approach, we have a prediction model for a target value or distribution, and the prediction model is learned with samples. Then, the prediction error is used as the intrinsic reward added to the actual reward from the environment, and the discounted sum of the actual and intrinsic rewards is maximized. The fundamental rationale behind this approach is that the prediction model is well learned for the frequently-observed state-action pairs in the sample history and hence the prediction error is small. On the other hand, for unobserved or less-observed state-action pairs in the sample history, the prediction model training is not enough and the prediction error is large. In this way, un- or less-explored state-action pairs are favored. Another successful method for exploration is policy entropy regularization with the representative method shown in (1). In (1), one can view the policy action entropy H(π(·|st)) as an intrinsic reward added to the actual reward rt. This method relies on the fact that the entropy attains maximum when the distribution is uniform (Cover & Thomas, 2006). Thus, maximizing the discounted sum of the actual reward rt and the policy action entropy as in (1) yields a policy that tries not only to maximize the actual reward but also to visit states with high action entropy and to take more uniform ac- tions for better exploration. Furthermore, in this method the weighting factor βin (1) can be learned adaptively based on the Lagrangian method to maintain a certain level of entropy (Haarnoja et al., 2018b). However, on the contrary to the prediction model-based method, the entropy regularization method does not exploit the previously-observed sample information to construct the intrinsic reward at current time tsince the intrinsic reward H(π(·|st)) depends only on the policy π(·|st), and π(·|st) for given st does not directly capture the sample distribution information from the replay buffer.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration In this paper, we consider the maximum entropy framework in off-policy learning and extend this framework by devising an efﬁcient way to exploit the sample information in the replay buffer so as to harness the merits of the two afore- mentioned approaches: taking more uniform actions and promoting un- or less-performed actions in the past. 4.2. Proposed Policy Objective Function In order to use the previous sample information in entropy- based exploration, we ﬁrst deﬁne the mixture distribution qπ,α mix(·|s) := απ(·|s) + (1−α)q(·|s), (2) where α∈[0,1] is the weighting factor, π(·|s) is the policy (action) distribution, and q(·|s) is the sample action distri- bution of the replay buffer Dwhich stores previous samples. Then, we propose maximizing the following objective func- tion J(π) = Eτ0∼π [∞∑ t=0 γt(rt + βH(qπ,α mix(·|st))) ] , (3) where we refer to H(qπ,α mix(·|st)) as the sample-aware en- tropy. Note that maximizing the sample-aware entropy en- hances sample-efﬁcient exploration because in this case the learning guides the policy to choose actions so that the mixture distribution qπ,α mix(·|st) becomes uniform. That is, π(·|st) will choose actions rare in the replay buffer (i.e., the density q(·|st) is low) with high probability and choose actions stored many times in the replay buffer (i.e., the den- sity q(·|st) is high) with low probability so as to make the mixture distribution uniform. Indeed, we can decompose the sample-aware entropy H(qπ,α mix) for given st as H(qπ,α mix) =− ∫ a∈A (απ+(1 −α)q) log(απ+(1 −α)q   =qπ,α mix ) (4) = ∫ απlog απ απ+ (1−α)q + ∫ (1 −α)qlog (1 −α)q απ+ (1−α)q − ∫ απlog(απ) − ∫ (1 −α)qlog((1 −α)q) (5) = Dα JS(π||q) +αH(π) + (1−α)H(q) +constant, (6) where Dα JS(π||q):=α ∫ πlog π απ+(1−α)q + (1 − α) ∫ qlog q απ+(1−α)q is the α-skew Jensen-Shannon (JS)-symmetrization of KL divergence (Nielsen, 2019). Dα JS reduces to the standard JS divergence for α= 1 2 and to zero for α= 0 or 1. When α= 1, H(qπ,α mix) reduces to the simple entropy and the problem reduces to (1). When α ∈ (0,1), on the other hand, all the ﬁrst three terms in the right-hand side (RHS) of (6) remain. Thus, the added regularized term in (3) will guide the policy to have more uniform actions due to H(π) and simultaneously to promote actions away from q due to Dα JS(π||q). Thus, the proposed policy objective function (3) has the desired properties. Note that the sample-aware entropy is included as reward not as an external regularization term added to the discounted return, and this targets optimization for high total sample-aware entropy of the entire trajectory. (An analytic toy example showing the efﬁciency of the sample-aware entropy regularization is provided in Appendix A.) The main challenge to realize policy design with (3) is how to compute the sample distribution q, which is necessary to compute the objective function. Explicit computation of qrequires a method such as discretization and counting for continuous state and action spaces. This should be done for each environment and can be a difﬁcult and tedious job for high dimensional environments. Even if such empirical qis obtained by discretization and counting, generalization of qto arbitrary state-action pairs is typically required to actually implement an algorithm based on function approximation and this makes the problem difﬁcult further. In the remainder of this paper, circumventing this difﬁculty, we develop a practical and efﬁcient algorithm to realize (3) without explicitly computing q. 4.3. Algorithm Construction Our algorithm construction for the objective function (3) is based on diverse policy iteration, which is a modiﬁcation of the soft policy iteration of Haarnoja et al. (2018a). Diverse policy iteration is composed of diverse policy evaluation and diverse policy improvement. Note that the sample action distribution qis updated as iteration goes on. However, it changes very slowly since the buffer size is much larger than the time steps of one iteration. Hence, for the purpose of algorithm derivation, we regard the action distribution q as a ﬁxed distribution in this section. As in typical policy iteration, for diverse policy iteration, we ﬁrst deﬁne the true diverse Q-function Qπ as Qπ(st,at) := 1 βrt + Eτt+1∼π [ ∞∑ l=t+1 γl−t−1 (1 βrl + H(qπ,α mix(·|sl)) )] , (8) by including the term H(qπ,α mix(·|sl)). Since Qπ(st,at) in- cludes H(qπ,α mix(·|sl)), it seemingly requires computation of q, as seen in (4)-(6). In order to circumvent this difﬁculty, we deﬁne the following ratio function: Rπ,α(st,at) = απ(at|st) απ(at|st) + (1−α)q(at|st), (9) and express the objective and all required loss functions in terms of the ratio function not q. For this, based on (4), we rewrite H(qπ,α mix(·|st)) as follows: H(qπ,α mix) =αEat∼π(·|st)[log Rπ,α(st,at) −log απ(at|st)] + (1−α)Eat∼q(·|st)[log Rπ,α(st,at) −log απ(at|st)]. (10)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Jπold(π(·|st)) := β{Eat∼π[Qπold(st,at) + α(log Rπ,α(st,at) −log απ(at|st))] + (1 −α)Eat∼q[log Rπ,α(st,at) −log απ(at|st)]}. (13) ˜Jπold(π(·|st)) := βEat∼π[Qπold(st,at) + α(log Rπold,α(st,at) −log π(at|st))], (14) The above equation is obtained by exploiting the entropy deﬁnition (4). As seen in (4), H(qπ,α mix) is the sum of −αEat∼π(·|st) log qπ,α mix and −(1 −α)Eat∼q(·|st) log qπ,α mix, but log qπ,α mix in both terms can be expressed as log απ(at|st) Rπ,α(st,at) by the deﬁnition of the ratio function (9). So, we obtain (10). Now, the H(qπ,α mix) expression in the RHS of (10) contains only the ratio function Rπ,α and the policy π, and thus ﬁts our purpose. Note that the expec- tation Eat∼q(·|st) will eventually be replaced by empirical expectation based on the samples in the replay buffer in a practical algorithm. So, it does not cause a problem. Thus, the added term H(qπ,α mix) in (3) and (8) is fully expressed in terms of the ratio function Rπ,α and the policy π. Now, we present the diverse policy iteration composed of diverse policy evaluation and diverse policy improvement. Diverse policy evaluation: We ﬁrst deﬁne a diverse action value function estimate Q: S×A→ R. Then, we deﬁne a modiﬁed Bellman backup operator acting on Qto estimate Qπ as TπQ(st,at) := 1 βrt + γEst+1∼P[V(st+1)], (11) where V(st) is the estimated diverse state value function given by the sum of Eat∼π[Q(st,at)] and H(qπ,α mix) for given st, i.e., V(st)= Eat∼π[Q(st,at)+αlog Rπ,α(st,at)−αlog απ(at|st)] +(1−α)Eat∼q[log Rπ,α(st,at)−log απ(at|st)], (12) where we used the expression (10) for H(qπ,α mix). Note that for the diverse policy evaluation, the policy πunder evalua- tion is given. Hence, the ratio function Rπ,α(st,at) is given for given πby its deﬁnition (9). Hence, V(st) in (12) is well deﬁned and thus the mapping TπQ(st,at) on the current estimate Q(st,at) in (11) is well deﬁned. By repeating the mapping Tπ on Q, the resulting sequence converges to Qπ . Proof is given in Lemma 1 in Appendix B. Diverse policy improvement: Now, consider diverse policy improvement. Suppose that we are given Qπold(·,·) for the current policy πold. (In this diverse policy improvement step, we use the notation πold for the given current policy to distinguish from the notation π as the optimization ar- gument.) Then, we construct the diverse policy objective function Jπold(π(·|st)) as shown in (13), where the notation π in (13) represents the optimization argument. Jπold(π) is the policy objective function estimated under Qπold. If we replace πold in Jπold(π) with πand view state st as the initial state, then (13) reduces to J(π) in (3). (This can be checked with (3), (8), (10) and (13).) Note that π in the Rπ,α and log(απ) terms inside the expectations in (13) is the optimization argument π. We update the policy from πold to πnew as πnew = arg max π∈Π Jπold(π). (15) Then, πnew satisﬁes Qπnew(st,at) ≥ Qπold(st,at), ∀(st,at) ∈S×A . Proof is given in Lemma 2 in Appendix B. Then, in a similar way to the proof of the convergence of the soft policy iteration (Haarnoja et al., 2018a), we can prove the convergence of the diverse policy iteration, stated in the following theorem. Theorem 1 (Diverse Policy Iteration) By repeating itera- tion of the diverse policy evaluation applying the Bellman operator (11) and the diverse policy improvement (15), any initial policy converges to the optimal policy π∗ s.t. Qπ∗ (st,at) ≥Qπ′ (st,at), ∀π′∈Π, ∀(st,at) ∈S×A . Furthermore, such π∗achieves maximum J in (3). Proof. See Appendix B.1. For proof of Theorem 1, we need the assumption of ﬁnite MDPs as in the proof of usual policy iteration or SPI. Later, we consider function approximation for the policy and the value functions to implement the diverse policy iteration in continuous state and action spaces, based on the conver- gence proof in ﬁnite MDPs. Although Theorem 1 proves convergence of the diverse policy iteration for ﬁnite MDPs and provides a basis for implementation with function approximation for continuous MDPs, actually ﬁnding the optimal policy by using The- orem 1 is difﬁcult due to the step (15) used in Theorem 1. The reason is as follows. In order to facilitate proof of monotone improvement by the step (15), we set π in the Rπ,α term in (13) as the optimization argument, as seen in Appendix B.1. Otherwise, proof of monotone improvement is not tractable. However, this setup causes a problem in practical implementation. For practical implementation with function approximation, we will eventually use parameter- ized estimates for the required functions, as we do shortly. For the policy π we will use πθ with parameter θ. Under this situation, let us consider the ratio function again. The ratio function Rπ,α for a given πis a mapping from (S,A) to [0,1), as seen in (9). In the case that π in Rπ,α is the optimization argument policy with parameter θ, we need toDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration deﬁne a mapping from (S,A) to [0,1) for each of all pos- sible values of θ. That is, the output value of Rπθ,α(st,at) depends not only on (st,at) but also on θ. To capture this situation, we need to set the input to the ratio functionRπθ,α as (st,at,θ). However, the dimension of the policy (neural network) parameter θis huge and thus implementation of Rπθ,α(st,at) as a function of (st,at,θ) is not simple. To circumvent this difﬁculty, we need to modify the policy ob- jective function so that it involves a much simpler form for the ratio function for easy implementation. For this, instead of using Rπ,α with πbeing the optimization argument, we use the ratio function Rπold,α for the given current policy πold so that πin the Rπ,α term in the policy objective func- tion is not the optimization argument anymore but ﬁxed as the given current policy πold. With this replacement, we manage to show the following result: Theorem 2 Consider the new objective function for policy improvement ˜Jπold(π(·|st)) in (14), where the ratio function inside the expectation in (14) is the ratio function for the given current policy πold. Suppose that the policy is param- eterized with parameter θ. Then, for parameterized policy πθ, the two objective functions Jπθold (πθ(·|st)) in (13) and ˜Jπθold (πθ(·|st)) in (14) have the same gradient direction for θat θ= θold for all st ∈S, where θold is the parameter of the given current policy πold. Proof. See Appendix B.2. Note that maximizing the new policy objec- tive function (14) is equivalent to minimizing DKL(π(·|st)||exp(Qπold(st,·)/α + Rπold,α(st,·)) = −˜Jπold(π(·|st))/β, and the improved policy ob- tained by maximizing (14) can be expressed as πnew(·|st) ∝exp(Qπold(st,·)/α+ Rπold,α(st,·)). Note also that the new policy objective function ˜Jπold(π(·|st)) is a simpliﬁed version in two aspects. First, we require the ratio function only for the given current policy. Second, the Eat∼q term in (13) disappeared. Note that π in the log π inside the expectation of ˜Jπold(π(·|st)) in (14) is still the optimization argument. However, this is not a problem since we have the parameter θfor the policy in implementation and this parameter will be updated. Now, the ratio function in the policy objective function ˜Jπold(π(·|st)) in (14) in the diverse policy improvement step is the ratio function for the given current policy. Furthermore, the ratio function in (12) in the diverse policy evaluation step is also for the given current policy. Hence, we need to implement and track only the ratio function for the current policy. Now, by Theorems 1 and 2, we can ﬁnd the optimal policy maximizing (3) by iterating the diverse policy evaluation(11) and the diverse policy improvement maximizing ˜Jπold(π(·|st)) in (14). The ﬁnal step to complete the proposed diverse policy itera- tion is learning of the ratio function for the current policy. For this, we deﬁne an estimate functionRα : S×A→ R for the true ratio functionRπ,α of the current policyπand adopt the learning method proposed in the works of Sugiyama et al. (2012); Goodfellow et al. (2014). That is, we ﬁrst deﬁne the objective function for Rα as Jratio(Rα(st,·)) = αEat∼π(·|st)[log Rα(st,at)] + (1 −α)Eat∼q(·|st)[log(1 −Rα(st,at))]. (16) Then, we learn Rα by maximizing the objective Jratio(Rα). Note that for given s, Jratio(Rα(s,·)) =∫ a[c1 log Rα(s,a) + c2 log(1 −Rα(s,a))]da, where c1 = απ, and c2 = (1 −α)q. The integral is maximized when the integrand for each a is maximized. The inte- grand f(Rα(s,a)) = c1 log Rα(s,a)+c2 log(1−Rα(s,a)) is a concave function of Rα(s,a), and its maximum is achieved when Rα(s,a) = c1/(c1 + c2) = απ/(απ+ (1− α)q). Hence, Jratio(Rα) is maximized when Rα(st,at) = απ/(απ+ (1 −α)q), which is exactly the desired ration function shown in (9). Therefore, the ratio function for the current policy can be estimated by maximizing the objective funtion Jratio(Rα). 4.4. Diversity Actor Critic Implementation We use deep neural networks to implement the policyπ, the ratio function Rα, and the diverse value functions Qand V, and their network parameters are θ, η, φ, and ψ, respec- tively. Based on ˜Jπold(π) in (14) and Jratio(Rα) in (16), we provide the practical objective functions: ˆJπ(θ) for the parameterized policy πθ, and ˆJRα(η) for the parameterized ratio function estimator Rα η, given by ˆJπ(θ) = Est∼D,at∼πθ[Qφ(st,at) + αlog Rα η(st,at) −αlog πθ(at|st)], (17) ˆJRα(η) = Est∼D[αEat∼πθ[log Rα η(st,at)] + (1 −α)Eat∼D[log(1 −Rα η(st,at))]], (18) where Ddenotes the replay buffer. Based on the Bellman operator Tπ in (11), we provide the loss functions: ˆLQ(φ) and ˆLV(ψ) for the parameterized value functions Qφ and Vψ, respectively, given by ˆLQ(φ) = E(st,at)∼D [1 2(Qφ(st,at) −ˆQ(st,at))2 ] , (19) ˆLV(ψ) = Est∼D [1 2(Vψ(st) −ˆV(st))2 ] , (20)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Algorithm 1 Diversity Actor Critic Initialize parameter θ, η, ψ, ¯ψ, ξ, φi, i= 1,2 for each iteration do Sample a trajectory τ of length N by using πθ Store the trajectory τ in the buffer D for each gradient step do Sample random minibatch of size M from D Compute ˆJπ(θ), ˆJRα(η), ˆLQ(φi), ˆLV(ψ) from the minibatch θ←θ+ δ∇θ ˆJπ(θ) η←η+ δ∇η ˆJRα(η) φi ←φi −δ∇φi ˆLQ(φi), i= 1,2 ψ←ψ−δ∇ψˆLV(ψ) Update ¯ψby EMA from ψ if α-adpatation is applied then Compute ˆLα(ξ) from the minibatch ξ←ξ−δ∇ξˆLα(ξ) end if end for end for where the target values ˆQand ˆV are deﬁned as ˆQ(st,at) = 1 βrt + γEst+1∼P[V¯ψ(st+1)] (21) ˆV(st) = Eat∼πθ[Qφ(st,at) + αlog Rα η(st,at) −αlog απθ(at|st)] + (1 −α)Eat∼D[log Rα η(st,at) −log απθ(at|st)]. (22) Here, ¯ψ is the network parameter of the target value V¯ψ updated by exponential moving average (EMA) of ψ for stable learning (Mnih et al., 2015). In addition, we use two Q-functions Qφi, i= 1,2 to reduce overestimation bias as proposed in (Fujimoto et al., 2018) and applied in SAC, and each Q-function is updated to minimize their loss function ˆLQ(φi). For the policy and the value function update, the minimum of two Q-functions is used for the policy update. Combining all up to now, we propose the diversity actor- critic (DAC) algorithm summarized as Algorithm 1. Here, note that DAC becomes SAC when α = 1, and becomes standard off-policy RL without entropy regularization when α = 0. When 0 < α <1, we accomplish sample-aware entropy regularization. Detailed implementation of DAC is provided in Appendix C. For DAC, we can consider the technique of SAC proposed in (Haarnoja et al., 2018b) using Q-function only for reducing complexity or automatic tun- ing of βfor balancing the entropy and the return. However, in the case of DAC, bothαand βaffect the entropy term, so both should be tuned simultaneously. 5. α-Adaptation In the proposed sample-aware entropy regularization, the weighting factor αbetween the policy and the sample dis- tribution plays an important role in controlling the ratio between the policy distribution π and the sample action distribution q. However, it is difﬁcult to ﬁnd optimal αfor each environment. To circumvent this αsearch, we propose an automatic adaptation method for αbased on max-min principle widely considered in game theory, robust learning, and decision making problems (Chinchuluun et al., 2008). That is, since we do not know optimal α, an alternative for- mulation is that we maximize the return while maximizing the worst-case sample-aware entropy, i.e., minαH(qπ,α mix). Then, the max-min approach can be formulated as follows: max π Eτ0∼π [∑ t γt(rt + βmin α [H(qπ,α mix) −αc]) ] (23) where cis a control hyperparameter for αadaptation. Note that we learn αto minimize the sample-aware entropy so that the entropy is maintained above a certain level to ex- plore the state and action spaces well. So, the αlearning objective is given by a Lagrangian form. Thus, when the α-learning model is parameterized with parameter ξ, the α learning objective is given by ˆLα(ξ) = Est∼D[H(qπθ,αξ mix )− αξc]. Detailed implementation of α-adaptation is given in Appendix C.1. 6. Experiments In this section, we evaluate the proposed DAC algorithm on various continuous-action control tasks and provide ablation study. We ﬁrst consider the pure exploration performance and then the performance on challenging sparse-reward or delayed Mujoco tasks. The source code of DAC based on Python Tensorﬂow is available at http://github. com/seungyulhan/dac/. 6.1. Pure Exploration Performance For comparison baselines, we ﬁrst considered the state-of- the-art entropy regularization methods: SAC and SAC-Div. SAC-Div is SAC combined with the exploration method in (Hong et al., 2018) that diversiﬁes the policy from the buffer distribution by simply maximizing JSAC(π) + αdD(π||q) for some divergence D, where JSAC(π) is given in (1). For SAC-Div, we considered the KL divergence and the adaptive scale αd with δd = 0 .2, as suggested in (Hong et al., 2018). The case of JS divergence used for SAC-Div is provided in Appendix F. Note that both SAC-Div and DAC contain a divergence term in their objective functions (DAC contains Dα JS, as seen in (6) and (3)), but there is an important difference. SAC-Div adds a single divergence term on the reward sum JSAC(π). So, SAC-Div keeps πDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) Continuous 4-room maze  (b) Number of state visitation (c) State visit histogram at 5k (left) 50k (middle) 300k (right) steps Figure 1: Pure exploration task: Continuous 4-room maze away from q, but does not guide learning of the policy to visit states on which the divergence between π and q is large. On the contrary, DAC contains the divergence term Dα JS as an intrinsic reward at each time step inside the reward sum of J(π), as seen in (3) and (6). Hence, DAC not only keeps πaway from qbut also learns a policy to visit states on which the divergence between π and q is large, to have large J(π), so that more actions different from q are possible. This situation is analogous to the situation of SAC in which the entropy is included as an intrinsic reward inside the sum of JSAC(π), as seen in (1), and hence for large JSAC(π), SAC learns a policy to visit states on which the policy action entropy is large. In addition to SAC and SAC-Div, we considered the recent high-performance state- based exploration methods: random network distillation (RND) (Burda et al., 2018) and MaxEnt(State) (Hazan et al., 2019). RND explores rare states by adding an intrinsic reward based on model prediction error, and MaxEnt(State) explores rare states by using a reward functional based on the entropy of state mixture distribution. Detailed simulation setup is provided in Appendix D. In order to see the pure exploration performance of DAC (α = 0 .5 is used), we considered state visitation on a 100 ×100 continuous 4-room maze. The maze environment was designed by modifying a continuous grid map avail- able at https://github.com/huyaoyu/GridMap, and it is shown in Fig. 1(a). State is the (x,y) position of the agent in the maze, action is (dx,dy) bounded by [−1,1] ×[−1,1], and the agent location after action be- comes (x+dx,y +dy). The agent starts from the left lower corner (0.5,0.5) and explores the maze without any reward. Fig. 1(b) shows the number of total different state visita- tions averaged over 30 seeds, where the number of state visitations is obtained based on quantized 1 ×1 squares. Here, the shaded region in the ﬁgure represents one standard deviation (1σ) from the mean. As seen in Fig. 1(b), DAC visited much more states than the other methods, which shows the superior exploration performance of DAC. Fig. 1(c) shows the corresponding state visit histogram of all seeds with 1 ×1 square quantization. Here, as the color of a state becomes brighter, the state is visited more times. It is seen that SAC/SAC-Div rarely visit the right upper room even at 300k time steps, RND and MaxEnt(State) visit the right upper room more than SAC/SAC-Div, and DAC visits the right upper room far earlier and more than the other methods. 6.2. Performance on Sparse-Reward Mujoco Tasks Then, we evaluated the performance on sparse-reward Mu- joco tasks, which have been widely used as difﬁcult en- vironments for RL in many previous studies (Hong et al., 2018; Mazoure et al., 2019; Burda et al., 2018). We con- sidered two versions. One was SparseMujoco, which is a sparse version of Mujoco (Todorov et al., 2012) in OpenAI Gym (Brockman et al., 2016), and the reward is 1 if the agent satisﬁes a certain condition, otherwise 0 (Hong et al., 2018; Mazoure et al., 2019). The other was DelayedMujoco (Zheng et al., 2018; Guo et al., 2018), which has the same state-action spaces as original Mujoco tasks but reward is sparsiﬁed. That is, rewards for Dtime steps are accumu- lated and the accumulated reward sum is delivered to the agent once every D time steps, so the agent receives no reward during the accumulation time. First, we ﬁxed α= 0.5 for DAC and tested DAC on Sparse- Mujoco. The result is shown in Fig. 2, which shows the performance averaged over 10 random seeds. For all per- formance plots, we used deterministic evaluation whichDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) SparseHalfCheetah-v1  (b) SparseHopper-v1  (c) SparseWalker2d-v1  (d) SparseAnt-v1 Figure 2: Performance comparison on Sparse Mujoco tasks generated an episode by deterministic policy for each iter- ation, and the shaded region in the ﬁgure represents one standard deviation (1σ) from the mean. It is seen that DAC has signiﬁcant performance gain over the competitive SAC and SAC-Div baselines. Figs. 3 (a) and (b) show the diver- gence Dα JS(π||q) curve and the corresponding number of discretized state visitation curve, respectively, on Sparse- HalfCheetah shown in Fig. 2(a). (The curves for the other tasks are provided in Appendix E.1. See Appendix E.1 about the discretization.) It is seen in Fig. 3(a) that the diver- gence of DAC is much higher than those of SAC/SAC-Div throughout the learning time. This implies that the policy of DAC choose more diverse actions from the policy distribu- tion far away from the sample action distribution q, so DAC visits more diverse states than the baselines, as seen in Fig. 3(b). Next, we tested DAC on DelayedMujoco and Humanoid- Standup. Note that HumanoidStandup is one of the difﬁcult high-action dimensional Mujoco tasks, so its reward is not sparsiﬁed for test. We considered three cases for αof DAC: α = 0.5, 0.8, and α-adaptation. Fig. 4 shows the result. Again, we can observe signiﬁcant performance improve- ment by DAC over the SAC baselines. We can also observe that the best αdepends on tasks. For example, α= 0.8 is the best for DelayedHalfCheetah, but α = 0.5 is the best for DelayedAnt. Thus, the result shows that α-adaptation method is necessary in order to adapt αproperly for each (a) Divergence Dα JS  (b) Number of state visitation Figure 3: (a) α-skewed JS symmetrization of KLD Dα JS(π||q) with α = 0.5 and (b) the corresponding mean number of state visitation task. Although the proposed α-adaptation in Section 5 is sub-optimal, DAC with our α-adaptation method has top- level performance across all the considered tasks and further enhances the performance in some cases such as Delayed- HalfCheetah and DelayedHopper tasks. We studied more on the α-adaptation proposed in Section 5 and the behavior of sample-awareness over the learning phase. Fig. 5 shows the learning curve of α, DJS(π||q) and the policy entropy H(π), which are intertwined in the DAC objective function as seen in (6). In the case of De- layedHalfCheetah, αincreases to one as time step goes on, and the initially nonzero JS divergence term DJS(π||q) di- minishes to zero as time goes. This means that the sample action distribution is exploited in the early phase of learning, and DAC operates like SAC as time goes. On the other hand, in the case of DelayedHopper, the learned αgradually settle down aroung 0.5, and the JS divergence term DJS(π||q) is non-zero throughout the learning phase. Thus, it is seen that the proposed α-adaptation learns the weighting factor α with a completely different strategy depending on the task, and this leads to better overall performance for each task as seen in Fig. 4. 6.3. Analysis on the Change of q We assumed that the sample action distribution q of the replay buffer Dis ﬁxed for theoretical development and proof of diverse policy iteration in Section 4.3. However, q changes as iteration goes in real situation, so there ex- ist a gap between the assumption and the real situation for DAC. Changing distribution was considered in some pre- vious work. Hazan et al. (2019) considered the change of previous distributions to guarantee convergence, but they still have a common objective function (i.e., the entropy of state distribution dπ induced by policy π) to maximize. In our case, on the other hand, the objective function (3) itself changes over time as q changes, so it is difﬁcult to show convergence with incorporation of the change of q. Thus, we assumed locally ﬁxed qbecause qchanges slowly when the buffer size is large. In order to investigate the impact of the lapse in the assumption and check the robustness ofDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) HumanoidStandup-v1  (b) Del.HalfCheetah-v1  (c) Del.Hopper-v1  (d) Del.Walker2d-v1  (e) Del.Ant-v1 Figure 4: Performance comparison on HumanoidStandup and Delayed Mujoco tasks (A zoomed version of the ﬁgure is available at Figure E.3 in Appendix E.) (a) DelayedHalfCheetah  (b) DelayedHopper-v1 Figure 5: Averaged learning curve for α-adaptation DAC with respect to the change speed of q, we performed an additional study. In the study, we maintained the buffer size of the replay buffer Das N =1000k. Then, instead of using original q, i.e., the sample action distribution of whole D, we now used q′, which is the sample action dis- tribution of the latest N′samples (we call D′) stored in D with N′≤1000k. The smaller N′is, the faster q′changes. Then, with others remaining the same, the ratio objective function ˆJRα(η) and the target value ˆV(st) in DAC were changed to incorporate q′as ˆJRα(η) =Est∼D[αEat∼πθ[log Rα η(st,at)] + (1−α)Eat∼q′[log(1 −Rα η(st,at))]], ˆV(st) =Eat∼πθ[Qφ(st,at) + αlog Rα η(st,at) −αlog απθ(at|st)] + (1−α)Eat∼q′[log Rα η(st,at) −log απθ(at|st)], where st is still drawn from the original buffer Dand only q′considers samples distribution of D′. Hence, st drawn from Dmay not belong to D′used to compute q′. So, we used generalization: To sample actions from q′for arbitrary states in D, we learned q′by using variational auto-encoder. Fig. 6 shows the corresponding performance. As shown, the performance degrades as q′changes faster by decreasing N′from 1000k to 1k (Note that the original DAC is the case when N′=1000k), but the performance is quite robust against the q change speed. Note that the performance is better than SAC even for N′=1k. We provided more results including the max average return table, more ablation study (control coefﬁcient c, entropy coefﬁcient β, and the effect of JS divergence) and the per- Figure 6: Robustness against the change speed of q formance comparison with various state-of-the-art RL algo- rithms in Appendix E. The results there also show that DAC yields top level performance. 7. Conclusion In this paper, we have proposed a sample-aware entropy framework for off-policy RL to overcome the limitation of simple policy entropy regularization. With the sample- aware entropy regularization, we can achieve diversity gain by exploiting sample history in the replay buffer in addition to policy entropy for sample-efﬁcient exploration. For prac- tical implementation of sample-aware entropy regularized RL, we have used the ratio function to make computation of the sample action distribution from the replay buffer unnec- essary, and have proposed the DAC algorithm with conver- gence proof. We have also provided an adaptation method for DAC to automatically control the ratio of the sample action distribution to the policy distribution. Numerical re- sults show that the proposed DAC algorithm signiﬁcantly outperforms other state-of-the-art RL algorithms. 8. Acknowledgement This work was supported in part by the ICT R&D program of MSIP/IITP(2016-0-00563, Research on Adaptive Ma- chine Learning Technology Development for Intelligent Au- tonomous Digital Companion) and in part by Basic Science Research Program through the National Research Founda- tion of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning(NRF2017R1E1A1A03070788).Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration References Achiam, J. and Sastry, S. Surprise-based intrinsic moti- vation for deep reinforcement learning. arXiv preprint arXiv:1703.01732, 2017. Agre, P. and Rosenschein, S. J. Computational theories of interaction and agency. Mit Press, 1996. Baldassarre, G. and Mirolli, M. Intrinsically motivated learning in natural and artiﬁcial systems. Springer, 2013. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based explo- ration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471–1479, 2016. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Ex- ploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Chentanez, N., Barto, A. G., and Singh, S. P. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 1281–1288, 2005. Chinchuluun, A., Pardalos, P. M., Migdalas, A., and Pit- soulis, L. Pareto optimality, game theory and equilibria. Springer, 2008. Cover, T. M. and Thomas, J. A. Elements of Information Theory. Wiley, 2006. Degris, T., White, M., and Sutton, R. S. Off-policy actor- critic. arXiv preprint arXiv:1205.4839, 2012. Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., Wu, Y ., and Zhokhov, P. Openai baselines. https://github. com/openai/baselines, 2017. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015. Fujimoto, S., van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y . Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014. Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. Q-prop: Sample-efﬁcient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016. Gu, S. S., Lillicrap, T., Turner, R. E., Ghahramani, Z., Sch¨olkopf, B., and Levine, S. Interpolated policy gradi- ent: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3846–3855, 2017. Guo, Y ., Oh, J., Singh, S., and Lee, H. Genera- tive adversarial self-imitation learning. arXiv preprint arXiv:1812.00950, 2018. Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Re- inforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018a. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V ., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b. Han, S. and Sung, Y . Dimension-wise importance sam- pling weight clipping for sample-efﬁcient reinforcement learning. International Conference on Machine Learning, 2019. Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Prov- ably efﬁcient maximum entropy exploration. In Interna- tional Conference on Machine Learning, pp. 2681–2691. PMLR, 2019. Hong, Z.-W., Shann, T.-Y ., Su, S.-Y ., Chang, Y .-H., Fu, T.- J., and Lee, C.-Y . Diversity-driven exploration strategy for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 10489–10500, 2018. Houthooft, R., Chen, X., Duan, Y ., Schulman, J., De Turck, F., and Abbeel, P. Vime: Variational information maxi- mizing exploration. In Advances in Neural Information Processing Systems, pp. 1109–1117, 2016. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y ., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y . Exploration in model-based reinforcement learning by empirically estimating learning progress. In Advances in neural information processing systems, pp. 206–214, 2012. Martin, J., Sasikumar, S. N., Everitt, T., and Hutter, M. Count-based exploration in feature space for reinforce- ment learning. arXiv preprint arXiv:1706.08090, 2017. Mazoure, B., Doan, T., Durand, A., Hjelm, R. D., and Pineau, J. Leveraging exploration in off-policy algorithms via normalizing ﬂows. arXiv preprint arXiv:1905.06893, 2019. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, 2015. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Bridging the gap between value and policy based rein- forcement learning. In Advances in Neural Information Processing Systems, pp. 2775–2785, 2017a. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Trust-pcl: An off-policy trust region method for continu- ous control. arXiv preprint arXiv:1707.01891, 2017b. Nielsen, F. On the jensen–shannon symmetrization of dis- tances relying on abstract means. Entropy, 21(5):485, 2019. O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V . Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised predic- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16–17, 2017. Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y ., Chen, X., Asfour, T., Abbeel, P., and Andrychow- icz, M. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017. Puterman, M. L. and Brumelle, S. L. On the convergence of policy iteration in stationary dynamic programming. Mathematics of Operations Research, 4(1):60–69, 1979. Rawlik, K., Toussaint, M., and Vijayakumar, S. On stochas- tic optimal control and reinforcement learning by ap- proximate inference. In Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013. Santos, M. S. and Rust, J. Convergence properties of policy iteration. SIAM Journal on Control and Optimization, 42 (6):2094–2115, 2004. Schulman, J., Chen, X., and Abbeel, P. Equivalence be- tween policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017a. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. Strehl, A. L. and Littman, M. L. An analysis of model- based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309– 1331, 2008. Sugiyama, M., Suzuki, T., and Kanamori, T. Density ratio estimation in machine learning. Cambridge University Press, 2012. Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. The MIT Press, Cambridge, MA, 1998. Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O. X., Duan, Y ., Schulman, J., DeTurck, F., and Abbeel, P. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural informa- tion processing systems, pp. 2753–2762, 2017. Todorov, E. General duality between optimal control and estimation. In 2008 47th IEEE Conference on Decision and Control, pp. 4286–4292. IEEE, 2008. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con- ference on, pp. 5026–5033. IEEE, 2012. Toussaint, M. Robot trajectory optimization using approxi- mate inference. In Proceedings of the 26th annual inter- national conference on machine learning, pp. 1049–1056. ACM, 2009. Wang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. Sample efﬁ- cient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016. Watkins, C. J. and Dayan, P. Q-learning. Machine Learning, 8(3):279–292, 1992. Wu, Y ., Mansimov, E., Grosse, R. B., Liao, S., and Ba, J. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5279–5288, 2017.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Zheng, Z., Oh, J., and Singh, S. On learning intrinsic rewards for policy gradient methods. In Advances in Neural Information Processing Systems, pp. 4644–4654, 2018. Ziebart, B. D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy . PhD thesis, ﬁgshare, 2010. Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement learning. 2008.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration A. A Simple Example of Efﬁciency of Sample-Aware Entropy Maximization Here, we provide a toy example showing the effectiveness of maximizing the sample-aware entropy deﬁned as the entropy of a mixture distribution qπ,α mix = απ+ (1 −α)q, where qis the sample action distribution of the replay buffer. For this simple toy example, we consider a discrete MDP case in order to show the intuition of sample-aware entropy maximization. Let us consider a simple 1-step MDP in which s0 is the unique initial state, there exist Na actions ( A = {A1,··· ,ANa}), s1 is the terminal state, and r is a deterministic reward function. Then, there exist Na state- action pairs in total. Let us assume that we already have Na −1 state-action samples in the replay buffer as R = {(s0,A1,r(s0,A1)),··· ,(s0,ANa−1,r(s0,ANa−1))}. In order to estimate the Q-function for all state-action pairs, the policy should sample the last action ANa (Then, we can reuse all samples inﬁnitely to estimate Q). Here, we will compare two exploration methods. 1) First, if we consider the simple entropy maximization, the policy that maximizes its entropy will choose all actions with equal probability 1/Na (uniformly). Then, Na samples should be taken on average by the policy to visit the action ANa. 2) Second, consider the sample-aware entropy maximization. Here, the sample action distribution qin the buffer becomes q(a0|s0) = 1 /(Na −1) for a0 ∈ {A1,··· ,ANa−1}and q(ANa|s0) = 0 , the mixture distribution becomes qπ,α mix = απ+(1 −α)q, and we set α= 1/Na. Then, the policy that maximizes the sample-aware entropy is given byπ(ANa|s0) = 1 because this policy makes qπ,α mix uniform and the sample-aware entropy is maximized. In this case, we only need one sample to visit the action ANa. In this way, the proposed sample-aware entropy maximization can enhance sample-efﬁciency for exploration by using the previous sample distribution and choosing a proper α. With this motivation, we propose the sample-aware entropy regularization for off-policy RL and an α-adaptation method.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration B. Proofs B.1. Proof of Theorem 1 To prove Theorem 1, we ﬁrst provide two lemmas. For a ﬁxed policy π, Qπ can be estimated by repeating the Bellman backup operator, as stated in Lemma 1 below. Lemma 1 is based on usual policy evaluation but has a new ingredient of the ratio function in the proposed sample-aware entropy case. Lemma 1 (Diverse Policy Evaluation) Deﬁne a sequence of diverse Q-functions as Qk+1 = TπQk, k≥0, where πis a ﬁxed policy and Q0 is a real-valued initial Q. Assume that the action space is bounded, and Rπ,α(st,at) ∈(0,1) for all (st,at) ∈S×A . Then, the sequence {Qk}converges to the true diverse state-action value Qπ. Proof. Let rπ,t := 1 βrt + γEst+1∼P[Eat+1∼π[αlog Rπ,α(st+1,at+1) − αlog απ(at+1|st+1)] + (1 − α)Eat+1∼q[log Rπ,α(st+1,at+1) −log απ(at+1|st+1)]]. Then, we can rewrite the modiﬁed Bellman equation (11) into the standard Bellman equation form for the true Qπ as follows: TπQ(st,at) = rπ,t + γEs+1∼P,a t+1∼π[Q(st+1,at+1)] (B.1) Under the assumption of a bounded action space and Rπ,α ∈(0,1), the reward rπ,t is bounded and the convergence is guaranteed as the usual policy evaluation (Sutton & Barto, 1998; Haarnoja et al., 2018a). □ Lemma 2 is about diverse policy improvement. Lemma 2 (Diverse Policy Improvement) Let πnew be the updated policy obtained by solving πnew = arg max π Jπold(π), where Jπold(π) is given in (13). Then, Qπnew(st,at) ≥Qπold(st,at), ∀(st,at) ∈S×A . Proof. Since πnew = arg max π Jπold(π), we have Jπold(πnew) ≥Jπold(πold). Expressing Jπold(πnew) and Jπold(πold) by using the deﬁnition of Jπold(π) in (13), we have Jπold(πnew(·|st)) = Eat∼πnew[Qπold(st,at) + αlog Rπnew,α(st,at) −αlog απnew(at|st)] + (1 −α)Eat∼q[log Rπnew,α(st,at) −log απnew(at|st)] ≥Jπold(πold(·|st)) = Eat∼πold[Qπold(st,at) + αlog Rπold,α(st,at) −αlog απold(at|st)] + (1 −α)Eat∼q[log Rπold,α(st,at) −log απold(at|st)] = Vπold(st) (B.2) by the deﬁnition of Vπ(st) in (12). Then, based on (B.2), we obtain the following inequality: Qπold(st,at) = 1 βrt + γEst+1∼P[Vπold(st+1)] (a) ≤ 1 βrt + γEst+1∼P{Eat+1∼πnew[ Qπold(st+1,at+1)   = 1 βrt+1+γEst+2∼P[Vπold(st+2)] +αlog Rπnew,α(st+1,at+1) −αlog απnew(at+1|st+1)] + (1−α)Eat+1∼q[log Rπnew,α(st+1,at+1) −log απnew(at+1|st+1)]} ... ≤Qπnew(st,at), for each (st,at) ∈S×A , (B.3) where Inequality (a) is obtained by applying Inequality(B.2) on Vπold(st+1), and Qπold(st+1,at+1) in the RHS of Inequality (a) is expressed as 1 βrt+1 + γEst+2∼P[Vπold(st+2)] and Inequality (B.2) is then applied on Vπold(st+2); this procedure is repeated to obtain Inequality (B.3). By (B.3), we have the claim. This concludes proof. □Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Now, we prove Theorem 1 based on the previous lemmas. Theorem 1 (Diverse Policy Iteration) By repeating iteration of the diverse policy evaluation and the diverse policy improvement, any initial policy converges to the optimal policy π∗s.t. Qπ∗ (st,at) ≥Qπ′ (st,at), ∀π′∈Π, ∀(st,at) ∈ S×A . Also, such π∗achieves maximum J, i.e., Jπ∗(π∗) ≥Jπ(π) for any π∈Π. Proof. Let Π be the space of policy distributions and let {πi,i = 0,1,2,···| πi ∈Π}be a sequence of policies generated by the following recursion: πi+1 = arg max π∈Π Jπi(π) with an arbitrary initial policy π0, (B.4) where the objective function Jπi(π) is deﬁned in (13). Proof of convergence of the sequence {πi,i = 0,1,2,···} to a local optimum is for arbitrary state space S. On the other hand, for proof of convergence of {πi,i = 0,1,2,···} to the global optimum, we assume ﬁnite MDP, as typically assumed for convergence proof in usual policy iteration (Sutton & Barto, 1998). For any state-action pair (s,a) ∈S×A , each Qπi(s,a) is bounded due to the discount factor γ(see (8)), and the sequence {Qπi(s,a),i = 0,1,2,···} is monotonically increasing by Lemma 2. Now, consider two terms Jπi+1 (πi+1(·|s)) and Jπi(πi+1(·|s)), which are expressed by the deﬁnition of Jπold(π) in (13) as follows: Jπi+1 (πi+1(·|s)) = β{Ea∼πi+1 [Qπi+1 (s,a) + α(log Rπi+1,α(s,a) −log απi+1(a|s))] + (1 −α)Ea∼q[log Rπi+1,α(s,a) −log απi+1(a|s)]} (B.5) Jπi(πi+1(·|s)) = β{Ea∼πi+1 [Qπi(s,a) + α(log Rπi+1,α(s,a) −log απi+1(a|s))] + (1 −α)Ea∼q[log Rπi+1,α(s,a) −log απi+1(a|s)]}. (B.6) Note in (B.5) and (B.6) that all the terms are the same forJπi+1 (πi+1(·|s)) and Jπi(πi+1(·|s)) except βEa∼πi+1 [Qπi+1 (s,a)] in Jπi+1 (πi+1(·|s)) and βEa∼πi+1 [Qπi(s,a)] in Jπi(πi+1(·|s)). Because {Qπi(s,a),i = 0,1,2,···} is monotonically increasing by Lemma 2, comparing (B.5) and (B.6) yields Jπi+1 (πi+1(·|s)) ≥Jπi(πi+1(·|s)). (B.7) Furthermore, we have for any s∈S, Jπi(πi+1(·|s)) ≥Jπi(πi(·|s)) (B.8) by the deﬁnition of πi+1 in (B.4). Combining (B.7) and (B.8), we have Jπi+1 (πi+1(·|s)) ≥Jπi(πi+1(·|s)) ≥Jπi(πi(·|s)) (B.9) for any state s ∈S. Therefore, the sequence {Jπi(πi(·|s)),i = 0,1,2,···} is monotonically increasing for any s ∈S. Furthermore, note from (B.5) that Jπi(πi(·|s)) is bounded for all i, because the Q-function and the entropy of the mixture distribution are bounded. (Note that the RHS of (B.5) except the term Ea∼πi+1 [Qπi+1 (s,a)] is nothing but the entropy of the mixture distribution H(qπi+1,α mix ). Please see (10) for this.) Note that Jπi(πi), which is obtained by setting πold = πi and π = πi in (13), is nothing but J(πi) with the desired original J deﬁned in (3). Hence, by (B.9) and the boundedness of the sequence {Jπi(πi)}, convergence to a local optimum of J by the sequence {πi,i = 0,1,2,···} is guaranteed by the monotone convergence theorem. Now, consider convergence to the global optimum. By the monotone convergence theorem,{Qπi(s,a),i = 0,1,2,···} and {Jπi(πi(·|s)),i = 0,1,2,···} pointwisely converge to their limit functionsQ∗: S×A→ R and J∗: S→ R, respectively. Here, note that J∗(s) ≥Jπi(πi(·|s)) for any i because the sequence {Jπi(πi(·|s)),i = 0 ,1,2,···} is monotonically increasing by (B.9). By the deﬁnition of pointwise convergence, for any s∈S, for any ϵ> 0, there exists a sufﬁciently large N(s)(> 0) depending on ssuch that Jπi(πi(·|s)) ≥J∗(s) −ϵ(1−γ) γ for all i ≥N(s). When Sis ﬁnite, we set ¯N = maxsN(s). Then, we have Jπi(πi(·|s)) ≥J∗(s) −ϵ(1 −γ) γ , ∀s∈S, ∀i≥ ¯N (B.10)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Furthermore, we have Jπi(πi(·|s)) ≥Jπi(π′(·|s)) −ϵ(1 −γ) γ , ∀s∈S, ∀i≥ ¯N, ∀π′∈Π. (B.11) (B.11) is valid by the following reason. Suppose that (B.11) is not true. Then, there exist some s′∈S and some π′∈Π such that Jπi(π′(·|s′)) (b) > Jπi(πi(·|s′)) + ϵ(1 −γ) γ (c) ≥J∗(s′), (B.12) where Inequality (b) is obtained by negating (B.11) and Inequality (c) is obtained by (B.10). Moreover, we have Jπi+1 (πi+1(·|s′)) (d) ≥Jπi(πi+1(·|s′)) = max π Jπi(π(·|s′)) (e) ≥Jπi(π′(·|s′)), (B.13) where Inequality (d) is valid due to (B.7) and Inequality (e) is valid by the deﬁnition of πi+1 given in (B.4). Combining (B.12) and (B.13) yields Jπi+1 (πi+1(·|s′)) ≥Jπi(πi+1(·|s′)) ≥Jπi(π′(·|s′)) >Jπi(πi(·|s′)) + ϵ(1 −γ) γ ≥J∗(s′). (B.14) However, this contradicts to the fact that J∗(s′) is the limit of the monotone-increasing sequence Jπi(πi(·|s′)). Therefore, (B.11) is valid. Based on (B.11), we have the following inequality regarding Qπi(st,at): For any (st,at), for all i≥ ¯N, Qπi(st,at) = 1 βrt + γEst+1∼P[Vπi(st+1)] = 1 βrt + γEst+1∼P[Jπi(πi(·|st+1))] (f) ≥ 1 βrt + γEst+1∼P [ Jπi(π′(·|st+1)) −ϵ(1 −γ) γ ] , ∀π′∈Π, (g) = 1 βrt + γEst+1∼P{Eat+1∼π[Qπi(st+1,at+1) + αlog Rπ′,α(st+1,at+1) −αlog απ′(at+1|st+1)] + (1 −α)Eat+1∼q[log Rπ′,α(st+1,at+1) −log απ′(at+1|st+1)]}−ϵ(1 −γ) ... (h) ≥Qπ′ (st,at) −ϵ, ∀π′∈Π, (B.15) where Inequality (f) is valid due to (B.11); Equality (g) is obtained by explicitly expressing Jπi(π′) using (13); we express Qπi(st+1,at+1) as Qπi(st+1,at+1) = 1 βrt+1 + γEst+2∼P[Vπi(st+2)] and repeat the same procedure on Vπi(st+2) = Jπi(πi(·|st+2)); and we obtain the last Inequality (h) by repeating this iteration. Here, the resulting constant term is −ϵ(1−γ)−ϵγ(1−γ)−ϵγ2(1−γ)−··· = −ϵ, as shown in the RHS of Inequality (g). Note that the uniformity condition ”∀s∈ S” in the Inequality (B.11) is required because we need to express Jπi(πi(·|st+1)), Jπi(πi(·|st+2)), Jπi(πi(·|st+3)), ··· in terms of Jπi(π′(·|st+1)), Jπi(π′(·|st+2)), Jπi(π′(·|st+3)), ···, respectively, by using (B.11) in the above recursive procedure and the support of each element of the sequence st+1, st+2, st+3,··· is Sin general. Since ϵ> 0 is arbitrary in the above, by taking i→∞ on both sides of (B.15), we have Qπ∞(s,a) ≥Qπ′ (s,a), ∀π′∈Π, ∀(s,a) ∈S×A (B.16) since the sequence {Qπi(s,a),i = 0,1,2,···} is monotonically increasing. Now, let us compare Jπ′(π′(·|s)) and Jπ∞(π′(·|s)). These two terms can be expressed in similar forms to (B.5) and (B.6), respectively. Then, only Qπ∞(s,a) and Qπ′ (s,a) are different in the expressed forms. Comparing Jπ′(π′(·|s)) and Jπ∞(π′(·|s)) as we did for (B.7), we have Jπ∞(π′(·|s)) ≥Jπ′(π′(·|s)) (B.17)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration due to Inequality (B.16). In addition, we have Jπi(πi(·|s)) ≥Jπi(π′(·|s)) −ϵ(1−γ) γ due to (B.11). Since ϵ> 0 is arbitrary, by taking i→∞, we have Jπ∞(π∞(·|s)) ≥Jπ∞(π′(·|s)). (B.18) Finally, combining (B.17) and (B.18) yields Jπ∞(π∞(·|s)) ≥Jπ∞(π′(·|s)) ≥Jπ′(π′(·|s)), ∀π′∈Π, ∀s∈S. (B.19) Recall that Jπ(π), which is obtained by setting πold = πand π= πin (13), is nothing but J(π) of the desired original J deﬁned in (3). Therefore, π∞is the optimal policy π∗maximizing J, and {πi}converges to the optimal policy π∗. This concludes the proof. □ Remark: Note that what we actually need for proof of convergence to the global optimum is the uniform convergence of Jπi(πi(·|s)) →J∗(s) as functions of sto obtain (B.11). The ﬁnite state assumption is one sufﬁcient condition for this. In order to guarantee convergence to global optimum in non-ﬁnite MDP (e.g. continuous state-space), we need more assumption as considered in (Puterman & Brumelle, 1979; Santos & Rust, 2004). Here, we do not further detail. In this paper, we just consider function approximation for the policy and the value functions to implement the diverse policy iteration in continuous state and action spaces, based on the convergence proof in ﬁnite MDP. B.2. Proof of Theorem 2 Remark: We deﬁned Jπold(π) as (13), which is restated below: Jπold(π(·|st)) := β{Eat∼π[Qπold(st,at) + α(log Rπ,α(st,at) −log απ(at|st))] + (1 −α)Eat∼q[log Rπ,α(st,at) −log απ(at|st)]}, (B.20) where πin the Rπ,α terms inside the expectations is the optimization argument. As mentioned in the main part of the paper, this facilitates proof of Lemma 2 and proof of Theorem 1, especially in Steps (B.2), (B.3), (B.5), (B.6), and (B.7). However, as explained in the main part of the paper, implementing the function Rπ,α(st,at) with optimization argument πis difﬁcult. Hence, we replaced Jπold(π) with ˜Jπold(π) in (14) by considering the ratio function Rπold,α(st,at) for only the current policy πold. Now, we prove the gradient equivalence of Jπold(π) and ˜Jπold(π) at θ= θold for parameterized policy πθ. Lemma 3 For the ratio function Rπ,α(st,at) deﬁned in (9), we have the following: log Rπ,α(st,at) −log απ(at|st) = log(1 −Rπ,α(st,at)) −log((1 −α)q(at|st)) (B.21) Proof. From the deﬁnition of the ratio function: Rπ,α(st,at) = απ(at|st) απ(at|st) + (1−α)q(at|st), (B.22) we have 1 −Rπ,α(st,at) = (1 −α)q(at|st) απ(at|st) + (1−α)q(at|st). (B.23) Hence, we have log 1 απ(at|st) + (1−α)q(at|st) = log Rπ,α(st,at) −log(απ(at|st)) (B.24) = log(1 −Rπ,α(st,at)) −log((1 −α)q(at|st)). (B.25) This concludes proof. □ Theorem 2 Consider the new objective function for policy improvement ˜Jπold(π(·|st)) in (14), where the ratio function inside the expectation in (14) is the ratio function for the given current policy πold. Suppose that the policy is parameterized with parameter θ. Then, for parameterized policy πθ, the two objective functions Jπθold (πθ(·|st)) and ˜Jπθold (πθ(·|st)) have the same gradient direction for θat θ= θold for all st ∈S, where θold is the parameter of the given current policy πold.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration Proof. With the parameterized πθ, the two objective functions are expressed as Jπθold (πθ(·|st)) = β(Eat∼πθ[Qπθold(st,at) + αlog Rπθ,α(st,at) −αlog απθ(at|st)] + (1 −α)Eat∼q[log Rπθ,α(st,at) −log απθ(at|st)]) (1) = β(Eat∼πθ[Qπθold(st,at) + αlog Rπθ,α(st,at) −αlog απθ(at|st)] + (1 −α)Eat∼q[log(1 −Rπθ,α(st,at)) −log(1 −α)q(at|st)]) (B.26) ˜Jπθold (πθ(·|st)) = βEat∼πθ[Qπθold(st,at) + αlog Rπθold,α(st,at) −αlog πθ(at|st)], (B.27) where Step (1) is valid by Lemma 3. Comparing (B.26) and (B.27), we can ignore the common Qπθold and log πθ terms, and the constant terms w.r.t. θthat yield zero gradient in (B.26) and (B.27). Therefore, we only need to show ∇θ{αEat∼πθ[log Rπθ,α] + (1−α)Eat∼q[log(1 −Rπθ,α)]}= ∇θEat∼πθ[αlog Rπθold,α] (B.28) at θ= θold. The gradient of the left-hand side (LHS) in (B.28) at θ= θold is expressed as ∇θ{αEat∼πθ[log Rπθ,α] + (1−α)Eat∼q[log(1 −Rπθ,α)]} = ∇θ { α ∫ at πθlog Rπθ,αdat + (1 −α) ∫ at qlog(1 −Rπθ,α)dat } = α ∫ at (∇θπθ) logRπθ,αdat + α ∫ at πθ(∇θlog Rπθ,α)dat + (1 −α) ∫ at q∇θlog(1 −Rπθ,α)dat = α ∫ at (∇θπθ)|θ=θold log Rπθ,α|θ=θolddat + α ∫ at πθ(∇θlog Rπθ,α)dat + (1 −α) ∫ at q∇θlog(1 −Rπθ,α)dat = α∇θ ∫ at πθlog Rπθold,αdat + α ∫ at πθ(∇θlog Rπθ,α)dat + (1 −α) ∫ at q∇θlog(1 −Rπθ,α)dat = ∇θEat∼πθ[αlog Rπθold,α] + αEat∼πθ[∇θlog Rπθ,α] + (1−α)Eat∼q[∇θlog(1 −Rπθ,α)]. (B.29) Here, the gradient of the last two terms in the RHS of (B.29) becomes zero, as shown below: αEat∼πθ[∇θlog Rπθ,α] + (1−α)Eat∼q[∇θlog(1 −Rπθ,α)] = αEat∼πθ [∇θRπθ,α Rπθ,α ] + (1 −α)Eat∼q [∇θ(1 −Rπθ,α) (1 −Rπθ,α) ] = αEat∼πθ [∇θRπθ,α Rπθ,α ] −(1 −α)Eat∼q [ ∇θRπθ,α (1 −Rπθ,α) ] = αEat∼πθ [∇θRπθ,α Rπθ,α ] −(1 −α)Eat∼q [απθ + (1 −α)q (1 −α)q ·∇θRπθ,α ] = αEat∼πθ [∇θRπθ,α Rπθ,α ] −Eat∼q [απθ + (1 −α)q q ·∇θRπθ,α ] (2) = αEat∼πθ [∇θRπθ,α Rπθ,α ] −Eat∼πθ [πθ + (1 −α)q πθ ·∇θRπθ,α ] = αEat∼πθ [∇θRπθ,α Rπθ,α ] −αEat∼πθ [πθ + (1 −α)q απθ ·∇θRπθ,α ] = αEat∼πθ [∇θRπθ,α Rπθ,α ] −αEat∼πθ [∇θRπθ,α Rπθ,α ] = 0, (B.30) where we used an importance sampling technique (i.e., measure change)Eat∼q[f(st,at)] = Eat∼πθ [ q(at|st) πθ(at|st) f(st,at) ] for Step (2). By (B.29) and (B.30), Jπθold (πθ(·|st)) and Jπθold (πθ(·|st)) have the same gradient at θ= θold. This concludes proof. □Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration C. Detailed DAC Implementation We deﬁned the target value ˆV(st) = Eat∼πθ[Qφ(st,at) + αlog Rα η(st,at) − αlog απθ(at|st)] + (1 − α)Eat∼D[log Rα η(st,at) −log απθ(at|st)] in (22). However, the probability of πfor actions sampled from Dcan have high variance, so we clip the term inside the expectation over at ∼D by action dimension for stable learning. Thus, the ﬁnal target value is given by ˆV(st) = Eat∼πθ[Qφ(st,at) + αlog Rα η(st,at) −αlog απθ(at|st)] + (1 −α)Eat∼D[clip(log Rα η(st,at) −log απ(at|st); −d,d)], (C.1) where d= dim(A) is the action dimension and clip(x; −d,d) is the clipping function to ﬁt into the range [−d,d]. We use (C.1) for actual implementation. In addition, we require Rπθ,α ∈(ϵ,1 −ϵ) in the proofs of Theorems 1 and 2 so thatlog Rπθ,α and log(1 −Rπθ,α) appearing in the proofs do not diverge. For practical implementation, we clipped the ratio function Rα as (ϵ,1 −ϵ) for small ϵ> 0 since some qvalues can be close to zero before the replay buffer stores a sufﬁcient amount of samples. However,πis always non-zero since we consider Gaussian policy. To compute the gradient of ˆJπ(θ) in (17), we use the reparameterization trick proposed by (Kingma & Welling, 2013; Haarnoja et al., 2018a). Note that the policy action at ∼πθ is the output of the policy neural network with parameter θ. So, it can be viewed as at = fθ(ϵt; st), where f is a function parameterized by θand ϵt is a noise vector sampled from spherical normal distribution N. Then, the gradient of ˆJπ(θ) is represented as ∇θ ˆJπ(θ) = Est∼D,ϵt∼N[∇a(Qφ(st,a) + αlog Rα η(st,a) −αlog πθ(a|st))|a=fθ(ϵt;st)∇θfθ(ϵt; st) −α(∇θlog πθ)(fθ(ϵt; st)|st)]. C.1. Detailed Implementation of the α-Adaptation In order to learn α, we parameterize αas a function of st using parameter ξ, i.e., α= αξ(st), and implement αξ(st) with a neural network. Then, ξis updated to minimize the following loss function of αobtained from (23): ˆLα(ξ) = Est∼D[H(qπθ,αξ mix ) −αξc] (C.2) In the αadaptation case, all the updates for diverse policy iteration are the same except that αis replaced with αξ(st). The gradient of ˆLα(ξ) with respect to ξcan be estimated as below: ∇ξˆLα(ξ) = ∇ξEst∼D[H(qπθ,αξ mix ) −αξc] =∇ξEst∼D[αξEat∼πθ[−log(αξπθ + (1 −αξ)q) −c] + (1−αξ)Eat∼q[−log(αξπθ + (1 −αξ)q)]] =Est∼D[(∇ξαξ)(Eat∼πθ[−log(αξπθ + (1 −αξ)q) −c] −Eat∼q[−log(αξπθ + (1 −αξ)q)])] + Est∼D[αξEat∼πθ[−∇ξlog(αξπθ + (1 −αξ)q)] + (1−αξ)Eat∼q[−∇ξlog(αξπθ + (1 −αξ)q)]] =Est∼D[(∇ξαξ)(Eat∼πθ[−log αξπθ + logRπθ,αξ −c] −Eat∼q[log Rπθ,αξ −log αξπθ])] + Est∼D [∫ at∈A (αξπθ + (1 −αξ)q)[−∇ξlog(αξπθ + (1 −αξ)q)    =0 ] ] =Est∼D[(∇ξαξ)(Eat∼πθ[−log αξπθ + logRπθ,αξ −c] −Eat∼q[log Rπθ,αξ −log αξπθ])] (C.3) Note that Rπθ,αξ can be estimated by the ratio function Rαξ η . Here, we use the same clipping technique as used in (C.1) for the last term of(C.3). For α-adaptation, we used regularization forαlearning and restricted the range ofαas 0.5 ≤α≤0.99 for αadaptation in order to maintain a certain level of entropy regularization and prevent saturation of Rα η.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration D. Simulation Setup We here provide the detailed simulation setup of DAC, SAC baselines, RND, and MaxEnt(State). For fair comparison, we use the common hyperparameter setup for DAC and SAC baselines except for the parts regarding entropy or divergence. The hyperparameter setup basically follows the setup in (Haarnoja et al., 2018a), which is given by Table D.1. Here, the entropy coefﬁcient βis selected based on the ablation study in Section F. For the policy space Π, we considered a Gaussian policy set widely considered in usual continuous RL. Also, we provide Table D.2, which shows the environment description, the corresponding entropy control coefﬁcient β, threshold for sparse Mujoco tasks, and reward delay Dfor delayed Mujoco tasks. SAC / SAC-Div DAC Learning rate δ 3 ·10−4 Discount factor γ 0.99 (0.999 for pure exploration) Horizon N 1000 Mini-batch size M 256 Replay buffer length 106 Smoothing coefﬁcient of EMA for V¯ψ 0.005 Optimizer Adam Num. of hidden layers (all networks) 2 Size of hidden layers (all networks) 256 Policy distribution Independent Gaussian distribution Activation layer ReLu Output layer for πθ, Qφ, Vψ , V¯ψ Linear Output layer for αξ, Rα η · Sigmoid Regularize coefﬁcient for αξ · 10−3 Control coefﬁcient cfor α-adaptation · − 2.0 ·dim(A) Table D.1: Hyperparamter setup State dim. Action dim. β Threshold SparseHalfCheetah-v1 17 6 0.02 5.0 SparseHopper-v1 11 3 0.04 1.0 SparseWalker2d-v1 17 6 0.02 1.0 SparseAnt-v1 111 8 0.01 1.0 State dim. Action dim. β Delay D HumanoidStandup-v1 376 17 1 · DelayedHalfCheetah-v1 17 6 0.2 20 DelayedHopper-v1 11 3 0.2 20 DelayedWalker2d-v1 17 6 0.2 20 DelayedAnt-v1 111 8 0.2 20 Table D.2: State and action dimensions of Mujoco tasks and the corresponding β In addition, we also compared the performance of DAC to two recent state-based exploration methods, RND (Burda et al., 2018) and MaxEnt(State) (Hazan et al., 2019), in Section 6. State-based exploration methods aim to ﬁnd rare states to enhance exploration performance. In order to explore rare states, RND adds an intrinsic reward based on prediction error rint t = ||ˆf(st+1) −f(st+1)||2 to the extrinsic reward rext t so that the total reward becomes rt = rext t + cintrint t , where ˆf is a prediction network and f is a randomly ﬁxed target network. Then, the agent goes to rare states since rare states have higher prediction errors. For our simulation, we considered MLP with 2 ReLu hidden layers of size 256 with 20-dimensional output for both networks of RND, and we used cint = 5 that performed well for considered tasks. On the other hand, MaxEnt(State) aims to maximize the entropy of state mixture distribution H(dπmix ) to explore rareDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration states, where dπ is the state distribution of a trajectory generated from π. In order to do that, MaxEnt(State) uses the reward rMaxEnt(State)(s) = −(log dπmix(s) + cs), where cs is a smoothing constant. MaxEnt(State) mainly considers large or continuous state space, so dπmix is computed by projection/Kernel density estimation. Then, MaxEnt(State) explores the state space better than a simple random policy on various tasks in continuous state spaces. For our simulation, we use previous 100K states stored in the buffer to estimate dπmix. Note that MaxEnt(State) is originally designed for pure exploration, but we use its reward functional as an intrinsic reward in order to learn sparse-rewarded tasks. In this case, we found that cint = 0.02 worked well for the considered tasks. For both RND and MaxEnt(State), we basically consider the same simulation setup with DAC and SAC baselines but use Gaussian policy with ﬁxed standard deviation σ= 0.3 for both RND and MaxEnt(State) to make fair comparison between action-based exploration and state-based exploration.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration E. More Results on Performance Comparison We provide more numerical results in this section. In Appendix E.1, we provide the remaining learning curves and max average return tables for the performance comparisons in the main paper. In Appendix E.2, we provide the performance comparison between DAC and RND/MaxEnt(State) on SparseMujocot tasks. In Appendix E.3, we compare the DAC with α adaptation to other general RL algorithms on HumanoidStandup and DelayedMujoco tasks. E.1. Performance Comparison with the SAC Baselines In this subsection, we provide more performance plots and tables for the performance comparison between DAC and SAC baselines. Fig. E.1 shows the divergence Dα JS curve (α= 0.5) and Fig. E.2 shows the mean number of discretized state visitation curve for remaining SparseMujoco tasks. Table. E.1 shows the corresponding max average return performance on sparse Mujoco tasks. Fig. E.3 shows the scaled version of the performance plots in Fig. E.2, and Table E.2 shows the corresponding max average return performance. Here, in order to show the tendency of state visitation in Fig. E.2, we discretized the state of each SparseMujoco task. For discretization, we simply consider 2 components of observations for each task: x,y axis position for SparseAnt, and x,z axis position for the other SparseMujoco tasks. We discretize the position by setting the grid spacing per axis to 0.01 in the range of (−10,10). For SAC/SAC-Div, the ratio functionRis estimated separately by the same way with DAC. (a) SparseHopper-v1  (b) SparseWalker-v1  (c) SparseAnt-v1 Figure E.1: α-skewed JS symmetrization of KLD Dα JS for DAC and SAC/SAC-Div (a) SparseHopper-v1  (b) SparseWalker2d-v1  (c) SparseAnt-v1 Figure E.2: The number of discretized state visitation on sparse Mujoco tasksDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) HumanoidStandup-v1  (b) Del.HalfCheetah-v1  (c) Del.Hopper-v1 (d) Del.Walker2d-v1  (e) Del.Ant-v1 Figure E.3: Performance comparison on HumanoidStandup and DelayedMujoco tasks DAC (α = 0.5) SAC SAC-Div SparseHalfCheetah 915.90 ±50.71 386.90 ±404.70 394.70 ±405.53 SparseHopper 900.30 ±3.93 823.70 ±215.35 817.40 ±253.54 SparseWalker2d 665.10 ±355.66 273.30 ±417.51 278.50 ±398.23 SparseAnt 935.80 ±37.08 963.80 ±42.51 870.70 ±121.14 Table E.1: Max average return of DAC algorithm and SAC baselines on SparseMujoco tasks DAC (α= 0.5) DAC ( α= 0.8) DAC ( α-adapt.) SAC SAC-Div HumanoidS 202491.81 ±25222.77 170832.05 ±12344.71 197302.37 ±43055.31 167394.36 ±7291.99 165548.76 ±2005.85 Del. HalfCheetah 6071.93±1045.64 6552.06 ±1140.18 7594.70±1259.23 3742.33±3064.55 4080.67 ±3418.07 Del. Hopper 3283.77±112.04 2836.81 ±679.05 3428.18±69.08 2175.31±1358.39 2090.64 ±1383.83 Del. Walker2d 4360.43±507.58 3973.37±273.63 4067.11 ±257.81 3220.92 ±1107.91 4048.11 ±290.48 Del. Ant 4088.12±578.99 3535.72 ±1164.76 4243.19±795.49 3248.43±1454.48 3978.34 ±1370.23 Table E.2: Max average return of DAC algorithms and SAC baselines on HumanoidStandup and DelayedMujoco tasksDiversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration E.2. Comparison to State-based Exploration Methods on Sparse Mujoco Tasks We compared the performance of DAC (α= 0.5) with RND/MaxEnt(State) on SparseMujoco tasks, and the performance of DAC (α-adapt.) with RND/MaxEnt(State) on DelayedMujoco tasks. Fig. E.5 shows the performance learning curve, and the corresponding max average return table in Table E.3. From the results, it is seen that DAC has better performance than RND/MaxEnt(State) on most Sparse/DelayedMujoco tasks. DAC has superiority not only in pure exploration but also in learning sparse rewarded tasks as compared to recent state-based exploration methods. (a) SparseHalfCheetah-v1  (b) SparseHopper-v1 (c) SparseWalker2d-v1  (d) SparseAnt-v1 Figure E.4: Performance comparison to RND/MaxEnt(State) on SparseMujoco tasks DAC (α= 0.5) RND MaxEnt(State) SAC SparseHalfCheetah 915.90±50.71 827.80±85.61 800.20 ±127.11 386.90 ±404.70 SparseHopper 900.30±3.93 648.10±363.75 879.50 ±30.96 823.70 ±215.35 SparseWalker2d 665.10±355.66 663.00 ±356.39 705.30±274.88 273.30±417.51 SparseAnt 935.80±37.08 920.60 ±107.50 900.00 ±70.02 963.80±42.51 DAC (α-adapt.) RND MaxEnt(State) SAC Del.HalfCheetah 7594.70±1259.23 7429.94±1383.75 6823.37 ±882.25 3742.33 ±3064.55 Del.Hopper 3428.18±69.08 2764.06±1220.86 3254.10 ±30.75 2175.31 ±1358.39 Del.Walker2d 4067.11±257.81 3514.97 ±1536.04 4430.61±347.02 3220.92±1107.91 Del.Ant 4243.19±795.49 1361.36±704.69 1246.80 ±323.50 3248.43 ±1454.48 Table E.3: Max average return of DAC, RND, and MaxEnt(State)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) Del.HalfCheetah-v1  (b) Del.Hopper-v1 (c) Del.Walker2d-v1  (d) Del.Ant-v1 Figure E.5: Performance comparison to RND/MaxEnt(State) on DelayedMujoco tasks E.3. Comparison to Recent General RL Algorithms We also compare the performance of DAC with α-adaptation to other state-of-the-art RL algorithms. Here, we consider various on-policy RL algorithms: Proximal Policy Optimization (Schulman et al., 2017b) (PPO, a stable and popular on-policy algorithm), Actor Critic using Kronecker-factored Trust Region (Wu et al., 2017) (ACKTR, actor-critic that approximates natural gradient by using Kronecker-factored curvature), and off-policy RL algorithms: Twin Delayed Deep Deterministic Policy Gradient (Fujimoto et al., 2018) (TD3, using clipped double-Q learning for reducing overestimation); and Soft Q-Learning (Haarnoja et al., 2017) (SQL, energy based policy optimization using Stein variational gradient descent). We used implementations in OpenAI baselines (Dhariwal et al., 2017) for PPO and ACKTR, and implementations in author’s Github for other algorithms. We provide the performance results as Fig. E.6 and Table E.4, and the results show that DAC has the best performance on all considered tasks among the compared recent RL algorithms.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) HumanoidStandup-v1  (b) DelayedHalfCheetah-v1  (c) DelayedHopper-v1 (d) DelayedWalker2d-v1  (e) DelayedAnt-v1 Figure E.6: Performance comparison to recent general RL algorithms DAC PPO ACKTR SQL TD3 SAC HumanoidS 197302.37 ±43055.31 160211.90 ±3268.37 109655.30 ±49166.15 138996.84 ±33903.03 58693.87 ±12269.93 167394.36 ±7291.99 Del. HalfCheetah 7594.70 ±1259.23 2247.92 ±640.69 3295.30 ±824.05 5673.34 ±1241.30 4639.85 ±1393.95 3742.33 ±3064.55 Del. Hopper 3428.18 ±69.08 2740.15 ±719.63 2864.81 ±1072.64 2720.32 ±127.71 2276.58 ±1471.66 2175.31 ±1358.39 Del. Walker2d 4067.11 ±257.81 2859.27 ±1938.50 1927.32 ±1647.49 3323.63 ±503.18 3736.72 ±1806.37 3220.92 ±1107.91 Del. Ant 4243.19 ±795.49 1224.33 ±521.62 2956.51 ±234.89 6.59 ±16.42 904.99 ±1811.78 3248.43 ±1454.48 Table E.4: Max average return of DAC and other RL algorithms F. More Ablation Studies In this section, we provide detailed ablation studies on the DelayedMucoco tasks. First, we focus on the DelayedHalfCheetah task because the tendencies of performance changes are similar for most environments and the performance changes on the DelayedHalfCheetah task are most noticeable. Then, we provide more ablation studies for remaining DelayedMujoco tasks in Appendix F.1. Control coefﬁcient cin (23): In the proposed α-adaptation (23) in Section 5, the control coefﬁcient caffects the learning behavior of α. Since H(π) and Dα JS are proportional to the action dimension, we tried a few values such as 0, −0.5d, −1.0dand −2.0d, where d= dim(A). Fig. F.1(a) shows the corresponding performance of DAC with α-adaptation on DelayedHalfCheetah. As seen in Fig. F.1(a), the performance depends on the change ofcas expected, and c= −2.0·dim(A)Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration (a) Control coefﬁcient c  (b) Entropy coefﬁcient β  (c) JS divergence Figure F.1: Averaged learning curve for ablation study performs well. We observed that −2.0dperformed well for all considered tasks. Hence, we set c= −2.0din (C.2). Entropy coefﬁcient β in (3): As mentioned in (Haarnoja et al., 2018a), the performance of SAC depends on β. It is expected that the performance of DAC depends on βtoo. Fig. F.1(b) shows the performance of DAC with ﬁxed α= 0.5 for three different values of β: β = 0.1, 0.2 and 0.4 on DelayedHalfCheetah. It is seen that the performance of DAC indeed depends on β. Although there exists performance difference for DAC depending on β, the performance of DAC is much better than SAC for a wide range of β. One thing to note is that the coefﬁcient of pure policy entropy regularization term for DAC is αβ, as seen in (3). Thus, DAC with α= 0.5 and β = 0.4 has the same amount of pure policy entropy regularization as SAC with β = 0.2. However, DAC with α= 0.5 and β = 0.4 has much higher performance than SAC with β = 0.2, as seen in Fig. Fig. F.1(b). So, we can see that the performance improvement of DAC comes from joint use of policy entropy H(π) and the sample action distribution from the replay buffer via Dα JS(π||q). The effect of JS divergence: In order to see the effect of the JS divergence on the performance, we provide an additional ablation study in which we consider a single JS divergence for SAC-Div by using the ratio function in Section 4.3. Fig. F.1(c) shows the performance comparison of SAC, SAC-Div(KL), SAC-Div(JS), and DAC. For SAC-Div(JS), we used δd = 0.5 for adaptive scaling in (Hong et al., 2018). It is seen that there is no signiﬁcant difference in performance between SAC-Div with JS divergence and SAC-Div with KL divergence. DAC still shows superiority to both SAC-Div(KL) and SAC-Div(JS). This shows that DAC has more advantages than simply using JS divergence.Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efﬁcient Exploration F.1. Ablation Studies for Remaining Tasks Here, we provide more ablation studies for remaining delayed Mujoco tasks in Figure F.2, Figure F.3, and Figure F.4. Control coefﬁcient c (a) DelayedHopper-v1  (b) DelayedWalker2d-v1  (c) DelayedAnt-v1 Figure F.2: Ablation study on c Entropy coefﬁcient β (a) DelayedHopper-v1  (b) DelayedWalker2d-v1  (c) DelayedAnt-v1 Figure F.3: Ablation study on β Effect of JS divergence over SAC-Div (a) DelayedHopper-v1  (b) DelayedWalker2d-v1  (c) DelayedAnt-v1 Figure F.4: Ablation study on SAC-Div with JS divergence",
      "references": [
        "Surprise-based intrinsic motivation for deep reinforcement learning.",
        "Computational theories of interaction and agency.",
        "Intrinsically motivated learning in natural and artificial systems.",
        "Unifying count-based exploration and intrinsic motivation.",
        "Openai gym.",
        "Exploration by random network distillation.",
        "Intrinsically motivated reinforcement learning.",
        "Pareto optimality, game theory and equilibria.",
        "Elements of Information Theory.",
        "Off-policy actor-critic.",
        "Openai baselines.",
        "Diversity is all you need: Learning skills without a reward function.",
        "Taming the noise in reinforcement learning via soft updates.",
        "Addressing function approximation error in actor-critic methods.",
        "Generative adversarial nets.",
        "Q-prop: Sample-efﬁcient policy gradient with an off-policy critic.",
        "Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning.",
        "Generative adversarial self-imitation learning.",
        "Reinforcement learning with deep energy-based policies.",
        "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
        "Soft actor-critic algorithms and applications.",
        "Dimension-wise importance sampling weight clipping for sample-efﬁcient reinforcement learning.",
        "Provably efﬁcient maximum entropy exploration.",
        "Diversity-driven exploration strategy for deep reinforcement learning.",
        "Vime: Variational information maximizing exploration.",
        "Auto-encoding variational bayes.",
        "Continuous control with deep reinforcement learning.",
        "Exploration in model-based reinforcement learning by empirically estimating learning progress.",
        "Count-based exploration in feature space for reinforcement learning.",
        "Leveraging exploration in off-policy algorithms via normalizing ﬂows.",
        "Human-level control through deep reinforcement learning.",
        "Bridging the gap between value and policy based reinforcement learning.",
        "Trust-pcl: An off-policy trust region method for continuous control.",
        "On the jensen–shannon symmetrization of distances relying on abstract means.",
        "Combining policy gradient and q-learning.",
        "Curiosity-driven exploration by self-supervised prediction.",
        "Parameter space noise for exploration.",
        "On the convergence of policy iteration in stationary dynamic programming.",
        "On stochastic optimal control and reinforcement learning by approximate inference.",
        "Convergence properties of policy iteration.",
        "Equivalence between policy gradients and soft q-learning.",
        "Proximal policy optimization algorithms.",
        "An analysis of model-based interval estimation for markov decision processes.",
        "Density ratio estimation in machine learning.",
        "Reinforcement learning: An introduction.",
        "# exploration: A study of count-based exploration for deep reinforcement learning.",
        "General duality between optimal control and estimation.",
        "Mujoco: A physics engine for model-based control.",
        "Robot trajectory optimization using approximate inference.",
        "Sample efﬁcient actor-critic with experience replay.",
        "Q-learning.",
        "Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation.",
        "On learning intrinsic rewards for policy gradient methods.",
        "Modeling purposeful adaptive behavior with the principle of maximum causal entropy.",
        "Maximum entropy inverse reinforcement learning."
      ],
      "meta_data": {
        "arxiv_id": "2006.01419v2",
        "authors": [
          "Seungyul Han",
          "Youngchul Sung"
        ],
        "published_date": "2020-06-02T06:51:25Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes “sample-aware” policy entropy regularization that mixes the current policy’s action distribution with the replay-buffer action distribution to encourage exploration of under-represented actions; derives a new objective whose added term decomposes into policy entropy plus an α-skew Jensen–Shannon divergence to the buffer distribution; develops a theoretically-grounded Diverse Policy Iteration scheme and the practical Diversity Actor-Critic (DAC) algorithm with convergence guarantees for finite MDPs; introduces an adaptive mechanism for automatically tuning the mixture weight α; demonstrates substantially improved exploration efficiency and return on continuous-control benchmarks versus SAC, SAC-Div, RND, MaxEnt(State), TD3, PPO, etc.",
        "methodology": "1) Define mixture q_mix=απ+(1-α)q where q is the empirical action distribution in the replay buffer; maximize expected return plus βH(q_mix). 2) Show H(q_mix)=αH(π)+(1-α)H(q)+D^α_JS(π||q)+const, linking exploration to a learnable Jensen–Shannon divergence. 3) Avoid explicit q via a density-ratio estimator R_α(s,a)=απ/(απ+(1-α)q) trained with a binary-classification style objective. 4) Formulate Diverse Policy Iteration: (a) Diverse policy evaluation using modified Bellman backup with sample-aware entropy; (b) Diverse policy improvement maximizing a surrogate using fixed ratio R_{π_old,α}. 5) Provide convergence proof for finite MDPs and gradient-equivalence proof for implementable surrogate. 6) Implement DAC with deep neural networks (2×256 MLPs) for policy, two Q-functions, value network, ratio network; use target network EMA, clipped double Q, re-parameterization; introduce max-min α-adaptation treating α as Lagrange variable, optimized online.",
        "experimental_setup": "Pure exploration: continuous 4-room maze (100×100 continuous grid), measure visited cells over 300k steps, 30 seeds. Control tasks: Sparse Mujoco (SparseHalfCheetah, SparseHopper, SparseWalker2d, SparseAnt), Delayed Mujoco (DelayedHalfCheetah, Hopper, Walker2d, Ant with delay 20), and HumanoidStandup. Rewards sparsified by thresholds or delayed accumulation. Networks as above, replay buffer 1M, discount 0.99, batch 256, Adam 3e-4, Gaussian policy. Baselines: SAC, SAC-Div (KL/JS), RND, MaxEnt(State), TD3, PPO, ACKTR, SQL. Metrics: average episode return over 10 seeds (30 for maze), state-visitation counts, divergence curves. Training for up to 3M environment steps (varies), deterministic evaluation each iteration.",
        "limitations": "1) Convergence proof assumes finite MDP and fixed replay distribution; in practice q changes and only empirical robustness is shown. 2) Requires learning an additional ratio network, increasing model complexity and hyper-parameter tuning (α, β, control c). 3) Performance sensitive to β and α values; α-adaptation heuristic may be sub-optimal. 4) Experiments limited to continuous control in Mujoco; no discrete or real-world tasks. 5) Assumes sufficiently large replay buffer and accurate density-ratio estimation; clipping heuristics used to stabilize training.",
        "future_research_directions": "• Provide theoretical guarantees under non-stationary replay distributions or establish tighter bounds. • Investigate alternative divergence measures or dynamic weighting schemes beyond α-skew JS. • Extend sample-aware entropy to on-policy algorithms, multi-agent, hierarchical or model-based RL. • Improve density-ratio estimation with more sample-efficient or uncertainty-aware methods. • Apply the approach to discrete large-scale domains (Atari, language) or real-robot settings. • Explore combination with intrinsic-motivation signals (curiosity, predictive error) or off-policy corrections for better sample reuse.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control",
      "full_text": "Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control Michal Nauman1,2 Mateusz Ostaszewski3 Krzysztof Jankowski2 Piotr Miło´s1,2,4 Marek Cygan2,5 Abstract Sample efficiency in Reinforcement Learning (RL) has traditionally been driven by algorithmic enhancements. In this work, we demonstrate that scaling can also lead to substantial improvements. We conduct a thorough investigation into the interplay of scaling model capacity and domain-specific RL enhancements. These empirical findings inform the design choices underlying our proposed BRO (Bigger, Regularized, Optimistic) algorithm. The key insight behind BRO is that strong regularization allows for effective scaling of the critic networks, which, paired with optimistic exploration, leads to superior performance. BRO achieves state-of-the- art results, significantly outperforming the leading model-based and model-free algorithms across 40 complex tasks from the DeepMind Control, MetaWorld, and MyoSuite benchmarks. BRO is the first model-free algorithm to achieve near-optimal policies in the notoriously challenging Dog and Humanoid tasks. 1 Introduction 0.0 0.2 0.4 0.6 0.8 1.0 Dog & Humanoid (7 tasks) 0.0 0.2 0.4 0.6 0.8 1.0 DeepMind Control (15 tasks) 0.0 0.2 0.4 0.6 0.8 1.0 MetaWorld (15 tasks) 0.0 0.2 0.4 0.6 0.8 1.0 MyoSuite (10 tasks) Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps. Deep learning has seen remarkable advancements in recent years, driven primarily by the development of large neural network models (Devlin et al., 2019; Tan & Le, 2019; Dosovitskiy et al., 2020). These advancements have significantly benefited fields like natural language processing and computer vision and have been percolating to RL as well (Padalkar et al., 2023; Zitkovich et al., 2023). Interestingly, some recent work has shown that the model scaling can be repurposed to achieve sample efficiency in discrete control (Schwarzer et al., 2023; Obando-Ceron et al., 2024), but these approaches cannot 0BRO website BRO code 1Ideas NCBR; 2University of Warsaw,3Warsaw University of Technology,4Polish Academy of Sciences, 5Nomagic. Correspondence to: Michal Nauman <nauman.mic@gmail.com> 38th Conference on Neural Information Processing Systems NeurIPS 2024 (spotlight) arXiv:2405.16158v3  [cs.LG]  3 Dec 20240 0.25 0.50 0.75 1.00 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM 0 10k 20k 30k 40k 50k Wallclock Time (Seconds) 0.0 0.2 0.4 0.6 0.8 1.0IQM BRO BRO (Fast) TD-MPC2 SR-SAC SAC TD3 CrossQ Figure 2: We report sample efficiency (left) and wallclock time (right) for BRO and BRO (Fast) (BRO with reduced replay ratio for increased compute efficiency), as well as baseline algorithms averaged over 40 tasks listed in Table 4. BRO achieves the best sample efficiency, whereas BRO (Fast) matches the sample efficiency of model-based TD-MPC2. In terms of wall clock efficiency, BRO runs approximately 25% faster than TD-MPC2. Remarkably, BRO (Fast) matches the wallclock efficiency of a standard SAC agent while achieving 400% better performance. The Y-axis reports the interquartile mean, with 1.0 representing the maximal possible performance. be directly translated to continuous action RL. As such, they rely on discrete action representation, whereas many physical control tasks have continuous, real-valued action spaces. Conventional practice in continuous deep RL has relied on small network architectures (Haarnoja et al., 2018; Hiraoka et al., 2021; Raffin et al., 2021; D’Oro et al., 2022), with the primary focus on algorithmic improvements. These enhancements aim to achieve better sample efficiency and address key challenges such as value overestimation (Fujimoto et al., 2018; Moskovitz et al., 2021; Cetin & Celiktutan, 2023), exploration (Chen et al., 2017; Ciosek et al., 2019; Nauman & Cygan, 2023a), and increasing the number of gradient steps (Nikishin et al., 2022; D’Oro et al., 2022). Additionally, evidence suggests that naive model capacity scaling can degrade performance (Andrychowicz et al., 2021; Bjorck et al., 2021). We challenge this status quo by posing a critical question: Can significant performance improvements in continuous control be achieved by combining parameter and replay scaling with existing algorithmic improvements? In this work, we answer this question affirmatively, identifying components essential to successful scaling. Our findings are based on a thorough evaluation of a broad range of design choices, which include batch size (Obando Ceron et al., 2024), distributional Q-values techniques (Bellemare et al., 2017; Dabney et al., 2018), neural network regularizations (Bjorck et al., 2021; Nauman et al., 2024), and optimistic exploration (Moskovitz et al., 2021; Nauman & Cygan, 2023a). Moreover, we carefully investigate the benefits and computational costs stemming from scaling along two axes: the number of parameters and the number of gradient steps. Importantly, we find that the former can lead to more significant performance gains while being more computationally efficient in parallelized setups. Our work culminates in developing the BRO (Bigger, Regularized, Optimistic) algorithm, a novel sample-efficient model-free approach. BRO significantly outperforms existing model-free and model- based approaches on 40 demanding tasks from the DeepMind Control, MetaWorld, and MyoSuite benchmarks, as illustrated in Figures 1 and 2. Notably, BRO is the first model-free algorithm to achieve near-optimal performance in challenging Dog and Humanoid tasks while being 2.5 times more sample-efficient than the leading model-based algorithm, TD-MPC2. The key BRO innovation is pairing strong regularization with critic model scaling, which, coupled with optimistic exploration, leads to superior performance. We summarize our contributions: • Extensive empirical analysis - we conduct an extensive empirical analysis focusing on critic model scaling in continuous deep RL. By training over 15, 000 agents, we explore the interplay between critic capacity, replay ratio, and a comprehensive list of design choices. • BroNet architecture & BRO algorithm- we introduce the BRO algorithm, a novel model-free ap- proach that combines BroNet architecture for critic scaling with domain-specific RL enhancements. BRO achieves state-of-the-art performance on 40 challenging tasks across diverse domains. 2• Scaling & regularization - we offer several insights, with the most important being: regularized critic scaling outperforms replay ratio scaling in terms of performance and computational efficiency; the inductive biases introduced by domain-specific RL improvements can be largely substituted by critic scaling, leading to simpler algorithms. 2 Bigger, Regularized, Optimistic (BRO) algorithm This section presents our novel Big, Regularized, Optimistic (BRO) algorithm and its design prin- ciples. The model-free BRO is a conclusion of extensive experimentation presented in Section 3, and significantly outperforms existing state-of-the-art methods on continuous control tasks from proprioceptive states (Figure 1). 2.1 Experimental setup We compare BRO against a variety of baseline algorithms. Firstly, we consider TD-MPC2 (Hansen et al., 2023), a model-based state-of-the-art that was shown to reliably solve the complex dog domains. Secondly, we consider SR-SAC (D’Oro et al., 2022), a sample-efficient SAC implementation that uses a large replay ratio of 32 and full-parameter resets. For completeness, we also consider CrossQ (Bhatt et al., 2023), a compute-efficient method that was shown to outperform ensemble approaches, as well as standard SAC (Haarnoja et al., 2018) and TD3 (Fujimoto et al., 2018). We run all algorithms with 10 random seeds, except for TD-MPC2, for which we use the results provided by the original manuscript (Hansen et al., 2023). We describe the process of hyperparameter selection for all considered algorithms in Appendix D, and share BRO pseudocode in Appendix (Pseudocode 1). We implement BRO based on the JaxRL (Kostrikov, 2021) and make the code available under the following link: https://github.com/naumix/BiggerRegularizedOptimistic Environments We consider a wide range of control tasks, encompassing a total of 40 diverse, complex continuous control tasks spanning three simulation domains: DeepMind Control (Tassa et al., 2018), MetaWorld (Yu et al., 2020), and MyoSuite (Caggiano et al., 2022) (a detailed list of environments can be found in Appendix C). These tasks include high-dimensional state and action spaces (with |S| and |A| reaching 223 and 39 dimensions), sparse rewards, complex locomotion tasks, and physiologically accurate musculoskeletal motor control. We run the algorithms for 1M environment steps and report the final performance unless explicitly stated otherwise. We calculate the interquartile means and confidence intervals using the RLiable package (Agarwal et al., 2021). (a) DeepMind Control (DMC)  (b) MetaWorld (MW)  (c) MyoSuite (MS) Figure 3: We consider a total of 40 tasks from DeepMind Control (DMC), MetaWorld (MW), and MyoSuite (MS). In particular, we chose the tasks with the biggest optimality gap according to previous evaluations (Hansen et al., 2023). We list all considered tasks in Table 4. 2.2 BRO outline and design choices The BRO algorithm is based on the well-established Soft Actor-Critic (SAC) (Haarnoja et al., 2018) (see also Appendix A) and is composed of the following key components: • Bigger – BRO uses a scaled critic network with the default of ≈ 5M parameters, which is approximately 7 times larger than the average size of SAC models (Haarnoja et al., 2018); as well as scaled training density with a default replay ratio1 of RR = 10, and RR = 2 for the BRO (Fast) version. 1The replay ratio refers to the number of gradient updates per one environment step. 30.55 1.05 2.83 4.92 26.31 Parameters (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM BroNet + BRO Vanilla + BRO Spectral + BRO 0.34 0.79 2.47 4.56 25.74 Parameters (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM BroNet + SAC Vanilla + SAC Spectral + SAC Figure 4: Scaling the critic parameter count for vanilla dense (Fujimoto et al., 2018), spectral normalization ResNet (Bjorck et al., 2021), and our BroNet for BRO (left), and SAC (right). We conclude that to achieve the best performance, we need both the right architecture (BroNet) and the correct algorithmic enhancements encapsulated in BRO. We report interquartile mean performance after 1M environment steps in tasks listed in Table 3, with error bars indicating 95% CI from 10 seeds. On the X-axis, we report the approximate parameter count of each configuration. • Regularized – the BroNet architecture, intrinsic to the BRO approach, incorporates strategies for regularization and stability enhancement, including the utilization of Layer Normalization (Ba et al., 2016) after each dense layer, alongside weight decay (Loshchilov & Hutter, 2017) and full-parameter resets (Nikishin et al., 2022). • Optimistic – BRO uses dual policy optimistic exploration (Nauman & Cygan, 2023a) and non- pessimistic (Nauman et al., 2024) quantile Q-value approximation (Dabney et al., 2018; Moskovitz et al., 2021) for balancing exploration and exploitation. The full details of the algorithm, along with the pseudo-code, are provided in Appendix B. Figure 7 summarizes the impact of removing components of BRO. We observe the biggest impact of scaling the critic capacity (scale) and replay ratio (RR), as well as using non-pessimistic Q-value, i.e. removing Clipped Double Q-learning (CDQ). Figure 5: BroNet architecture employed for actor and critic. Each fully connected layer is augmented with Layer Norm, which is essential to unlocking scaling. We use ≈ 5M parameters and N = 2in the default setting. Scaling critic network and BroNet architec- ture The key contribution of this paper is showing how to enable scaling the critic network. We recall that naively increasing the critic ca- pacity does not necessarily lead to performance improvements and that successful scaling de- pends on a carefully chosen suite of regular- ization techniques (Bjorck et al., 2021). Fig- ure 5 shows our BroNet architecture, which, up to our knowledge, did not exist previously in the literature. The architecture begins with a dense layer followed by Layer Norm (Ba et al., 2016) and ReLU activation. Subsequently, the network comprises ResNet blocks, each consist- ing of two dense layers regularized with Layer Norm. Consequently, the ResNet resembles the FFN sub-layer utilized in modern LLM archi- tectures (Xiong et al., 2020), differing primarily in the placement of the Layer Norms. Crucially, we find that BroNet scales more effectively than other architectures (Figure 4 (left)). However, the right choice of architecture and scaling is not a silver bullet. Figure 4 (right) shows that when these are plugged into the standard SAC algorithm naively, the performance is weak. The 41 2 5 10 15 Replay Ratio 0.4 0.5 0.6 0.7 0.8 0.9IQM 0.55 M 1.05 M 2.83 M 4.92 M 26.31 M 5k 10k 20k 40k 80k Wallclock Time (Seconds) 0.4 0.5 0.6 0.7 0.8 0.9IQM RR=1 RR=2 RR=5 RR=10 RR=15 Figure 6: To account for sample efficiency, we report the performance averaged at 250k, 500k, 750k, and 1M environment steps across different 5 replay ratios and 5 critic model sizes. All agents were evaluated in tasks listed in Table 3, and 10 random seeds per variant. The left figure shows performance scaling with increasing replay ratios (shapes) and model sizes (colors). The right figure examines the tradeoff between performance and computational cost when scaling replay ratios versus critic model sizes. Increasing model size leads to substantial performance improvements at lower compute costs compared to increasing the replay ratio. We present more scaling results in Appendix E, including a description of model sizes in Table 7. important elements are additional regularization (weight decay and network resets) and optimistic exploration (see below). Interestingly, we did not find benefits from scaling the actor networks, further discussed in Section 3. Scaling replay ratio and relation to model scaling Increasing replay ratio (D’Oro et al., 2022) is another axis of scaling. We investigate mutual interactions by measuring the performance across different model scales (from 0.55M to 26M) and RR settings (from RR = 1 to RR = 15). Figure 6 reveals that the model scaling has a much stronger impact plateauing at≈ 5M parameters. Increasing the replay ratio also leads to noticeable benefits. For example, a 26M model with RR = 1 achieves significantly better performance than a small model with RR = 15, even though the 26M model requires three times less wallclock time. Importantly, model scaling and increasing replay ratio work well in tandem and are interchangeable to some degree. We additionally note that the replay ratio has a bigger impact on wallclock time than the model size. This stems from the fact that scaling replay ratio leads to inherently sequential calculations, whereas scaling model size leads to calculations that can be parallelized. For these reasons, BRO (Fast) with RR = 2 and 5M network offers an attractive trade-off, being already very sample efficient and fast at the same time. Optimistic exploration and Q-values BRO utilizes two mechanisms to increase optimism. We observe significant improvements stemming from these techniques in both BRO and BRO (Fast) agents (Figure 7). They are particularly pronounced in the early stages of the training and for smaller models (Figure 9a). The initial mechanism involves deactivating Clipped Double Q-learning (CDQ) (Fujimoto et al., 2018), a commonly employed technique in reinforcement learning aimed at mitigating Q-value overestimation. For further clarification, refer to Appendix B.6, particularly Eq. 8 where we take the ensemble mean instead of minimum for Q-value calculation. This is surprising, perhaps, as it goes against conventional wisdom. However, some recent work has already suggested that regularization might effectively combat the overestimation (Nauman et al., 2024). We observe a much stronger effect. In Figures 7 & 9a, we compare the performance of BRO with BRO that uses CDQ. This analysis indicates that using risk-neutral Q-value approximation in the presence of network regularization unlocks significant performance improvements without value overestimation (Table 1). The second mechanism is optimistic exploration. We implement the dual actor setup (Nauman & Cygan, 2023a), which employs separate policies for exploration and temporal difference updates. The exploration policy follows an optimistic upper-bound Q-value approximation, which has been shown to improve the sample efficiency of SAC-based agents (Ciosek et al., 2019; Moskovitz et al., 2021; Nauman & Cygan, 2023a). In particular, we optimize the optimistic actor towards a KL-regularized Q-value upper-bound (Nauman & Cygan, 2023a), with Q-value upper-bound calculated with respect 540 50 60 70 80 90 100 BRO − WD − Quantile − Dual π + RR=1 + CDQ − Scale BRO 40 50 60 70 80 90 100 BRO (Fast) % of Base Model Performance Figure 7: Impact of removing various BRO components on its performance. We report the percentage of the final performance for BRO (left) and BRO (Fast) (right). The y-axis shows the components that are ablated: -Scale denotes using a standard-sized network, +CDQ denotes using pessimistic Clipped Double Q-learning (which is removed by default in BRO), +RR=1 uses the standard replay ratio, -Dual π removes optimistic exploration, and -Quantile and -WD stand for removing quantile Q-values and weight decay, respectively. We report the interquartile mean and 95% CIs for tasks in Table 3, with 10 random seeds. The results indicate that the Scale, CDQ, and RR=1 components are the most impactful for BRO. Since BRO (Fast) has RR=2 by default, reducing it to one does not significantly affect its performance. to epistemic uncertainty calculated according to the methodology presented in Moskovitz et al. (2021). As shown in Figure 7, using dual actor optimistic exploration yields around10% performance improvement in the BRO model. Others We mention two other design choices. First, we use a smaller batch size of 128 than the typical one of 256. This is computationally beneficial while having a marginal impact on performance, which we show in Figure 15. Secondly, we use quantile Q-values (Bellemare et al., 2017; Dabney et al., 2018). We find that quantile critic representation improves performance (Figure 7), particularly for smaller networks. This improvement, however, diminishes for over-parameterized agents (Figure 9a). On top of the performance improvements, the distribution setup allows us to estimate epistemic uncertainties, which we leverage in the optimistic exploration according to the methodology presented in Moskovitz et al. (2021). 3 Analysis This section summarizes the results of 15,000 experiments, detailed in Table 2, which led us to develop the BRO algorithms. These experiments also provided numerous insights that we believe will be of interest to the community. We adhered to the experimental setup described in Section 2.1. We also present additional experimental results in Appendix E. Scaling model-free critic allows superior performance We recall that the most important finding is that skillful critic model scaling combined with simple algorithmic improvements can lead to extremely sample-efficient performance and the ability to solve the most challenging environments. We deepen these observations in experiments depicted in Figure 8. Namely, we let the other algo- rithms, including state-of-the-art model-based TD-MPC2, run for 3M steps on the most challenging tasks in the DMC suite (Dog Stand, Dog Walk, Dog Trot, Dog Run, Humanoid Stand, Humanoid Walk, and Humanoid Run). TD-MPC2 eventually achieves BRO performance levels, but it requires approximately 2.5 more environment steps. Algorithmic improvements matter less as the scale increases The impact of algorithmic im- provements varies with the size of the critic model. As shown in Figure 9a, while techniques like smaller batch sizes, quantile Q-values, and optimistic exploration enhance performance for 1.05M and 4.92M models, they do not improve performance for the largest 26.3M models. We hypoth- esize this reflects a tradeoff between the inductive bias of domain-specific RL techniques and the overparameterization of large neural networks. Despite this, these techniques still offer performance 6Figure 8: IQM return learning curves for four Dog and three Humanoid environments from the DMC benchmark, plotted against the number of environment steps. Notably, the model-based approach (TD-MPC2) requires approximately 2.5 times more steps to match BRO performance. gains with lower computing costs. Notably, full-parameter resets (Nikishin et al., 2022; D’Oro et al., 2022) are beneficial; the largest model without resets nearly matches the performance of the BRO with resets. Scaling actor is not effective Previous works underscore the relative importance of critic and actor networks in off-policy algorithms like SAC (Fujimoto et al., 2018; D’Oro et al., 2022; Li et al., 2022). For instance, Nikishin et al. (2022) found that critic regularization is significantly more important than actor regularization. We confirm this result by showing that, for off-policy continuous control actor-critic algorithms, increasing critic capacity leads to much better results than increasing the actor model size, which in some cases might be even detrimental (Figure 9a). As such, practitioners can achieve performance improvements by prioritizing critic capacity over actor capacity while adhering to memory limitations. Target networks yield small but noticeable performance benefits Using target networks doubles the memory costs (Schwarzer et al., 2020; Bhatt et al., 2023; Lee et al., 2024), which can be a significant burden for large models. In Figure 9b, we compare the performance of standard BRO and BRO (Fast) agents against their versions without target networks. Consistent with established understanding, we find that using target networks yields a small but significant performance improve- ment. However, we observe substantial variation in these effects among benchmarks and specific environments (Figure 9b & Figure 16). For example, the majority of performance improvements in DMC and MS environments are attributable to specific tasks. Table 1: Comparison of BroNet, Spectral (Bjorck et al., 2021), and Vanilla MLP architectures in notriously hard Dog environments. All metrics except return are averaged over time steps. All architectures are combined with BRO. BroNet Spectral Vanilla Final return 763.5 73.5 167. ||∇||2 35.5 88. 9.61E+04 Mean Q-values 58.06 153.85 1.20E+05 TD-error 0.38 4.31E+04 6.03E+07 Architecture matters (especially in complex environments) By break- ing down the results from Figure 4 into individual environments, the BroNet architecture achieves better performance in all of them, but the differences are most pronounced in the Dog environments. Therefore, we deepened our analysis with extra metrics to understand these discrepan- cies better. Table 1 demonstrates that BroNet outperforms the other archi- tectures regarding final performance. The Vanilla MLP exhibits instabilities across all measured metrics, including gradient norm, overestimation, and TD error. While using the Spectral architecture maintains moderate gradient norms and overestimation, it struggles significantly with minimizing the TD error. In (Nauman et al., 2024), the authors indicate that the gradient norm and overestimation are strong indicators of poor performance in Dog environments. However, these results suggest that identifying 7(a) Design Choices  (b) Target Networks Figure 9: (Left) We analyze the importance of BRO components dependent on the critic model size. Interestingly, most components become less important as the critic capacity grows. (Right) We report the performance of BRO variants with and without a target network. All algorithm variants are run with 10 random seeds. a single cause for the challenges in training a reinforcement learning agent is difficult, highlighting the complexity of these systems and the multifaceted nature of their performance issues. What did not work? While researching BRO, we tested a variety of techniques that were found to improve the performance of different RL agents; however, they did not work in our evaluations. Firstly, we found that using N-step returns (Sutton & Barto, 2018; Schwarzer et al., 2023) does not improve the performance in the tested environments. We conjecture that the difference betweenN- step effectiveness in Atari and continuous control benchmarks stems from the sparser reward density in the former. Furthermore, we evaluated categorical RL (Bellemare et al., 2017) and HLGauss (Imani & White, 2018; Farebrother et al., 2024) Q-value representations, but found that these techniques are not directly transferable to a deterministic policy gradient setup and introduce training instabilities when applied naively, resulting in a significant amount of seeds not finishing their training. Finally, we tested a variety of scheduling mechanisms considered by Schwarzer et al. (2023) but found that the performance benefits are marginal and highly task-dependent while introducing much more complexity associated with hyperparameter tuning. A complete list of tested techniques is presented in Appendix B.8. Are current benchmarks enough? As illustrated in Figure 10, even complex tasks like Dog Walk or Dog Trot can be reliably solved by combining existing algorithmic improvements with critic model scaling within 1 million environment steps. However, some tasks remain unsolved within this limit (e.g., Humanoid Run or Acrobot Swingup). Tailoring algorithms to single tasks risks overfitting to specific issues. Therefore, we advocate for standardized benchmarks that reflect the sample efficiency of modern algorithms. This standardization would facilitate consistent comparison of approaches, accelerate advancements by focusing on a common set of challenging tasks, and promote the development of more robust and generalizable RL algorithms. On that note, in Appendix F, we report BRO performance at earlier stages of the training. BroNet architecture is useful beyond continuous control We design additional experiments to test the effectiveness of the naive application of BroNet to popular offline reinforcement learning problems in two offline RL benchmarks: AntMaze (6 tasks); and Adroit (9 tasks) (Fu et al., 2020). We run Behavioral Cloning (BC) in pure offline (Sutton & Barto, 2018), Implicit Q-Learning (IQL) offline + fine-tuning (Kostrikov et al., 2022), as well as online reinforcement learning with offline data (Ball et al., 2023). We run all these algorithms with the default MLP network, as well as BroNet backbone. As shown in Figure 11, we find that the naive application of BroNet leads to performance improvements across all tested algorithms. 8Figure 10: Our experiments cover 40 of the hardest tasks from DMC (locomotion), MW (manip- ulation), and MS (physiologically accurate musculoskeletal control) considered in previous work (Hansen et al., 2023). In those tasks, the state-of-the-art model-free SR-SAC (D’Oro et al., 2022) achieves more than 80% of maximal performance in 18 out of 40 tasks, whereas our proposed BRO in 33 out of 40 tasks. BRO makes significant progress in the most complex tasks of the benchmarks. -0.50 -0.25 0 0.25 0.5 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Aggregate (15 tasks) BC IQL SAC -0.50 -0.25 0 0.25 0.5 Environment Step (M) 0.0 0.1 0.2 0.3 0.4 0.5IQM AntMaze (6 tasks) -0.50 -0.25 0 0.25 0.5 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Adroit (9 tasks) Figure 11: We test three scenarios: offline (comparing vanilla BC to BroNet-based BC), offline fine- tuning (comparing vanilla IQL to BroNet-based IQL), and online with offline data (comparing vanilla SAC to BroNet-based SAC). The solid line represents BRO-based and the dashed line represents vanilla variants. Negative values on the X-axis refer to offline training. 10 seeds per task. 4 Related Work 4.1 Sample efficiency through algorithmic improvements A significant effort in RL has focused on algorithmic improvements. One recurring theme is controlling value overestimation (Fujimoto et al., 2018; Moskovitz et al., 2021; Cetin & Celiktutan, 2023). For instance, Fujimoto et al. (2018) proposed Clipped Double Q-learning (CDQ), which updates policy and value networks using a lower-bound Q-value approximation. However, since a pessimistic lower-bound can slow down learning, Moskovitz et al. (2021) introduced an approach that tunes pessimism online. Recently, Nauman et al. (2024) showed that layer normalization can improve performance without value overestimation, eliminating the need for pessimistic Q-learning. A notable effort has also focused on optimistic exploration (Wang et al., 2020; Moskovitz et al., 2021). Various methods have been developed to increase sample efficiency via exploration that is greedy with respect to a Q-value upper bound. These include closed-form transformations of the pessimistic policy (Ciosek et al., 2019) or using a dual actor network dedicated to exploration (Nauman & Cygan, 2023a). 4.2 Sample efficiency through scaling Recent studies demonstrated the benefits of model scaling when pre-training on large datasets (Driess et al., 2023; Schubert et al., 2023; Taiga et al., 2023) or in pure offline RL setups (Kumar et al., 2023). Additionally, model scaling has proven advantageous for model-based online RL (Hafner et al., 2023; Hansen et al., 2023; Wang et al., 2024). However, in these approaches, most of the model scale is dedicated to world models, leaving the value network small. Notably, Schwarzer et al. (2023) found that increasing the scale of the encoder network improves performance for DQN agents, but did not study increasing the capacity of the value network. Various studies indicate that naive scaling of the value model leads to performance deterioration (Bjorck et al., 2021; Obando-Ceron et al., 2024; 9Farebrother et al., 2024). For example, Bjorck et al. (2021) demonstrated that spectral normalization enables stable training with relatively large ResNets with performance improvements. In addition to model size scaling, the community has investigated the effectiveness of replay ratio scaling (i.e., increasing the number of gradient steps for every environment step) (Hiraoka et al., 2021; Nikishin et al., 2022; Li et al., 2022). Recent works have shown that a high replay ratio can improve performance across various algorithms in both continuous and discrete MDPs, provided the neural networks are regularized (Li et al., 2022; D’Oro et al., 2022). In this context, layer normalization and full-parameter resets have been particularly effective (Schwarzer et al., 2023; Lyle et al., 2024; Nauman et al., 2024). 5 Limitations and Future Work BRO’s larger model size compared to traditional baselines like SAC or TD3 results in higher memory requirements, potentially posing challenges for real-time inference in high-frequency control tasks. Future research could explore techniques such as quantization or distillation to improve inference speed. While BRO is designed for continuous control problems, its effectiveness in discrete settings remains unexplored. Further investigation is needed to assess the applicability and performance of BRO’s components in discrete action MDPs. Additionally, our experimentation primarily focuses on continuous control tasks using proprioceptive state representations. Future research is needed to investigate the tradeoff between scaling the critic and the state encoder in image-based RL. 6 Conclusions Our study underscores the efficacy of scaling a regularized critic model in conjunction with existing algorithmic enhancements, resulting in sample-efficient methods for continuous-action RL. The proposed BRO algorithm achieves markedly superior performance within 1 million environment steps compared to the state-of-the-art model-based TD-MPC2 and other model-free baselines. Notably, it achieves over 90% success rates in MetaWorld and MyoSuite benchmarks, as well as over 85% of maximal returns in the DeepMind Control Suite, and near-optimal policies in the challenging Dog and Humanoid locomotion tasks. While some tasks remain unsolved within 1 million environment steps, our findings underscore the need for new standardized benchmarks focusing on sample efficiency to drive consistent progress in the field. The BRO algorithm establishes a new standard for sample efficiency, providing a solid foundation for future research to build upon and develop even more robust RL algorithms. Acknowledgements This research was partially supported by the National Science Centre, Poland, under grant nos. 2020/39/B/ST6/01511 and 2023/51/D/ST6/01609, as well as by the Warsaw University of Technology and the University of Warsaw through the Excellence Initiative: Research University (IDUB) program. We also gratefully acknowledge the Polish high-performance computing infrastructure, PLGrid (HPC Center: ACK Cyfronet AGH), for providing computational resources and support under grant no. PLG/2024/017159. Marek Cygan was partially supported by an NCBiR grant POIR.01.01.01-00- 0433/20. Piotr Miło´s research was supported by the National Science Center (Poland) grant number 2019/35/O/ST6/03464. References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021. Andrychowicz, M., Raichuk, A., Sta´nczyk, P., Orsini, M., Girgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et al. What matters in on-policy reinforcement learning? a large- scale empirical study. In ICLR 2021-Ninth International Conference on Learning Representations, 2021. 10Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Ball, P. J., Smith, L., Kostrikov, I., and Levine, S. Efficient online reinforcement learning with offline data. Proceedings of the 40th International Conference on Machine Learning, 2023. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279, 2013. Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In International conference on machine learning, pp. 449–458. PMLR, 2017. Bhatt, A., Palenicek, D., Belousov, B., Argus, M., Amiranashvili, A., Brox, T., and Peters, J. Cross q: Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity. In The Twelfth International Conference on Learning Representations, 2023. Bjorck, N., Gomes, C. P., and Weinberger, K. Q. Towards deeper deep reinforcement learning with spectral normalization. Advances in neural information processing systems, 34:8242–8255, 2021. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym, 2016. Caggiano, V ., Wang, H., Durandau, G., Sartori, M., and Kumar, V . Myosuite–a contact-rich simulation suite for musculoskeletal motor control. arXiv preprint arXiv:2205.13600, 2022. Cetin, E. and Celiktutan, O. Learning pessimism for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 6971–6979, 2023. Chen, R. Y ., Sidor, S., Abbeel, P., and Schulman, J. Ucb exploration via q-ensembles.arXiv preprint arXiv:1706.01502, 2017. Ciosek, K. and Whiteson, S. Expected policy gradients for reinforcement learning. Journal of Machine Learning Research, 21(2020), 2020. Ciosek, K., Vuong, Q., Loftin, R., and Hofmann, K. Better exploration with optimistic actor critic. Advances in Neural Information Processing Systems, 32, 2019. Dabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pp. 1096–1105. PMLR, 2018. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional trans- formers for language understanding. North American Chapter of the Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1423. D’Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Bellemare, M. G., and Courville, A. Sample- efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, 2022. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2020. Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y ., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V ., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model. arXiv preprint arXiv: 2303.03378, 2023. Farebrother, J., Orbay, J., Vuong, Q., Taïga, A. A., Chebotar, Y ., Xiao, T., Irpan, A., Levine, S., Castro, P. S., Faust, A., et al. Stop regressing: Training value functions via classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024. 11François-Lavet, V ., Fonteneau, R., and Ernst, D. How to discount deep reinforcement learning: Towards new dynamic strategies. arXiv preprint arXiv: 1512.02011, 2015. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pp. 1587–1596. PMLR, 2018. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V ., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Hansen, N., Su, H., and Wang, X. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv: 2310.16828, 2023. Hiraoka, T., Imagawa, T., Hashimoto, T., Onishi, T., and Tsuruoka, Y . Dropout q-functions for doubly efficient reinforcement learning. In International Conference on Learning Representations, 2021. Hussing, M., V oelcker, C., Gilitschenski, I., Farahmand, A.-m., and Eaton, E. Dissecting deep rl with high update ratios: Combatting value overestimation and divergence. arXiv preprint arXiv:2403.05996, 2024. Imani, E. and White, M. Improving regression performance with distributional losses. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2157–2166. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/imani18a.html. Kearns, M. and Singh, S. Bias-variance error bounds for temporal difference updates. In Annual Conference Computational Learning Theory , 2000. URL https://api.semanticscholar. org/CorpusID:5053575. Kostrikov, I. JAXRL: Implementations of Reinforcement Learning algorithms in JAX, 10 2021. URL https://github.com/ikostrikov/jaxrl. Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022. Kumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine, S. Offline q-learning on diverse multi- task data both scales and generalizes. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4-k7kUavAj. Lee, H., Cho, H., Kim, H., Kim, D., Min, D., Choo, J., and Lyle, C. Slow and steady wins the race: Maintaining plasticity with hare and tortoise networks. In Forty-first International Conference on Machine Learning, 2024. Li, Q., Kumar, A., Kostrikov, I., and Levine, S. Efficient deep reinforcement learning requires regulating overfitting. In The Eleventh International Conference on Learning Representations, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. International Conference on Learning Representations, 2017. Lyle, C., Zheng, Z., Khetarpal, K., van Hasselt, H., Pascanu, R., Martens, J., and Dabney, W. Disentangling the causes of plasticity loss in neural networks. arXiv preprint arXiv: 2402.18762, 2024. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . Spectral normalization for generative adversarial networks. International Conference on Learning Representations, 2018. 12Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried- miller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015. Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M. Tactical optimism and pessimism for deep reinforcement learning. Advances in Neural Information Processing Systems, 34:12849–12863, 2021. Nauman, M. and Cygan, M. On the theory of risk-aware agents: Bridging actor-critic and economics. In ICML 2024 Workshop: Aligning Reinforcement Learning Experimentalists and Theorists, 2023a. Nauman, M. and Cygan, M. Decoupled actor-critic. arXiv preprint arXiv:2310.19527, 2023b. URL https://arxiv.org/pdf/2310.19527v3. Nauman, M., Bortkiewicz, M., Miło ´s, P., Trzcinski, T., Ostaszewski, M., and Cygan, M. Over- estimation, overfitting, and plasticity in actor-critic: the bitter lesson of reinforcement learn- ing. In Proceedings of the 41st International Conference on Machine Learning , 2024. URL https://arxiv.org/pdf/2403.00514. PMLR 235:37342-37364. Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In International conference on machine learning , pp. 16828–16847. PMLR, 2022. Obando Ceron, J., Bellemare, M., and Castro, P. S. Small batch deep reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Obando-Ceron, J., Sokar, G., Willi, T., Lyle, C., Farebrother, J., Foerster, J., Dziugaite, G. K., Precup, D., and Castro, P. S. Mixtures of experts unlock parameter scaling for deep rl. arXiv preprint arXiv:2402.08609, 2024. Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22 (268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html. Schubert, I., Zhang, J., Bruce, J., Bechtle, S., Parisotto, E., Riedmiller, M., Springenberg, J. T., Byravan, A., Hasenclever, L., and Heess, N. A generalist dynamics model for control. arXiv preprint arXiv: 2305.10912, 2023. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. Data-efficient rein- forcement learning with self-predictive representations. In International Conference on Learning Representations, 2020. Schwarzer, M., Ceron, J. S. O., Courville, A., Bellemare, M. G., Agarwal, R., and Castro, P. S. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pp. 30365–30380. PMLR, 2023. Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018. Tai, J. J., Towers, M., and Tower, E. Shimmy: Gymnasium and PettingZoo Wrappers for Commonly Used Environments. URL https://github.com/Farama-Foundation/shimmy. Taiga, A. A., Agarwal, R., Farebrother, J., Courville, A., and Bellemare, M. G. Investigating multi-task pretraining and generalization in reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= sSt9fROSZRO. Tan, M. and Le, Q. V . Efficientnet: Rethinking model scaling for convolutional neural networks. International Conference on Machine Learning, 2019. 13Tassa, Y ., Doron, Y ., Muldal, A., Erez, T., Li, Y ., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Van Seijen, H., Van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis of expected sarsa. In 2009 ieee symposium on adaptive dynamic programming and reinforcement learning, pp. 177–184. IEEE, 2009. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, S., Liu, S., Ye, W., You, J., and Gao, Y . Efficientzero v2: Mastering discrete and continuous control with limited data. arXiv preprint arXiv:2403.00564, 2024. Wang, Y ., Wang, R., Du, S. S., and Krishnamurthy, A. Optimism in reinforcement learning with gen- eralized linear function approximation. In International Conference on Learning Representations, 2020. Xiong, R., Yang, Y ., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y ., Wang, L., and Liu, T.-Y . On layer normalization in the transformer architecture.International Conference on Machine Learning, 2020. Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094–1100. PMLR, 2020. Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 2165–2183. PMLR, 2023. 14Broader Impact The work presented in this study, while academic and based on simulated benchmarks, advances the development of more capable autonomous agents. Although our contributions do not directly cause any negative societal impacts, we encourage the community to remain mindful of such potential consequences when extending our research. A Background We consider an infinite-horizon Markov Decision Process (MDP) (Puterman, 2014) which is described with a tuple (S, A, r, p, γ), where states S and actions A are continuous, r(s′, s, a) is the transition reward, p(s′|s, a) is the transition kernel and γ ∈ (0, 1] is the discount factor. The policy π(a|s) is a state-conditioned action distribution with its entropy denoted as H(π(s)). Soft Value (Haarnoja et al., 2018) is the sum of expected discounted return and state entropies from following the policy at a given state V π(s) = Ea∼π,s′∼p [r(s′, s, a) + αH(π(s)) + γV π(s′)] , (1) with α denoting the entropy temperature parameter. Q-value is the expected discounted return from performing an action and following the policy thereafter. Qπ(s, a) = Es′∼p [r(s′, s, a) + γV π(s′)] (2) A policy is said to be optimal if it maximizes the expected value of the possible starting states s0, such that ˙π = arg maxπ∈Π Es0∼pV π(s0), with ˙π denoting the optimal policy and Π denoting the considered set of policies (e.g., Gaussian). Soft values and soft Q-values are related through the following equation: V π(s) = Ea∼π [Qπ(s, a) − log π(a|s)] (3) This relation is often approximated via a single action sampled according to the policy a ∼ π(s). In off-policy actor-critic, there is continuous gradient-based learning of both Q-values (critic) and the policy (actor). The critic parameters θ are updated by minimizing SARSA temporal-difference on transitions T = (s, a, r, s′), with T being sampled from a replay buffer of transitions (Fujimoto et al., 2018; Haarnoja et al., 2018) according to: θ = arg min θ ET∼D \u0000 Qθ(s, a) − r(s′, s, a) − γV¯θ(s) \u0001 , (4) V¯θ(s) = Q¯θ(s′, a′) − α log πϕ(a′|s′)), (5) with a′ ∼ πϕ. In this setup, Qθ is the critic network, Q¯θ is the value target network, and D is the replay buffer (Mnih et al., 2015). Qθ is trained to approximate the Q-value under the policy from which the bootstrap is sampled (Van Seijen et al., 2009; Sutton & Barto, 2018). The policy parameters ϕ are updated to seek locally optimal values approximated by the critic (Ciosek & Whiteson, 2020) according to: ϕ = arg max ϕ Es∼D (Qθ(s, a) − α log πϕ(a|s)) , with a ∼ πϕ. (6) B BRO additional details B.1 Base agent BRO uses the well-established Soft Actor-Critic (SAC) (Haarnoja et al., 2018) as its base. SAC is a stochastic policy, maximum entropy algorithm (see Eq. 1) that employs online entropy temperature adjustment and an ensemble of two critic networks. SAC models the policy via a Tanh-transformed Gaussian distribution whose parameters are modeled by the actor network. B.2 Architecture details In the proposed architecture, we adopt the transformer feedforward blueprint from Vaswani et al. (2017) with a novel layer normalization configuration, as shown in Figure 5. Dropout is omitted. All Dense layers in the BRO have a default width of 512 units, with a linear layer at both the input and output stages. To increase the model’s depth, we add new residual blocks exclusively. While a similar transformer backbone has been used in previous work (Bjorck et al., 2021), our design choices, detailed in Section E, led us to use layer normalization instead of spectral normalization. 15B.3 Scaling In Figure 4, we examine the scaling capabilities of SAC and BRO agents using a vanilla dense network (Fujimoto et al., 2018; Haarnoja et al., 2018; Moskovitz et al., 2021), a spectral normalization ResNet (Bjorck et al., 2021; Cetin & Celiktutan, 2023), and a layer normalization ResNet inspired by previous work (Nauman et al., 2024). As shown in Figure 4, increasing the model capacity of a vanilla-dense agent can lead to performance degradation beyond a certain model size. However, for the regularized architectures, we observe behavior similar to the empirically observed scaling properties of supervised models, where increasing model size leads to diminishing returns in performance improvements. Furthermore, we find that the layer normalization ResNet achieves better scaling properties than the spectral normalization architecture. Interestingly, the BRO agent consistently outperforms the baseline SAC across all architectures and network sizes, suggesting an interaction between parameter scaling and other algorithmic design choices. The highest performing SAC agent achieves around 25% of maximal performance, whereas our proposed BRO agent achieves more than90%. Given that the BRO agent performs similarly at 4.92 million and 26.31 million parameters, we use the smaller model to reduce the computational burden. B.4 Scaling replay ratio Replay Ratio (RR) scaling in small models leads to diminishing performance increases as RR values rise, eventually plateauing at higher RRs (Nikishin et al., 2022; D’Oro et al., 2022). Unfortunately, increasing RR also results in linear increases in computing costs, as each gradient step must be calculated sequentially. This naturally becomes a burden as the model sizes increase. In Figure 6, we investigate the performance of the BRO agent across different model scales (from0.55 million to 26.31 million parameters) and RR settings (from RR=1 to RR=15), measuring both performance and wall-clock efficiency. We find that with the BRO regularized critic architecture, critic scaling leads to performance and sample efficiency gains that match those of scaled RR. Scaling both RR and model size produces the best-performing agents. Interestingly, scaling the model size can lead to significant performance improvements even if RR is low while being more computationally efficient due to parallelized workloads (Figure 6). For example, a 5 million parameter BRO model with RR=1 outperforms a 1 million parameter BRO agent with RR=15 despite being five times faster in terms of wall-clock time. This observation challenges the notion that a sample-efficient RL algorithm must use high replay settings. B.5 Batch Size Inspired by recent findings that reducing the batch size can result in significant performance gains for discrete action RL (Obando Ceron et al., 2024), we reduce the number of transitions used for gradient calculation from 256 to 128. As shown in Figures 9a & 15, this batch size reduction leads to a marginal improvement in aggregate performance and decreased memory requirements of the algorithm. Interestingly, we find that batch size significantly affects performance, with no single value performing best across all tasks. B.6 Risk-Neutral Temporal Difference Using a pessimistic lower-bound Q-value approximation for actor-critic updates, known as Clipped Double Q-learning (CDQ) (Fujimoto et al., 2018; Haarnoja et al., 2018), is a popular method to counteract Q-value overestimation, though it introduces bias. Formally, it modifies the value estimate in Eq. 5 to a lower-bound estimation V lbθ(s) ≈ Qlbθ(s, a) − α log πϕ(a|s), a ∼ πϕ(s), (7) where Qlbθ(s, a) is a lower-bound Q-value estimation derived from a critic ensemble, often using two networks (Fujimoto et al., 2018; Haarnoja et al., 2018) Qlbθ(s, a) = min(Q1θ(s, a), Q2θ(s, a)). (8) Recent studies have shown that techniques like layer normalization or full-parameter resets can be more effective at combating overestimation than pessimistic Q-value approximation (Nauman et al., 2024). Since our critic architecture leverages multiple regularization techniques, we disable CDQ and use the ensemble mean instead of the minimum to calculate the targets for actor and critic 16updates. In Figures 7 & 9a, we compare the performance of the baseline BRO to a BRO agent that uses CDQ. Our findings indicate that using risk-neutral Q-value approximation in the presence of network regularization unlocks significant performance improvements without increasing value overestimation. B.7 Optimistic Exploration Optimism is an algorithmic design principle that balances exploration and exploitation (Ciosek et al., 2019; Moskovitz et al., 2021). The dual actor setup (Nauman & Cygan, 2023a,b) employs separate policies for exploration and temporal difference updates, with the exploration policy pursuing an optimistic upper-bound Q-value approximation. This approach has been shown to improve the sample efficiency of SAC-based agents (Nauman & Cygan, 2023a). We implement the optimistic policy such that the Q-value upper bound is calculated based on the epistemic uncertainty estimated via the quantile critic ensemble (Moskovitz et al., 2021). Figure 7 shows that using a dual policy setup leads to performance improvements. We observe that these results are particularly pronounced in the early training stages and for smaller networks (Figure 9a). Algorithm 2 BRO training step with changes with respect to standard SAC colored red. 1: Input: πp ϕ - pessimistic actor; πo η - optimistic actor; Qk θ,i - kth quantile of ith critic; Qk ¯θ,i - kth quantile of ith target critic; α - temperature; βo - optimism; τ - KL weight; 2: Hyperparameters: KL∗ - target KL; K - number of quantiles 3: Sample action from the optimistic actor s′, r= ENV.STEP (a) a ∼ πo η 4: Add transition to the replay buffer BUFFER .ADD (s, a, r, s′) 5: for i = 1 to ReplayRatio do 6: Sample batch of transitions s, a, r, s′ ∼ BUFFER .SAMPLE 7: Calculate critic target value without CDQ Qk ¯θ,µ(s′, a′) = 1 2 (Qk ¯θ,1(s′, a′) + Qk ¯θ,2(s′, a′)) with a′ ∼ πp ϕ(s′) 8: Update critic using pessimistic actor θ ← θ − ∇θHuber \u0000 Qθ(s, a), (r + γQµ ¯θ (s′, a′) − α log πp ϕ(a′|s′) \u0001 9: Calculate pessimistic actor value without CDQ Qµ θ (s, a) = 1 2K PK i=1(Qk θ,1(s, a) + Qk θ,2(s, a)) with a ∼ πp ϕ(s) 10: Update pessimistic actor ϕ ← ϕ + ∇ϕ \u0000 Qµ θ (s, a) − α log πp ϕ(a|s) \u0001 11: Calculate optimistic actor value Qo θ(s, a) = 1 2K PK i=1(Qk θ,1(s, a) +Qk θ,2(s, a) +βo|Qk θ,1(s, a) −Qk θ,2(s, a)|) with a ∼ πp ϕ(s) 12: Update optimistic actor η ← η + ∇η \u0000 Qµ θ (s, a) + βoQσ θ (s, a) − τKL \u0000 πp ϕ(s)|πo η(s) \u0001\u0001 with a ∼ πo η(s) 13: Update entropy temperature α ← α − ∇αα \u0000 H∗ − H(s) \u0001 14: Update optimism βo ← βo − ∇βo (βo − βp)( 1 |A|KL(πp ϕ|πo η) − KL∗) 15: Update KL weight τ ← τ + ∇τ τ( 1 |A|KL(πp ϕ|πo η) − KL∗) 16: Update target network ¯θ ← POLYAK(θ, ¯θ) 17: end for B.8 Approaches Examined During the Development of BRO Examined approaches are listed in Table 2. Methods incorporated into BRO include regulariza- tion techniques (LayerNorm, Weight Decay, removing CDQL), optimistic exploration, quantile distributional RL, resets and increased replay ratio. 17Table 2: Approaches examined during BRO development. Methods incorporated into BRO are highlighted in bold. Methods Group Specific Method Source Exploration DAC (Nauman & Cygan, 2023a) Value Regularization CDQL (removed) (Fujimoto et al., 2018) N-Step Returns (Sutton & Barto, 2018) Network Regularization LayerNorm (Ba et al., 2016) Weight Decay (Loshchilov & Hutter, 2017) Spectral Norm (Miyato et al., 2018) Scheduling N-Step Schedule (Kearns & Singh, 2000) Discount Schedule (François-Lavet et al., 2015) Pessimism Schedule Entropy Schedule Learning Rate Schedule (Andrychowicz et al., 2021) Distributional RL HL Gauss (Imani & White, 2018) Categorical (Bellemare et al., 2017) Quantile (Dabney et al., 2018) Plasticity Regularization Resets (Nikishin et al., 2022; D’Oro et al., 2022) Learning Replay Ratio (Nikishin et al., 2022; D’Oro et al., 2022) C Tested Environments We tested BRO on a variety of 40 tasks from DeepMind Control Suite (Tassa et al., 2018), MyoSuite (Caggiano et al., 2022) and MetaWorld (Yu et al., 2020). Selected tasks cover various challenges, from simple to hard, in locomotion and manipulation. Table 4 presents the environments with specified dimensions of states and actions. BRO is a versatile agent that can successfully perform tasks of different difficulty and various action and state spaces. Our selection of 40 tasks focuses on the most challenging tasks from the DeepMind Control Suite (DMC), MetaWorld (MW), and MyoSuite (MS) benchmarks, as identified in previous studies (Hansen et al., 2023). We chose these hard tasks because many easy tasks from these benchmarks can be solved by modern algorithms within 100k environment steps (Hansen et al., 2023; Wang et al., 2024). In the MetaWorld environment, we follow the TD-MPC2 evaluation protocol (Hansen et al., 2023). As such, the environment issues a truncate signal after 200 environment steps, after which we assess if the agent achieved goal success within the 200th step. We do not implement any changes to how goals are defined in the original MetaWorld and we use V2 environments. Table 3: List of tasks from DeepMind Control and MetaWorld on which the agents were ablated. The table also contains the dimensions of action and observation space. Task Observation dimension Action dimension DEEP MIND CONTROL Acrobot-Swingup 6 1 Dog-Trot 223 38 Hopper-Hop 15 4 Humanoid-Run 67 24 Humanoid-Walk 67 24 METAWORLD Assembly 39 4 Coffee-Push 39 4 Hand-Insert 39 4 Push 39 4 Stick-Pull 39 4 18Table 4: List of tasks from DeepMind Control, MetaWorld, and MyoSuite on which the agents were tested. The table also contains the dimensions of action and observation space. Task Observation dimension Action dimension DEEP MIND CONTROL Acrobot-Swingup 6 1 Cheetah-Run 17 6 Dog-Run 223 38 Dog-Trot 223 38 Dog-Stand 223 38 Dog-Walk 223 38 Finger-Turn-Hard 12 2 Fish-Swim 24 5 Hopper-Hop 15 4 Humanoid-Run 67 24 Humanoid-Stand 67 24 Humanoid-Walk 67 24 Pendulum-Swingup 3 1 Quadruped-Run 78 12 Walker-Run 24 6 METAWORLD Basketball 39 4 Assembly 39 4 Button-Press 39 4 Coffee-Pull 39 4 Coffee-Push 39 4 Disassemble 39 4 Hammer 39 4 Hand-Insert 39 4 Push 39 4 Reach 39 4 Stick-Pull 39 4 Sweep 39 4 Lever-Pull 39 4 Pick-Place 39 4 Push-Back 39 4 MYOSUITE Key-Turn 93 39 Key-Turn-Hard 93 39 Obj-Hold 91 39 Obj-Hold-Hard 91 39 Pen-Twirl 83 39 Pen-Twirl-Hard 83 39 Pose 108 39 Pose-Hard 108 39 Reach 115 39 Reach-Hard 115 39 D Hyperparameters Hyperparameters of BRO and other baselines are listed in Table 5. BRO (Fast) shares the same parameters as BRO except replay ratio 2 which significantly speeds the algorithm without sacrificing performance that much. BRO features the BRONet architecture and resets of all parameters done every 250k steps until 1M steps with additional resets at steps 15k and 50k. The selection of 19hyperparameters for BRO was based on the values reported in the main building blocks of BRO and extensive experimentation coupled with ablations studies. Table 5: Hyperparameter values for actor-critic agents used in the experiments. Parameter BRO SAC TD3 SR-SAC CrossQ Batch size (B) 128 256 Replay ratio 10 2 32 1 Critic hidden depth RESIDUAL 2 2 2 Critic hidden size 512 256 2048 Actor depth RESIDUAL 2 2 2 Actor size 256 Num quantiles 100 N/A KL target 0.05 N/A Initial optimism 1.0 N/A Std multiplier 0.75 N/A Actor learning rate 3e-4 1e-3 Critic learning rate 3e-4 1e-3 Temperature learning rate 3e-4 N/A 3e-4 Optimizer ADAM W ADAM Discount (γ) 0.99 Initial temperature (α0) 1.0 N/A 1.0 Exploratory steps 2,500 10,000 25,000 10,000 5,000 Target entropy (H∗) |A|/2 N/A |A|/2 |A| Polyak weight (τ) 0.005 N/A We compare BRO against official and widely used implementations of CrossQ, SAC, SR-SAC, TD3 and TD-MPC2 with open source repositories listed in Table 6. As the official results do not cover all 40 benchmarking tasks, we ran the baselines independently (except TD-MPC2, where all official results were available). SAC and TD3 are commonly used baselines; therefore, their hyperparameters vary across different implementations. To account for this fact, we ran 2 versions of these baselines: tuned and original. If not specified otherwise, we report the results of the tuned versions with hyperparameters in Table 5. The original versions of SAC and TD3 both feature a replay ratio of 1 and in the case of SAC, target entropy (H∗) equal to the action space dimension |A|. The performance of both variants of the implementations can be observed in Figure 24. Table 6: Links to the repositories of the used baselines. All are distributed under MIT license. Baseline Source code link CrossQ (Bhatt et al., 2023) github.com/adityab/CrossQ SAC (Haarnoja et al., 2018) github.com/denisyarats/pytorch_sac SAC (tuned version) github.com/ikostrikov/jaxrl SR-SAC (D’Oro et al., 2022)github.com/proceduralia/high_replay_ratio_continuous_control TD3 (Fujimoto et al., 2018) github.com/sfujim/TD3 TD3 (tuned version) github.com/ikostrikov/jaxrl TD-MPC2 (Hansen et al., 2023)github.com/nicklashansen/tdmpc2 As other baselines were developed and tested on only a subset of our 40 selected tasks, we observed that achieving similar performance on new tasks was challenging. This can be especially observed in the case of CrossQ, which is a state-of-the-art algorithm on selected tasks from OpenAI Gym 20(Brockman et al., 2016), but as it was tested only on a fraction of DeepMind Control Suite tasks, its performance does not transfer to our selection of tasks. Originally, CrossQ authors tested their agent on DeepMind Control Suite using Shimmy (Tai et al.) contrary to other agents that use the original codebase (Tassa et al., 2018). We run CrossQ using the DMC wrappers (D’Oro et al., 2022). Comparison between 2 variants of SAC and TD3 (Original and Tuned) is presented in Figure 24. Tuned versions feature a higher value of replay ratio (2 instead of 1) than the original and lower target entropy in the case of SAC (|A|/2 instead of |A|). Table 7: Description of the considered model sizes. Size Number of block Hidden Size 0.55M 1 BroNet block hidden size of 128 1.05M 1 BroNet block hidden size of 256 2.83M 1 BroNet block hidden size of 512 4.92M 2 BroNet blocks hidden size of 512 26.31M 3 BroNet blocks hidden size of 1024 E Additional Experiments E.1 Scaling and Time Experiments The execution time was measured for all agents for each of the 40 tasks averaged over 2 random seeds. We ran each agent for 105k steps with initial 5k exploration steps, 100k training steps, and 1 evaluation. We benchmark all 25 variants of BRO with 5 different model sizes and 5 values of replay ratio. Figure 12 different algorithms performance compared to execution time. Experiments were conducted on an NVIDIA A100 GPU with 10GB of RAM and 8 CPU cores of AMD EPYC 7742 processor. All tasks were run separately so the agents could use all resources independently. 0.0 0.2 0.4 0.6 0.8 1.0 Normalized execution time 0.0 0.2 0.4 0.6 0.8IQM performance BRO BRO-Fast TD-MPC2 SR-SAC SAC TD3 CrossQ Wallclock time and performance of agents Figure 12: Scatterplot of the performance of agents plotted against normalized execution time. Increasing the model size and replay ratio both improve the performance. However, the former is more efficient in terms of execution time due to GPU parallelism. For example, the largest BRO variant (26.31M parameters) with replay ratio 5 has similar execution time as the smallest one (0.55M parameters) with replay ratio 15, but the performance is much greater (Figures 13). E.2 Additional BroNet Analysis We examine various architectural blueprints on 5 DMC and 5 MetaWorld environments (see Table 3), each with over 10 seeds per task. Our starting point was the transformer-based design by Bjorck et al. (2021), termed Spectral. This architecture incorporates recent transformer advancements, moving Layer Norm to the beginning of the residual block to prevent vanishing gradients in deep networks. While Spectral performs better than the vanilla MLP, its performance on the DMC benchmark, particularly the Dog environment, is weaker. This aligns with findings from Nauman et al. (2024); Hussing et al. (2024), indicating that Layer Norm is crucial for stability in such complex tasks. To analyze the importance of layer normalization in the BroNet architecture, we replaced spectral norms with Layer Norms in the residual blocks, resulting in the BRO wo first LN architecture (Figure 5). This modification improves performance but still lags behind the full BRO architecture. Furthermore, 211 2 5 10 15 Replay ratio 26.31M 4.92M 2.83M 1.05M 0.55M Model size Mean execution time across 40 tasks 1 2 5 10 15 Replay ratio 26.31M 4.92M 2.83M 1.05M 0.55M Model size IQM Performance across 40 tasks Figure 13: Heatmaps of execution time and IQM performance across 40 tasks of 25 BRO variants with various model sizes and replay ratio values. Black lines connect the same interpolated values. we examine a simple MLP architecture with Layer Norm before each activation function. Since BRO consists of two residual blocks, we compare it with a 5-layer model, (Dense + LN) x 5. Figure 14 shows that Layer Norm after each Dense layer is effective, and in aggregated IQM, this model is comparable to BRO. However, skip connections in BRO are beneficial for managing complex environments like Dog. In conclusion, BroNet architecture uses Layer Norm and residual blocks for superior robustness and performance in challenging tasks. Figure 14: Comparison of five architecture designs across different environments: The top plot shows results on 5 DMC and 5 MetaWorld environments, the middle plot focuses on the 5 DMC environ- ments, and the bottom plot highlights the Dog Trot environment. BRO and Spectral architectures each consist of 2 residual blocks. (Dense + LN) x 5 represents standard MLP networks with 5 hidden layers, each incorporating Layer Norm before activation. Lastly, BRO wo first LN refers to the BRO architecture without Layer Norm in the first Dense block, before the residual connection. E.3 Additional analysis Batch sizes We ablate of the minibatch size impact on BRO and BRO (Fast) performance across different benchmarks is depicted in Figure 15. The figure shows that using half or even a quarter of the original minibatch size (256) does not significantly hurt BRO’s performance. Figure 15: Performance of BRO and BRO (Fast) with different minibatch sizes for: D&H (Dogs and Humanoid), DMC (DeepMind Control), MW (MetaWorld), and MS (MyoSuite). 22Target network We investigate the performance benefits stemming from using a target network with the BRO agent. We present these results in Figure 16. Interestingly, we observe that the target network yields performance improvements only in specific environments. Figure 16: We compare BRO against BRO without target network. 10 seeds per task, 1M steps. More baselines To evaluate BRO performance beyond maximum entropy objectives, we tested BRO and BRO (Fast) with a TD3 backbone. BRO with a SAC backbone slightly outperformed TD3, though TD3 remains a viable option. Furthermore, we compare BRO performance to three additional baselines. These results are presented in Figure 17. 0 0.25 0.50 0.75 1.00 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM 15 DeepMind Control Tasks 0 0.25 0.50 0.75 1.00 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog and Humanoid BRO BRO (Fast) BROTD3 BROTD3 (Fast) SMR TD7 REDQ Figure 17: We run BRO and BRO (Fast) with a TD3 backbone (BROTD3). 10 seeds. Longer training We expanded BRO training beyond 1M environment steps, although in a single- task setup. We trained BRO and BRO (Fast) for 3M and 5M steps respectively on 7 Dog and Humanoid tasks and compared them to TD-MPC2 and SR-SAC. BRO significantly outperforms these baselines and notably almost solves the Dog Run tasks at 5M steps (achieving over 80% of possible returns). We show the 3M results in Figure 18. 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog Run 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog Stand 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog Trot 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog Walk 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Humanoid Run 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Humanoid Stand 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Humanoid Walk 0 500 1000 1500 2000 2500 3000 Environment Step (M) 0.0 0.2 0.4 0.6 0.8 1.0IQM Dog and Humanoid BRO BRO (Fast) TD-MPC2 SR-SAC Figure 18: We run BRO on complex tasks for 3M steps. 5 seeds. Image-based tasks To analyze the impact of BroNet on image-based RL tasks, we experiment with 3 tasks from the Atari 100k (Bellemare et al., 2013) benchmark. Here, we changed the regular 23Q-network of the SR-SPR (RR=2) model (D’Oro et al., 2022) to a BroNet, and considered changing the reset schedules to better fit the plasticity of the BroNet model. As depicted in the Table below, applying BroNet to discrete, image-based tasks is a promising avenue for future research. Table 8: We replace the Q-network in SR-SPR with BroNet, while keeping the standard convolutional encoder. We test two values of reset interval (RI) and shrink-and-perturb (SP) and find that these hyperparameters impact the performance of the BroNet agent. 5 seeds. Task SR-SPR SR-SPR-BroNet SR-SPR-BroNet SR-SPR-BroNet (RI=20k;SP=0.8) (RI=20k;SP=0.8) (RI=20k;SP=0.5) (RI=5k;SP=0.8) Pong -10.5 4.8 10.2 -12.0 Seaquest 714.7 399.0 420.7 782.0 Breakout 24.5 13.5 13.7 33.3 Multi-Task RL Finally, we evaluate whether the increased model size improves the agent’s ca- pability in a multi-task setup (Yu et al., 2020; Hansen et al., 2023). To this end, we compare the BRO (Fast) to SAC on the MT50 benchmark from MetaWorld, which comprises 50 different tasks. To accommodate the task diversity, we increase the width of all models twofold and pass a one-hot vector representing the task identifier in the network input. Otherwise, both algorithms are run with the same hyperparameters as the main experiments. We present the results in Figure 19. 0 125 250 375 500 Environment Step (Thousand) 0.0 0.1 0.2 0.3 0.4 0.5 MetaWorld MT50 SAC BRO (Fast) Figure 19: We compare BRO (Fast) with SAC on the multi-task benchmark. 3 seeds F Training Curves We present the aggregated performance of BRO compared to other baselines at Dog & Humanoid tasks, DeepMind Control Suite, Metaworld and MyoSuite in Figure 20 together with summarized performance results at 100k, 200k, 500k and 1M steps in Table 9. The performance on each of the 40 individual tasks in shown in Figures 21, 23, 22. Figure 20: BRO aggregated performance over 1M steps on 40 tasks from DeepMind Control Suite, MetaWorld and MyoSuite. Y -axis represents the IQM of normalized rewards. 240.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500 600reward acrobot_swingup 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward cheetah_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500reward dog_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800 1000reward dog_stand 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward dog_trot 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward dog_walk 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800 1000reward finger_turn_hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward fish_swim 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500 600reward hopper_hop 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 50 100 150 200 250 300 350reward humanoid_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward humanoid_stand 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward humanoid_walk 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward pendulum_swingup 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward quadruped_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward walker_run Figure 21: Results of 15 tasks from DeepMind Control Suite for BRO and other baselines run for 1M steps. We present the IQM of rewards and 95% confidence intervals. 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success assembly-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success basketball-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success button-press-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success coffee-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success coffee-push-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success disassemble-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success hammer-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success hand-insert-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success lever-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success pick-place-wall-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success push-back-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success push-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success reach-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success stick-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success sweep-v2 Figure 22: Results of 15 tasks from MetaWorld for BRO and other baselines run for 1M steps. We present the IQM of success rate and 95% confidence intervals. 250.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-key-turn 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-key-turn-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-obj-hold 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8success myo-obj-hold-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pen-twirl 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pen-twirl-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pose 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.00 0.05 0.10 0.15 0.20 0.25success myo-pose-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-reach 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-reach-hard Figure 23: Results of 10 tasks from MyoSuite for BRO and other baselines run for 1M steps. We present the IQM of success rate and 95% confidence intervals. Table 9: Summary of IQM results of BRO and other agents evaluated on40 tasks from DeepMind Control Suite, Metaworld and MyoSuite achieved at 100k, 200k, 500k and 1M steps. BRO achieves better results than other state-of-the-art agents (both model-based and model-free) while featuring great sample efficiency. Step BRO BRO (Fast) TD-MPC2 SR-SAC SAC TD3 CrossQ AGGREGATED 40 TASKS 100k 0.254 0.113 0.204 0.046 0.007 0.008 0.004 200k 0.560 0.384 0.519 0.083 0.043 0.054 0.009 500k 0.862 0.772 0.745 0.373 0.167 0.157 0.037 1M 0.945 0.878 0.842 0.595 0.316 0.294 0.042 DEEP MIND CONTROL SUITE 100k 0.230 0.123 0.128 0.089 0.030 0.046 0.031 200k 0.442 0.282 0.332 0.195 0.088 0.091 0.038 500k 0.726 0.613 0.532 0.412 0.259 0.261 0.092 1M 0.836 0.794 0.673 0.487 0.391 0.381 0.106 METAWORLD 100k 0.247 0.113 0.452 0.046 0.000 0.000 0.000 200k 0.642 0.571 0.835 0.062 0.018 0.047 0.000 500k 0.984 0.929 0.952 0.421 0.108 0.084 0.000 1M 1.000 0.976 0.978 0.769 0.303 0.250 0.000 MYOSUITE 100k 0.392 0.124 0.088 0.000 0.000 0.000 0.020 200k 0.748 0.400 0.394 0.015 0.024 0.008 0.140 500k 0.888 0.776 0.688 0.285 0.140 0.120 0.354 1M 0.980 0.876 0.775 0.538 0.276 0.256 0.474 DOG & HUMANOID 100k 0.038 0.008 0.014 0.007 0.006 0.013 0.011 200k 0.238 0.056 0.058 0.024 0.015 0.040 0.013 500k 0.661 0.469 0.302 0.107 0.080 0.133 0.025 1M 0.864 0.772 0.527 0.099 0.155 0.221 0.040 260.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500 600reward acrobot_swingup 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward cheetah_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500reward dog_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800 1000reward dog_stand 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward dog_trot 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward dog_walk 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800 1000reward finger_turn_hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward fish_swim 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 100 200 300 400 500 600reward hopper_hop 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 50 100 150 200 250 300 350reward humanoid_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward humanoid_stand 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward humanoid_walk 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward pendulum_swingup 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward quadruped_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0 200 400 600 800reward walker_run 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success assembly-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success basketball-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success button-press-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success coffee-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success coffee-push-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success disassemble-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success hammer-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success hand-insert-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success lever-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success pick-place-wall-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success push-back-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success push-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success reach-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success stick-pull-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success sweep-v2 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-key-turn 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-key-turn-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-obj-hold 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8success myo-obj-hold-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pen-twirl 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pen-twirl-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-pose 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.00 0.05 0.10 0.15 0.20 0.25success myo-pose-hard 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-reach 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e6 0.0 0.2 0.4 0.6 0.8 1.0success myo-reach-hard 40 Tasks from DeepMind Control Suite, MetaWorld and MyoSuite Figure 24: Results of different variants of SAC and TD3 on 40 tasks. 27",
      "references": [
        "Deep reinforcement learning at the edge of the statistical precipice.",
        "What matters in on-policy reinforcement learning? a large- scale empirical study.",
        "Layer normalization.",
        "Efficient online reinforcement learning with offline data.",
        "The arcade learning environment: An evaluation platform for general agents.",
        "A distributional perspective on reinforcement learning.",
        "Cross q: Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity.",
        "Towards deeper deep reinforcement learning with spectral normalization.",
        "Openai gym, 2016.",
        "Myosuite–a contact-rich simulation suite for musculoskeletal motor control.",
        "Learning pessimism for reinforcement learning.",
        "Ucb exploration via q-ensembles.",
        "Expected policy gradients for reinforcement learning.",
        "Better exploration with optimistic actor critic.",
        "Implicit quantile networks for distributional reinforcement learning.",
        "Bert: Pre-training of deep bidirectional transformers for language understanding.",
        "Sample- efficient reinforcement learning by breaking the replay ratio barrier.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Palm-e: An embodied multimodal language model.",
        "Stop regressing: Training value functions via classification for scalable deep rl.",
        "How to discount deep reinforcement learning: Towards new dynamic strategies.",
        "D4rl: Datasets for deep data-driven reinforcement learning.",
        "Addressing function approximation error in actor-critic methods.",
        "Soft actor-critic algorithms and applications.",
        "Mastering diverse domains through world models.",
        "Td-mpc2: Scalable, robust world models for continuous control.",
        "Dropout q-functions for doubly efficient reinforcement learning.",
        "Dissecting deep rl with high update ratios: Combatting value overestimation and divergence.",
        "Improving regression performance with distributional losses.",
        "Bias-variance error bounds for temporal difference updates.",
        "JAXRL: Implementations of Reinforcement Learning algorithms in JAX.",
        "Offline reinforcement learning with implicit q-learning.",
        "Offline q-learning on diverse multi- task data both scales and generalizes.",
        "Slow and steady wins the race: Maintaining plasticity with hare and tortoise networks.",
        "Efficient deep reinforcement learning requires regulating overfitting.",
        "Decoupled weight decay regularization.",
        "Disentangling the causes of plasticity loss in neural networks.",
        "Spectral normalization for generative adversarial networks.",
        "Human-level control through deep reinforcement learning.",
        "Tactical optimism and pessimism for deep reinforcement learning.",
        "On the theory of risk-aware agents: Bridging actor-critic and economics.",
        "Decoupled actor-critic.",
        "Over- estimation, overfitting, and plasticity in actor-critic: the bitter lesson of reinforcement learning.",
        "The primacy bias in deep reinforcement learning.",
        "Small batch deep reinforcement learning.",
        "Mixtures of experts unlock parameter scaling for deep rl.",
        "Open x-embodiment: Robotic learning datasets and rt-x models.",
        "Markov decision processes: discrete stochastic dynamic programming.",
        "Stable-baselines3: Reliable reinforcement learning implementations.",
        "A generalist dynamics model for control.",
        "Data-efficient reinforcement learning with self-predictive representations.",
        "Bigger, better, faster: Human-level atari with human-level efficiency.",
        "Reinforcement learning: An introduction.",
        "Shimmy: Gymnasium and PettingZoo Wrappers for Commonly Used Environments.",
        "Investigating multi-task pretraining and generalization in reinforcement learning.",
        "Efficientnet: Rethinking model scaling for convolutional neural networks.",
        "Deepmind control suite.",
        "A theoretical and empirical analysis of expected sarsa.",
        "Attention is all you need.",
        "Efficientzero v2: Mastering discrete and continuous control with limited data.",
        "Optimism in reinforcement learning with generalized linear function approximation.",
        "On layer normalization in the transformer architecture.",
        "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.",
        "Rt-2: Vision-language-action models transfer web knowledge to robotic control."
      ],
      "meta_data": {
        "arxiv_id": "2405.16158v3",
        "authors": [
          "Michal Nauman",
          "Mateusz Ostaszewski",
          "Krzysztof Jankowski",
          "Piotr Miłoś",
          "Marek Cygan"
        ],
        "published_date": "2024-05-25T09:53:25Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces BRO (Bigger, Regularized, Optimistic), a model-free reinforcement learning algorithm for continuous control that pairs strongly regularised, large-capacity critic networks (BroNet) with optimistic exploration. Demonstrates that scaling the critic’s parameter count (≈5 M) together with moderate replay ratios yields superior sample efficiency and wall-clock speed. BRO achieves new state-of-the-art performance across 40 challenging tasks (DeepMind Control, MetaWorld, MyoSuite), including first near-optimal policies on Dog and Humanoid tasks within 1 M steps, outperforming leading model-based (TD-MPC2) and model-free baselines.",
        "methodology": "BRO builds on Soft Actor-Critic. Key ingredients: (1) BroNet critic architecture – residual MLP with LayerNorm after every dense layer plus weight-decay and periodic full-parameter resets, enabling safe scaling to ~5–26 M parameters; actor remains standard small network. (2) Moderate replay ratio (default RR=10, BRO-Fast RR=2). (3) Removal of Clipped Double Q; uses risk-neutral quantile critics (100 quantiles) and dual-actor setup: a pessimistic policy for updates and an optimistic policy for exploration whose objective includes upper-confidence Q bound derived from epistemic uncertainty. (4) Additional regularisation: small batch (128), weight decay, target networks. These components collectively improve sample and compute efficiency.",
        "experimental_setup": "Evaluated BRO, BRO-Fast and 5 baselines (TD-MPC2, SR-SAC, CrossQ, SAC, TD3) on 40 continuous-control tasks: 15 DeepMind Control Suite, 15 MetaWorld v2 sparse-reward tasks, 10 MyoSuite musculoskeletal tasks. Each algorithm run for 1 M environment steps (some extended to 3 M in analysis) with 10 random seeds (TD-MPC2 results reused from paper). Metrics: normalized return or success rate, aggregated using inter-quartile mean (IQM) with 95% CIs via RLiable. Ablations (>15 000 runs) varied critic size (0.55–26 M params), replay ratio, architecture, batch size, exploration strategy, target networks. Wall-clock measured on A100 GPU.",
        "limitations": "BRO’s large critic networks (≈5 M params default, up to 26 M tested) increase GPU memory footprint and may hinder real-time inference on embedded hardware. Study is limited to proprioceptive state inputs; applicability to image-based or multimodal observations untested. Only continuous action domains examined—effectiveness in discrete MDPs unknown. Benchmarks, while diverse, are simulated; transfer to real-world robotics not validated. Hyper-parameters (e.g., replay ratio, reset schedule) tuned empirically and may require retuning for other settings.",
        "future_research_directions": "1) Investigate model compression (quantisation, distillation) to reduce inference cost of large critics. 2) Extend BRO components to vision-based RL, exploring trade-offs between encoder and critic scaling. 3) Adapt optimistic exploration and BroNet scaling to discrete or hybrid action spaces. 4) Combine BRO with model-based elements or offline pre-training for further sample efficiency. 5) Evaluate on real-robot tasks and develop standardised sample-efficiency benchmarks that include unsolved tasks (e.g., Humanoid Run) to avoid overfitting.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Sample-Efficient Agnostic Boosting",
      "full_text": "arXiv:2410.23632v1  [cs.LG]  31 Oct 2024 Sample-Efﬁcient Agnostic Boosting Udaya Ghai Amazon ughai@amazon.com Karan Singh T epper School of Business Carnegie Mellon University karansingh@cmu.edu Abstract The theory of boosting provides a computational framework f or aggregating ap- proximate weak learning algorithms, which perform margina lly better than a ran- dom predictor, into an accurate strong learner. In the reali zable case, the success of the boosting approach is underscored by a remarkable fact th at the resultant sam- ple complexity matches that of a computationally demanding alternative, namely Empirical Risk Minimization (ERM). This in particular impl ies that the realizable boosting methodology has the potential to offer computatio nal relief without com- promising on sample efﬁciency. Despite recent progress, in agnostic boosting, where assum ptions on the condi- tional distribution of labels given feature descriptions a re absent, ERM outstrips the agnostic boosting methodology in being quadratically m ore sample efﬁcient than all known agnostic boosting algorithms. In this paper, we make progress on closing this gap, and give a substantially more sample efﬁci ent agnostic boost- ing algorithm than those known, without compromising on the computational (or oracle) complexity. A key feature of our algorithm is that it leverages the abil- ity to reuse samples across multiple rounds of boosting, whi le guaranteeing a generalization error strictly better than those obtained b y blackbox applications of uniform convergence arguments. W e also apply our approac h to other previ- ously studied learning problems, including boosting for re inforcement learning, and demonstrate improved results. 1 Introduction A striking observation in statistical learning is that give n a small number of samples it is possible to learn the best classiﬁer from an almost exponentially lar ge class of predictors. In fact, it is possible to do using a conceptually straightforward proced ure – Empirical Risk Minimization (ERM) – that ﬁnds a classiﬁer that is maximally consistent with the collected samples. Substantiating this observation, the fundamental theorem of statistical learn ing (e.g., [ SSBD14]) states that with high probability ERM can guarantee ε−excess population error with respect to the best classiﬁer f rom a ﬁnite, but large hypothesis class H given merely magnostic ≈ (log |H|)/ε2 identically distributed and independent (IID) samples of pairs of features and labels fr om the population distribution. Under an additional assumption – that of realizability – guaranteeing that there exists a perfect classiﬁer in the hypothesis class achieving zero error, a yet quadratically smaller number mrealizable ≈ (log |H|)/ε of samples sufﬁce. In the absence of such assumption, the lea rning problem is said to take place in the agnostic setting, i.e., under of a lack of belief in the ability of any h ypothesis to perfectly ﬁt the observed data. This ability to generalize to the population distribution a nd successfully (P AC) learn from an al- most exponentially large, and hence expressive, hypothesi s class given limited number of examples suggests that the primary bottleneck for efﬁcient learning is computational. Indeed, even with mod- 38th Conference on Neural Information Processing Systems ( NeurIPS 2024).Sample Complexity Oracle Complexity [KK09] (log |B|)/γ4ε4 1/γ2ε2 [BCHM20] (log |B|)/γ4ε6 1/γ2ε2 Theorem 4 (log |B|)/γ3ε3 (log |B|)/γ2ε2 Theorem 9 (in Appendix B) (log |B|)/γ3ε3 + (log |B|)3/γ2ε2 1/γ2ε2 ERM (no boosting) (log |B|)/γ2ε2 +∞ (inefﬁcient) T able 1: A comparison between sample and oracle complexitie s (i.e., number of weak learning calls) of the present results and previous works, in each cas e to achieve ε-excess population error. Here we suppress polylogarithmic factors. W e make progress on closing the sample complexity gap between ERM, which is computationally inefﬁcient, and boos ting-based approaches. The γ-weak leaner outputs a hypothesis from the base class B, which is usually substantially smaller than H against which the ﬁnal agnostic learning guarantee holds. I n practice, boosting is used with learners with small values of log |B|. See Deﬁnition 1 for details. See the paragraph following Theorem 1 in [ KK09] and Section 3.3 in [ BCHM20] for derivation of these bounds. See also Theorem 2.14 in [AGHM21] for a bound on the expressivity of the boosted class to deriv e ERM’s sample complexity. est sample requirements, ﬁnding a maximally consistent hyp othesis within an almost exponentially large class via, say, enumeration or global search, is gener ally computationally intractable. It is against this backdrop that the theory of boosting offer s a compelling alternative. The starting point is the realization that often, both in practice and in t heory, it is easy to construct simple, yet inaccurate rules-of-thumbs (or weak learners ) that perform ever so slightly better than a random classiﬁer. A natural question then arises (paraphrased fro m [ Sch90]’s abstract): can one convert such mediocre learning rules into one that performs extreme ly well? Boosting algorithms offer a positive resolution to this question by providing convenie nt and computationally efﬁcient reductions that aggregate such weak learners into a proﬁcient learner w ith an arbitrarily high accuracy. Realizable Boosting. Consider the celebrated Adaboost algorithm [ FS97] which operates in the (noiseless) realizable binary classiﬁcation setting. On a ny distribution consistent with a ﬁxed label- ing function (or concept), a weak learner here promises an accuracy strictly better t han half, since guessing labels randomly gets any example correct with prob ability half. Given access to such a weak learner and mrealizable boosting ≈ (log |H|)/ε samples1, Adaboost makes ≈ log 1/ε calls to the weak learner to produce a classiﬁer with (absolute) error at most ε on any distribution consistent with the same labeling function. Thus, not only is Adaboost c omputationally efﬁcient provided, of course, such weak learners can be found, but also its sample c omplexity is no worse than that of ERM. This underscores the fact that the realizable boosting meth odology has the potential to offer computational relief, without compromising on sample efﬁc iency. Agnostic Boosting. In practice, realizability is an onerous assumption; it is t oo limiting for the ob- served feature values alone to determine the label complete ly and deterministically, and that such a relation can be perfectly captured by an in-class hypothesi s. Agnostic learning forgoes such assump- tions. In their absence, bounds on absolute error are unachi evable, e.g., when labels are uniformly random irrespective of features, no classiﬁer can achieve a ccuracy better than half. Instead, in ag- nostic learning, the goal of the learner is to output a hypoth esis with small excess error with respect to the best in-class classiﬁer. If an in-class hypothesis is perfect on a given distribution, this relative error translates to an absolute error bound, thus generaliz ing the realizable case perfectly. Indeed, such model agnosticism has come to be a lasting hallmark of mo dern machine learning. Early attempts at realizing the promise of boosting in the ag nostic setting were met with limited success: while they did boost the weak learner’s accuracy, t he ﬁnal hypothesis produced was not competitive with the best in-class hypothesis. W e survey so me of these in the related work section. 1 In the introduction, for simplicity , we suppress polynomia l dependencies in the weak learner’s edge γ, and polylogarithmic terms. A recent sample complexity lower bo und due to [ GLR22] implies that this equivalence continues to hold even taking into account poly (γ) dependencies as long as γ is not exponentially small. 2Episodic model Rollouts w . ν-resets [BHS22] 1/γ4ε5 1/γ4ε6 Theorem 7 1/γ3ε4 1/γ3ε5 T able 2: Sample complexity of reinforcement learning given γ-weak learner over the policy class, for two different modes of accessing the underlying MDP , in t erms of ε and γ, suppressing other terms. A later result, and the work most related to ours, is due to [ KK09]. A weak learner in this setting returns a classiﬁer with a correlation against the true labe ls that is γ (say 0.1) times that of the best in-class hypothesis. Random guesses of the labels prod uce a correlation of zero; hence, a weak classiﬁer interpolates the performance of the best in- class hypothesis with that of a random one. Given access to such a weak learner, the boosting algori thm of [ KK09] makes ≈ 1/ε2 calls to the weak learner and draws magnostic boosting ≈ (log |H|)/ε4 samples to produce a learning rule (not necessarily in the hypotheses class, hence improper) with ε-excess error. The dependency of the sample complexity in the target accuracy is thus quadrat ically worse than that of ERM. This gap persists untarnished for other known agnostic boosting algorithms too. In this work, we seek to diminish this fundamental gap and construct a more sample-e fﬁcient agnostic boosting algorithm. Our main result is an efﬁcient boosting algorithm that upon r eceiving ≈ (log |H|)/ε3 samples pro- duces an improper learning rule with ε-excess error on the population loss. This is accomplished b y the careful reuse of samples across rounds of boosting. W e al so extend these guarantees to inﬁnite function classes, and give applications of our main result i n reinforcement learning, and in agnos- tically learning halfspaces, in each case improving known r esults. W e detail key contributions and technical challenges in achieving them next. 1.1 Contributions and technical innovations Contribution 1: Sample-efﬁcient Agnostic Booster . W e provide a new potential-based agnostic boosting algorithm (Algorithm 1) that achieves ε-excess error when given a γ-weak learner operating with a base class B. In Theorem 4, we prove that the sample complexity of this new algorithm sc ales as (log |B|)/γ3ε3, improving upon all known approaches to agnostic boosting. See T able 1. A key innovation in our algorithm design and the source of our sample efﬁciency is the careful recursive reuse of samples between rounds of boosting, via a second-order estimator of the potential in Line 5.II. In contrast, [ KK09] draws fresh samples for every round of boosting. A second coupled innovation, this time in our analysis, is to circumvent a uniform convergence argument on the boosted hypothesis class , which would otherwise result in a sample complexity that scales as ≈ 1/ε4. Indeed, the algorithm in [ BCHM20] reuses the entire training dataset across all rounds of boosting. This approach succeeds in boosting t he accuracy on the empirical distri- bution; however, success on the population distribution now relies on a uniform convergence (or sample compression) argument on the boosted class, the comp lexity of which grows linearly with the number of rounds of boosting since boosting algorithms a re inevitably improper (i.e., output a ﬁnal hypothesis by aggregating weak learners, hence outsi de the base class). Instead, we use a martingale argument on the smaller base hypothesis class to show that the empirical distributions constructed by our data reuse scheme and fed to the weak learn er track the performance of any base hypothesis on the population distribution. This is encapsu lated in Lemma 6. Finally, while we follow the potential-based framework lai d in [ KK09], we ﬁnd it necessary to alter the branching logic dictating what gets added to the en semble at every step. At each step, the algorithm makes progress via including the weak hypothesis or making a step “backward” via adding in a negation of the sign of the current ensemble. W e note that there is a subtle error in [ KK09] (see Appendix A), that although for their purposes is rectiﬁable without a c hange in the claimed sample complexity, leads to 1/ε4 sample complexity here in spite of the above modiﬁcations. A t the leisure of 1/ε4 sample complexity, the ﬁx is to test which of these alternati ves fares better by drawing fresh samples every round. However, given a smaller budget, the er ror of the negation of the sign of the ensemble, which lies outside the base class, is not efﬁcient ly estimable. Instead, in Line 9 we give a 3different branching criteria that can be evaluated using th e performance of the weak hypothesis on past data alone. Contribution 2: T rading off Sample and Oracle Complexity . Although Theorem 4 offers an unconditional improvement on the sample complexity, it mak es more calls to the weak learners than previous works. T o rectify this, we give a second guarantee o n the performance of Algorithm 1 in Theorem 9 (in Appendix B), with the oracle complexity matching that of known results . The resultant sample complexity improves known results for all practical (i.e., sub-constant) regimes of ε. This is made possible by a less well known variant of Freedma n’s inequality [ Pin94, Pin20] that applies to random variables bounded with high probability. Contribution 3: Extension to Inﬁnite Classes. Although in our algorithm the samples fed to the weak leaner are independent conditioned on past sources of r andomness, our relabeling and data reuse across rounds introduces complicated inter-depende ncies between samples. For example, a sample drawn in the past can simultaneously be used as is in th e present round (i.e., in Line 5.I), in addition to implicitly being used to modify the label of a dif ferent sample via the weak hypothesis it induced in the past (i.e., in Line 5.II). Thus, the textbook m achinery of extending ﬁnite hypothesis results to inﬁnite class applicable to IID samples via symme trization and Rademacher complexity (see, e.g., [ SSBD14, MR T18, BBL05]) is unavailable to us. Instead, we ﬁrst derive L1 covering number based bounds in Theorem 19 (in Appendix F). Through a result in empirical process theory [VDVW97], we translate these to a VC (B)/γ3ε3 sample complexity bound, where VC (B) is the VC dimension of class B, in Theorem 5. Contribution 4: Applications in Reinforcement Learning an d Agnostic Learning of Halfspaces. Building on earlier reductions from reinforcement learnin g to supervised learning [ KL02], [ BHS22] initiated the study of function-approximation compatible reinforcement learning, given access to a weak learner over the policy class. By applying our agnostic booster to this setting, we improve their sample complexity by poly (ε, γ) factors for binary-action MDPs, as detailed in T able 2. Also, following [ KK09], we apply our agnostic boosting algorithm to the problem of learning halfspaces over the Boolean hypercube and exhibit improved boosting-b ased results in Theorem 8. Contribution 5: Experiments. In preliminary experiments in Section 7, we demonstrate that the sample complexity improvements of our approach do indeed ma nifest in the form of improved em- pirical performance over previous agnostic boosting appro aches on commonly used datasets. 2 Related work The possibility of boosting was ﬁrst posed in [ KV94], and was resolved positively in a remarkable result due to [ Sch90] for the realizable case. The Adaboost algorithm [ FS97] paved the way for its practical applications (notably in [ VJ01]). W e refer the reader to [ SF13] for a comprehensive text that surveys the many facets of boosting, including its connections to game theory and online learning. See also [ HLR23, GLR22, AGHM21] for recent developments. The fact that Adaboost and its natural variants are brittle i n presence of label noise and lack of realizability [ LS08] prompted the search for boosting algorithms in the realiza ble plus label noise [DIK+21, KS03] and agnostic learning models [ BDLM01, MM02, KMV08, Kal07, CBC16]. In general, these boosting models are incomparable: although agnostic learning implies success in the random noise model, agnostic weak learning also constitute s a stronger assumption. Early agnostic boosting results could not boost the learner’s accuracy to m atch that of the best in-class hypothesis; this limitation was tied to their notion of agnostic weak lea rning. Our work is most closely related to [ KK09, Fel10]; we use the same notion of agnostic weak learning. Boosting has also been extended to the online setting. [ BKL15, CLL12, JGT17] study boosting in the mistake bound (realizable) model, while [ BCHM20, R T22, HS21] focus on regret minimization. Our scheme of data reuse is inspired by variance reduction te chniques [ SLRB17, JZ13, FLLZ18, ABS23] in convex optimization, although there considerations of uniform convergence and general- ization are absent, and our algorithm does not admit a natura l gradient descent interpretation. 42 − z (z + 2)e−z Figure 1: The two components of the piecewise potential func tion φ(z), with (z + 2)e−z plotted in blue, and 2 − z in red. Note that φ(z) is the point-wise maximum of the two. 3 Problem setting Let D ∈ ∆( X × {± 1}) be the joint population distribution over features, chosen from X , and (signed) binary labels with respect to which a classiﬁer’s h : X → {± 1} performance may be as- sessed. The performance criterion we consider is the 0-1 los s over the true labels and the classiﬁer’s predictions. lD(h) = E(x,y )∼D [1(h(x) ̸= y)] = Pr (x,y )∼D[h(x) ̸= y] Relatedly, one may measure the correlation between the clas siﬁer’s predictions and the true labels. corrD(h) = E(x,y )∼D [yh(x)] Note that for signed binary labels, we have that lD(h) = 1 2 (1 − corrD(h)). Therefore, these notions are equivalent in that a classiﬁer that maximizes the correl ation with true labels also minimizes the 0-1 loss, and vice versa, even in a relative error sense. Deﬁnition 1 (Agnostic W eak Learner) . A learning algorithm is called a γ-agnostic weak learner with sample complexity m : R+ × R+ → N ∪ { +∞} with respect to a hypothesis class H and a base hypothesis class B if, for any ε0, δ0 > 0, upon being given m(ε0, δ0) independently and identically distributed samples from any distribution D′ ∈ ∆( X × {± 1}), it can output a base 2 hypothesis W ∈ B such that with probability 1 − δ0 we have corrD′ (W) ≥ γ max h∈H corrD′ (h) − ε0. As remarked in [ KK09], typically m(ε, δ) = O((log |B|/δ)/ε2), and we use this fact in compil- ing T able 1. However, following [ KK09, BCHM20], we state our main result for ﬁxed ε0, δ0 in Theorem 4, where a necessary and irreducible ε0 term shows up in our ﬁnal accuracy. Although not explicitly mentioned in our weak learning deﬁn ition, our algorithm falls within the distribution-speciﬁc boosting framework [ KK09, Fel10]. In particular, like previous work on ag- nostic boosting, Algorithm 1 can be implemented by relabeling examples , instead of adaptively reweighing them. Thus, the overall marginal distribution o f any D′ fed to the learner on the feature space X is the same as that induced by the population distribution D on X . Under such promise on inputs, distribution-speciﬁc weak learners may be easier t o ﬁnd. 4 The algorithm and main results Notations. Given two functions f, g : X → R and generic scalars α, β ∈ R, we use af + bg to de- note a function such that (αf +βg)(x) = αf(x)+βg(x) for all x ∈ X . Given a function f : X → R, we take sign(f) to be a function such that for all x ∈ X , sign(f)(x) = 1(f(x) ≥ 0) − 1(f(x) < 0). Deﬁne a ﬁltration sequence {Ft : t ∈ N≥0}, where Ft capture all source of randomness the algo- rithm is subject to in the ﬁrst t iterations. For brevity, we deﬁne Et[·] = E[·|Ft]. For any feature-label dataset ˆD, we use E ˆD and corr ˆD to denote the empirical average and empirical correlation o ver ˆD. 2T ypically , the base class B is (often substantially) smaller than H. For example, decision stumps are a common example of the base class. 5Algorithm 1 Agnostic Boosting via Sample Reuse 1: Inputs: Sampling oracle for D supported on X × {± 1}, γ-agnostic weak learning oracle W, step-size η, mixing parameter σ, number of iterations T , per-iteration sample size S, resampling parameter m, branching tolerance τ, post-selection sample size S0, potential φ : R → R. 2: Initialize a zero hypothesis H1 = 0. 3: for t = 1 to T do 4: Sample S IID examples from the distribution D to create a dataset ˆDt. 5: Construct a sampling distribution Dt that samples (x, y) uniformly from ˆDt if t = 1 , and for t > 1 produces IID samples (x, ˆy) as follows: I With probability 1 − σ, return a sample (x, ˆy) from Dt−1. II With remaining probability σ, draw η′ ∼ Unif[0, η], pick (x, y) uniformly from ˆDt, construct a pseudo label ˆy = {+1 with probability pt(x, y, η ′), −1 with probability 1 − pt(x, y, η ′), and return (x, ˆy), where pt(x, y, η ′) = 1 2 − σφ′(yHt−1(x))y + ηφ′′(y(Ht−1(x) + η′ht−1(x)))ht−1(x) 2(η + σ) . 6: Sample m samples from Dt to create another dataset ˆD′ t. 7: Call the weak learning oracle on ˆD′ t to get Wt = W( ˆD′ t). 8: Measure the empirical correlation of Wt on ˆD′ t as corr ˆD′ t (Wt) = ∑ (x, ˆy)∈ ˆD′ t ˆyWt(x). 9: Set ht = Wt/γ if corr ˆD′ t (Wt) > τ else ht = − sign(Ht). 10: Update Ht+1 = Ht + ηht. 11: end for 12: Sample S0 IID examples from the distribution D to create a dataset ˆD0. 13: Output the hypothesis h = arg max h∈{sign(Ht):t∈[T ]} ∑ (x,y )∈ ˆD0 yh(x). Potential function. Deﬁne the potential function φ : R → R as φ(z) = {2 − z if z ≤ 0, (z + 2)e−z if z > 0. (1) W e can use this to assign a population potential to any real-v alued hypothesis H : X → R as Φ D(H) = E(x,y )∼D [φ(yH(x)] . T o maximize correlation between true labels and the hypothe sis H’s outputs, one wants H(x) > 0 whenever y = +1 for most samples drawn from the underlying distribution. Si nce φ is a mono- tonically decreasing function, equivalently, higher clas siﬁer accuracy typically corresponds to lower values of population potential. However, there are limits t o the utility of this argument: a low value of the potential alone does not translate to successful lear ning. In agnostic learning, one is con- cerned not with the error of the learned classiﬁer per se , but with its excess error over the best in-class hypothesis. W e will provide a precise relation bet ween the potential and the excess error in Lemma 3. W e will use the following properties of φ (proved in Appendix C). Going forward, these will be the sole characteristics of φ we will appeal to. The potential we use is similar to the one us ed in [KK09, Dom00], but has been modiﬁed to remove a jump discontinuity in the s econd derivative at z = 0 as our approach requires a twice continuously differentiab le potential. Lemma 2. W e make the following elementary observations about φ: I. φ is convex and in C2, i.e., it is two-times continuously differentiable everyw here. II. φ is non-negative on R and φ(0) = 2 . III. F or all z ∈ R, φ′(z) ∈ [−1, 0]. Further , for any z < 0, φ′(z) = −1. IV . φ is 1-smooth, i.e., ∀z ∈ R, φ′′(z) ≤ 1. 6For any real-valued hypotheses H, h and g on X , to ease analysis, we introduce Φ ′ D(H, h) = E(x,y )∼D[φ′(yH(x))yh(x)], Φ ′′ D(H, h, g ) = E(x,y )∼D[φ′′(yH(x))h(x)g(x)]. Equivalently, Φ ′ D(H, h) can be characterized as the D-induced semi inner product between the func- tional derivative ∂Φ D(H)/∂H and h. But for ease of presentation, we forgo this formal interpre ta- tion in favor of the literal one stated above. A key property of the above potential is stated in the next lem ma (proved in Appendix C). It gives us a strategy to control the relative error of the learned hyp othesis, the quantity on the right, by individually minimizing both terms on the left, as we discus s next. Lemma 3. F or any distribution D ∈ ∆( X × {± 1}), real-valued hypothesis H : X → R, and binary hypothesis h∗ : X → {± 1}, we have Φ ′ D(H, sign(H)) − Φ ′ D(H, h∗) ≥ corrD(h∗) − corrD(sign(H)). Description of the algorithm. Each round of Algorithm 1 adds some multiple of either the weak hypothesis Wt or −sign(Ht) to the current ensemble Ht; this choice is dictated by the empirical correlation of the weak hypothesis on the dataset it was fed. The construction of the relabeled dis- tribution Dt via our data reuse scheme ensures that if any hypothesis in th e base class B produces sufﬁcient correlation on it, its addition to the ensemble mu st decrease the potential Φ associated with the ensemble. Concretely, as we prove in Lemma 6, for all h ∈ B , corr Dt (h) closely tracks −Φ ′(Ht, h). The key to our improved sample complexity is the fact that th is invariant can be main- tained while sampling only S ≈ 1/γε fresh samples each round, by repurposing samples from earlier rounds to construct the dataset fed to the weak learn er. The mixing parameter σ controls the proportion of these two sources of samples used to construct Dt. However, this explanation is opaque when it comes to motivat ing the need for −sign(Ht). Let’s rectify that: let h∗ ∈ arg minh∈H corrD(h) be the best in-class hypothesis. If −Φ ′ D(Ht, h∗) is sufﬁciently large, so is corrDt (h∗) by Lemma 6, which assures us a non-negligible weak learning edge. As such −Φ ′ D(Ht, Wt) is large and the potential drops. If the weak hypothesis fail s to make sufﬁcient progress, the algorithms adds − sign(Ht) to the ensemble, which, again by Lemma 6 corresponds to decreasing the potential value by some funct ion of −Φ ′ D(Ht, − sign(Ht)) = Φ ′ D(Ht, sign(Ht)), using linearity of Φ ′ D in its second argument. Thus, when run for sufﬁciently many iterations, because the potential is bounded above at i nitialization both the terms on the left side of Lemma 3 must become small. This then implies a bounded correlation g ap between the best in-class hypothesis, and the majority vote of the ensembles considered in some iteration; the last line picks the best of these. 4.1 Main result for ﬁnite hypotheses classes The key feature of our algorithm is that it is designed to reus e samples across successive rounds of boosting. The soundness of this scheme, which Lemma 6 substantiates, is based on the observation that in each round Ht changes by a small amount, thereby inducing an incremental c hange in the distribution fed to the weak learner. Although our algorith m needs a total number of iterations comparable to [ KK09], this data reuse lowers the number of fresh samples needed e very iteration to just 1/γε, instead of 1/γ2ε2, resulting in improved sample complexity, as we show next. Theorem 4 (Main result for ﬁnite hypotheses class) . Choose any ε, δ > 0. There exists a choice of η, σ, T, τ, S 0, S, m satisfying3 T = O((log |B|)/γ2ε2), η = O(γ2ε/log |B|), σ = η/γ, τ = O(γε), S = O(1/γε), S0 = O(1/ε2), m = m(ε0, δ0) + O(1/γ2ε2) such that for any γ-agnostic weak learning oracle (as deﬁned in Deﬁnition 1) with ﬁxed tolerance ε0 and failure probability δ0, Algorithm 1 when run with the potential deﬁned in (1) produces a hypothesis h such that with probability 1 − 10δ0T − 10δT , corrD(h) ≥ max h∈H corrD(h) − 2ε0 γ − ε, while making T = O((log |B|)/γ2ε2) calls to the weak learning oracle, and sampling T S + S0 = O((log |B|)/γ3ε3) labeled examples from D. 3 Here, the O notation suppresses polylogarithmic factors. 7In Appendix B, we provide a different result (Theorem 9), also using Algorithm 1, where the learner makes O(1/γ2ε2) call to weak learner, exactly matching the oracle complexit y of existing results, while drawing O((log |B|)/γ3ε3 + (log |B|)3/γ2ε2) samples. 4.2 Extensions to inﬁnite classes As mentioned in Section 1.1, the reuse of samples prohibits us from appealing to symmetr ization and Rademacher complexity based arguments. Instead, our ge neralization to inﬁnite classes is based on L1 covering numbers. Using empirical process theory [ VDVW97], we upper bound L1 cover- ing number by a suitable function of VC dimension, yielding t he following result (proved in Ap- pendix F). Theorem 5 (Main result for VC Dimension) . There exists a setting of parameters such that for any for any γ-agnostic weak learning oracle with ﬁxed tolerance ε0 and failure probability δ0, Algo- rithm 1 produces a hypothesis h such that with probability 1 − 10δ0T − 10δT , corrD(h) ≥ max h∈H corrD(h) − 2ε0 γ − ε, while making T = O(VC(B)/γ2ε2) calls to the weak learning oracle, and sampling T S + S0 = O(VC(B)/γ3ε3) labeled examples from D. 5 Sketch of the analysis In this section, we provide a brief sketch of the analysis out lined in the proof of Theorem 4. Our intent is to convey the plausibility of a 1/ε3 sample complexity result, up to the exclusion of other factors. Hence, ≈ and ≲ inequalities below only hold up to constants and polynomial factors in other paramters, e.g., in γ, log |B|. Formal proofs are reserved for Appendix D. Bounding the correlation gap (Theorem 4). A central tool in bounding the correlation gap is Lemma 3. W e want to ensure for some t, since our algorithm at the end picks the best one, that −Φ ′ D(Ht, − sign(Ht))   want ≲ ε + (−Φ ′ D(Ht, h∗))   want ≲ ε ≥ corrD(h∗) − corrD(sign(Ht)). On the other hand, using 1-smoothness of φ, we can upper bound Φ D on successive iterates as Φ D(Ht+1) ≤ Φ D(Ht) + ηΦ ′ D(Ht, ht) + η2/2γ2. Rearranging this to telescope the sum produces − 1 T T∑ t=1 Φ ′ D(Ht, ht) ≤ ∑ T t=1(Φ D(Ht) − Φ D(Ht+1)) ηT + η 2γ2 ≤ 2 ηT + η 2γ2 Hence, by setting a η ≈ 1/ √ T , we know that there exists a t where −Φ ′ D(Ht, ht) ≲ 1/ √ T . In Lemma 6, we establish that the core guarantee our data resue scheme p rovides: that for all h in the base class B and for h∗, the correlation on the resampled distribution Dt constructed by the algorithm tracks the previously stated quantity of interes t −Φ ′ D(Ht, ·). Lemma 6. There exists a C > 0 such that with probability 1 − δ, for all t ∈ [T ] and h ∈ B ∪ { h∗}, we have ⏐ ⏐ ⏐Φ ′ D(Ht, h) + ( 1 + η σ ) corrDt (h) ⏐ ⏐ ⏐≤ C ( σ + η γ )( 1√ σS √ log |B|T δ + log |B|T δ )    :=εGen . Using the deﬁnition of weak learner, we know that corr Dt (h∗) ≲ corrDt (Wt)/γ. Now , using Lemma 6 twice and the linearity of Φ ′ D(H, ·), we get −Φ ′ D(Ht, h∗) ≲ corrDt (h∗) + εGen ≲ corrDt (Wt)/γ + εGen ≲ −Φ ′ D(Ht, Wt/γ) + 2 εGen. Now , if at each step we could choose ht ∈ {− sign(Ht), Wt/γ}, which ever maximized −Φ ′ D(Ht, ·), we would have for some t that max{−Φ ′ D(Ht, − sign(Ht)), −Φ ′ D(Ht, h∗) − 2εGen} ≤ − Φ ′ D(Ht, ht) ≲ 1/ √ T . 8Alas, − sign(Ht) is not in B, thus corr Dt (− sign(Ht)) can be really far from −Φ ′ D(Ht, − sign(Ht)), i.e., Lemma 6 does not apply to −sign(Ht). T o circumvent this, instead of choosing the maximizer, in the algorithm and in the actual proof, we use a relaxed crit eria for choosing between Wt/γ and −sign(Ht) that depends on the correlation of Wt on Dt alone, and hence can be efﬁciently evaluated. The spirit of this modiﬁcation is to adopt −sign(Ht) only if Wt by itself fails to make enough progress, the threshold for which can be stated in the terms of target accuracy. Generalization over B via sample reuse (Lemma 6). Here, we sketch a proof of Lemma 6 that ties the previous proof sketch together, and forms the basis of our sample reuse scheme. Our start- ing point is the following claim (Claim 10) that uses the fact that φ is second-order differentiably continuous to arrive at the fact that for any t and h : X → R, we have Φ ′ D(Ht, h) ≈ Φ ′ D(Ht−1, h) + ηΦ ′′ D(Ht−1, h, ht−1). Simultaneously, using Et−1 to condition on the randomness in the ﬁrst t− 1 rounds, by construction, our data reuse and relabeling scheme gives us Et−1[corrDt (h)] ≈ (1 − σ)corrDt−1 (h) − σ2 σ + η Φ ′ D(Ht−1, h) − ησ σ + η Φ ′′ D(Ht−1, h, ht−1). Thus, suitably scaling and adding the two together, we get th e identity that Et−1[∆ t] = Φ ′ D(Ht, h) + ( 1 + η σ ) Et−1 [corrDt (h)] = (1 − σ)Φ ′ D(Ht−1, h) + (1 − σ) ( 1 + η σ ) corrDt−1 (h) = (1 − σ)∆ t−1, where ∆ t = Φ ′ D(Ht, h) + ( 1 + η σ ) corrDt (h). Thus, ∆ t forms a martingale-like sequence; the sign of ∆ t is indeterminate. T o establish concentration, we apply Fre edman’s inequality, noting that the conditional variance of the associated martingale diff erence sequence scales as 1/ √ S. A union bound over B ∪ { h∗} yields the claim. Relatedly, to reach a better oracle comple xity in Theorem 9, we show a uniform high-probability on the martingale differ ence sequence, and then apply a variant of Freedman’s inequality [ Pin94, Pin20] that can adapt to martingale difference sequences that are bounded with high probability instead of almost surely. 6 Applications In this section, we detail the implications of our results fo r previously studied learning problems. 6.1 Boosting for reinforcement learning [ BHS22] initiated the approach of boosting weak learners to constr uct a near-optimal policy for re- inforcement learning. Plugging Algorithm 1 into their meta-algorithm yields the following result for binary-action MDPs, improving upon the sample complexity i n [ BHS22]. Here, V π is the expected discounted reward of a policy π, V ∗ is its maximum. β is the discount factor of the underlying MDP , and C∞, D∞ and E, Eν are distribution mismatch and policy completeness terms (r elated to the inherent Bellman error). In the episodic model , the learner interacts with the MDP in episodes. In the ν-reset model , the learner can seed the initial state with a ﬁxed well dispe rsed distribution ν as a means to exploration. See Appendix G for a complete statement of results and details of the setting. Theorem 7 (Informal; stated formally in Theorem 22). Let W be a γ-weak learner for the policy class Π operating with a base class B, with sample complexity m(ε0, δ0) = (log |B|/δ0)/ε2 0. Fix tolerance ε and failure probability δ. In the episodic access model, there is an algorithm using that uses the weak learner W to produce a policy π such that with probability 1 − δ, we have V ∗−V π ≤ (C∞E)/(1 − β)+ε, while sampling O((log |B|)/γ3ε4) episodes of length O((1−β)−1). In the ν-reset access model, there is a setting of parameters such that Algo rithm 2 when given access to W produces a policy π such that with probability 1−δ, we have V ∗−V π ≤ (D∞Eν )/(1 − β)2+ε, while sampling O((log |B|)/γ3ε5) episodes of length O((1 − β)−1). 6.2 Agnostically learning halfspaces W e apply our algorithm in a black-box manner to agnostically learn halfspaces over the n- dimensional boolean hypercube when the data distribution h as uniform marginals on features. The 9aim is this section is not to obtain the best known bounds, but rather to provide an example illustrat- ing that agnostic boosting is both a viable and ﬂexible appro ach to construct agnostic learners, and where our improvements carry over. Following [ KK09], we use ERM over the parities of degree at most d, for d ≈ 1/ε4, as our weak learners; the the weak learner’s edge here is γ = n−d. An application of our boosting algorithm (proved in Appendix I) to this problem improves the sample complexity of O(ε−8n80ε−4 ) indicated in [ KK09]. Theorem 8. Let D be any distribution over {±1}n × {±1} with uniform distribution over features. By H = {sign(w⊤x − θ) : ( w, θ) ∈ Rn+1}, denote the class of halfspaces. There exists some d such that running Algorithm 1 with ERM over parities of degree at most d produces a classiﬁer h such that lD(¯h) ≤ minh∈H lD(h) + ε, while using O(ε−7n60ε−4 ) samples in npoly(1/ε ) time. Note that ERM over the class of halfspaces directly, althoug h considerably more sample efﬁcient, takes (1/ε)poly(n) time, i.e., it is exponentially slower for moderate values o f ε. There are known sta- tistical query lower bounds [ DKZ20] requiring npoly(ε−1) queries for agnostic learning of halfspaces with Gaussian marginals, suggesting that a broad class of al gorithms, regardless of the underlying parametrization, might not fare any better. For completene ss, we note that a better sample complex- ity is attainable by direct L1-approximations of halfspaces via low-degree polynomials [DGJ+10], instead of the approach taken in [ KK09, KOS04] and mirrored here which ﬁrst constructs an L2- approximation, but such structural improvements apply equ ally to presented and compared results. 7 Experiments In T able 3, we report the results of preliminary experiments with Algo rithm 1 against the ag- nostic boosting algorithms in [ KK09] and [ BCHM20] as baselines on UCI classiﬁcation datasets [SWHB89, HRFS99, SED+88], using decision stumps [ PVG+11] as weak learners. W e also in- troduce classiﬁcation noise of 5%, 10% and 20% during traini ng to measure the robustness of the algorithms to label noise. Accuracy is estimated using 30-fold cross validation with a grid search over the mixing weight σ and the number of boosters T . The algorithm in [ KK09] does not reuse samples between rounds, while [ BCHM20] uses the same set of samples across all rounds. In con- trast, Algorithm 1 blends fresh and old samples every round, with σ controlling the proportion of each. See Appendix J for additional details. W e note that the Ionsphere dataset i ncludes 351 sam- ples, while Diabetes contains 768, and Spambase contains 46 01. The beneﬁts of sample reuse are less stark in a data-rich regime. This could explain some of t he under-performance on Spambase, disregarding which the proposed algorithm substantially o utperforms the alternatives. Dataset No Added Noise 5% Noise [KK09] [BCHM20] Ours [KK09] [BCHM20] Ours Ionosphere 0.92 ± 0.02 0.89 ± 0.03 0.97 ± 0.02 0.90 ± 0.03 0.88 ± 0.03 0.97 ± 0.03 Diabetes 0.83 ± 0.03 0.78 ± 0.02 0.87 ± 0.03 0.83 ± 0.03 0.77 ± 0.02 0.88 ± 0.03 Spambase 0.69 ± 0.02 0.79 ± 0.01 0.78 ± 0.02 0.81 ± 0.02 0.79 ± 0.01 0.78 ± 0.02 German 0.77 ± 0.02 0.75 ± 0.02 0.83 ± 0.02 0.78 ± 0.02 0.75 ± 0.02 0.85 ± 0.02 Sonar 0.66 ± 0.07 0.91 ± 0.03 0.88 ± 0.07 0.84 ± 0.05 0.88 ± 0.03 0.94 ± 0.05 W aveform 0.88 ± 0.01 0.78 ± 0.01 0.91 ± 0.01 0.88 ± 0.01 0.77 ± 0.01 0.90 ± 0.01 Dataset 10% Noise 20% Noise [KK09] [BCHM20] Ours [KK09] [BCHM20] Ours Ionosphere 0.93 ± 0.02 0.89 ± 0.02 0.97 ± 0.02 0.92 ± 0.03 0.90 ± 0.03 0.96 ± 0.03 Diabetes 0.83 ± 0.03 0.78 ± 0.02 0.88 ± 0.03 0.82 ± 0.02 0.78 ± 0.02 0.88 ± 0.02 Spambase 0.83 ± 0.02 0.80 ± 0.01 0.79 ± 0.02 0.84 ± 0.01 0.79 ± 0.01 0.79 ± 0.01 German 0.78 ± 0.02 0.75 ± 0.02 0.84 ± 0.02 0.78 ± 0.02 0.74 ± 0.02 0.84 ± 0.02 Sonar 0.85 ± 0.04 0.91 ± 0.03 0.88 ± 0.04 0.88 ± 0.04 0.88 ± 0.04 0.93 ± 0.04 W aveform 0.88 ± 0.01 0.77 ± 0.01 0.91 ± 0.01 0.88 ± 0.01 0.77 ± 0.01 0.90 ± 0.01 T able 3: Cross-validated accuracies of Algorithm 1 compared to the agnostic boosting algorithms from [ KK09] and [ BCHM20] on 6 datasets. The ﬁrst column reports accuracy on the origi nal datasets, and the next three report performance with 5%, 10% and 20% label noise added during training. The proposed algorithm simultaneously outperfo rms both the alternatives on 18 out of 24 instances. 108 Conclusion W e give an agnostic boosting algorithm with a substantially lower sample requirement than ones known, enabled by efﬁcient recency-aware data reuse betwee n boosting iterations. Improving our oracle complexity or proving its optimality, and closing th e sample complexity gap to ERM are interesting directions for future work. References [ABS23] Naman Agarwal, Brian Bullins, and Karan Singh. V ari ance-reduced conservative pol- icy iteration. In Shipra Agrawal and Francesco Orabona, edi tors, Proceedings of The 34th International Conference on Algorithmic Learning The ory, volume 201 of Pro- ceedings of Machine Learning Research , pages 3–33. PMLR, 20 Feb–23 Feb 2023. [AGHM21] Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. B oosting simple learners. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theor y of Computing , pages 481–489, 2021. [BBL05] Stéphane Boucheron, Olivier Bousquet, and Gábor Lu gosi. Theory of classiﬁcation: A survey of some recent advances. ESAIM: probability and statistics , 9:323–375, 2005. [BCHM20] Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Mo ran. Online agnostic boosting via regret minimization. Advances in Neural Information Processing Systems , 33:644– 654, 2020. [BDLM01] Shai Ben-David, Philip M Long, and Y ishay Mansour. Agnostic boosting. In Computa- tional Learning Theory: 14th Annual Conference on Computat ional Learning Theory, COLT 2001 and 5th European Conference on Computational Lear ning Theory, Euro- COLT 2001 Amsterdam, The Netherlands, July 16–19, 2001 Proc eedings 14 , pages 507–516. Springer, 2001. [BHS22] Nataly Brukhim, Elad Hazan, and Karan Singh. A boost ing approach to reinforcement learning. Advances in Neural Information Processing Systems , 35:33806–33817, 2022. [BKL15] Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Op timal and adaptive algorithms for online boosting. In International Conference on Machine Learning , pages 2323– 2331. PMLR, 2015. [CBC16] Shang-Tse Chen, Maria-Florina Balcan, and Duen Hor ng Chau. Communication ef- ﬁcient distributed agnostic boosting. In Artiﬁcial Intelligence and Statistics , pages 1299–1307. PMLR, 2016. [CLL12] Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An o nline boosting algorithm with theoretical justiﬁcations. In Proceedings of the 29th International Coference on Inter- national Conference on Machine Learning , pages 1873–1880, 2012. [DGJ+10] Ilias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal , Rocco A Servedio, and Emanuele V iola. Bounded independence fools halfspaces. SIAM Journal on Com- puting, 39(8):3441–3462, 2010. [DIK+21] Ilias Diakonikolas, Russell Impagliazzo, Daniel M Kane , Rex Lei, Jessica Sorrell, and Christos Tzamos. Boosting in the presence of massart noise. In Conference on Learn- ing Theory , pages 1585–1644. PMLR, 2021. [DKZ20] Ilias Diakonikolas, Daniel Kane, and Nikos Zariﬁs. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gauss ian marginals. Advances in Neural Information Processing Systems , 33:13586–13596, 2020. [Dom00] C Domingo. Madaboost: a modiﬁcation of adaboost. In Proc. of the 13th Conference on Computational Learning Theory, COLT’00 , 2000. [Dud74] Richard M Dudley. Metric entropy of some classes of s ets with differentiable bound- aries. Journal of Approximation Theory , 10(3):227–236, 1974. 11[Fel10] V italy Feldman. Distribution-speciﬁc agnostic bo osting. Innovations in Theoretical Computer Science (ITCS) , pages 241–250, 2010. [FLLZ18] Cong Fang, Chris Junchi Li, Zhouchen Lin, and T ong Z hang. Spider: Near-optimal non-convex optimization via stochastic path-integrated d ifferential estimator. Advances in neural information processing systems , 31, 2018. [Fre75] David A Freedman. On tail probabilities for marting ales. the Annals of Probability , pages 100–118, 1975. [FS97] Y oav Freund and Robert E Schapire. A decision-theore tic generalization of on-line learning and an application to boosting. Journal of computer and system sciences , 55(1):119–139, 1997. [GLR22] Kasper Green Larsen and Martin Ritzert. Optimal wea k to strong learning. Advances in Neural Information Processing Systems , 35:32830–32841, 2022. [Hau95] David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis dimension. Journal of Combinatorial Theory, Series A, 69(2):217–232, 1995. [HLR23] Mikael Møller Høgsgaard, Kasper Green Larsen, and M artin Ritzert. AdaBoost is not an optimal weak to strong learner. In Proceedings of the 40th International Confer- ence on Machine Learning , Proceedings of Machine Learning Research, pages 13118– 13140. PMLR, 2023. [HRFS99] Mark Hopkins, Erik Reeber, George Forman, and Jaap Suermondt. Spambase. UCI Machine Learning Repository, 1999. DOI: https://doi.org/ 10.24432/C53G6X. [HS21] Elad Hazan and Karan Singh. Boosting for online conve x optimization. In Interna- tional Conference on Machine Learning , pages 4140–4149. PMLR, 2021. [JGT17] Y oung Hun Jung, Jack Goetz, and Ambuj T ewari. Online multiclass boosting. Ad- vances in neural information processing systems , 30, 2017. [JZ13] Rie Johnson and T ong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems , 26, 2013. [Kal07] Satyen Kale. Boosting and hard-core set constructi ons: a simpliﬁed approach. In Electronic Colloquium on Computational Complexity (ECCC) , volume 14. Citeseer, 2007. [KK09] V arun Kanade and Adam Kalai. Potential-based agnost ic boosting. Advances in neural information processing systems , 22, 2009. [KL02] Sham Kakade and John Langford. Approximately optima l approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learn- ing, pages 267–274, 2002. [KMV08] Adam T auman Kalai, Y ishay Mansour, and Elad V erbin. On agnostic boosting and parity learning. In Proceedings of the fortieth annual ACM symposium on Theory o f computing, pages 629–638, 2008. [KOS04] Adam R Klivans, Ryan O’Donnell, and Rocco A Servedio . Learning intersections and thresholds of halfspaces. Journal of Computer and System Sciences , 68(4):808–840, 2004. [KS03] Adam Kalai and Rocco A Servedio. Boosting in the prese nce of noise. In Proceedings of the thirty-ﬁfth annual ACM symposium on Theory of computi ng, pages 195–205, 2003. [KV94] Michael Kearns and Leslie V aliant. Cryptographic li mitations on learning boolean formulae and ﬁnite automata. Journal of the ACM (JACM) , 41(1):67–95, 1994. 12[LS08] Philip M Long and Rocco A Servedio. Random classiﬁcat ion noise defeats all convex potential boosters. In Proceedings of the 25th international conference on Machin e learning, pages 608–615, 2008. [MM02] Y ishay Mansour and David McAllester. Boosting using branching programs. Journal of Computer and System Sciences , 64(1):103–112, 2002. [MR T18] Mehryar Mohri, Afshin Rostamizadeh, and Ameet T alw alkar. F oundations of machine learning. MIT press, 2018. [Pin94] Iosif Pinelis. Optimum bounds for the distribution s of martingales in banach spaces. The Annals of Probability , pages 1679–1706, 1994. [Pin20] Iosif Pinelis. Extension of bernstein’s inequalit y when the random variable is bounded with large probability. MathOverﬂow , 2020. URL:https://m athoverﬂow .net/q/371436 (version: 2020-09-11). [Put14] Martin L Puterman. Markov decision processes: discrete stochastic dynamic pr ogram- ming. John Wiley & Sons, 2014. [PVG+11] F . Pedregosa, G. V aroquaux, A. Gramfort, V . Michel, B. Th irion, O. Grisel, M. Blon- del, P . Prettenhofer, R. W eiss, V . Dubourg, J. V anderplas, A . Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Mach ine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. [R T22] V inod Raman and Ambuj T ewari. Online agnostic multic lass boosting. Advances in Neural Information Processing Systems , 35:25908–25920, 2022. [Sch90] Robert E Schapire. The strength of weak learnabilit y. Machine learning , 5:197–227, 1990. [SED+88] Jack W Smith, James E Everhart, WC Dickson, William C Know ler, and Robert Scott Johannes. Using the adap learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual symposium on computer applicatio n in medical care , page 261. American Medical Informatics Association, 1988. [SF13] Robert E Schapire and Y oav Freund. Boosting: Foundat ions and algorithms. Kyber- netes, 42(1):164–166, 2013. [SLRB17] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Mi nimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming , 162:83–112, 2017. [SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms . Cambridge university press, 2014. [SWHB89] V . Sigillito, S. Wing, L. Hutton, and K. Baker. Iono sphere. UCI Machine Learning Repository, 1989. DOI: https://doi.org/10.24432/C5W01B . [VDVW97] Aad W V an Der V aart and Jon A W ellner. W eak convergence and empirical processes: with applications to statistics . Springer New Y ork, 1997. [VJ01] Paul V iola and Michael Jones. Rapid object detection using a boosted cascade of simple features. In Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001 , volume 1, pages I–I. Ieee, 2001. 13Appendix Limitations. The primary contribution of this work is theoretical. Exten sively demonstrating the empirical efﬁcacy of the proposed approach and fully charac terizing when it fares best is left to future work. Further, in comparison to realizable boosting (with or without label noise), the existence of an agnostic weak leaner here is a stronger assumption. Nev ertheless, we believe, especially given the recent interest in agnostic boosting, the current work presents a substantial and concrete improvement over the state of the art, and is a ﬁrst step in mak ing agnostic boosting practical. Organization of the appendix. In Appendix A, we point out an error in the branching criteria in [ KK09]. Next, in Appendix B, we give a second guarantee on our algorithm that improves th e oracle complexity at a vanishing (in ε) cost to the sample complexity. In Appendices C and D, we prove the main result and the lemmas leading up to it. Appendi x E provides a proof of the improved result in Appendix B. Appendix F furnishes a proof for the claims concerning extensions to in ﬁnite classes. In Appendices G and H, we deﬁne the reinforcement learning setup, formally state the RL boosting result along with accompanying algorithms, and prove it. Appendix I substantiates our improved sample complexity bound for learning halfspac es. Finally, in Appendix J, we provide additional experimental details, and Appendix K provides a practical guide for adapting the proposed algorithm. A Branching criteria in [ KK09] At a high level, the boosting algorithm in [ KK09] is an iterative one, that adds either the weak hypothesis or the negation of the sign of the present ensembl e to the ensemble mixture in each of the T ≈ 1/ε2 rounds. The algorithm makes use of samples to fulﬁll two obje ctives: (a) provide m ≈ 1/ε2 samples to the weak learner to obtain a weak hypothesis that g eneralizes, (b) use s ≈ 1/ε2 samples to decide whether to add the weak hypothesis or the vo ting classiﬁer to the current mixture, by comparing the empirical performance on both on said sampl es. The algorithm uses fresh samples for part (a) every round; this is sound and contributes T m ≈ 1/ε4 to the net sample complexity. However, as described in [ KK09], the algorithm reuses the same s samples for part (b) across all rounds. This means these s samples determine which of the two choices gets added to the m ixture at the end of the ﬁrst step, and hence H1. In the next step, however, because H1 through relabelling of new samples determines the weak hypothesis, these sample s are no longer IID with respect to the weak hypothesis or − sign(H1), since they have already played a demonstrable part in deter mining it. This effects occur and compound at all time steps, not jus t the ﬁrst. In the analysis, on top of page 7, the analysis in [ KK09] uses a Chernoff-Hoeffding bound to say that the performanc e of the weak hypothesis and − sign(H1) on these s samples transfers approximately to the population. Howeve r, this inequality may only be applied to IID random variables. In the case of [ KK09], there is a simple and satisfactory ﬁx: resample these s examples from the population distribution every round. The sample complexit y, now T (m + s) instead of T m + s, remains O(1/ε4), and no change to the performance gurantee needs to be made. In our adaptation, this highlights an additional challenge . Even if one were to sate the weak learner with a total fewer number of samples, i.e., perform part (a) w ith fewer samples, through a uniform convergence result on the base hypothesis (crucially, not t he boosted hypothesis , whose complexity grows with number of iterations), part (b) requires s fresh samples, and hence 1/ε4 samples in total, in determining which of the weak hypothesis or −sign of the ensemble provides a greater magnitude of descent. Here, note that − sign(Ht) lies outside the base class. T o circumvent this, we give a branching criteria to decide which component to add to the present mixture, based on the performance of the weak hypothesis, the only component w hose performance is estimable with bounded generalization error, alone on the reused data. Con cretely, we introduce a threshold τ to choose the weak hypothesis, if it makes at least τ progress, and otherwise, choose − sign(Ht) even if in truth it is worse than the weak hypothesis. This deviati on requires careful handling in the proof. 14B Improved oracle complexity Here, we provide a different result where Algorithm 1 makes O(1/γ2ε2) call to weak learner, matching exatcly the oracle complexity of existing results , while drawing O((log |B|)/γ3ε3 + (log |B|)3/γ2ε2) samples. Notice that the second term in the sample complexit y has a smaller order, whenever ε is sub-constant. The proof may be found in Appendix E. Theorem 9 (Improved oracle complexity for ﬁnite hypotheses class) . Choose any ε, δ > 0. There exists a choice of η, σ, T, τ, S 0, S, m satisfying T = O(1/γ2ε2), η = O(γ2ε), σ = η/γ, τ = O(γε), S = O((log |B|)/γε + (log |B|)3), S0 = O(1/ε2), m = m(ε0, δ0) + O(1/γ2ε2) such that for any γ-agnostic weak learning oracle (as deﬁned in Deﬁnition 1) with ﬁxed tolerance ε0 and failure probability δ0, Algorithm 1 when run with the potential deﬁned in (1) produces a hypothesis h such that with probability 1 − 10δ0T − 10δT , corrD(h) ≥ max h∈H corrD(h) − 2ε0 γ − ε, while making T = O(1/γ2ε2) calls to the weak learning oracle, and sampling T S + S0 = O((log |B|)/(γ3ε3) + (log |B|)3/(γ2ε2)) labeled examples from D. C Proof of auxiliary lemmas Proof of Lemma 2. Part 2 is evident from the deﬁnition. T aking care that right a nd left ﬁrst (and second) derivatives exist and are equal at z = 0 , by explicit computation, we have φ′(z) = {−1 if z ≤ 0, −(z + 1)e−z if z > 0, φ′′(z) = {0 if z ≤ 0, ze−z if z > 0. From this, one may immediately verify Part 1, since φ′′ is non-negative. Non-negativity of φ′′ also implies that φ′ is non-decreasing, and hence, being clearly non-positive, is between [−1, 0]; this is Part 3. By elementary calculus, φ′′ is maximize at z = 1 where it equals 1/e implying the conclusion in Part 4. Proof of Lemma 3. By the deﬁnition of Φ ′ D, we have Φ ′ D(H, sign(H)) − Φ ′ D(H, h∗) = E(x,y )∼D[φ′(yH(x))y(sign(H)(x) − h∗(x))] =E(x,y )∼D[1(yH(x) ≥ 0)φ′(yH(x))y(sign(H)(x) − h∗(x))] + E(x,y )∼D[1(yH(x) < 0)φ′(yH(x))y(sign(H)(x) − h∗(x))]. Note that whenever yH(x) < 0, φ′(yH(x)) = −1, which Lemma 2.III attests to. On the other hand, if yH(x) ≥ 0, since h∗ is a binary classiﬁer, ysign(H)(x) = 1 ≥ yh∗(x), and since, again by Lemma 2.III, φ′(yH(x)) ≥ − 1, we have in this case φ′(yH(x))y(sign(H)(x) − h∗(x)) ≥ − y(sign(H)(x) − h∗(x)) Plugging these into the previous derivation, we arrive at Φ ′ D(H, sign(H)) − Φ ′ D(H, h∗) ≥E(x,y )∼D[1(yH(x) ≥ 0)y(h∗(x) − sign(H)(x))] + E(x,y )∼D[1(yH(x) < 0)y(h∗(x) − sign(H)(x))] =E(x,y )∼D[y(h∗(x) − signH(x))] =corrD(h∗) − corrD(sign(H)), ﬁnishing the proof of the claim. D Proofs for the main result Proof of Theorem 4. Let h∗ ∈ arg minh∈H corrD(h). Using the update rule for Ht+1 and the fact that φ is 1-smooth (Lemma 2.IV), we arrive at Φ D(Ht+1) = E(x,y )∼D[φ(y(Ht(x) + ηht(x)))] ≤ E(x,y )∼D [ φ(yHt(x)) + ηyφ′(yHt(x))ht(x) + ( ηht(x)y)2/2 ] ≤ Φ D(Ht) + ηΦ ′ D(Ht, ht) + η2/2γ2, 15using the deﬁnition of Φ ′ D, and that ht is either a binary classiﬁer, or a 1/γ-scaled version of it. Rearranging this to telescope the sum produces − 1 T T∑ t=1 Φ ′ D(Ht, ht) ≤ ∑ T t=1(Φ D(Ht) − Φ D(Ht+1)) ηT + η 2γ2 ≤ 2 ηT + η 2γ2 (2) where we use the fact that Φ D(0) = 2 , and that it is non-negative (Lemma 2.II). Hence, ∃t ∈ [T ], such that −Φ ′ D(Ht, ht) is small. Our proof strategy going forward is to use this fact to imply that both the terms on left side of inequality in Lemm a 3, namely, −Φ ′ D(−sign(Ht)) and −Φ ′ D(Ht, h∗), are small for some t, implying a small correlation gap on the population distrib ution. Note that if corr D′ t (Wt) ≥ τ, the algorithm sets ht = Wt/γ, else it chooses ht = −sign(Ht). W e analyze these cases separately. For both, Lemma 6 which relates the empirical correlation on Dt with Φ ′ D will prove indispensable. Going forward, we will deﬁne εGen as indicated above to capture the generalization error over the base hypothesis class. For brevity of notation, we condition the analysis going for ward on three events. Let EA be that event that for all t, |corrDt (Wt) − corr ˆD′ t (Wt)| ≤ ε′/10. Since ˆD′ t is constructed from IID samples from Dt, setting m ≥ 100/ε′2√ log T/δ , Pr(EA) ≥ 1−δ, by an application of Hoeffding’s inequality and union bound over t. Here, note that unlike S setting a higher value of m doesn’t increase the number of points sampled from D, since D′ t is resampled from already collected data. Similarly, denot e by EB the event that for all t, |corrD(sign(Ht)) − corr ˆD0 (sign(Ht))| ≤ ε′′/10. Again, by Hoeffding’s inequality and union bound over t, choosing S0 = 100 /ε′′2√ log T/δ , we have Pr(EB) ≥ 1 − δ, since the samples in ˆD0 were chosen independently of those used to compute Ht’s. Finally, we will take the success of Lemma 6 (call this EC ) for granted in the analysis below , for brevity of notation, conditioning our analysis on all three events. Case A: When ht = Wt/γ. When this happens, corr Dt (Wt) ≥ corrD′ t (Wt) − ε′/10 ≥ τ − ε′/10. Applying Lemma 6 and noting that Φ ′ D(Ht, ·) is linear in its argument, we have −Φ ′ D ( Ht, Wt γ ) = − Φ ′ D(Ht, Wt) γ ≥ 1 γ ( 1 + η σ ) corrDt (Wt) − εGen γ ≥ 1 γ ( 1 + η σ )( τ − ε′ 10 ) − εGen γ Rearranging this Φ ′ D ( Ht, Wt γ ) ≤ − 1 γ ( 1 + η σ )( τ − ε′ 10 ) + εGen γ . (3) Case B: When ht = −sign(Ht). Here corr Dt (Wt) ≤ corrD′ t (Wt)+ ε′/10 ≤ τ +ε′/10. Applying Lemma 6, and using the weak learning condition (Deﬁnition 1), we have that ( 1 + η σ )( τ + ε′ 10 ) ≥ ( 1 + η σ ) corrDt (Wt) ≥ γ ( 1 + η σ ) corrDt (h∗) − ( 1 + η σ ) ε0 ≥ − γΦ ′ D(Ht, h∗) − ( 1 + η σ ) ε0 − γεGen Now , we invoke the linearity of Φ ′ D(Ht, ·) and Lemma 3 to observe that Φ ′ D(Ht, −sign(Ht)) = −Φ ′ D(Ht, sign(Ht)) ≤ − Φ ′ D(Ht, h∗) − (corrD(h∗) − corrD(sign(Ht))) ≤ − (corrD(h∗) − corrD(sign(Ht))) + 1 γ ( 1 + η σ )( τ + ε′ 10 + ε0 ) + εGen . (4) 16Combining the two. In either case, combining Equations ( 3) and ( 4), we have Φ ′ D(Ht, ht) ≤ max { − 1 γ ( 1 + η σ )( τ − ε′ 10 ) + εGen γ , − (corrD(h∗) − corrD(sign(Ht))) + 1 γ ( 1 + η σ )( τ + ε 10 + ε0 ) + εGen } , or using the identity − max(−f(x)) = min f(x), −Φ ′ D(Ht, ht) ≥ min { 1 γ ( 1 + η σ )( τ − ε′ 10 ) − εGen γ , (corrD(h∗) − corrD(sign(Ht))) − 1 γ ( 1 + η σ )( τ + ε′ 10 + ε0 ) − εGen } . Now , set τ = ( 1 + η σ )−1 ( 4 ηT + η γ2 + εGen γ ) γ + ε′ 10. Now , either there exists some t such corrD(h∗) − corrD(sign(Ht)) ≤ 1 γ ( 1 + η σ ) (2τ + ε0) + ( 1 − 1 γ ) εGen = 8 ηT + 2η γ2 + ( 1 + 1 γ ) εGen + ( 1 + η σ )(ε0 γ + ε′ 5γ ) , (5) or the minimum operator in the last expression always accept s the ﬁrst clause, in which case for all t, −Φ ′ D(Ht, ht) ≥ 1 γ ( 1 + η σ )( τ − ε′ 10 ) − εGen γ = 4 (ηT ) + η γ2 , which contradicts Equation ( 2). Henceforth let t∗ be the iteration for which Equation ( 5) holds. Finally, given the event EB, we have corrD(h) ≥ corr ˆD0 (h) − ε′′ 10 ≥ corr ˆD0 (sign(Ht∗ )) − ε′′ 10 ≥ corrD(sign(Ht∗ )) − ε′′ 5 . Compiling this with the inequality in Equation ( 5), we get corrD(h∗) − corrD(h) ≤ 8 ηT + 2η γ2 + 2εGen γ + 1 γ ( 1 + η σ )( ε0 + ε′ 5 ) + ε′′ 5 . Setting ε′ = ε/5γ, ε′′ = ε/5 and plugging in the proposed hyper-parameters with appropr iate constants yields the claimed result. Proof of Lemma 6. Fix any hypothesis h ∈ B . For any η′ ≥ 0, deﬁne Hη′ t = Ht + η′ht, and ∆ t = Φ ′ D(Ht, h) + ( 1 + η σ ) E(x,y )∼Dt [yh(x)] . First, we derive recursive expansions of Φ ′ D(Ht, h) and Dt, the two quantities we wish to relate. Claim 10. F or any t and h : X → R, we have Φ ′ D(Ht, h) = Φ ′ D(Ht−1, h) + ηEη′ ∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)]. Claim 11. F or any t, Et−1[E(x,y )∼Dt [yh(x)]] = (1 − σ)E(x,y )∼Dt−1 [yh(x)] − σ2 σ + η Φ ′ D(Ht−1, h) − ησ σ + η Eη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)]. 17Adding (1 + η/σ) times the last expression to the expansion of Φ ′ D(Ht, h), we have Et−1[∆ t] = Φ ′ D(Ht, h) + ( 1 + η σ ) Et−1 [ E(x,y )∼Dt [yh(x)] ] = (1 − σ)Φ ′ D(Ht−1, h) + (1 − σ) ( 1 + η σ ) E(x,y )∼Dt−1 [yh(x)] = (1 − σ)∆ t−1. Using this, we conclude that ∆ ′ t = (1 − σ)−t∆ t forms a martingale sequence with respect to the {Ft : t ∈ N≥0} ﬁltration sequence, as ∆ ′ t = Et−1[(1 − σ)−t∆ t] = (1 − σ)t−1∆ t−1 = ∆ ′ t−1. The associated martingale difference sequence δ′ t = ∆ ′ t − ∆ ′ t−1 can be bounded both in worst-case and second-moment terms as we show next. Claim 12. F or all t, |δ′ t| ≤ (1 − σ)−t2(σ + η/γ) and ∑ t s=1 Es−1[δ′ s 2] ≤ 8(σ 2+η2/γ 2) σ (1−σ )2t S . Now , we are ready to apply Freedman’s inequality for marting ales. Theorem 13 (Freedman’s inequality [ Fre75]). Consider a real-valued martingale {Yk : k ∈ Z≥0} with respect to some ﬁltration sequence {Σ k : k ∈ Z≥0}, and let {Xk : k ∈ Z>0} be the associated difference sequence. Assume that the difference sequence i s uniformly bounded: |Xk| ≤ R almost surely for k ∈ Z>0. Deﬁne the predictable quadratic variation process: Wk := ∑ k j=1 Ej−1 ( X2 j ) for k ∈ Z>0, where Ej−1 ( · ):= E ( · |Σ j−1 ) . Then, for all t ≥ 0 and σ2 > 0, Pr ( ∃k ≥ 0 : Yk ≥ t and Wk ≤ σ2) ≤ exp { − −t2/2 σ2 + Rt/3 } . Applying Freedman’s inequality to ∆ ′ t, since the total conditional variance of our martingale dif fer- ence sequence is bounded almost surely as shown before, we ge t for any r ≥ 4(1 − σ)−t ( σ + η γ ) max { 1√ σS √ log 2 δ , log 2 δ } that Pr(∆ ′ t ≥ r) ≤ exp { − (r2 2 )/( 8(σ2 + η2/γ2) σ(1 − σ)2tS + 2(σ + η/γ)r (1 − σ)t )} ≤ δ. Hence, using ∆ ′ t = (1 − σ)−t∆ t, with probability 1 − δ, we have ∆ t ≤ 4 ( σ + η γ )( 1√ σS √ log 2 δ + log 2 δ ) . T aking a union bound over the choice of h from B ∪ { h∗} and over t, along with repeating the argument on the martingale −∆ t to furnish the promised two-sided bound, concludes the clai m. Proof of Claim 10. Using the fundamental theorem of calculus and that φ ∈ C 2, which Lemma 2.I certiﬁes, we get Φ ′ D(Ht, h) = E(x,y )∼D [φ′(yHt(x))yh(x)] = E(x,y )∼D [φ′(y(Ht−1(x) + ηht−1(x)))yh(x)] = E(x,y )∼D [ φ′(yHt−1(x))yh(x) + ∫ η 0 φ′′(y(Ht−1(x) + η′ht−1(x)))y2ht−1(x)h(x)dη′ ] = E(x,y )∼D [φ′(yHt−1(x))yh(x)] + ηEη′ ∼Unif[0,η ]E(x,y )∼D[h(x)ht−1(x)φ′′(yHη′ t−1(x))] = Φ ′ D(Ht−1, h) + ηEη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)]. where use the fact that for binary labels y ∈ {− 1, 1}, y2 = 1 , and the deﬁnitions of Φ ′ D and Φ ′′ D. 18Proof of Claim 11. First, we establish a recursive structure on the random rand om distribution Dt without any conditional expectation in place. Claim 14. F or any t, we have E(x,y )∼Dt [yh(x)] = (1 − σ)E(x,y )∼Dt−1 [yh(x)] − σ σ + η E(x,y )∼ ˆDt [ σφ′(yHt−1(x))yh(x) + ηEη′∼Unif[0,η ][φ′′(yHη′ t−1(x))ht−1(x)h(x)] ] . Using the fact that ˆDt identically samples data points from D, we arrive at Et−1[E(x,y )∼Dt [yh(x)]] =(1 − σ)E(x,y )∼Dt−1 [yh(x)] − σ2 σ + η E(x,y )∼D [φ′(yHt−1(x))yh(x)] + ησ σ + η Eη′∼Unif[0,η ]E(x,y )∼D [ φ′′(yHη′ t−1(x))ht−1(x)h(x) ] =(1 − σ)E(x,y )∼Dt−1 [yh(x)] − σ2 σ + η Φ ′ D(Ht−1, h) − ησ σ + η Eη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)]. Proof of Claim 14. By the deﬁnition of Dt, we have E(x,y )∼Dt [yh(x)] =(1 − σ)E(x,y )∼Dt−1 [yh(x)] − σE(x,y )∼ ˆDt [ h(x) ( σ σ + η φ′(yHt−1(x))y + η η + σ Eη′∼Unif[0,η ][φ′′(yHη′ t−1(x))ht−1(x)] )] =(1 − σ)E(x,y )∼Dt−1 [yh(x)] − σ σ + η E(x,y )∼ ˆDt [ σφ′(yHt−1(x))yh(x) + ηEη′∼Unif[0,η ][φ′′(yHη′ t−1(x))ht−1(x)h(x)] ] where in the ﬁrst equality we use the fact that for any binary r andom variable Y supported on {−1, 1} with Pr(Y = 1) = p, we have E[Y ] = 2 p − 1. Proof of Claim 12. Using the recursive expansion of Φ ′ D (Claim 10) and Dt (Claim 14), we have δ′ t =(1 − σ)−t (Φ ′ D(Ht, h) − (1 − σ)Φ ′ D(Ht−1, h)) + (1 − σ)−t ( 1 + η σ )( E(x,y )∼Dt [yh(x)] − (1 − σ)E(x,y )∼Dt−1 [yh(x)] ) =(1 − σ)−t ( σΦ ′ D(Ht−1, h) + ηEη′ ∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)] ) − (1 − σ)−tE(x,y )∼ ˆDt [ σφ′(yHt−1(x))yh(x) + ηEη′ ∼Unif[0,η ][φ′′(yHη′ t−1(x))ht−1(x)h(x)] ] =(1 − σ)−tσ ( Φ ′ D(Ht−1, h) − E(x,y )∼ ˆDt [φ′(yHt−1(x))yh(x)] ) + (1 − σ)−tη ( Eη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1) − E(x,y )∼ ˆDt [φ′′(yHη′ t−1(x))ht−1(x)h(x)]] ) . Using Lemma 2, φ′, φ′′ are uniformly bounded in magnitude by one, Φ ′ D(H, ·) and Φ ′′ D(H, ·, g) are uniformly bounded in magnitude by one for any H : X → R and 1/γ-uniformly bounded function g. Hence, |δ′ t| ≤ (1 − σ)−t2(σ + η/γ). T o bound the conditional variance, we use the identity 19(a + b)2 ≤ 2a2 + 2b2 in the ﬁrst line below to show Et−1[δ′ t 2] ≤2(1 − σ)−2t ( σ2Et−1 [ Φ ′ D(Ht−1, h) − E(x,y )∼ ˆDt [φ′(yHt−1(x))yh(x)] ] 2 + η2Et−1 [ Eη′ ∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1) − E(x,y )∼ ˆDt [φ′′(yHη′ t−1(x))ht−1(x)h(x)]] ] 2 ) =2(1 − σ)−2tS−1 ( σ2E(x,y )∼D [Φ ′ D(Ht−1, h) − φ′(yHt−1(x))yh(x)]2 + η2E(x,y )∼D [ Eη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1) − φ′′(yHη′ t−1(x))ht−1(x)h(x)] ] 2 ) ≤ 8(σ2 + η2/γ2) (1 − σ)2tS , where we use the fact that the S samples consisting ˆDt are independent and identically sampled from D conditioned on Ft−1 in the second equality, and that the expectation of any funct ion over ˆDt is the equivalent sample average. Finally, using an identit y on geometric sums, we get t∑ s=1 Es−1[δ′ s 2] ≤ 8(σ2 + η2/γ2) S (1 − σ)−2(t+1) − 1 (1 − σ)−2 − 1 ≤ 8(σ2 + η2/γ2) σ(1 − σ)2tS , where we appeal to the inequality 1 − (1 − σ)2 = 2 σ − σ2 ≥ σ for any σ ∈ [0, 1]. E Proofs for improved oracle complexity Proof of Theorem 9. The overall proof is similar to that of Theorem 4, with the exception of the following bound on εGen , which we state now (and prove next) in a new lemma relating th e empir- ical correlation on Dt with Φ ′ D. The principal difference between Lemma 6 and Lemma 15 is the presence of a 1/ √ S in both terms on the right in Lemma 15, however this comes at the cost of a higher polynomial dependence in log |B| in the second term on the right side of Lemma 15. Lemma 15. There exists a C > 0 such that with probability 1 − δ, for all t ∈ [T ] and h ∈ B ∪ { h∗}, we have ⏐ ⏐ ⏐Φ ′ D(Ht, h) + ( 1 + η σ ) corrDt (h) ⏐ ⏐ ⏐≤ C ( σ + η γ )( 1√ σS √ log |B|T δ + 1√ S ( log |B|T δ )3/ 2)    :=εGen . From here, with the new deﬁnition of εGen as deﬁned above, identically following the steps in the proof of Theorem 4, which we skip to avoid repetition, we arrive at corrD(h∗) − corrD(h) ≤ 8 ηT + 2η γ2 + 2εGen γ + 1 γ ( 1 + η σ )( ε0 + ε′ 5 ) + ε′′ 5 . Setting ε′ = ε/5γ, ε′′ = ε/5 and plugging in the proposed hyper-parameters with appropr iate constants yields the claimed result. Proof of Lemma 15. The proof of the present lemma is largely similar to that of Le mma 6, with two exceptions: (a) we use a variant of Freedman’s inequalit y that applies when the martingale difference sequences (for us, δ′ t are bounded with high probability, instead of admitting an a lmost sure absolute bound; (b) to apply this result, we establish a high probability upper bound on δ′ t that scales as 1/ √ S. Fix any hypothesis h ∈ B . For any η′ ≥ 0, deﬁne Hη′ t = Ht + η′ht, and ∆ t = Φ ′ D(Ht, h) + ( 1 + η σ ) E(x,y )∼Dt [yh(x)] . 20As before, adding (1 + η/σ) times the expression in Claim 11 to the the expansion of Φ ′ D(Ht, h) in Claim 10, we observe that Et−1[∆ t] = (1 − σ)∆ t−1, and hence conclude that ∆ ′ t = (1 − σ)−t∆ t forms a martingale sequence with respect to the {Ft : t ∈ N≥0} ﬁltration sequence. As shown in Claim 12, the associated martingale difference sequence δ′ t = ∆ ′ t − ∆ ′ t−1 admits a total variance bound of ∑ t s=1 Es−1[δ′ s 2] ≤ 8(σ 2+η2/γ 2) σ (1−σ )2t S . Now , we state a variant of Freedman’s inequality that applie s when martingale difference sequences are bounded with high probability. Theorem 16 (Freedman’s inequality with high probability bounds [ Pin94, Pin20]). Fix a positive integer n. Consider a real-valued martingale {Yk : k ∈ Z≥0} with respect to some ﬁltration sequence {Σ k : k ∈ Z≥0}, and let {Xk : k ∈ Z>0} be the associated difference sequence. Assume that the difference sequence is uniformly bounded with high probability for some R, δ′: Pr ( max k∈[n] |Xk| ≥ R ) ≤ δ′. Deﬁne the predictable quadratic variation process: Wn := ∑ n j=1 Ej−1 ( X2 j ) , where Ej−1 ( · ):= E ( · |Σ j−1 ) . Then, for all t ≥ 0 and σ2 > 0, Pr ( Yn ≥ t and Wn ≤ σ2) ≤ exp { − −t2/2 σ2 + Rt/3 } + δ′. T o apply this variant of Freedman’s inequality, we establis h a high probability bound on ∆ ′ t, that is, ignoring logarithmic terms, substantially better than the almost sure bound in Claim 12. Claim 17. There exists a universal constant C, such that for any t, we have Pr ( max s∈[t] |δ′ s| ≥ C (1 − σ)t ( σ + η γ ) 1√ S √ log t δ ) ≤ δ. Combining this with the almost sure bound on the total condit ional variance of our martingale dif- ference sequence, we get for any r ≥ 4(1 − σ)−t ( σ + η γ ) max { 1√ σS √ log 2 δ , C√ S ( log 2t δ )3/ 2} that Pr(∆ ′ t ≥ r) ≤ exp { − (r2 2 )/ ( 8(σ2 + η2/γ2) σ(1 − σ)2tS + C(σ + η/γ)r (1 − σ)t √ S √ log t δ )} + δ ≤ 2δ. Hence, using ∆ ′ t = (1 − σ)−t∆ t, with probability 1 − 2δ, we have ∆ t ≤ 4 ( σ + η γ )( 1√ σS √ log 2 δ + C√ S ( log 2t δ )3/ 2) . T aking a union bound over the choice of h from B ∪ { h∗} and over t, along with repeating the argument on the martingale −∆ t to furnish the promised two-sided bound, concludes the clai m. 21Proof of Claim 17. The proof begins similarly to the one for Claim 12. Using the recursive expan- sion of Φ ′ D (Claim 10) and Dt (Claim 14), we have δ′ t =(1 − σ)−t (Φ ′ D(Ht, h) − (1 − σ)Φ ′ D(Ht−1, h)) + (1 − σ)−t ( 1 + η σ )( E(x,y )∼Dt [yh(x)] − (1 − σ)E(x,y )∼Dt−1 [yh(x)] ) =(1 − σ)−t ( σΦ ′ D(Ht−1, h) + ηEη′ ∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1)] ) − (1 − σ)−tE(x,y )∼ ˆDt [ σφ′(yHt−1(x))yh(x) + ηEη′ ∼Unif[0,η ][φ′′(yHη′ t−1(x))ht−1(x)h(x)] ] =(1 − σ)−tσ   Φ ′ D(Ht−1, h) − E(x,y )∼ ˆDt [φ′(yHt−1(x))yh(x)]    :=A1    + (1 − σ)−tη   Eη′∼Unif[0,η ][Φ ′′ D(Hη′ t−1, h, ht−1) − E(x,y )∼ ˆDt [φ′′(yHη′ t−1(x))ht−1(x)h(x)    :=A2 ]]   . However, this time, we note that, conditioned on Ft−1, A1 and A2 are averages of S zero-mean IID random variables, where each constituent is absolutely bou nded by one in magnitude (Lemma 2). Hence, by Hoeffding’s inequality, with 1 − 2δ, we have that max{|A1|, |A2|} ≤ 100√ S √ log 2 δ . Note that since this statement holds true for all realizatio ns in Ft−1 – in words, it is a statement about the randomness in the present round alone – it remains true wh en not subject to such ﬁltration’s, i.e., by marginalizing over Ft−1. A union bound over t now yields the claim F Proofs for extensions to inﬁnite classes Deﬁnition 18 (Covering number) . Given a set F in linear space V, a semi norm ∥·∥ on V, N (ε, F, ∥· ∥) is the size of the smallest set G such that for all f ∈ F , there exists a g ∈ G such that ∥f − g∥ ≤ ε. Proof of Theorem 5. First, we state (and subsequently prove) the following samp le complexity result using L1 covering numbers. W e note that it may be possible to improve t his result for speciﬁc classes of functions, i.e., monotonic functions, by applying chain ing techniques [ Dud74] to L2 distances. Theorem 19 (Main result for Covering Number) . Deﬁne L1, DX (f, g ) = Ex∼DX [|f(x) − g(x)|], for any two functions f, g : X → R, where DX is the marginal distribution of D on the features set X . There exists a setting of parameters such that for any for any γ-agnostic weak learning oracle with ﬁxed tolerance ε0 and failure probability δ0, Algorithm 1 produces a hypothesis h such that with probability 1 − 10δ0T − 10δT , corrD(h) ≥ max h∈H corrD(h) − 2ε0 γ − ε, while making T = O((log N (ε/10γ, B, L1, DX ))/γ2ε2) calls to the weak learning oracle, and sam- pling T S + S0 = O((log N (ε/10γ, B, L1, DX ))/γ3ε3) labeled examples from D. The claim follows immediately by the application of the foll owing result from [ VDVW97] to place an upper bound of the L1 covering in terms of the VC dimension in Theorem 19. This result originally due to [ Hau95] was established for distances deﬁned by n-point empirical measures, for some ﬁnite n. The version in [ VDVW97] works on arbitrary distributions, by ﬁrst proving that by a result proven for empirical-type measures transfers witho ut loss to any distribution. Theorem 20 (Theorem 2.6.4 in [ VDVW97]). There exists a universal constant C such that for any VC class B, any probability measure DX , and 0 ≤ ε ≤ 1, N (ε, B, L1, DX ) ≤ C · VC(B) · (4e ε )VC(B) . 22This concludes the proof. Proof of Theorem 19. W e ﬁrst invoke Lemma 6 but on a minimal εCov-cover (say BCov) of B with respect to the L1, DX semi norm to get that there exists a C > 0 such that with probability 1 − δ, for all t ∈ [T ] and h ∈ B Cov ∪ {h∗}, we have ⏐ ⏐ ⏐Φ ′ D(Ht, h) + ( 1 + η σ ) corrDt (h) ⏐ ⏐ ⏐ ≤ C ( σ + η γ )( 1√ σS √ log T log N (εCov , B, L1, DX ) δ + log T log N (εCov , B, L1, DX ) δ )    :=ε′ Gen . Fix any g ∈ B . Now , by virtue of the εCov-cover, there exists a h ∈ B such that L1, DX = Ex∼DX [|f(x) − g(x)|] ≤ εCov. Now , using the deﬁnition of Φ ′ D and a uniform (absolute) upper bound on φ′ (Lemma 2.III), we get |Φ ′ D(Ht, h) − Φ ′ D(Ht, g)| = |E(x,y )∼D[yφ′(yHt(x))(h(x) − g(x))]| ≤ E(x,y )∼D[|yφ′(yHt(x))| · | h(x) − g(x)|] ≤ E(x,y )∼D[|h(x) − g(x)|] ≤ Ex∼DX [|h(x) − g(x)|] ≤ εCov. Similarly, using the fact that the marginal distribution of Dt on X is the same as DX , we get |corrDt (h) − corrDt (g)| ≤ Ex∼DX [|h(x) − g(x)|] ≤ εCov. Combining these, we have that with probability 1 − δ, for all t ∈ [T ] and h ∈ B Cov ∪ {h∗}, we have ⏐ ⏐ ⏐Φ ′ D(Ht, h) + ( 1 + η σ ) corrDt (h) ⏐ ⏐ ⏐≤ ε′ Gen + 2εCov. From here, proceeding identically as the steps in the proof o f Theorem 4 yields corrD(h∗) − corrD(h) ≤ 8 ηT + 2η γ2 + 2(ε′ Gen + εCov) γ + 1 γ ( 1 + η σ )( ε0 + ε′ 5 ) + ε′′ 5 . Setting ε′ = ε/5γ, ε′′ = ε/5, εCov = ε/10γ and plugging in the proposed hyper-parameters with appropriate constants yields the claimed result. G Boosting for reinforcement learning MDP . In this section, we consider a Markov Decision Process M = ( S, A, r, P, β, µ 0), where S is a set of states, A = {±1} is a binary set of actions, r : S × A → [0, 1] determines the (expected) reward at any state-action pair, P : S × A → S captures the transition dynamics of the MDP , i.e., P (s′|s, a) is the probability of moving to state s′ upon taking action a at state s, β ∈ [0, 1) is the discount factor, and µ0 is the initial state distribution. Without any loss, one may restrict their consideration to Markovian policies [ Put14] of the form π : S → ∆( A), where an agent at each point in time chooses action a at state s independently with probability π(a|s). For any state s ∈ S , action a ∈ A , and distribution µ ∼ ∆( S) over states, deﬁne state-action and state-value functions as Qπ (s, a) = E [ ∞∑ t=0 βtr(st, at) ⏐ ⏐ ⏐ ⏐π, s0 = s, a0 = a ] , V π (s) = E [ ∞∑ t=0 βtr(st, at) ⏐ ⏐ ⏐ ⏐π, s0 = s ] , V π µ = Es∼µ [V π (s)] . 23Algorithm 2 RL Boosting adapted from [ BHS22] 1: Input: iteration budget T , state distribution µ, step sizes ηt, post-selection sample size P 2: Initialize a policy π0 ∈ Π arbitrarily. 3: for t = 1 to T do 4: Run Algorithm 1 to get π′ t, while using Algorithm 3 to produce a distribution over state- actions (ignore ˆQ) by executing the current policy πt−1 starting from the initial state distribu- tion µ. 5: Update πt = (1 − ηt)πt−1 + ηtπ′ t. 6: end for 7: Run each policy πt for P rollouts to compute an empirical estimate ˆV π t of the expected return. 8: return π = πt′ where t′ = arg max t ˆV π t . W e will abbreviate V π µ 0 = V π , since it captures the value of any policy starting from the c anonical starting state distribution. Finally, the occupancy measu re µπ µ ′ induced by a policy π starting from an initial state distribution µ′ is stated below . W e will take µπ = µπ µ 0 as a matter of convention. Accessing the MDP . Following [ BHS22], we consider two models of accessing the MDP; we will furnish a different result for each. In the episodic model , the learner interacts with the MDP in a limited number of episodes of reasonable length (i.e., ≈ (1− β)−1), and the starting state of MDP is always drawn from µ0. In the second, termed rollouts with ν-resets, the learner’s interaction is still limited to a small number of episodes, however, the MDP now sa mples its starting state from ν. It is important to stress that in both cases, the learner’s obje ctive is the same, to maximize V π starting from µ0. However, ν could be more spread out over the state space than µ0, and provide an implicit source of explanation, and the learner’s guarantee as shown next beneﬁts from its dependence on a milder notion of distribution mismatch in this case. W eak Leaner . For convenience, we restate our weak learning deﬁnition, th is time using π to denote policies, instead of h. Note that our deﬁnition is equivalent to that used by [ BHS22], because for binary actions a random policy induces an accuracy of half re gardless of the distribution over features and labels. One may use the identity corr D(π) = 1 − 2lD(π) to observe this. In fact, our assumption of weak learning might ostensibly seem weaker, since it oper ates with 0/1 losses (equivalently, correlations), whereas the losses in previous work are assu med general linear. However, for binary actions, this difference is insubstantial and purely styli stic. Deﬁnition 21 (Agnostic W eak Learner) . A learning algorithm is called a γ-agnostic weak learner with sample complexity m : R+ × R+ → N ∪ { +∞} with respect to a policy class Π and a base policy class B if, for any ε0, δ0 > 0, upon being given m(ε0, δ0) independently and identically distributed samples from any distribution D′ ∈ ∆( X × {± 1}), it can output a base policy W ∈ B such that with probability 1 − δ0 we have corrD′ (W) ≥ γ max π ∈Π corrD′ (π) − ε0. Policy Completeness and Distribution Mismatch. π∗ ∈ arg maxπ V π be a reward maximizing policy, and V ∗ be its value. Let Π be the convex hull of the boosted policy class, i.e., the outp uts of the boosting algorithm. For any state distribution µ′, deﬁne the policy completeness Eµ ′ term as Eµ ′ = max π ∈Π min π ′∈Π Es∼µ π µ ′ [max a∈A Qπ (s, a) − Ea∼π ′(·|s)Qπ (s, a)]. In words, this term captures how well the greedy policy impro vement operator is approximated by Π in an state-averaged sense over the distribution induces by any policy in Π . Finally, we deﬁne distribution mismatch coefﬁcients below . C∞ = max π ∈Π ∥µπ ∗ /µπ ∥∞, D ∞ = ∥µπ ∗ /ν∥∞. Theorem 22. Let W be a γ-weak learner for the policy class Π operating with a base class B, with sample complexity m(ε0, δ0) = (log |B|/δ0)/ε2 0. Fix tolerance ε and failure probability δ. In the episodic access model, there is a setting of parameters such that Algo rithm 2 when given access to W produces a policy π such that with probability 1 − δ, we have V ∗ − V π ≤ C∞E 1 − β + ε, 24Algorithm 3 Trajectory Sampler adapted from [ BHS22] 1: Sample state s0 ∼ µ and action a′ ∼ Unif(A). 2: Sample s ∼ µπ as follows: at every step h, with probability β, execute π; else, accept sh. 3: T ake action a′ at state sh, then continue to execute π, and use a termination probability of 1 − β. Upon termination, set R(sh, a′) as the sum of rewards from time h onwards. 4: Deﬁne the vector ˆQ, such that for all a ∈ A, ˆQ(a) = 2 R(sh, a′) · Ia=a′ . 5: With probability C ˆQ(a′), set y = a′ else set y ∈ A − { a′}, where C = (1 − β)/2. 6: return (sh, ˆQ, y). while sampling O ( C5 ∞ log |B| (1 − β)9γ3ε4 ) episodes of length O((1 − β)−1). In the ν-reset access model, there is a setting of parameters such that Algorithm 2 when given access to W produces a policy π such that with probability 1 − δ, we have V ∗ − V π ≤ D∞Eν (1 − β)2 + ε, while sampling O ( D5 ∞ log |B| (1 − β)15γ3ε5 ) episodes of length O((1 − β)−1). H Proofs for boosting for reinforcement learning Proof of Theorem 22. The proof here closely follows that of Theorem 7 in [ BHS22], and we only indicate the necessary departures. Since we utilize the out er algorithm from previous work, the associated guarantees naturally carry over. The departure comes from our substitution of the internal boosting procedure in Algorithm 2 with Algorithm 1; in fact, in its place [ BHS22] use a result of [HS21] which can be seen as a generalization of [ KK09], e.g., it uses fresh samples for every round of boosting, to other non–binary action sets preserving its ≈ 1/ε4 sample complexity. In this view , the improvement in sample complexity by using the present pa per’s apprach seems natural. For the episodic model, applying the second part of Theorem 9 in [ BHS22], while noting the smooth- ness of V π , and combining the result with Lemma 18 and Lemma 11 in [ BHS22], we have with probability 1 − T δ V ∗ − V ¯π ≤ C∞E 1 − β + 4C2 ∞ (1 − β)3T + C∞ 1 − β ( max π ∈Π ,t ∈[T ] E(s, ˆQ,y )∼Dt [ ˆQ⊤(π(·|s) − π′ t(·|s))] ) , where π′ t is the output of the internal boosting algorithm in Line 4 of A lgorithm 2, and Dt is the distribution produced by Algorithm 3 with πt−1 selected as the policy for execution. Here, by explicit calculation, noting a′ ∼ Unif(A), one may verify for any t that E(s, ˆQ,y )∼Dt [y|s] = 1 − β 2 E(s, ˆQ,y )∼Dt [ ˆQ(+1) − ˆQ(−1)|s]. Recall that A = {±1}, and hence π : S → ∆( {±1}). Hence, for any policy pair π1, π2 we have (1 − β)E(s, ˆQ,y )∼Dt [ ˆQ⊤(π1(·|s) − π2(·|s))] = E(s, ˆQ,y )∼Dt Ea1∼π 1(s)Ea2∼π 2(s)[y(a1 − a2)]. Therefore, all we need to ensure is that output of Algorithm 1 as instantiated in Algorithm 2 every round has an excess correlation gap over the best policy Π no more that (1 − β)2ε/C∞, which Algorithm 1 assures us can be accomplished with O ( C3 ∞ log |B| (1−β )6γ 3ε3 ) samples. The total number of samples is T = O ( C2 ∞ (1−β )3ε ) times greater. 25Similarly, for the ν-reset model, applying the ﬁrst part of Theorem 9 in [ BHS22], and combining the result with Lemma 19 and Lemma 11 in [ BHS22], we have V ∗ − V ¯π ≤ D∞Eν (1 − β)2 + 2D∞ (1 − β)3 √ T + D∞ (1 − β)2 ( max π ∈Π ,t ∈[T ] E(s, ˆQ,y )∼Dt [ ˆQ⊤(π(·|s) − π′ t(·|s))] ) + 96D∞ (1 − β)3√ P log 1 δ Again, we need to ensure is that output of Algorithm 1 as instantiated in Algorithm 2 every round has an excess correlation gap over the best policy Π no more that (1 − β)3ε/D∞, which Algorithm 1 assures us can be accomplished with O ( D3 ∞ log |B| (1−β )9γ 3ε3 ) samples. The total number of samples is T = O ( D2 ∞ (1−β )6ε2 ) times greater. I Proofs for agnostic learning of halfspaces Proof of Theorem 8. Using an approximation result of [ KOS04], we observe that ERM on the Fourier basis χS(x) = ∏ i∈S xi, namely parities on subsets S, can be used to produce a weak learner. This result guarantees that an n-dimensional halfspace can be approximated with uniform weighting on the hypercube to ε2 ℓ2-error using degree-limited Bn,d = {±χS : |S| ≤ d} as a basis, where d = 20 ε−4. As a result, at least one h ∈ B n,d must have high correlation. Lemma 23 (Lemma 5 in [ KMV08]). Let D be any data distribution over {±1}n × {− 1, 1} with marginal distribution Unif ({±1}n) on the features. F or any ﬁxed ε and d = 20 ε−4, there exists some h ∈ B n,d such corrD(h) ≥ maxc∈H corrD(c) − ε nd The result follows directly from the preceding lemma, which provides a weak learner for the task, and Theorem 4. W e note that |Bn,d | < n d and γ = n−d, so log |Bn,d | γ3ε3 ≤ dn3d log(n) ε3 . J Additional experimental details For all algorithms, we perform a grid search on the number of b oosting rounds with T ∈ {25, 50, 100}. For Algorithm 1 we search over σ ∈ { 0.1, 0.25, 0.5} as well. Rather than us- ing a ﬁxed η, our implementation uses an adaptive step-size scheme prop ortional to the empiri- cal correlation on the current relabeled distribution. Our experiments were performed using the fractional relabeling scheme stated in [ KK09], intended to reduce the stochasticity the algorithm is subject to. In particular, rather than sampling labels, w e provide both (x, y) and (x, −y) in our dataset with weights 1+wt(x,y ) 2 and 1−wt(x,y ) 2 respectively. For the baselines, wt(x, y) = −φ′ mad (Hty) = min(1 , exp(−Ht(x)y)), where φmad is the Madaboost potential [ Dom00]. For the implementation of the proposed algorithm, for greater repr oducibility, we use the weighting function wt(x, y) = (1 − σ)φ′(Ht−1(x)y) − φ′(Ht(x)y), which is analytically equivalent to computing the expectation of pt(x, y, η ′) in Algorithm 1 over η′ ∼ Unif[0, η]. The runtime used for all experiments was a Google Cloud Engine VM instance with 2 vCPUs (Intel Xeon 64-bit @ 2.20 GHz) and 12 GB RAM. K Guide for practical adaptation of Algorithm 1 For many hyperparamters, our theory provides strong clues ( the link between η and σ). Brieﬂy, in practice, given a ﬁxed dataset with mtotal data points, there are two parameters we think a practi- tioner should concern herself with: the mixing parameter σ and the number of rounds of boosting T , 26while the rest can be determined, or at least guessed well, gi ven these. The ﬁrst choice σ dictates the relative weight ascribed to reused samples across rounds of boosting, while T , apart from determin- ing the running time and generalization error, implicitly l imits the algorithm to using mtotal /T fresh samples each round. For η, one may use the adaptive step-size schedule suggested in th e previous section, which also obviates the difﬁculty of selecting γ. Similarly, our branching criteria, whose necessity is explained in Appendix A, although theoretically both sound and necesssary, is over ly conservative. In practice, we believe choosing the better o f −sign(Ht) and Wt whichever produces the greatest empirical distribution on the relabeled distr ibution will perform best. 27This figure \"graph.png\" is available in \"png\"  format from: http://arxiv.org/ps/2410.23632v1",
      "references": [
        "Variance-reduced conservative policy iteration",
        "Boosting simple learners",
        "Theory of classification: A survey of some recent advances",
        "Online agnostic boosting via regret minimization",
        "Agnostic boosting",
        "A boosting approach to reinforcement learning",
        "Optimal and adaptive algorithms for online boosting",
        "Communication efficient distributed agnostic boosting",
        "An online boosting algorithm with theoretical justifications",
        "Bounded independence fools halfspaces",
        "Boosting in the presence of Massart noise",
        "Near-optimal SQ lower bounds for agnostically learning halfspaces and ReLUs under Gaussian marginals",
        "Madaboost: a modification of Adaboost",
        "Metric entropy of some classes of sets with differentiable boundaries",
        "Distribution-specific agnostic boosting",
        "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator",
        "On tail probabilities for martingales",
        "A decision-theoretic generalization of on-line learning and an application to boosting",
        "Optimal weak to strong learning",
        "Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik-Chervonenkis dimension",
        "AdaBoost is not an optimal weak to strong learner",
        "Spambase",
        "Boosting for online convex optimization",
        "Online multiclass boosting",
        "Accelerating stochastic gradient descent using predictive variance reduction",
        "Boosting and hard-core set constructions: a simplified approach",
        "Potential-based agnostic boosting",
        "Approximately optimal approximate reinforcement learning",
        "On agnostic boosting and parity learning",
        "Learning intersections and thresholds of halfspaces",
        "Boosting in the presence of noise",
        "Cryptographic limitations on learning boolean formulae and finite automata",
        "Random classification noise defeats all convex potential boosters",
        "Boosting using branching programs",
        "Foundations of machine learning",
        "Optimum bounds for the distributions of martingales in Banach spaces",
        "Extension of Bernstein’s inequality when the random variable is bounded with large probability",
        "Markov decision processes: discrete stochastic dynamic programming",
        "Scikit-learn: Machine learning in Python",
        "Online agnostic multiclass boosting",
        "The strength of weak learnability",
        "Using the adap learning algorithm to forecast the onset of diabetes mellitus",
        "Boosting: Foundations and algorithms",
        "Minimizing finite sums with the stochastic average gradient",
        "Understanding machine learning: From theory to algorithms",
        "Ionosphere",
        "Weak convergence and empirical processes: with applications to statistics",
        "Rapid object detection using a boosted cascade of simple features"
      ],
      "meta_data": {
        "arxiv_id": "2410.23632v1",
        "authors": [
          "Udaya Ghai",
          "Karan Singh"
        ],
        "published_date": "2024-10-31T04:50:29Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a new potential-based agnostic boosting framework that reuses training examples across boosting rounds, reducing sample complexity from the prior O((log|B|)/γ^4ε^4) to O((log|B|)/γ^3ε^3) without increasing oracle (weak-learner) calls. Provides an alternative variant matching previous oracle complexity with still-lower samples, extends theory to infinite hypothesis classes via VC dimension, and applies the booster to reinforcement learning and agnostic half-space learning, achieving improved theoretical bounds and empirical gains on several UCI datasets.",
        "methodology": "Introduces Algorithm 1 that builds an ensemble by either adding the current weak hypothesis or the negated ensemble sign, chosen via a new branching rule. It constructs a relabeled data distribution mixing old and newly drawn samples (controlled by σ) and employs a second-order estimator of a smooth convex potential φ to allow sample reuse. A martingale analysis shows empirical correlations track potential gradients, avoiding uniform convergence over the growing ensemble. A refined Freedman inequality gives high-probability bounds; L1 covering numbers extend results to infinite classes. Variants trade sample for oracle complexity. Same booster is embedded in RL (Algorithm 2) with trajectory relabeling (Algorithm 3).",
        "experimental_setup": "Primarily theoretical; empirical validation on six UCI classification datasets (Ionosphere 351 ex., Diabetes 768, Spambase 4601, German Credit, Sonar, Waveform). Weak learners are decision stumps; algorithms compared are KK09 and BCHM20. 30-fold cross-validation with grid search over boosting rounds T {25,50,100} and mixing parameter σ {0.1,0.25,0.5}. Datasets are evaluated in original form and with 5%,10%,20% injected label noise. Accuracy averaged with standard deviations reported; experiments run on a 2-vCPU GCP VM.",
        "limitations": "Contributions are mainly theoretical; empirical tests are small-scale and limited to tabular datasets, leaving real-world and high-dimensional performance untested. Assumes existence of a γ-agnostic weak learner, a stronger requirement than realizable boosting. Main theorem uses more weak-learner calls than earlier work (addressed by variant) and still leaves a quadratic sample gap to ERM. Algorithm currently targets binary labels/actions and may be sensitive to hyper-parameters (σ, T, η).",
        "future_research_directions": "1) Further reduce sample complexity to match ERM while retaining efficiency. 2) Lower oracle complexity simultaneously with sample gains. 3) Extend framework to multi-class classification and multi-action RL. 4) Develop practical agnostic weak learners for complex data (e.g., deep networks) and test on large-scale benchmarks. 5) Investigate adaptive or data-dependent tuning of mixing and step-size parameters. 6) Combine with noise-tolerant or differential-privacy techniques.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Label-invariant Augmentation for Semi-Supervised Graph Classification",
      "full_text": "LABEL -INVARIANT AUGMENTATION FOR SEMI -SUPERVISED GRAPH CLASSIFICATION Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu Brandeis University {hanyue, chunhuizhang, chuxuzhang, hongfuliu}@brandeis.edu ABSTRACT Recently, contrastiveness-based augmentation surges a new climax in the computer vision domain, where some operations, including rotation, crop, and ﬂip, combined with dedicated algorithms, dramatically increase the model generalization and robustness. Following this trend, some pioneering attempts employ the similar idea to graph data. Nevertheless, unlike images, it is much more difﬁcult to design reasonable augmentations without changing the nature of graphs. Although exciting, the current graph contrastive learning does not achieve as promising performance as visual contrastive learning. We conjecture the current performance of graph contrastive learning might be limited by the violation of the label-invariant augmentation assumption. In light of this, we propose a label-invariant augmentation for graph-structured data to address this challenge. Different from the node/edge modiﬁcation and subgraph extraction, we conduct the augmentation in the representation space and generate the augmented samples in the most difﬁcult direction while keeping the label of augmented data the same as the original samples. In the semi-supervised scenario, we demonstrate our proposed method outperforms the classical graph neural network based methods and recent graph contrastive learning on eight benchmark graph-structured data, followed by several in-depth experiments to further explore the label-invariant augmentation in several aspects. Keywords Graph Contrastive Learning ·Semi-supervised Classiﬁcation 1 Introduction Contrastive augmentation aims to expand training data in both volume and diversity in a self-supervised fashion to increase model robustness and generalization. Common sense and domain knowledge are employed to design the contrastive augmentation operations. Denoising auto-encoder [ 1, 2] is one of the pioneering studies to apply perturbations to generate contrastive samples for tablet data, which takes a corrupted input and recovers the original undistorted input. For visual data, some operations, including rotation, crop, and ﬂip, combined with dedicated algorithms, signiﬁcantly improve the learning performance in diverse tasks [ 3, 4, 5, 6, 7]. Treating the augmented and original samples as positive pairs and the augmented samples from different source samples as negative pairs, contrastive learning aims to learn the augment-invariant representations by increasing the similarity of positive pairs and the dissimilarity of negative pairs [8]. These positive pairs increase the model robustness due to the assumption that the augmented operations preserve the nature of images and make the augmented samples have consistent labels with the original ones. The negative pairs work as the instance-level discrimination, which is expected to enhance the model generalization, but might deteriorate the downstream task since the negative pairs contain the augmented samples from different source samples but with the same category. The recent BYOL [9] and SimSiam [10] demonstrate the negative effect of the negative pairs and conclude that the current performance of contrastive learning can be further boosted even without negative pairs. Following this trend, some pioneering attempts employ contrastive augmentation to graph data. GraphCL [11] is the ﬁrst work to address the graph contrastive learning problem with four types of augmentations, including node dropping, edge perturbation, attribute masking, and subgraph extraction. Later, JOAO [12] extends GraphCL by automatically selecting one type of graph augmentation from the above four types plus non-augmentation. GRACE [13] treats the original graph data and the novel-level augmented data as two views and learns the graph representation by maximizing arXiv:2205.09802v1  [cs.CV]  19 May 2022Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation the agreement between the two views. Similarly, MVGRL [ 14] conducts the contrastive multi-view representation learning on both node and graph levels. Beyond the above studies to augment graphs, simGRACE [15] perturbs the model parameters for contrastive learning, which can be regarded as an ensemble of model perturbation or a robust regularization. Although exciting, the above studies point out that the effectiveness of graph contrastive learning heavily hinges on ad-hoc data augmentations, which need to be carefully designed or selected per dataset and request more domain knowledge. Contributions. We conjecture these hand-crafted graph augmentations might change the nature of the original graph and violate the label-invariant assumption in the downstream tasks. Different from treating graph contrastive learning in a pre-trained perspective, we aim to incorporate the downstream classiﬁcation task into the representation learning, where the label information is fully used for both decision boundary learning and graph augmentation. Speciﬁcally, we propose Graph Label-invariant Augmentation (GLA), which conducts augmentation in the representation space and augments the most difﬁcult sample while keeping the label of the augmented sample the same as the original sample. Our major contributions are summarized as follows: • We propose a label-invariant augmentation strategy for graph contrastive learning, which involves labels in the downstream task to guide the contrastive augmentation. It is worthy to note that we do not generate any graph data. Instead, we directly generate the label-consistent representations as the augmented graphs during the training phase. • In the rich representation space, we aim to generate the most difﬁcult sample for the model and increase the model generalization. Rather than formulating it as an expensive bi-level optimization problem, we choose a lightweight technique by randomly generating a set of qualiﬁed candidates and selecting the most difﬁcult one, i.e., minimizing the maximum loss or worst case loss over the augmented candidates. • We conduct a series of semi-supervised experiments on eight graph benchmark datasets in a fair setting and compare our label-invariant augmentation with classical graph neural network based methods and recent graph contrastive methods by running the codes provided by the original authors. Extensive results demonstrate our label-invariant augmentation can achieve better performance in general cases without generating real augmented graphs and any speciﬁc domain knowledge. Besides algorithmic performance, we also provide rich and in-depth experiments to explore label-invariant augmentation in several aspects. 2 Related Work Here we introduce the related work of graph neural networks and graph contrastive learning for graph classiﬁcation. Node classiﬁcation, although related, is not covered here due to its different setting. Graph Neural Network. Graph Neural Networks (GNNs) have been employed on various graph learning tasks and achieved promising performance [16]. To extract the representation of each node, GNNs pass node embeddings from its connected neighbor nodes and apply feedforward neural networks to transform the aggregated features. As a pioneer study in GNNs, graph convolutional network (GCN) ﬁrstly aims to generalize the convolution mechanism from image to graph [16, 17, 18]. Based on GCN, instead of simply summing and averaging connected neighboring node’s embedding, graph attention networks [19, 20, 21] adopt an attention mechanism that builds self-attention to score each connected neighboring nodes’ embedding to identify the more important nodes and enhance the effectiveness of message passing. Then in order to break prior GNN’s limitations on message passing over long distances on large graphs, graph recurrent neural networks [22, 23] apply the gating mechanism from RNNs to propagation on graph topology. Simultaneously, for dealing with the noise introduced from more than 3 layers of graph convolution, DeepGCN [ 24, 25] uses skip connections and enables GCN to achieve better results with deeper layers. Recently, GAE [ 26] and Infomax [ 27] achieve state-of-the-art performance on several benchmark datasets. GAE extends the variational auto-encoder to graph neural networks for unsupervised learning, while Infomax learns the unsupervised representation on graphs to enlarge mutual information between local (node-level) and global (graph-level) representations in one graph. Graph Contrastive Learning. Recently, many studies has been devoted to the graph contrastive learning area in diverse angles, including graph augmentation, negative sample selection, and view fusion. GraphCL [11] summarizes four types of graph augmentations to learn the invariant representation across different augmented views. Built on GraphCL, JOAO [12] proposes a learnable module to automatically select augmentation for different datasets to alleviate the human labor in combinations of these augmentations. Differently, MVGRL [14] contrasts node and graph encodings across views which enriches more negative samples for contrastive learning. Later, InfoGCL [ 28] diminishes the mutual information between contrastive parts among views while preserving the task-relevant representation. Beyond augmenting graphs, SimGRACE [15] disturbs the model weights and then learns the invariant high-level representation at the output end to alleviate the design of graph augmentation. 2Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Different from the above methods that separate the pre-train and ﬁne-tuning phases, we aim to employ the label information in downstream tasks to guide the augmentation process. Speciﬁcally, in this study, we propose a label- invariant augmentation strategy for graph-level representation learning. 3 Methodology A graph can be represented by G= (V,X,A ), where V = {v1,v2,...,v n}is the set of vertexes, X ∈Rn×d denotes the features of each vertex, and A ∈ {0,1}n×n represents the adjacency matrix. Given a set of labeled graphs S= {(G1,y1),(G2,y2),..., (GM,yM)}where M is the number of labeled graphs, and yi ∈Y is the corresponding categorical label of graph Gi ∈G (1≤i≤M), and another set of unlabeled graphs T = {GM+1,...,G N}, where N is the number of all graphs, M<N, the semi-supervised graph classiﬁcation problem can be deﬁned as learning a mapping function from graphs to categorical labels f : G→Y to predict the labels of T. In this section, we ﬁrst illustrate our motivation supported by empirical evidence, then we elaborate on our Graph Label-invariant Augmentation (GLA) method for semi-supervised graph classiﬁcation. 3.1 Motivation Figure 1: Performance gains (%) of GraphCL and JOAOv2 under different augmentation settings on MUTAG [29] dataset compared to none augmentation setting. Augmentation plays an important role in neural network training. It not only improves the robustness of learned representation but also introduces rich data for training. For graph-structured data, GraphCL [11] proposes four types of augmentations: node dropping, edge perturba- tion, attribute masking, and subgraph sampling. How- ever, it is widely noticed that the effectiveness of graph contrastive learning highly depends on the chosen types of augmentations for speciﬁc graph data [ 12, 12]. To illustrate the difference between various augmentation combinations, we conduct experiments on MUTAG [29] in a semi-supervised graph classiﬁcation task, where the label rate is set to 50%. Figure 1 shows the performance gains (classiﬁcation accuracy %) of different augmenta- tion combinations compared to none augmentation. We can see that different augmentation combinations result in different performances, and JOAOv2 automatically selects data augmentations but cannot guarantee to outperform GraphCL with all augmentation combinations. Moreover, some augmentation combinations work worse than none augmentation, which demonstrates that some augmentations hurt the model training. We further ﬁne-tune a model with 100% labeled graphs from MUTAG, and then feed the augmented graphs (randomly selected from the four augmentation types with an augmentation ratio of 0.2) to this model, ﬁnding that about 80% augmented graphs have the same labels with their corresponding original graphs. It indicates that most augmentations are reasonable, which is one of the reasons that GraphCL works well. While on the other hand, there are still about 20% augmented graphs getting different labels from the original ones. Motivated by this, we design a label-invariant augmentation strategy for graph contrastive learning. 3.2 Label-invariant Augmentation Framework Overview. Figure 2 illustrates the framework of our proposed Graph Label-invariant Augmentation (GLA) for semi-supervised graph classiﬁcation, which mainly consists of four components: Graph Neural Network Encoder, Classiﬁer, Label-invariant Augmentation, and Projection Head. We ﬁrst use GNN Encoder to get graph-level original representation for the input graph. Then Label-invariant Augmentation, together with Classiﬁer, is utilized to generate augmented representation from original representation under a label-invariant constraint. For an unlabeled graph, we expect that the labels represented by original prediction and augmented prediction are the same. For a labeled graph, we expect that the label represented by augmented prediction is the same as the ground truth label. A cross-entropy loss is used to keep reﬁning the classiﬁer with labeled graphs. Finally, a Projection Head is adopted to generate projections for contrastive loss. We use Θ = {θG,θC,θP }to denote the trainable parameters set, where θG, θC, and θP denote the parameters of GNN Encoder, Classiﬁer and Projection Head, respectively. Details of each component are as follows. Graph Neural Network Encoder. Graph Neural Network Encoder aims to get graph-level representations for graph- structured data. It is ﬂexible to adopt various GNNs for this part. We follow GraphCL [11] and utilize ResGCN [30], which takes Graph Convolutional Network (GCN) [31] as the backbone, to extract node-level representations from the input graph, and then a global sum pooling layer is used to obtain its graph-level representation. The computation of the 3Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Projection  Head GNN  Encoder Classifier Maximizing Agreement Label-invariant Augmentation Input Graph … Augmented Representations Figure 2: Framework of our Graph Label-invariant Augmentation (GLA) for semi-supervised graph classiﬁcation. Given an input graph, a Graph Neural Network (GNN) Encoder is employed to encode the input graph into a graph- level representation (original representation). Then we perturb the original representation to get multiple augmented representations. A classiﬁer is adopted to verify whether the augmentations are label-invariant or not. We select the “hardest” augmented representation, i.e., the one that has the least probability of belonging to the same class as original representation, from all augmented representations that satisfy the label-invariant constraint. On top of GNN Encoder, we build a projection head to get projections for both original representation and label-invariant augmented representation. We maximize the agreement between projections via a contrastive loss for all graphs and reﬁne the classiﬁer via a cross-entropy loss with labeled graphs. GCN layer with the parameter θG is described as follows: G(l+1) = σ( ˜D−1 2 ˜A˜D−1 2 G(l)θ(l) G ), (1) where ˜A= A+ In is the adjacency matrix Awith added self-connections, In ∈Rn×n is the identity matrix, ˜Dis the degree matrix of ˜A, and θ(l) G is a layer-speciﬁc trainable weight matrix. G(l) denotes the matrix in the l-th layer, and G(0) = X. We employ σ(·) = ReLU(·) as the activation function. Then on top of the ResGCN, we use a global sum pooling layer to get graph-level representations from node-level representations as follows: H = Pooling(G). (2) Here we use HO to denote the original representation of the input graph andHA to denote the augmented representation of the augmented graph. The augmentation method will be described in the Label-invariant Augmentation part. Classiﬁer. Based on the graph-level representations, we employ fully-connected layers with the parameter θC for prediction: C(l+1) = Softmax(σ(C(l) ·θ(l) C )), (3) where C(l) denotes the embeddings in the l-th layer, and the input layer C(0) = HO or C(0) = HA for the original representation and augmented representation, respectively. In our experiments, we adopt a 2-layer multilayer perceptron and obtain predictions CO and CA for original representation HO and augmented representation HA. Label-invariant Augmentation. Instead of augmenting graph data by node dropping, edge perturbation, attribute masking, or subgraph sampling as recent graph contrastive learning methods [11, 12], we conduct the augmentation in the representation space by adding a perturbation to the original representation HO so that we do not need to generate any graph data. In our experiment, we ﬁrst calculate the centroid of original representations for all graphs and get the average value of euclidean distances between each original representation and the centroid as d, that is: d= 1 N N∑ i=1 ∥HO i − 1 N N∑ j=1 HO j ∥. (4) Then the augmented representation HA is calculated by: HA = HO + ηd∆, (5) where ηscales the magnitude of the perturbation, and ∆ is a random unit vector. To achieve the label-invariant augmentation, each time, we randomly generate multiple perturbations and select the qualiﬁed augmentation candidates that obey the label-invariant property. Among these qualiﬁed candidates, we choose the most difﬁcult one, i.e., the one closest to the decision boundary of the classiﬁer, to increase the model generalization ability. 4Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Table 1: Statistics of datasets for semi-supervised graph classiﬁcation Datasets Category #Class #Graph Avg. #Node Avg. #Edge MUTAG Biochemical Molecules 2 188 17.93 19.79 PROTEINS Biochemical Molecules 2 1113 39.06 72.82 DD Biochemical Molecules 2 1178 284.32 715.66 NCI1 Biochemical Molecules 2 4110 29.87 32.30 COLLAB Social Networks 3 5000 74.49 2457.78 RDT-B Social Networks 2 2000 429.63 497.75 RDT-M5K Social Networks 5 4999 508.52 594.87 GITHUB Social Networks 2 12725 113.79 234.64 Projection Head. We employ fully-connected layers with the parameter θP to get projections for contrastive learning from graph-level representations, which is shown as: P(l+1) = σ(P(l) ·θ(l) P ). (6) We adopt a 2-layer multilayer perceptron and get projections PO and PA from original representation HO and augmented representation HA. Objective Function. Our objective function consists of contrastive loss and classiﬁcation loss. For contrastive loss, we utilize the normalized temperature-scaled cross-entropy loss (NT-Xent) [11] but only keep the positive-pair part as follows: LP = −(PO)⊤PA ∥PO∥∥PA∥. (7) Maximizing the agreement between original projection and augmented projection would increase the robustness of the model. For classiﬁcation loss, we adopt cross-entropy, which is deﬁned as: LC = − c∑ i=1 (YO i log PO i + YO i log PA i ), (8) where YO is the label of the input graph, and cis the number of graph categories. We only calculate LC for labeled graphs. The improvement of the classiﬁer would help with the label-invariant augmentation, which in turn beneﬁts the training of the classiﬁer. Combining Eq. (7) and (8), our overall objective function can be written as follows: minΘ LP + αLC, (9) where αis a trade-off hyperparameter to balance the contrastive loss and classiﬁcation loss. Discussion. From the perspective of information usage for model training, our proposed method is the same as the semi-supervised learning task by recent graph contrastive learning methods [ 11, 12, 14, 15], which use structure information of all graphs and label information of a subset of all graphs for model training. From the perspective of training strategy, the previous methods ﬁrst pre-train a model via a contrastive loss and then ﬁne-tune the model for downstream tasks. While our proposed method focuses on semi-supervised classiﬁcation, we merge the pre-train and ﬁne-tuning phases into one integrated phase. During our training phase, the augmented samples increase the model robustness and generalization ability, and the classiﬁer helps to generate better augmented samples, which in turn beneﬁts classiﬁcation performance. 4 Experiments In this section, we ﬁrst describe our semi-supervised settings of experiments and baseline methods for comparison. Then we show the algorithmic performance of these methods on eight graph benchmark datasets in a fair setting. Finally, we provide some insightful experiments to demonstrate the effectiveness of the proposed Graph Label-invariant Augmentation (GLA) method. 4.1 Experiment Settings Datasets. We select eight public graph classiﬁcation benchmark datasets from TUDataset [32] for evaluation, including MUTAG [29], PROTEINS [33], DD [34], NCI1 [35], COLLAB [36], RDT-B [36], RDT-M5K [36], and GITHUB [37]. 5Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Table 2: Semi-supervised graph classiﬁcation results (Accuracy % ±Standard Deviation %) on eight benchmark datasets. The best and second-best results are highlighted in red and blue, respectively. Label Methods MUTAG PROTEINS DD NCI1 COLLAB RDT-B RDT-M5K GITHUB Avg. Rank GAE 83.63±0.81 74.31±0.33 77.33±0.36 77.20±0.22 77.46±0.11 90.75±0.17 54.81±0.18 65.22±0.11 75.09 5.00 Infomax 84.68±1.12 74.84±0.28 77.07±0.45 79.49±0.17 77.30±0.19 90.65±0.17 55.37±0.20 66.45±0.06 75.73 4.25 MVGRL 83.16±0.98 75.56±0.44 77.08±0.56 72.41±0.18 75.28±0.12 88.20±0.16 53.16±0.06 64.71±0.04 73.70 6.12 30% SimGRACE 83.68±0.84 74.38±0.30 76.27±0.38 78.52±0.17 78.66±0.24 90.60±0.17 55.54±0.16 66.81±0.14 75.56 4.50 GraphCL 85.20±0.98 74.12±0.30 78.60±0.37 79.22±0.09 77.90±0.20 90.35±0.18 56.07±0.15 67.63±0.13 76.14 3.38 JOAOv2 85.67±0.91 75.02±0.30 77.16±0.30 78.69±0.18 79.88±0.17 91.65±0.15 55.23±0.14 67.96±0.10 76.41 2.62 GLA (Ours) 86.32±1.25 75.65±0.37 77.49±0.40 79.71±0.13 78.78±0.12 91.05±0.25 55.85±0.22 65.16±0.19 76.25 2.12 GAE 84.12±0.90 74.75±0.38 78.35±0.31 79.56±0.16 80.47±0.14 90.95±0.19 55.69±0.16 67.09±0.13 76.37 5.88 Infomax 87.37±1.11 75.38±0.38 78.26±0.38 80.80±0.13 79.70±0.11 91.50±0.26 56.51±0.18 67.70±0.09 77.15 3.75 MVGRL 85.79±0.23 76.72±0.34 78.60±0.46 74.09±0.10 76.08±0.05 88.55±0.06 54.04±0.06 64.89±0.05 74.84 5.62 50% SimGRACE 86.32±0.88 75.09±0.35 78.39±0.35 79.78±0.24 80.48±0.15 91.45±0.16 56.50±0.20 67.71±0.16 76.97 4.50 GraphCL 87.28±0.71 75.29±0.29 78.73±0.46 80.17±0.19 80.40±0.16 91.45±0.25 56.83±0.19 68.71±0.09 77.36 3.25 JOAOv2 86.78±0.79 75.74±0.29 78.52±0.45 80.10±0.17 81.50±0.18 92.10±0.18 56.51±0.17 68.97±0.11 77.53 2.75 GLA (Ours) 90.00±0.94 76.19±0.28 80.22±0.37 80.66±0.28 80.84±0.12 91.65±0.22 56.63±0.13 66.59±0.14 77.85 2.25 GAE 87.31±0.66 75.47±0.38 79.37±0.36 79.78±0.17 80.78±0.12 91.50±0.19 56.25±0.16 68.42±0.14 77.36 5.62 Infomax 88.33±0.73 75.92±0.38 79.28±0.33 82.85±0.16 81.04±0.12 92.15±0.13 56.63±0.18 68.88±0.14 78.14 3.62 MVGRL 87.95±0.35 77.81±0.35 79.51±0.34 74.43±0.08 76.42±0.08 88.65±0.23 54.40±0.11 65.00±0.08 75.52 5.25 70% SimGRACE 87.37±0.71 76.52±0.36 78.90±0.29 81.80±0.15 81.88±0.23 92.45±0.13 56.58±0.09 68.19±0.15 77.96 4.12 GraphCL 88.33±0.86 76.36±0.25 79.03±0.29 82.50±0.13 81.08±0.17 91.85±0.14 56.91±0.17 69.19±0.08 78.16 3.62 JOAOv2 87.78±0.76 76.46±0.27 79.11±0.38 81.70±0.26 82.16±0.17 92.20±0.19 56.67±0.16 69.96±0.11 78.26 3.25 GLA (Ours) 91.05±0.86 77.45±0.38 80.71±0.29 83.24±0.14 81.54±0.14 91.70±0.17 57.01±0.14 67.11±0.18 78.73 2.50 Table 1 shows the statistics of these datasets. The ﬁrst four datasets include biochemical molecules and proteins, and the last four datasets are about social networks. The numbers of graphs in these datasets range from 188 to 12725, the average node numbers range from 17.93 to 508.52, and the average edge numbers are from 19.79 to 2457.78, indicating the diversity of these datasets. Compared Methods and Implementation . We choose two heuristic self-supervised methods, GAE [ 26] and In- fomax [27], and four recent graph contrastive learning methods, MVGRL [ 14], GraphCL [11], JOAOv2 [12], and SimGRACE [15], for comparison on semi-supervised graph classiﬁcation task. GAE performs adjacency matrix recon- struction by using a graph convolutional network (GCN) [31] encoder and a simple inner product decoder. Infomax is based on global-local representation consistency enforcement, which maximizes the mutual information between global and local representation. MVGRL proposes to learn node and graph level representations by node diffusion and contrasting encodings. GraphCL presents four types of graph augmentations. Based on GraphCL, JOAOv2 is designed as a uniﬁed bi-level optimization framework to automatically select graph augmentations. SimGRACE perturbs parameters of graph encoder for contrastive learning, which does not require data augmentations. For GAE, Infomax, and GraphCL, we adopt the implementations and default hyperparameter settings provided by the source codes of GraphCL [11]. For other compared methods, we follow the implementations and hyperparameter settings in their corresponding source codes. The compared methods are pre-trained ﬁrst and then are ﬁne-tuned for the semi-supervised graph classiﬁcation task. For our proposed Graph Label-invariant Augmentation (GLA) method, we perform contrastive learning and graph classiﬁer learning synchronously. The implementation details of GLA are as follows. We implement the networks based on GraphCL [11] by PyTorch, set the magnitude of perturbation ηto 1.0, and the weight of classiﬁcation loss αto 1.0, which is the same with GraphCL. We adopt Adam optimizer [38] to minimize the objective function in Eq. (9). Evaluation Protocol. We evaluate the models with 10-fold cross-validation. We randomly shufﬂe a dataset and then evenly split it into 10 parts. Each fold corresponds to one part of data as the test set and another part as the validation set to select the best epoch, where the rest folds are used for training. We select 30%, 50%, 70% graphs from the training set as labeled graphs for each fold, then conduct semi-supervised learning. For a fair comparison, we use the same training/validation/test splits for all compared methods on each dataset. Semi-supervised graph classiﬁcation results are reported by the average accuracy across 10 folds and their standard deviations. 4.2 Algorithmic Performance Table 2 shows the prediction results of two self-supervised and ﬁve graph contrastive learning methods under the semi-supervised graph classiﬁcation setting with 30%, 50%, and 70% label ratios on eight benchmark datasets, where the best and second-best results are highlighted in red and blue, respectively, and the last column is the average rank score across all datasets. Although different algorithms achieve their best performances on different datasets, the contrastiveness-based methods perform better than the non-contrastiveness-based methods in general, which indicates the effectiveness of the graph augmentation. Our proposed GLA achieves the best ranking scores under all 30%, 50%, and 70% label ratios in experiments, the second-best average performance under 30% label ratio, and the best average 6Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation /uni00000016/uni00000013/uni00000008/uni00000010/uni00000021/uni00000018/uni00000013/uni00000008/uni00000016/uni00000013/uni00000008/uni00000010/uni00000021/uni0000001a/uni00000013/uni00000008 /uni00000003 /uni00000014 /uni00000014/uni00000011/uni00000018 /uni00000015 /uni00000015/uni00000011/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000002a/uni00000044/uni0000004c/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002d/uni00000032/uni00000024/uni00000032/uni00000059/uni00000015 (a) performance gain /uni00000030/uni00000039/uni0000002a/uni00000035/uni0000002f/uni00000036/uni0000004c/uni00000050/uni0000002a/uni00000035/uni00000024/uni00000026/uni00000028/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni0000002d/uni00000032/uni00000024/uni00000032/uni00000059/uni00000015/uni0000002a/uni0000002f/uni00000024/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c  (b) label-invariant rate distribution /uni00000016/uni00000013/uni00000008/uni00000018/uni00000013/uni00000008/uni0000001a/uni00000013/uni00000008 /uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a /uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036 /uni00000027/uni00000027 /uni00000026/uni00000032/uni0000002f/uni0000002f/uni00000024/uni00000025 (c) GLA’s label-invariant rate Figure 3: Performance gain and label-invariant rates. (a) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and GLA. (b) shows the label-invariant rate distributions of different augmentation methods over eight datasets. (c) shows the label-invariant rates of our GLA over different semi-supervised settings. performance under 50% and 70% label ratios. In our algorithmic design, we employ the decision boundary learned from the labeled samples to verify the label-invariant augmentation. It is worthy to note that the quality of the decision boundary depends on the number of labeled samples. We conjecture that a 30% label ratio is not sufﬁcient enough to learn a high-quality decision boundary, resulting in our GLA performing slightly worse than JOAOv2 on average. With more labeled samples, our GLA delivers the best average performance over other competitive methods. Different from other graph contrastive learning methods, our augmentation method aims to generate label-invariant augmentations, which decreases the possibility of getting “bad” augmentations, thus resulting in better performance. Besides the general comparison in Table 2, we dive into details and discover several interesting ﬁndings. Figure 3(a) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and our GLA, the top three methods in our experiments. In addition to seeing that the increased performance of all three methods well aligns with more labeled samples, our GLA receives more performance gains than GraphCL and JOAOv2. By such comparisons, we can roughly eliminate the effect of more labeled samples and attribute the extra gains to the label-invariant augmentation. It also veriﬁes our aforementioned conjecture that the high-quality decision boundary is beneﬁcial to the label-invariant augmentation, further bringing in the performance boost. Moreover, we further verify our motivation by checking the label-invariant property of different contrastive methods. While we do not have a ground truth classiﬁer, we use ﬁne-tuned classiﬁers in the representation spaces learned by these contrastive methods with a 100% label ratio as the surrogates of the ground truth classiﬁer. Then we use these classiﬁers to assess how many of the augmented representations belong to the same class as their corresponding original representations. Figure 3(b) presents the distributions of label-invariant rates across eight baseline datasets for all graph contrastive methods. As our GLA trained under different label ratios would generate different augmentations, we put the results of GLA’s label-invariant rates under 30%, 50%, and 70% label ratios together for plotting. We can see that GLA has the highest label-invariant rates on average compared to other methods. It is also noticed that the label-invariant rates of different contrastive methods keep the same ranking with the performance in Table 2 (the last column), which veriﬁes our motivation for designing a label-invariant augmentation strategy. Moreover, we further demonstrate our GLA’s label-invariant rates along with different label ratios in Figure 3(c), which accords with our expectation that more labeled samples lead to a high-quality decision boundary and further promote the label-invariant rate in GLA. 4.3 In-depth Exploration Here we further demonstrate several in-depth explorations of our GLA in terms of negative pairs, augmentation space, and strategy. Negative Pairs. The existing graph contrastive learning methods treat the augmented graphs from different source samples as negative pairs and employ the instance-level discrimination on these negative pairs. Since these methods separate the pre-train and ﬁne-tuning phases, the negative pairs contain the augmented samples from different source samples but with the same category in the downstream tasks. Here we explore the effect of negative pairs on our GLA. Figure 4(a) shows the performance of our GLA with and without negative pairs on four datasets. We can see the performance with negative pairs signiﬁcantly drops compared with our default setting without negative pairs, which behaves consistently on all four datasets. Different from the existing graph contrastive methods, our GLA integrates the pre-train and ﬁne-tuning phases, where the negative pairs designed in a self-supervised fashion are not beneﬁcial to the downstream tasks. This ﬁnding is also in accord with the recent studies [10, 9] in the visual contrastive learning area. 7Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a/uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036/uni00000027/uni00000027/uni00000031/uni00000026/uni0000002c/uni00000014 /uni00000003 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000003a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000031/uni00000048/uni0000004a/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056/uni00000003/uni0000000b/uni0000002a/uni0000002f/uni00000024/uni0000000c /uni0000003a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000031/uni00000048/uni0000004a/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056 (a) w/o negative pairs /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001b/uni00000016 /uni0000001b/uni0000001a /uni0000001c/uni00000014/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (b) η on MUTAG /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000014 /uni0000001a/uni00000018 /uni0000001a/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (c) η on PROTEINS /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a/uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036/uni00000027/uni00000027/uni00000031/uni00000026/uni0000002c/uni00000014 /uni00000003 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002b/uni00000044/uni00000055/uni00000047/uni00000048/uni00000056/uni00000057/uni00000003/uni0000000b/uni0000002a/uni0000002f/uni00000024/uni0000000c /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000028/uni00000044/uni00000056/uni0000004c/uni00000048/uni00000056/uni00000057 (d) augmentation strategy /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000017 /uni0000001a/uni0000001b /uni0000001b/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (e) η on DD /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000017 /uni0000001a/uni0000001b /uni0000001b/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (f) η on NCI1 Figure 4: In-depth exploration of GLA. (a) contrastive loss with/without negative pairs, (d) performance of different label-invariant augmentation strategies, (b,c,e,f) performance of magnitude of perturbation ηon different datasets under 50% label ratio. Augmentation Space. Different from the most graph contrastive learning methods that directly augment raw graphs, our GLA conducts the augmentation in the representation space, as we believe the raw graphs can be mapped into the representation space, and this space is much easier to augment than the original graph space. In Eq. (5), we design our representation augmentation with a random unit vector scaled by the magnitude of the perturbation η. Figure 4(b,c,e,f) show the performance of our GLA with different values of η on four datasets, where we provide GraphCL and GraphCL+Label-Invariant as references. GraphCL+Label-Invariant takes the augmented graph from GraphCL and ﬁlters the augmented samples that violate the label-invariant property by the downstream classiﬁer. Comparing the two references, we can see that the label-invariant property beneﬁts not only our GLA but also other contrastive methods in most cases. For our GLA, although the ηvalues corresponding to the best performance vary on different datasets, the default setting with η= 1 delivers satisfying performance in general, which outperforms GraphCL+Label-Invariant and indicates the superior of the representation augmentation over the raw graph augmentation. Augmentation Strategy. In the representation space, there might exist multiple qualiﬁed candidates that obey the label-invariant property. Our GLA chooses the most difﬁcult augmentation for the model. Here we demonstrate the performance of different augmentation strategies among qualiﬁed candidates, including the most difﬁcult augmentation, random augmentation, and the easiest augmentation in Figure 4(d), where the random augmentation can be regarded as GraphCL+Label-Invariant. We can see that the most difﬁcult augmentation increases the model generalization and indeed brings in signiﬁcant improvements over the other two ways. This also provides good support for our representation augmentation, where we can ﬁnd the most difﬁcult augmentation in the representation space, but it is difﬁcult to directly generate the raw graphs that are challenging to the downstream classiﬁer. 5 Conclusion In this paper, we consider the graph contrastive learning problem. Different from the existing methods from the pre-train perspective, we propose a novel Graph Label-invariant Augmentation (GLA) algorithm which integrates the pre-train and ﬁne-tuning phases to conduct the label-invariant augmentation in the representation space by perturbations. Speciﬁcally, GLA ﬁrst checks whether the augmented representation obeys the label-invariant property and chooses the most difﬁcult sample from the qualiﬁed samples. By this means, GLA achieves the contrastive augmentation without generating any raw graphs and also increases the model generalization. Extensive experiments in the semi-supervised setting on eight benchmark graph datasets demonstrate the effectiveness of our GLA. Moreover, we also provide extra experiments to verify our motivation and explore the in-depth factors of GLA in the effect of negative pairs, augmentation space, and strategy. 8Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation References [1] Minmin Chen, Kilian Weinberger, Fei Sha, and Yoshua Bengio. Marginalized denoising auto-encoders for nonlinear representations. In International Conference on Machine Learning, 2014. [2] Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In International Conference on Machine Learning, 2012. [3] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In European Conference on Computer Vision, 2020. [4] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in Neural Information Processing Systems, 33:8765–8775, 2020. [5] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In AAAI Conference on Artiﬁcial Intelligence, 2021. [6] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020. [7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020. [9] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271–21284, 2020. [10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. [11] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823, 2020. [12] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In International Conference on Machine Learning, 2021. [13] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph Representation Learning and Beyond, 2020. [14] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In International Conference on Machine Learning, 2020. [15] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In ACM Web Conference, 2022. [16] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations, 2017. [17] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning, 2019. [18] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018. [19] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [20] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In The World Wide Web Conference, 2019. [21] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph attention network for recommendation. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019. [22] Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Dufﬁeld, Mingyuan Zhou, and Xiaoning Qian. Variational graph recurrent neural networks. Advances in Neural Information Processing Systems, 32, 2019. [23] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Trafﬁc graph convolutional recurrent neural network: A deep learning framework for network-scale trafﬁc learning and forecasting. IEEE Transactions on Intelligent Transportation Systems, 21(11):4883–4894, 2019. 9Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation [24] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In The IEEE International Conference on Computer Vision, 2019. [25] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2021. [26] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016. [27] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. International Conference on Learning Representations, 2(3):4, 2019. [28] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-aware graph contrastive learning. 34:30414–30425, 2021. [29] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2):786–797, 1991. [30] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classiﬁcation. arXiv preprint arXiv:1905.04579, 2019. [31] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [32] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond, 2020. [33] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47–i56, 2005. [34] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of Molecular Biology, 330(4):771–783, 2003. [35] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classiﬁcation. Knowledge and Information Systems, 14(3):347–375, 2008. [36] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015. [37] Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs. In ACM International Conference on Information and Knowledge Management, 2020. [38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 10",
      "references": [
        "Marginalized denoising auto-encoders for nonlinear representations",
        "Marginalized denoising autoencoders for domain adaptation",
        "Contrastive learning for unpaired image-to-image translation",
        "Debiased contrastive learning",
        "Contrastive clustering",
        "Supervised contrastive learning",
        "Improved baselines with momentum contrastive learning",
        "A simple framework for contrastive learning of visual representations",
        "Bootstrap your own latent-a new approach to self-supervised learning",
        "Exploring simple siamese representation learning",
        "Graph contrastive learning with augmentations",
        "Graph contrastive learning automated",
        "Deep Graph Contrastive Representation Learning",
        "Contrastive multi-view representation learning on graphs",
        "Simgrace: A simple framework for graph contrastive learning without data augmentation",
        "Semi-supervised classification with graph convolutional networks",
        "Simplifying graph convolutional networks",
        "Large-scale learnable graph convolutional networks",
        "Graph attention networks",
        "Heterogeneous graph attention network",
        "Kgat: Knowledge graph attention network for recommendation",
        "Variational graph recurrent neural networks",
        "Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting",
        "Deepgcns: Can gcns go as deep as cnns?",
        "Training graph neural networks with 1000 layers",
        "Variational graph auto-encoders",
        "Deep graph infomax",
        "Infogcl: Information-aware graph contrastive learning",
        "Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity",
        "Are powerful graph neural nets necessary? a dissection on graph classification",
        "Tudataset: A collection of benchmark datasets for learning with graphs",
        "Protein function prediction via graph kernels",
        "Distinguishing enzyme structures from non-enzymes without alignments",
        "Comparison of descriptor spaces for chemical compound retrieval and classification",
        "Deep graph kernels",
        "Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs",
        "Adam: A method for stochastic optimization"
      ],
      "meta_data": {
        "arxiv_id": "2205.09802v1",
        "authors": [
          "Han Yue",
          "Chunhui Zhang",
          "Chuxu Zhang",
          "Hongfu Liu"
        ],
        "published_date": "2022-05-19T18:44:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Graph Label-invariant Augmentation (GLA), a contrastive framework for semi-supervised graph classification that (1) performs augmentations directly in representation space instead of modifying the raw graph, (2) enforces a label-invariant constraint by retaining only perturbations predicted to share the original label, and (3) selects the hardest (most confusing) augmented sample to maximize generalization. The method jointly learns representations and the downstream classifier, removing the usual pre-train/finetune split and outperforming six strong baselines on eight benchmark datasets.",
        "methodology": "GLA contains: (a) a GNN encoder (ResGCN backbone) that outputs graph-level vectors; (b) a classifier producing label probabilities; (c) a label-invariant augmentation module that adds random unit-vector perturbations η·d·Δ (d is mean distance to dataset centroid) to each original embedding, keeps only candidates whose predicted label matches the source (ground-truth for labeled graphs, classifier prediction for unlabeled), and chooses the candidate with maximum classification loss; (d) a projection head for contrastive learning. Training minimizes the sum of: (i) a positive-only NT-Xent contrastive loss between original and augmented projections (no negative pairs), and (ii) cross-entropy on labeled graphs. Parameters are optimized jointly (Adam).",
        "experimental_setup": "Datasets: MUTAG, PROTEINS, DD, NCI1 (biochemical) and COLLAB, RDT-B, RDT-M5K, GITHUB (social), all from TUDataset. Graph counts 188–12 725; average nodes 18–509. Semi-supervised protocol: 10-fold cross-validation; within each training fold, 30%, 50%, or 70% of graphs are labeled, remainder unlabeled. Models compared: GAE, Deep Graph Infomax, MVGRL, GraphCL, JOAOv2, SimGRACE, and GLA. All baselines use authors’ code; GLA uses η=1, α=1, batch optimization with Adam. Evaluation metric: classification accuracy (mean±std across folds). Additional ablations assess effect of negative pairs, perturbation magnitude, and augmentation-selection strategy.",
        "limitations": "1) Quality of label-invariant filtering depends on the classifier, so performance declines when the labeled proportion is small (e.g., 30%). 2) Perturbation magnitude η is tuned manually and optimal values vary across datasets. 3) Representation-space perturbations may overlook meaningful structural changes available in raw graph augmentations. 4) Study restricted to graph-level classification on static, modest-sized graphs; scalability to very large or dynamic graphs is untested. 5) Method relies on availability of at least some labels; not directly applicable to fully unsupervised settings.",
        "future_research_directions": "• Develop adaptive or learnable perturbation strategies (e.g., adversarial or gradient-based) to replace random search.\n• Extend label-invariant augmentation to node-level tasks, heterogeneous or dynamic graphs, and larger-scale datasets.\n• Investigate fully unsupervised variants that approximate label invariance without ground-truth labels.\n• Study theoretical guarantees on representation robustness and generalization under label-invariant perturbations.\n• Explore integration with raw graph augmentations to combine structural and representation-space views.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning",
      "full_text": "Published as a conference paper at ICLR 2021 ON DATA-AUGMENTATION AND CONSISTENCY - BASED SEMI -SUPERVISED LEARNING Atin Ghosh & Alexandre H. Thiery Department of Statistics and Applied Probability National University of Singapore atin.ghosh@u.nus.edu a.h.thiery@nus.edu.sg ABSTRACT Recently proposed consistency-based Semi-Supervised Learning (SSL) methods such as the Π-model, temporal ensembling, the mean teacher, or the virtual ad- versarial training, have advanced the state of the art in several SSL tasks. These methods can typically reach performances that are comparable to their fully super- vised counterparts while using only a fraction of labelled examples. Despite these methodological advances, the understanding of these methods is still relatively limited. In this text, we analyse (variations of) the Π-model in settings where analytically tractable results can be obtained. We establish links with Manifold Tangent Classiﬁers and demonstrate that the quality of the perturbations is key to obtaining reasonable SSL performances. Importantly, we propose a simple exten- sion of the Hidden Manifold Model that naturally incorporates data-augmentation schemes and offers a framework for understanding and experimenting with SSL methods. 1 I NTRODUCTION Consider a dataset D= DL ∪DU that is comprised of labelled samples DL = {xi,yi}i∈IL as well as unlabelled samples DU = {xi}i∈IU . Semi-Supervised Learning (SSL) is concerned with the use of both the labelled and unlabeled data for training. In many scenarios, collecting labelled data is difﬁcult or time consuming or expensive so that the amount of labelled data can be relatively small when compared to the amount of unlabelled data. The main challenge of SSL is in the design of methods that can exploit the information contained in the distribution of the unlabelled data (Zhu, 2005; Chapelle et al., 2009). In modern high-dimensional settings that are common to computer vision, signal processing, Natural Language Processing (NLP) or genomics, standard graph/distance based methods (Blum & Chawla, 2001; Zhu & Ghahramani, 2002; Zhu et al., 2003; Belkin et al., 2006; Dunlop et al., 2019) that are successful in low-dimensional scenarios are difﬁcult to implement. Indeed, in high-dimensional spaces, it is often difﬁcult to design sensible notions of distances that can be exploited within these methods. We refer the interested reader to the book-length treatments (Zhu, 2005; Chapelle et al., 2009) for discussion of other approaches. The manifold assumption is the fundamental structural property that is exploited in most modern approaches to SSL: high-dimensional data samples lie in a small neighbourhood of a low-dimensional manifold (Turk & Pentland, 1991; Basri & Jacobs, 2003; Peyré, 2009; Cayton, 2005; Rifai et al., 2011a). In computer vision, the presence of this low-dimensional structure is instrumental to the success of (variational) autoencoder and generative adversarial networks: large datasets of images can often be parametrized by a relatively small number of degrees of freedom. Exploiting the unlabelled data to uncover this low-dimensional structure is crucial to the design of efﬁcient SSL methods. A recent and independent evaluation of several modern methods for SSL can be found in (Oliver et al., 2018). It is found there that consistency-based methods (Bachman et al., 2014; Sajjadi et al., 2016; Laine & Aila, 2016; Tarvainen & Valpola, 2017; Miyato et al., 2018; Luo et al., 2018; Grill et al., 2020), the topic of this paper, achieve state-of-the art performances in many realistic scenarios. 1 arXiv:2101.06967v1  [stat.ML]  18 Jan 2021Published as a conference paper at ICLR 2021 Contributions: consistency-based semi-supervised learning methods have recently been shown to achieve state-of-the-art results. Despite these methodological advances, the understanding of these methods is still relatively limited when compared to the fully-supervised setting (Saxe et al., 2013; Advani & Saxe, 2017; Saxe et al., 2018; Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017). In this article, we do not propose a new SSL method. Instead, we analyse consistency-based methods in settings where analytically tractable results can be obtained, when the data-samples lie in the neighbourhood of well-deﬁned and tractable low-dimensional manifolds, and simple and controlled experiments can be carried out. We establish links with Manifold Tangent Classiﬁers and demonstrate that consistency-based SSL methods are in general more powerful since they can better exploit the local geometry of the data-manifold if efﬁcient data-augmentation/perturbation schemes are used. Furthermore, in section 4.1 we show that the popular Mean Teacher method and the conceptually more simple Π-model approach share the same solutions in the regime when the data-augmentations are small; this conﬁrms often reported claim that the data-augmentation schemes leveraged by the recent SSL, as well as fully unsupervised algorithms, are instrumental to their success. Finally, in section 4.3 we propose an extension of the Hidden Manifold Model (Goldt et al., 2019; Gerace et al., 2020). This generative model allows us to investigate the properties of consistency-based SSL methods, taking into account the data-augmentation process and the underlying low-dimensionality of the data, in a simple and principled manner, and without relying on a speciﬁc dataset. For gaining understanding of SSL, as well as self-supervised learning methods, we believe it to be important to develop a framework that(i) can take into account the geometry of the data(ii) allows the study of the inﬂuence of the quality of the data-augmentation schemes (iii) does not rely on any particular dataset. While the understanding of fully-supervised methods have largely been driven by the analysis of simpliﬁed model architectures (eg. linear and two-layered models, large dimension asymptotic such as the Neural Tangent Kernel), these analytical tools alone are unlikely to be enough to explain the mechanisms responsible for the success of SSL and self-supervised learning methods Chen et al. (2020); Grill et al. (2020), since they do not, and cannot easily be extended to, account for the geometry of the data and data-augmentation schemes. Our proposed framework offers a small step in that direction. 2 C ONSISTENCY -BASED SEMI -SUPERVISED LEARNING For concreteness and clarity of exposition, we focus the discussion on classiﬁcation problems. The arguments described in the remaining of this article can be adapted without any difﬁculty to other situations such as regression or image segmentation. Assume that the samples xi ∈X ⊂ RD can be represented as D-dimensional vectors and that the labels belong to C ≥2 possible classes, yi ∈Y≡{ 1,...,C }. Consider a mapping Fθ : RD →RC parametrized by θ ∈Θ ⊂R|Θ|. This can be a neural network, although that is not necessary. For x∈X, the quantity Fθ(x) can represent probabilistic output of the classiﬁer, or , for example, the pre-softmax activations. Empirical risk minimization consists in minimizing the function LL(θ) = 1 |DL| ∑ i∈IL ℓ(Fθ(xi),yi) for a loss function ℓ: RC ×Y↦→ R. Maximum likelihood estimation corresponds to choosing the loss function as the cross entropy. The optimal parameter θ∈Θ is found by a variant of stochastic gradient descent (Robbins & Monro, 1951) with estimated gradient ∇θ { 1 |BL| ∑ i∈BL ℓ(Fθ(xi),yi) } for a mini-batch BL of labelled samples. Consistency-based SSL algorithms regularize the learning by enforcing that the learned function x↦→Fθ(x) respects local derivative and invariance constraints. For simplicity, assume that the mapping x ↦→Fθ(x) is deterministic, although the use of drop- out (Srivastava et al., 2014) and other sources of stochasticity are popular in practice. The Π- model (Laine & Aila, 2016; Sajjadi et al., 2016) makes use of a stochastic mapping S : X× Ω →X that maps a sample x∈X and a source of randomness ω∈Ω ⊂RdΩ to another sample Sω(x) ∈X. The mapping S describes a stochastic data augmentation process. In computer vision, popular data- augmentation schemes include random translations, rotations, dilatations, croppings, ﬂippings, elastic 2Published as a conference paper at ICLR 2021 deformations, color jittering, addition of speckle noise, and many more domain-speciﬁc variants. In NLP, synonym replacements, insertions and deletions, back-translations are often used although it is often more difﬁcult to implement these data-augmentation strategies. In a purely supervised setting, data-augmentation can be used as a regularizer. Instead of directly minimizing LL, one can minimize instead θ↦→ 1 |DL| ∑ i∈IL Eω[ℓ(Fθ[Sω(xi)],yi)]. In practice, data-augmentation regularization, although a simple strategy, is often crucial to obtaining good generalization properties (Perez & Wang, 2017; Cubuk et al., 2018; Lemley et al., 2017; Park et al., 2019). The idea of regularizing by enforcing robustness to the injection of noise can be traced back at least to (Bishop, 1995). In the Π-model, the data-augmentation mapping S is used to deﬁne a consistency regularization term, R(θ) = 1 |D| ∑ i∈IL∪IU Eω {Fθ[Sω(xi)] −Fθ⋆(xi) 2} . (1) The notation θ⋆ designates a copy of the parameter θ, i.e. θ⋆ = θ, and emphasizes that when differentiating the consistency regularization term θ↦→R(θ), one does not differentiate through θ⋆. In practice, a stochastic estimate of ∇R(θ) is obtained as follows. For a mini-batch Bof samples {xi}i∈B, the current value θ⋆ ∈Θ of the parameter and the current predictions fi ≡Fθ⋆(xi), the quantity ∇ { 1 |B| ∑ i∈B Fθ[Sω(xi)] −fi 2 } is an approximation of ∇R(θ). There are indeed many variants (eg. use of different norms, different manners to inject noise), but the general idea is to force the learned function x↦→Fθ(x) to be locally invariant to the data-augmentation schemeS. Several extensions such as the Mean Teacher (Tarvainen & Valpola, 2017) and the V AT (Miyato et al., 2018) schemes have been recently proposed and have been shown to lead to good results in many SSL tasks. The recently proposed and state-of-the-art BYOL approach Grill et al. (2020) is relying on mechanisms that are very close to the consistency regularization methods discussed on this text. If one recalls the manifold assumption, this approach is natural: since the samples corresponding to different classes lie on separate manifolds, the function Fθ : X→ RC should be constant on each one of these manifolds. Since the correct value of Fθ is typically well approximated or known for labelled samples (xi,yi) ∈DL, the consistency regularization term equation 1 helps propagating these known values across these manifolds. This mechanism is indeed similar to standard SSL graph-based approaches such as label propagation (Zhu & Ghahramani, 2002). Graph-based methods are difﬁcult to directly implement in computer vision, or NLP, when a meaningful notion of distance is not available. This interpretation reveals that it is crucial to include the labelled samples in the regularization term equation 1 in order to help propagating the information contained in the labelled samples to the unlabelled samples. Our numerical experiments suggest that, in the standard setting when the number of labelled samples is much lower than the number of unlabeled samples, i.e. |DL|≪|D U|, the formulation equation 1 of the consistency regularization leads to sub-optimal results and convergence issues: the information contained in the labelled data is swamped by the number of unlabelled samples. In all our experiments, we have adopted instead the following regularization term R(θ) = 1 |DL| ∑ i∈IL Eω {Fθ[Sω(xi)] −Fθ⋆(xi) 2} + 1 |DU| ∑ j∈IU Eω {Fθ[Sω(xj)] −Fθ⋆(xj) 2} (2) that balances the labelled and unlabelled data samples more efﬁciently. Furthermore, it is clear that the quality and variety of the data-augmentation scheme S : X× Ω →X is pivotal to the success of consistency-based SSL methods. We argue in this article that it is the dominant factor contributing 3Published as a conference paper at ICLR 2021 to the success of this class of methods. Effort spent on building efﬁcient local data-augmentation schemes will be rewarded in terms of generalization performances. Designing good data-augmentation schemes is an efﬁcient manner of injecting expert/prior knowledge into the learning process. It is done by leveraging the understanding of the local geometry of the data manifold. As usual and not surprisingly (Niyogi et al., 1998; Montavon et al., 2012), in data-scarce settings, any type of domain-knowledge needs to be exploited and we argue that consistency regularization approaches to SSL are instances of this general principle. 3 A PPROXIMATE MANIFOLD TANGENT CLASSIFIER It has long been known (Simard et al., 1998) that exploiting the knowledge of derivatives, or more generally enforcing local invariance properties, can greatly enhance the performance of standard classiﬁers/regressors (Haasdonk & Keysers, 2002; Chapelle & Schölkopf, 2002). In the context of deep-learning, the Manifold Tangent Classiﬁer (Rifai et al., 2011a) is yet another illustration of this idea. Consider the data manifold M⊂X ⊂ RD and assume that the data samples lie on a neighbourhood of it. For x∈M, consider as well the tangent plane Tx to Mat x. Assuming that the manifold Mis of dimension 1 ≤d≤D, the tangent plane Tx is also of dimension dwith an orthonormal basis ex 1,..., ex d ∈RD. This informally means that, for suitably small coefﬁcients ω1,...,ω d ∈R, the transformed sample x∈X deﬁned as x = x+ d∑ j=1 ωjex j also lies, or is very close to, the data manifold M. A possible stochastic data-augmentation scheme can therefore be deﬁned as Sω(x) = x+ Vω where Vω = ∑d j=1 ωjex j. If ω is a multivariate d- dimensional centred Gaussian random vector with suitably small covariance matrix, the perturbation vector Vω is also centred and normally distributed. To enforce that the function x→Fθ(x) is locally approximately constant along the manifold M, one can thus penalize the derivatives of Fθ at xin the directions Vω. Denoting by Jx ∈RC,D the Jacobian with respect to x∈RD of Fθ at x∈M, this can be implemented by adding a penalization term of the type Eω[∥JxVω∥2] = Tr ( Γ ⊗JT xJx ) , where Γ ∈RD,D is the covariance matrix of the random vector ω→Vω. This type of regularization of the Jacobian along the data-manifold is for example used in (Belkin et al., 2006). More generally, if one assumes that for any x,ω ∈X× Ω we have Sεω(x) = x+ εD(x,ω) + O(ε2), for some derivative mapping D : X× Ω →X, it follows that lim ε→0 1 ε2 Eω [ ∥Fθ[Sεω(x)] −Fθ(x)∥2] = Eω [ ∥JxD(x,ω)∥2] = Tr ( Γx,S ⊗JT x Jx ) where Γx,S is the covariance matrix of the X-valued random vector ω↦→D(x,ω) ∈X. This shows that consistency-based methods can be understood as approximated Jacobian regularization methods, as proposed in (Simard et al., 1998; Rifai et al., 2011a). 3.1 L IMITATIONS In practice, even if many local dimension reduction techniques have been proposed, it is still relatively difﬁcult to obtain a good parametrization of the data manifold. The Manifold Tangent Classiﬁer (MTC) (Rifai et al., 2011a) implements this idea by ﬁrst extracting in an unsupervised manner a good representation of the dataset Dby using a Contractive-Auto-Encoder (CAE) (Rifai et al., 2011b). This CAE can subsequently be leveraged to obtain an approximate basis of each tangent plane Txi for xi ∈D, which can then be used for penalizing the Jacobian of the mapping x↦→Fθ(x) in the direction of the tangent plane to Mat x. The above discussion shows that the somewhat simplistic approach consisting in adding an isotropic Gaussian noise to the data samples is unlikely to deliver satisfying results. It is equivalent to penalizing the Frobenius norm ∥Jx∥2 F of the Jacobian of the mapping x↦→Fθ(x); in a linear model, that is equivalent to the standard ridge regularization. This mechanism does not take at all into account the local-geometry of the data-manifold. Nevertheless, in medical imaging applications where scans are often contaminated by speckle noise, this class of approaches which can be thought off as adding artiﬁcial speckle noise, can help mitigate over- ﬁtting (Devalla et al., 2018). 4Published as a conference paper at ICLR 2021   cop I f µ labelledsamplesoooo unlabeledsamples localdataaugmentation Figure 1: Left: Jacobian (i.e. ﬁrst order) Penalization method are short-sighted and do not exploit fully the data-manifold Right: Data-Augmentation respecting the geometry of the data-manifold. There are many situations where, because of data scarcity or the sheer difﬁculty of unsupervised representation learning in general, domain-speciﬁc data-augmentation schemes lead to much better regularization than Jacobian penalization. Furthermore, as schematically illustrated in Figure 1, Jacobian penalization techniques are not efﬁcient at learning highly non-linear manifolds that are common, for example, in computer vision. For example, in “pixel space\", a simple image translation is a highly non-linear transformation only well approximated by a ﬁrst order approximation for very small translations. In other words, if x∈X represents an image and g(x,v) is its translated version by a vector v, the approximation g(x,v) ≈x+∇vg(x), with ∇vg(x) ≡limε→0 (g(x,εv )−g(x)/ε, becomes poor as soon as the translation vector vis not extremely small. In computer vision, translations, rotations and dilatations are often used as sole data-augmentation schemes: this leads to a poor local exploration of the data-manifold since this type transformations only generate a very low dimensional exploration manifold. More precisely, the exploration manifold emanating from a sample x0 ∈X, i.e. {S(x0,ω) : ω∈Ω}, is very low dimensional: its dimension is much lower than the dimension dof the data-manifold M. Enriching the set of data-augmentation degrees of freedom with transformations such as elastic deformation or non-linear pixel intensity shifts is crucial to obtaining a high-dimensional local exploration manifold that can help propagating the information on the data-manifold efﬁciently (Cubuk et al., 2019a; Park et al., 2019). 4 A SYMPTOTIC PROPERTIES 4.1 F LUID LIMIT Consider the standard Π-model trained with a standard Stochastic Gradient Descent (SGD). Denote by θt ∈Θ the current value of the parameter and η >0 the learning rate. We have θk+1 = θk −η∇θ { 1 |BL| ∑ i∈BL ℓ( Fθk (xi), yi ) + λ |BL| ∑ j∈BL Fθk (Sω[xj]) −fj  2 + λ |BU| ∑ k∈BU Fθk (Sω[xk]) −fk  2} (3) for a parameter λ> 0 that controls the trade-off between supervised and consistency losses, as well as subsets BL and BU of labelled and unlabelled data samples, and fj ≡Fθ⋆(xj) for θ⋆ ≡θk as discussed in Section 2. The right-hand-side is an unbiased estimate of η∇θ [ LL(θk) +λR(θk) ] with variance of order O(η2), where the regularization term R(θk) is described in equation 2. It follows from standard ﬂuid limit approximations (Ethier & Kurtz, 2009)[Section 4.8] for Markov processes that, under mild regularity and growth assumptions and as η→0, the appropriately time-rescaled trajectory {θk}k≥0 can be approximated by the trajectory of the Ordinary Differential Equation (ODE). 5Published as a conference paper at ICLR 2021 Proposition 4.1 Let D([0,T],R|Θ|) be the usual space of càdlàg R|Θ|-valued functions on a bounded time interval [0,T] endowed with the standard Skorohod topology. Consider the update equation 3 with learning rate η >0 and deﬁne the continuous time process θ η (t) = θ[t/η]. The sequence of processes θ η ∈D([0,T],R|Θ|) converges weakly in D([0,T],R|Θ|) and as η →0 to the solution of the ordinary differential equation ˙θt = −∇ ( L(θt) + λR(θt) ) . (4) The article (Tarvainen & Valpola, 2017) proposes themean teacher model, an averaging approach related to the standard Polyak-Ruppert averaging scheme (Polyak, 1990; Polyak & Juditsky, 1992), which modiﬁes the consistency regularization term equation 2 by replacing the parameter θ⋆ by an exponential moving average (EMA). In practical terms, this simply means that, instead of deﬁning fj = Fθ⋆(xj), with θ⋆ = θk in equation 3, one sets fj = Fθavg,k (xj) where the EMA process {θavg,k}k≥0 is deﬁned through the recursion θavg,k = (1−αη) θavg,k−1 +αηθ k where the coefﬁcient α >0 controls the time-scale of the averaging process. The use of the EMA process {θavg,k}k≥0 helps smoothing out the stochasticity of the process θk. Similarly to Proposition 4.1, as η→0, the joint process (θ η t,θ η avg,t) ≡(θη [t/η],θη avg,[t/η]) converges as η →0 to the solution of the following ordinary differential equation   ˙θt = −∇ ( L(θt) + λR(θt,θavg,t) ) ˙θavg,t = −α(θavg,t −θt) (5) where the notation R(θt,θavg,t) designates the same quantity as the one described in equation 2, but with an emphasis on the dependency on the EMA process. At convergence (θt,θavg,t) → (θ∞,θavg,∞), one must necessarily have that θ∞= θavg,∞, conﬁrming that, in the regime of small learning rate η→0, the Mean Teachermethod converges, albeit often more rapidly, towards the same solution as the more standard Π-model. This indicates that the improved performances of the Mean Teacher approach sometimes reported in the literature are either not statistically meaningful, or due to poorly executed comparisons, or due to mechanisms not captured by the η→0 asymptotic. Indeed, several recently proposed consistency based SSL algorithms (Berthelot et al., 2019; Sohn et al., 2020; Xie et al., 2019) achieve state-of-the-art performance across diverse datasets without employing any exponential averaging processes. These results are achieved by leveraging more sophisticated data augmentation schemes such as Rand-Augment (Cubuk et al., 2019b) , Back Translation (Artetxe et al., 2017) or Mixup (Zhang et al., 2017). 4.2 M INIMIZERS ARE HARMONIC FUNCTIONS To understand better the properties of the solutions, we consider a simpliﬁed setting further exploited in Section 4.3. Assume that F: X≡ RD →R and Y≡ R and that, for every yi ∈Y≡ R, the loss function f ↦→ℓ(f,yi) is uniquely minimized at f = yi. We further assume that the data-manifold M⊂ RD can be globally parametrized by a smooth and bijective mapping Φ : Rd →M⊂ RD. Similarly to the Section 2, we consider a data-augmentation scheme that can be described as Sεω(x) = Φ(z+ εω) for z = Φ−1(x) and a sample ωfrom a Rd-valued centred and isotropic Gaussian distribution. We consider a ﬁnite set of labelled samples {xi,yi}i∈IL, with xi = Φ(zi) and zi ∈Rd for i ∈IL. We choose to model the large number of unlabelled data samples as a continuum distributed on the data manifold Mas the push-forward measure Φ♯µ(dz) of a probability distribution µ(dz) whose support is Rd through the mapping Φ. This means that an empirical average of the type (1/|DU|) ∑ i∈Iu ϕ(xi) can be replaced by ∫ ϕ[Φ(z)] µ(dz). We investigate the regime ε→0 and, similarly to Section 2, the minimization of the consistency-regularized objective LL(θ) + λ ε2 ∫ Rd Eω {Fθ[Sεω(Φ(z))] −Fθ(Φ(z)) 2} µ(dz). (6) For notational convenience, set fθ ≡ Fθ ◦Φ. Since Sεω[Φ(z)] = Φ( z + εω), as ε → 0 the quantity 1 ε2 Eω {Fθ[Sεω(Φ(z))] −Fθ(Φ(z)) 2} converges to ∥∇zfθ∥2 and the objective function equation 6 approaches the quantity G(fθ) ≡ 1 |DL| ∑ i∈IL ℓ(fθ(zi),yi) + λ ∫ Rd ∥∇zfθ(z)∥2 µ(dz). (7) 6Published as a conference paper at ICLR 2021 A minimizer f : Rd →R of the functional G that is consistent with the labelled data, i.e. f(zi) = yi for i ∈IL, is a minimizer of the energy functional f ↦→ ∫ Rd ∥∇zfθ(z)∥2 µ(dz) subject to the constraints f(zi) = yi. It is the variational formulation of the Poisson equation {∆f(z) = 0 for z∈Rd \\{zi}i∈IL f(zi) = yi for i∈IL. (8) Note that the solution does not depend on the regularization parameter λin the regime of ε →0: this indicates, as will be discussed in Section 4.3 in detail, that the generalization properties of consistency-based SSL methods will typically be insensitive to this parameter, in the regime of small data-augmentation at least. Furthermore, equation 8 shows that consistency-based SSL methods are indeed based on the same principles as more standard graph-based approaches such as Label Propagation (Zhu & Ghahramani, 2002): solutions are gradient/Laplacian penalized interpolating functions. In Figure 2, we consider the case where D= d= 2 with trivial mapping Φ(x) = x. We consider labelled data situated on the right (resp. left) boundary of the unit square and corresponding to the label y = 0 (resp. y = 1). For simplicity, we choose the loss function ℓ(f,y) = 1 2 (f −y)2 and parametrize Fθ ≡fθ with a neural network with a single hidden layer with N = 100 neurons. As expected, the Π-model converges to the solution to the Poisson equation 8 in the unit square with boundary condition f(u,v) = 0 for u= 0 and f(u,v) = 1 for u= 1. Figure 2: Labelled data samples with class y= 0 (green triangle) and y= +1 (red dot) are placed on the Left/Right boundary of the unit square. Unlabelled data samples (blue stars) are uniformly placed within the unit square. We consider a simple regression setting with loss functionℓ(f,y) = 1 2 (f−y)2. Left: Randomly initialized neural network. Middle: labelled/unlabelled data Right: Solution of f obtained by training a standard Π-model. It is the harmonic function f(u,v) = u, as described by equation 8. 4.3 G ENERATIVE MODEL FOR SEMI -SUPERVISED LEARNING As has been made clear throughout this text, SSL methods crucially rely on the dependence structure of the data. The existence and exploitation of a much lower-dimensional manifold Msupporting the data-samples is instrumental to this class of methods. Furthermore, the performance of consistency- based SSL approaches is intimately related to the data-augmentation schemes they are based upon. Consequently, in order to understand the mechanisms that are at play when consistency-based SSL methods are used to uncover the structures present in real datasets, it is important to build simpliﬁed and tractable generative models of data that(1) respect these low-dimensional structures and(2) allow the design of efﬁcient data-augmentation schemes. Several articles have investigated the inﬂuence of the dependence structures that are present in the data on the learning algorithm (Bruna & Mallat, 2013; Mossel, 2016). Here, we follow the Hidden Manifold Model (HMM) framework proposed in (Goldt et al., 2019; Gerace et al., 2020) where the authors describe a model of synthetic data concentrating near low-dimensional structures and analyze the learning curve associated to a class of two-layered neural networks. Low-dimensional structure: Similarly to Section 4.2, assume that the D-dimensional data-samples xi ∈X can be expressed as xi = Φ(zi) ∈RD for a ﬁxed smooth mapping Φ : Rd →RD. In other words, the data-manifold Mis d-dimensional and the mapping Φ can be used to parametrize it. The mapping Φ is chosen to be a neural network with a single hidden layer with H neurons, although 7Published as a conference paper at ICLR 2021 0 20 40 60 80 100 Epoch 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Test NLL = 1.0 = 10.0 = 100.0 Unregularized 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.3 0.4 0.5 0.6Test NLL = 0.03 = 0.10 = 0.30 = 1.00 Figure 3: Left: For a ﬁxed data-augmentation scheme, generalization properties for λspanning two orders of magnitude. Right: Inﬂuence of the quantity of the data-augmentation of the generalization properties. other choices are indeed possible. For z = (z1,...,z d) ∈Rd, set Φ(z) = A1→2 ϕ(A0→1z+ b1) for matrices A0→1 ∈RH,d and A1→2 ∈RD,H, bias vector b1 ∈RH and non-linearity ϕ: R →R applied element-wise. In all our experiments, we use the ELU non-linearity. We adopt the standard normalization A0→1 i,j = w(1) i,j/ √ dand A1→2 i,j = w(2) i,j/ √ Hfor weights w(k) i,j drawn i.i.d from a centred Gaussian distribution with unit variance; this ensures that, if the coordinate of the input vectorz∈Rd are all of order O(1), so are the coordinates of x= Φ(z). Data-augmentation: consider a data sample xi ∈M on the data-manifold. It can also be ex- pressed as xi = Φ(zi). We consider the natural data-augmentation process which consists in setting Sεω(xi) = Φ( zi + εω) for a sample ω ∈Rd from an isotropic Gaussian distribution with unit covariance and ε> 0. Crucially, the data-augmentation scheme respect the low-dimensional structure of the data: the perturbed sample Sεω(xi) belongs to the data-manifold Mfor any perturbation vector εω. Note that, for any value of ε, the data-augmentation preserves the low-dimensional manifold: perturbed samples Sεω(xi) exactly lie on the data-manifold. The larger ε, the more efﬁcient the data-augmentation scheme; this property is important since it allows to study the inﬂuence of the amount of data-augmentation. Classiﬁcation: we consider a balanced binary classiﬁcation problem with |DL|≥ 2 labelled training examples {xi,yi}i∈IL where xi = Φ(zi) and yi ∈Y ≡{−1,+1}. The sample zi ∈Rd corre- sponding to the positive (resp. negative) class are assumed to have been drawn i.i.d from a Gaussian distribution with identity covariance matrix and mean µ+ ∈Rd (resp. mean µ−∈Rd). The distance ∥µ+ −µ−∥quantiﬁes the hardness of the classiﬁcation task. Neural architecture and optimization: Consider ﬁtting a two-layered neural network Fθ : RD → R by minimising the negative log-likelihood LL(θ) ≡(1/|DL|) ∑ iℓ[Fθ(xi),yi] where ℓ(f,y) = log(1 + exp[−yf]). We assume that there are |DL|= 10 labelled data pairs {xi,yi}i=IL, as well as |DU|= 1000 unlabelled data samples, that the ambient space has dimension D= 100 and the data manifold Mhas dimension d= 10. The function Φ uses H = 30 neurons in its hidden layer. In all our experiments, we use a standard Stochastic Gradient Descent (SGD) method with constant learning rate and momentum β = 0.9. For minimizing the consistency-based SSL objective LL(θ) + λR(θ), with regularization R(θ) given in equation 2, we use the standard strategy (Tarvainen & Valpola, 2017) consisting in ﬁrst minimizing the un-regularized objective alone LL for a few epochs in order for the function Fθ to be learned in the neighbourhood of the few labelled data-samples before switching on the consistency-based regularization whose role is to propagate the information contained in the labelled samples along the data manifold M. Insensitivity to λ: Figure 3 (Left) shows that this method is relatively insensitive to the parameter λ, as long as it is within reasonable bounds. This phenomenon can be read from equation 8 that does not depend on λ. Much larger or smaller values (not shown in Figure 3) of λdo lead, unsurprisingly, to convergence and stability issues. Amount of Data-Augmentation: As is reported in many tasks Cubuk et al. (2018); Zoph et al. (2019); Kostrikov et al. (2020), tuning the amount data-augmentation in deep-learning applications is often a delicate exercise that can greatly inﬂuence the resulting performances. Figure 3 (Right) 8Published as a conference paper at ICLR 2021 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.4 0.6 0.8 1.0Test NLL k=5 k=6 k=7 k=8 k=9 k=10 5 6 7 8 9 10 k: Data Augmentation Dimension 0.2 0.4 0.6 0.8 1.0Test NLL Generalization at Epoch 200 Figure 4: Learning curve test (NLL) of the Π-model with λ = 10 for different “quality\" of data- augmentation. The data manifold is of dimension d = 10 in an ambient space of dimension D = 100 . For xi = Φ( zi) and 1 ≤k ≤d, the data-augmentation scheme is implemented as Sεω[k](xi) = Φ(zi + εω[k]) where ω[k] is a sample from a Gaussian distribution whose last (d−k) coordinates are zero. In other words, the data-augmentation scheme only explores kdimensions out of the ddimensions of the data-manifold. We use ε = 0.3 in all the experiments. Left: Learning curves (Test NLL) for data-augmentation dimension k∈[5,10] Right: Test NLL at epoch N = 200 (see left plot) for data-augmentation dimension k∈[5,10]. reports the generalization properties of the method for different amount of data-augmentation. Too low an amount of data-augmentation (i.e. ε= 0.03) and the ﬁnal performance is equivalent to the un-regularized method. Too large an amount of data-augmentation (i.e. ε= 1.0) also leads to poor generalization properties. This is because the choice of ε= 1.0 corresponds to augmented samples that are very different from the distribution of the training dataset (i.e. distributional shift), although these samples are still supported by the data-manifold. 0 50 100 150 200 250 300 Epochs 0.3 0.4 0.5 0.6 0.7 0.8Test NLL MT: MT=0.900 MT: MT=0.950 MT: MT=0.990 MT: MT=0.995 -model Figure 5: Mean-Teacher (MT) learning curves (Test NLL) for different values of the exponential smoothing parameter βMT ∈(0,1). For βMT ∈{0.9,0.95,0.99,0.995}, the ﬁnal test NLL obtained through the MT approach is identical to the test NLL obtained through the Π-model. In all the experiments, we used λ= 10 and used SGD with momentum β = 0.9. Quality of the Data-Augmentation: to study the inﬂuence of the quality of the data-augmentation scheme, we consider a perturbation process implemented asSεω[k](xi) = Φ(zi+ω[k]) for xi = Φ(zi) where the noise term ω[k] is deﬁned as follows. For a data-augmentation dimension parameter 1 ≤k≤dwe have ω[k] = (ξ1,...,ξ k,0,..., 0) for i.i.d standard Gaussian samples ξ1,...,ξ k ∈R. This data-augmentation scheme only explores the ﬁrst k dimensions of the d-dimensional data- manifold: the lower k, the poorer the exploration of the data-manifold. As demonstrated on Figure 4, lower quality data-augmentation schemes (i.e. lower values of k∈[0,d]) hurt the generalization performance of the Π-model. Mean-Teacher versus Π-model: we implemented the Mean-Teacher (MT) approach with an expo- nential moving average (EMA) process θavg,k = βMT θavg,k−1 + (1 −βMT) θk for the MT parameter θavg,k with different scales βMT ∈{0.9,0.95,0.99,0.995}, as well as a Π-model approach, with 9Published as a conference paper at ICLR 2021 λ= 10 and ε= 0.3. Figure 5 shows, in accordance with Section 4.1, that the different EMA schemes lead to generalization performances similar to a standard Π-model. 5 C ONCLUSION Consistency-based SSL methods rely on a subtle trade-off between the exploitation of the labelled samples and the discovery of the low-dimensional data-manifold. The results presented in this article highlight the connections with more standard methods such as Jacobian penalization and graph- based approaches and emphasize the crucial role of the data-augmentation scheme. The analysis of consistency-based SSL methods is still in its infancy and our numerical simulations suggest that the variant of the Hidden Manifold Model described in this text is a natural framework to make progress in this direction. REFERENCES Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017. Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017. Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems, pp. 3365–3373, 2014. Ronen Basri and David W Jacobs. Lambertian reﬂectance and linear subspaces. IEEE Transactions on Pattern Analysis & Machine Intelligence, (2):218–233, 2003. Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame- work for learning from labeled and unlabeled examples. Journal of machine learning research, 7 (Nov):2399–2434, 2006. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 5050–5060, 2019. Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation, 7 (1):108–116, 1995. Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the Eighteenth International Conference on Machine Learning, pp. 19–26. Morgan Kaufmann Publishers Inc., 2001. Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence, 35(8):1872–1886, 2013. Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, 12 (1-17):1, 2005. Olivier Chapelle and Bernhard Schölkopf. Incorporating invariances in non-linear support vector machines. In Advances in neural information processing systems, pp. 609–616, 2002. Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542–542, 2009. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113–123, 2019a. 10Published as a conference paper at ICLR 2021 Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no separate search. arXiv preprint arXiv:1909.13719, 2019b. Sripad Krishna Devalla, Prajwal K Renukanand, Bharathwaj K Sreedhar, Giridhar Subramanian, Liang Zhang, Shamira Perera, Jean-Martial Mari, Khai Sing Chin, Tin A Tun, Nicholas G Strouthidis, et al. Drunet: a dilated-residual U-net deep learning network to segment optic nerve head tissues in optical coherence tomography images. Biomedical optics express, 9(7): 3244–3265, 2018. Matthew M Dunlop, Dejan Slepˇcev, Andrew M Stuart, and Matthew Thorpe. Large data and zero noise limits of graph-based semi-supervised learning algorithms. Applied and Computational Harmonic Analysis, 2019. Stewart N Ethier and Thomas G Kurtz. Markov processes: characterization and convergence, volume 282. John Wiley & Sons, 2009. Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborová. Gener- alisation error in learning with random features and the hidden manifold model. arXiv preprint arXiv:2002.09339, 2020. Sebastian Goldt, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. Modelling the inﬂuence of data structure on learning in neural networks. arXiv preprint arXiv:1909.11500, 2019. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. Bernard Haasdonk and Daniel Keysers. Tangent distance kernels for support vector machines. In Object recognition supported by user interaction for service robots, volume 2, pp. 864–868. IEEE, 2002. Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8896–8905, 2018. Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018. Grégoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe, Alexandre Tkatchenko, Anatole V Lilienfeld, and Klaus-Robert Müller. Learning invariant representations of molecules for atomization energy prediction. In Advances in Neural Information Processing Systems, pp. 440–448, 2012. Elchanan Mossel. Deep learning and hierarchal generative models. arXiv preprint arXiv:1612.09057, 2016. Partha Niyogi, Federico Girosi, and Tomaso Poggio. Incorporating prior information in machine learning by creating virtual examples. Proceedings of the IEEE, 86(11):2196–2209, 1998. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems, pp. 3235–3246, 2018. 11Published as a conference paper at ICLR 2021 Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Luis Perez and Jason Wang. The effectiveness of data augmentation in image classiﬁcation using deep learning. arXiv preprint arXiv:1712.04621, 2017. Gabriel Peyré. Manifold models for signals and images. Computer Vision and Image Understanding, 113(2):249–260, 2009. Boris T Polyak. New stochastic approximation type procedures. Automat. i Telemekh, 7(98-107):2, 1990. Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.SIAM journal on control and optimization, 30(4):838–855, 1992. Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011a. Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pp. 833–840. Omnipress, 2011b. Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp. 400–407, 1951. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma- tions and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163–1171, 2016. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren- dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. 2018. Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017. Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of the trade, pp. 239–274. Springer, 1998. Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. arXiv preprint arXiv:2001.07685, 2020. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis- tency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195–1204, 2017. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pp. 1–5. IEEE, 2015. Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3 (1):71–86, 1991. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. 2019. 12Published as a conference paper at ICLR 2021 Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propaga- tion. Technical report, Citeseer, 2002. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912–919, 2003. Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2005. Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data augmentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019. 13",
      "references": [
        "High-dimensional dynamics of generalization error in neural networks.",
        "Unsupervised neural machine translation.",
        "Learning with pseudo-ensembles.",
        "Lambertian reﬂectance and linear subspaces.",
        "Manifold regularization: A geometric frame- work for learning from labeled and unlabeled examples.",
        "Mixmatch: A holistic approach to semi-supervised learning.",
        "Training with noise is equivalent to tikhonov regularization.",
        "Learning from labeled and unlabeled data using graph mincuts.",
        "Invariant scattering convolution networks.",
        "Algorithms for manifold learning.",
        "Incorporating invariances in non-linear support vector machines.",
        "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews].",
        "A simple framework for contrastive learning of visual representations.",
        "Autoaugment: Learning augmentation policies from data.",
        "Autoaugment: Learning augmentation strategies from data.",
        "Randaugment: Practical data augmentation with no separate search.",
        "Drunet: a dilated-residual U-net deep learning network to segment optic nerve head tissues in optical coherence tomography images.",
        "Large data and zero noise limits of graph-based semi-supervised learning algorithms.",
        "Markov processes: characterization and convergence, volume 282.",
        "Gener- alisation error in learning with random features and the hidden manifold model.",
        "Modelling the inﬂuence of data structure on learning in neural networks.",
        "Bootstrap your own latent: A new approach to self-supervised learning.",
        "Tangent distance kernels for support vector machines.",
        "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.",
        "Temporal ensembling for semi-supervised learning.",
        "Smart augmentation learning an optimal data augmentation strategy.",
        "Smooth neighbors on teacher graphs for semi-supervised learning.",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning.",
        "Learning invariant representations of molecules for atomization energy prediction.",
        "Deep learning and hierarchal generative models.",
        "Incorporating prior information in machine learning by creating virtual examples.",
        "Realistic evaluation of deep semi-supervised learning algorithms.",
        "Specaugment: A simple data augmentation method for automatic speech recognition.",
        "The effectiveness of data augmentation in image classiﬁcation using deep learning.",
        "Manifold models for signals and images.",
        "New stochastic approximation type procedures.",
        "Acceleration of stochastic approximation by averaging.",
        "The manifold tangent classiﬁer.",
        "Contractive auto-encoders: Explicit invariance during feature extraction.",
        "A stochastic approximation method.",
        "Regularization with stochastic transforma- tions and perturbations for deep semi-supervised learning.",
        "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.",
        "On the information bottleneck theory of deep learning.",
        "Opening the black box of deep neural networks via information.",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.",
        "Deep learning and the information bottleneck principle.",
        "Eigenfaces for recognition.",
        "Unsupervised data augmentation for consistency training.",
        "mixup: Beyond empirical risk minimization.",
        "Learning from labeled and unlabeled data with label propagation.",
        "Semi-supervised learning using gaussian ﬁelds and harmonic functions.",
        "Semi-supervised learning literature survey.",
        "Learning data augmentation strategies for object detection."
      ],
      "meta_data": {
        "arxiv_id": "2101.06967v1",
        "authors": [
          "Atin Ghosh",
          "Alexandre H. Thiery"
        ],
        "published_date": "2021-01-18T10:12:31Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Provides a principled analysis of consistency-based semi-supervised learning (SSL) methods, particularly the Π-model and Mean Teacher, in settings where closed-form results are attainable; establishes connections with Manifold Tangent Classifiers, proves that Mean Teacher converges to the same solution as the Π-model when augmentations are small, demonstrates that data-augmentation quality dominates performance, and introduces an extension of the Hidden Manifold Model that explicitly incorporates data augmentation to create a dataset-agnostic test-bed for studying SSL.",
        "methodology": "Combines theoretical derivations and controlled synthetic experiments: (1) rewrites consistency loss as approximate Jacobian regularisation linking it to tangent-space methods, (2) uses fluid-limit (ODE) analysis of SGD to show equivalence between Π-model and Mean Teacher, (3) derives harmonic-function solution for the small-augmentation limit, and (4) designs a generative Hidden Manifold Model where data lie on a low-dimensional manifold parameterised by a two-layer network and are perturbed by Gaussian noise along manifold directions; experiments vary augmentation magnitude, dimensionality, and regularisation weight.",
        "experimental_setup": "Experiments are performed on synthetic data produced by the Hidden Manifold Model: ambient dimension D=100, manifold dimension d=10, 10 labelled and 1,000 unlabelled samples generated from two Gaussian class centres; data augmentation realised as ε-scaled Gaussian noise along k≤d manifold directions; classification performed with a two-layer neural network trained with SGD (momentum 0.9), evaluating negative log-likelihood on a held-out test set; additional 2-D toy example (unit square) illustrates convergence to harmonic function; hyper-parameters (λ, ε, augmentation dimension k, EMA decay βMT) are systematically varied.",
        "limitations": "Findings rely on simplified settings: small synthetic datasets, low-capacity networks, Gaussian manifolds, and small perturbation assumptions; no evaluation on real benchmarks (e.g., CIFAR, ImageNet), so practical relevance is unverified; analysis covers only Π-model and Mean Teacher, excluding stronger modern SSL algorithms; Hidden Manifold Model may not capture complex real-world data geometry or augmentation artefacts; conclusions about λ-insensitivity and augmentation dominance might not generalise to large-scale tasks.",
        "future_research_directions": "1) Apply the proposed framework to more realistic generative models and real image or text datasets to validate theoretical insights; 2) Extend analysis to other state-of-the-art SSL/self-supervised methods such as VAT, FixMatch, BYOL, and MixMatch; 3) Develop principled procedures for designing high-quality, geometry-aware data augmentations; 4) Study regimes with large or learned augmentations beyond the small-ε approximation; 5) Explore higher-capacity models and deeper architectures within the Hidden Manifold framework to assess scalability and practical impact.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection",
      "full_text": "Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection Chuandong Liu1,2 , Chenqiang Gao 1,2*, Fangcen Liu 1,2 , Pengcheng Li 1,2 , Deyu Meng 3,4 , Xinbo Gao 1 1School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China 2Chongqing Key Laboratory of Signal and Information Processing, Chongqing, China 3Xi’an Jiaotong University, Xi’an, China 4Macau University of Science and Technology, Taipa, Macau Abstract State-of-the-art 3D object detectors are usually trained on large-scale datasets with high-quality 3D annotations. However, such 3D annotations are often expensive and time-consuming, which may not be practical for real ap- plications. A natural remedy is to adopt semi-supervised learning (SSL) by leveraging a limited amount of labeled samples and abundant unlabeled samples. Current pseudo- labeling-based SSL object detection methods mainly adopt a teacher-student framework, with a single fixed threshold strategy to generate supervision signals, which inevitably brings confused supervision when guiding the student net- work training. Besides, the data augmentation of the point cloud in the typical teacher-student framework is too weak, and only contains basic down sampling and flip-and-shift (i.e., rotate and scaling), which hinders the effective learn- ing of feature information. Hence, we address these is- sues by introducing a novel approach of Hierarchical Su- pervision and Shuffle Data Augmentation (HSSDA), which is a simple yet effective teacher-student framework. The teacher network generates more reasonable supervision for the student network by designing a dynamic dual-threshold strategy. Besides, the shuffle data augmentation strategy is designed to strengthen the feature representation ability of the student network. Extensive experiments show that HSSDA consistently outperforms the recent state-of-the-art methods on different datasets. The code will be released at https://github.com/azhuantou/HSSDA. 1. Introduction Kinds of important applications, especially autonomous driving, have been motivating the rapid development of 3D *Corresponding author. Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene ( a) RGB ImageFull labeled scene (a) (b) Ours Teacher High threshold Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene RGB ImageFull labeled scene (a) (b) Ours Student High threshold Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR Figure 1. Illustration of (a) the previous teacher compared to our teacher framework and (b) the previous student compared to our student framework. The black dashed box includes the RGB im- age and the corresponding fully annotated 3D point cloud (green box). The left side of the yellow dotted line in (a) represents the pseudo-labeling scene generated by the single threshold of the vanilla teacher network, causing the student network may be severely misled due to missing mined objects (high threshold) or false positive objects (low threshold), while our proposed teacher network generates three groups of pseudo labels(shown as green, red, blue) to provide hierarchical supervision for the student net- work. (b) shows our student network adopts stronger shuffled data augmentation than the vanilla student network to learn the stronger ability of feature representation. object detection by the range sensor data (e.g., LiDAR point cloud). Up to now, many point-based and point-voxel-based methods [11,32,33,53] have been proposed. Despite the im- pressive progress, a large amount of accurate instance-level 3D annotations have to be provided for training, which is more time-consuming and expensive than 2D object annota- tion. This hinders the application and deployment of exist- 1 arXiv:2304.01464v1  [cs.CV]  4 Apr 2023ing advanced detection models. To this end, how to reduce the dependence on huge annotated datasets has achieved growing interest in the object detection community. As one of the important machine learning schemes for reducing data annotation, semi-supervised learning (SSL) aims to improve the generalization ability of model train- ing with a small number of labeled data together with large-scale unlabeled samples. In the semi-supervised ob- ject detection community, most works focus on 2D object detection [14, 18, 36, 42, 45], among which the teacher- student model is the mainstream framework. Specifically, the teacher network with weakly augmented labeled data generates pseudo labels to train the student network with strong data augmentation. This pipeline has been widely verified to be effective, in which the quality of pseudo labels and the data augmentation strategy are the key and many works have been proposed to tackle them [2,7,35,36]. Ben- efiting from 2D semi-supervised object detection, several 3D semi-supervised object detection methods have been proposed [25, 41, 49, 55], which still mainly adopted the teacher-student model. These methods, as well as 2D semi- supervised object detection methods [22,36,58], mainly use a hard way, e.g., a score threshold, to get pseudo labels to train the student network. This kind of strategy is difficult to guarantee the quality of pseudo labels. Taking the score threshold strategy as an example, if the threshold is too low, the pseudo labels will contain many false objects, while if it is too high, the pseudo labels will miss many real ob- jects which will be improperly used as background (see in Fig. 1 (a)). As exhibited in [41], only about 30% of the ob- jects can be mined from the unlabeled scenes even at the end of the network training. Thus, both of those two cases will bring the student network confused supervision, which harms the performance of the teacher-student model. This would inevitably happen for the single threshold strategy, even adopting some optimal threshold search method [41]. Thus, how to produce reasonable pseudo labels from the teacher network output is an important issue to address for better training the student networks. Besides the quality of pseudo labels, data augmentation is also the key to the teacher-student model as mentioned previously. Extensive works in 2D semi-supervised object detection have shown that strong data augmentation is very important to learn the strong feature representation ability of the student network. Thus, kinds of strong data aug- mentation strategies, e.g., Mixup [50], Cutout [7], and Mo- saic [4] have been widely adopted. However, current 3D semi-supervised object detection methods adopt some weak data augmentation strategies, e.g., flip-and-shift. These kinds of data augmentations are not able to well drive the student network to learn strong feature representation abil- ity. Thus, the good effect of data augmentation in 2D semi- supervised object detection does not appear obviously in 3D semi-supervised object detection. To tackle the above issues of the quality of pseudo labels and data augmentation, we propose a Hierarchical Super- vision and Shuffle Data Augmentation (HSSDA) method for 3D semi-supervised object detection. We still adopt the teacher-student model as our mainframe. For obtain- ing more reasonable pseudo labels for the student network, we design a dynamic dual-threshold strategy to divide the output of the teacher network into three groups: (1) high- confidence level pseudo labels, (2) ambiguous level pseudo labels, and (3) low-confidence level pseudo labels, as shown in Fig. 1 (a). This division provides hierarchical supervision signals for the student network. Specifically, the first group is used as the strong labels to learn the student network, while the second join learning through a soft-weight way. The higher the score is, the more it affects learning. The third group is much more likely to tend to be false objects. We directly delete them from the point cloud to avoid con- fusing parts of the object point cloud into the background. For strengthening the feature representation ability of the student network, we design a shuffle data augmentation strategy in this paper. As shown in Fig. 1 (b), we first gener- ate shuffled scenes by splitting and shuffling the point cloud patches in BEV (bird-eye view) and use them as inputs to the student model. Next, the feature maps extracted from the detector backbone are unshuffled back to the original point cloud geometry location. To summarize, our contributions are as follows: • We propose a novel hierarchical supervision gener- ation and learning strategy for the teacher-student model. This strategy can provide the student network hierarchical supervision signal, which can fully utilize the output of the teacher network. • We propose a shuffle data augmentation strategy that can strengthen the feature representation ability of the student network. • Our proposed hierarchical supervision strategy and shuffle data augmentation strategy can be directly ap- plied to the off-the-shelf 3D semi-supervised point cloud object detector and extensive experiments demonstrate that our method has achieved state-of-the- art results. 2. Related Work 2.1. 3D Object Detection 3D object detection is a fundamental task in the au- tonomous driving area. The mainstream 3D object detection methods can be roughly divided into three types: voxel- based methods [9, 15, 21, 46, 56, 57], point-based meth- ods [26, 33, 34, 47, 48, 54], and point-voxel-based methods 2[10, 20, 30–32]. For voxel-based methods, voxelization is a common operation that transforms irregular point clouds into voxel grids for applying traditional 2D or 3D convo- lution. In V oxelnet [59], the voxel-wise encoding layer was adopted for collective feature representation extraction from the voxel-wise LiDAR point cloud. V oxSeT [9] presented a novel transformer-based framework that encoded features from a larger receptive field. Point-based approaches di- rectly use the raw point cloud to capture spatial structure information for feature extraction through networks of the PointNet series [27, 28]. PointRCNN [33] is the represen- tative, which directly generates point-level RoIs and uses the point-level features for further refinement. To acceler- ate the inference speed for applications, IA-SSD [52] pro- posed an efficient downsampling way and a contextual cen- troid perception module to capture geometrical structure. Point-V oxel-based methods combined voxel representations with point representations from the point cloud. Built on the PV-RCNN [31], PV-RCNN++ [32] leveraged a Vector- Pool aggregation to learn structure features and a sector- ized proposal-centric keypoints sampling strategy to obtain more keypoints. All the above fully supervised methods can be easily embedded into our HSSDA framework, e.g., PV- RCNN [31] and V oxel-RCNN [6]. 2.2. Semi-supervised Learning (SSL) SSL can greatly reduce the annotations for model train- ing and most existing works focus on image classifica- tion, which can be broadly divided into two types: con- sistency regularization [2, 3, 23, 43] and pseudo-labeling methods [1, 12, 44]. The former approaches assume the model’s predictions to be consistent under input perturba- tions/augmentations (e.g., different contrast, flip, etc.) and penalize the inconsistency of predictions. Techniques range from simple augmentation to more complex transforma- tions such as MixUp [50], as well as stronger automatic augmentation such as Cutout [7] and CTAugment [2]. The latter methods exploit pseudo-labeling, where the model first is trained trains on labeled data and then iteratively generates the pseudo labels of unlabeled data to add highly confident predictions for training. It is revisited in deep neu- ral networks to learn from large amounts of unlabeled data. Notably, perturbation mechanisms of the above two types of methods play a key role in promoting the model robustness against noise in network parameters or structure but have not been explored in SSL for 3D object detection. 2.3. Semi-supervised Object Detection Inspired by the SSL works in image classification, SSL is also applied to the 2D object detection to alleviate the heavy annotation problem [14, 18, 36, 42]. STAC [36] gen- erated pseudo labels for unlabeled data in an offline manner. To further improve the quality of pseudo labels, Instant- Teachering [58] rectified the false predictions via the co- rectify scheme and experimented with MixUp [50] and Mo- saic [4]. This work aims to tackle a more challenging task, SSL for 3D object detection, where large spaces of 7 Degrees-of-Freedom of 3D objects need to be searched. Recently, several works have been proposed in the 3D SSL domain. SESS [55] and 3DIoUMatch [41] are the pioneer approaches for 3D object detection from indoor and outdoor point cloud data. Similar to 2D SSL meth- ods, SESS [55] leveraged a triple consistency regularization strategy to align the 3D proposal from the teacher and stu- dent network. Following the pseudo-labeling line in SSL, 3DIoUMatch [41] designed a series of filtering strategies such as objectness, classification, and localization threshold to obtain high-quality pseudo labels, and a unique IoU esti- mation branch to further deduplicate the predictions. Differ- ent from 3DIoUMatch, Proficient Teachers [49] developed several necessary modules to improve the recall and preci- sion of pseudo labels and removed the necessity of thresh- old setting. DetMatch [25] generated more precise pseudo labels by matching 2D and 3D detection from each modal- ity. These works employed the EMA weight update strat- egy to train a student network and then gradually update the teacher network. Although achieving impressive perfor- mances with high-quality pseudo labels, the missing-mined objects and heuristic strong augmentation are ignored. By contrast, our HSSDA leverages the hierarchical supervision and shuffle data augmentation to alleviate these issues and further improve performance. 3. Method 3.1. Preliminary Problem Definition We first provide the definition of semi-supervised 3D object detection. In detail, the model is trained with a set of labeled scenes Ds = {ps i, ys i}Ns i=1 and a set of unlabeled scenes Du = {pu i }Nu i=1, where pi ∈ Rn×{3+r} represents a point cloud scene pi which has n points with three-dimensional coordinates and additional r- dimensional information (e.g., color, intensity) that can be treated as extra features, Ns and Nu are the numbers of la- beled and unlabeled point cloud scenes, respectively. Gen- erally speaking, Nu >> Ns. For a scene ps i, the annotation ys i is composed of both 7-dimensional location information which includes center, size, and orientation, and category of the 3D bounding boxes. Teacher-Student Framework Similar to the main- stream researches [16, 41, 55], our learning paradigm also builds up on the teacher-student framework which includes two 3D detectors with the same configurations. Here, we can use any off-the-shelf state-of-the-art 3D object detec- tor, e.g., PV-RCNN [31] and V oxel-RCNN [6]. Following those works, we build the teacher detector via exponential 3Weak  Aug. Detection Head Detection Head High-confidence  level supervision Ambiguous level  supervision Weak Aug. Teacher Network Clean  scene  Detector  Backbone  Split & Shuffle Detector  Backbone  Detection Head Student Network Shuffled Features Unshuffled Features  EMA Update Hierarchical supervision  ̂𝑝𝑝𝑖𝑖 𝑢𝑢 Low-confidence level  supervision ~ Unlabeled  scene Augmented scene Pair of predictions Augmented scene  ... Groundtruth Labeled scenes Labeled  scenes Mined  scenes Confident scene set Labeled  scenes Mined  scenes Detector  Backbone  Dynamic dual-threshold generation  Augmented scene set Weak  Aug. Predictions Predictions Predictions c ir Predictions c ir Pair of predictions u ip u ip u ir u ir IoU consistency  constraint Confidence score  constraint Objectness score  constraint Dual-threshold-based  division Position information (,)high low cls clsττ (,)high low obj objττ (,)high low iou iouττClass 1: Class 2: (,)high low cls clsττ (,)high low obj objττ (,)high low iou iouττ...  : Figure 2. Overview of the proposed HSSDA pipeline. We propose a dual-threshold strategy to help the teacher network to generate hierarchical supervision to train the student network. Besides, we also propose a data augmentation strategy to strengthen the ability of feature representation of the student network. moving average (EMA) [39]: θi+1 t = θi t · α + θi s · (1 − α), (1) where α is the EMA decay rate, θt and θs represent the pa- rameters of the teacher and student networks, respectively, and i denotes the training step. 3.2. Overview The pipeline of our HSSDA framework is illustrated in Fig. 2, which is derived from the basic teacher-student mu- tual learning framework. In the burn-in stage of training, we train the detector in a fully supervised manner follow- ing OpenPCDet [40] with the labeled scenes and keep the same setting as the used detector. Then, both the teacher network and student network are initialized with the same pre-trained weight parameters. In the mutual learning stage, there are three steps in each training epoch. The first step is to generate three kinds of dual-thresholds for each class in a global view, as shown on the left of the pink dotted line in Fig. 2. Specifically, we construct a confident scene set Dc composed of labeled scenes and mined scenes (in the first epoch, it just contains all labeled scenes). Then we sequentially input each scene from Dc and its weak augmentation (rotation and scaling) into the teacher network to produce a pair of predictions. Based on those pairs of predictions and the object informa- tion from Dc, we design a dynamic dual-threshold gener- ation strategy to obtain three kinds of dual-thresholds for each class in terms of confidence score, objectness score, and IoU consistency: (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou , τlow iou ). The second step (see the right of the pink dotted line in Fig. 2) is mainly to mine the hierarchical pseudo la- bels. Specifically, each unlabeled scene pu i and its weak augmented scene ˜pu i are sequentially input to the teacher de- tector to produce a pair of predictions. Through three mea- sure rules based on the dual-thresholds obtained in the first step, we can generate the hierarchical supervision: (1) high- confidence level pseudo labels, (2) ambiguous level pseudo labels, and (3) low-confidence level pseudo labels. We add all high-confidence pseudo labels into the confident scene set Dc for the next dual-threshold generation. We also add the ground-truth from labeled scenes into the first group for following student network training. In the third step, we use the hierarchical supervision composed of three groups of pseudo labels to train the student network with our designed shuffle data augmentation, and then update the teacher net- work by the EMA strategy according to Eq. (1). After the mutual training step, we use the 3D detector from the student network as our final detector. Through the above procedure, we can see that our designed framework can train any off-the-shelf 3D detector which consists of a backbone and a detection head. 3.3. Dynamic dual-threshold generation The dual threshold generation is dynamically conducted in each training epoch. Algorithm 1 describes the whole process of generating dual-thresholds for one class in one training epoch, given a confidence setDc = {pc i , yc i }Nc i=1, its pairs of predictions rc, ˜rc through the 3D detector, and an IoU matching threshold τpair. Here τpair is determined ex- perientially and is used to judge if two 3D bounding boxes match. Initially, we create three empty sets Pcls, Pobj and Piou to collect the confidence score, objectness score and IoU. These three sets will be used to search three optimal dual-thresholds. Concretely, for the i-th scene, we can fetch the predicted bounding boxes br i and ground truth bounding 4Algorithm 1: Dynamic dual-threshold generation Input: confident scene set Dc, pairs of predictions rc and ˜rc, IoU matching threshold τpair. Output: (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou , τlow iou ) 1 initialize empty sets Pcls, Pobj, and Piou; 2 for each {pc i , yc i } ∈Dc do 3 fetch bounding boxes bgt i from yc i ; 4 fetch bounding boxes br i from rc i ; 5 fetch bounding boxes ˜br i from ˜rc i ; 6 for bgt ij in bgt i do 7 compute the matrix M ←IoU (bgt ij , br i ); 8 if max(M) > τpair then 9 choose index k with max(M); 10 Pcls = Pcls ∪ sc cls, 11 sc cls is the confidence score of rc ik; 12 Pobj = Pobj ∪ sc obj, 13 sc obj is the objectness score of rc ik; 14 Piou = Piou ∪ vc, 15 vc = max(IoU (br ik,˜br i )); 16 (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou τlow iou ) ← JNB (Pcls, Pobj, Piou); boxes bgt i from rc i and yc i , respectively. Further, we utilize the common IoU-based strategy to pair a predicted box br ik for each ground truth bgt ij in the i-th scene, aiming to ob- tain the predicted confidence scores and predicted object- ness scores of ground truth objects, and collect these scores into the confidence score set Pcls and objectness score set Pobj, respectively. In this way, we can distinguish predic- tions with different classifications and objectness reliability in a global view. At the same time, we can also get the consistency IoU set Piou based on the rc i and ˜rc i , which fa- cilitates grading predictions with different localization qual- ity in a consistency constraints manner. After handling all scenes, we solve the dual-threshold search problem through a global clustering algorithm. Specifically, we adopt the Jenks Natural Breaks (JNB) [13] algorithm to search the natural turning points or breakpoints based on the three sets. As shown in Fig. 3 (a) and (b), we can automatically obtain the dual-threshold τhigh cls and τlow cls for each class. Similarly, we can automatically obtain the other two dual-thresholds τobj and τiou, which are detector-agnostic and category- aware. As mentioned in Sec. 3.2, in each training epoch, the confident scene Dc will be updated, so all three dual- thresholds will dynamically change during training. 3.4. Hierarchical supervision generation As shown in Fig. 2, with the three dual-thresholds for each class, we divide the mined object pseudo labels from * * 0.99 * 0.99 0.99 0.64 * * 0.94 0.71 * * * * * 0.82 0.94 0.11 0.82 Numbers of (× 102) Score Score Score Score Numbers of (× 103) Numbers of (× 103)  Numbers of (× 102) (a) (b) (d)(c) unlabeled scene Detection Head IoU-based  consistency filtering High-confidence  level supervision Ambiguity-level  supervision Weak Aug. Teacher Confidence-based  score filtering Dynamic dual-threshold  strategy  clean scene  Detector  Backbone  Split & Shuffle Detector  Backbone  Detection Head Student Shuffled Features Unshuffled Features  EMA Update Objectness-based  score filtering Hierarchical supervision  𝑝𝑝𝑖𝑖 𝑢𝑢 𝑝𝑝𝑖𝑖 𝑢𝑢𝐴𝐴𝐴𝐴𝐴𝐴. 𝑝𝑝𝑖𝑖 𝑢𝑢′𝐴𝐴𝐴𝐴𝐴𝐴. 𝑝𝑝𝑖𝑖 𝑢𝑢′ ̂𝑝𝑝𝑖𝑖 𝑢𝑢 𝑝𝑝𝑖𝑖 𝑢𝑢𝑆𝑆𝑆𝑆 Low-confidence level supervision Figure 3. Selection of the dual-threshold based on the Jenks Natu- ral Breaks [13]. In each subplot, the blue line represents the sorted scores from the sequence pool P and the red and green dash lines indicate the high threshold and low threshold, respectively. (a) and (b) show the process of dynamic confidence IoU selection for ‘Car’ and ‘Pedestrian’. Similarly, (c) and (d) show the selection of dynamic consistency IoU threshold. unlabeled scenes into hierarchical supervision which in- cludes (1) high-confidence level pseudo labels, (2) ambigu- ous level pseudo labels and (3) low-confidence level pseudo labels. Specifically, the high-confidence level pseudo la- bel ˜yi is chosen when the predicted result ru i of unlabeled scenes from the teacher network meets all three following inequalities simultaneously,i.e., su cls > τhigh cls , su obj > τhigh obj and vu > τhigh iou , where su cls and su obj are the confidence score and the objectness score of the chosen predicted re- sult, respectively, and vu is the consistency IoU between chosen ru i and ˜ru i . Besides, we group the rest of the predic- tions as ambiguous level pseudo labels, which meet all three following inequalities simultaneously, i.e., su cls > τlow cls , su obj > τlow obj and vu > τlow iou . As for those predictions that neither belong to the high-confidence level nor ambiguous level, we mark them as low-confidence level predictions. As mentioned previously, we also add the ground-truth from labeled scenes into the group of high-confidence level pseudo labels which provide strong object label supervi- sion for the student network, while the ambiguous level group will supervise the student network training through a soft-weight way which will be introduced in Sec. 3.6. Following [17], we leverage the points removal strategy to eliminate noise information based on the group of low- confidence level pseudo labels. 3.5. Shuffle Data Augmentation Weak-strong data augmentation plays a significant role in the teacher-student style framework, which guides the model to learn strong feature representation. As mentioned previously, the data augmentation of the student network of 5current methods is too weak to learn the strong ability of feature representation. However, due to the huge modality difference between 2D images and 3D point cloud, it is not feasible to directly apply the widely-used data augmenta- tion strategies, e.g., color transformation or Mixup [50] to point cloud for object detection. Thus, we propose a shuffle data augmentation strategy for the student network. Specif- ically, as shown in Fig. 2, given a sceneˆpu i from the teacher network, we first clip the range of point cloud scene into [x1, x2] for the X axis, [y1, y2] for the Y axis and compress the Z axis to form BEV (bird-eye-view) grid. Then we split each scene into R × C(e.g., 2 × 2) patches and generate the same shaped R×C position information to shuffle each scene patch. By feeding the shuffled patches to the detector backbone for feature extraction, the teacher network with a weak branch and the student network with a strong branch can achieve obvious differences, which is beneficial for the student network to learn more complex and peculiar infor- mation from hierarchical supervision. Then we leverage the position information to unshuffle the patch features for further regression and classification in the detection head. Although shuffled patches make it difficult to distinguish the edges or parts of objects in scenes, the unshuffle opera- tion before the detection head restores the original location from the feature space. Hence, our student network deliv- ers more effort into learning with weaker features, which would strengthen the ability of feature representation, and especially benefit to detect objects with weak feature due to small sizes, e.g., ‘Pedestrian’, ‘Cyclist’, which will be shown in experiments. 3.6. Training Objective Function Following [18, 38], we freeze the optimization of the teacher detector, and the student detector is trained on both unlabeled scenes with the hierarchical supervision and la- beled scenes with the ground-truth. More specifically, the training objective consists of a supervised loss for labeled and unlabeled scenes. Ls = X i Lcls (ps i , ys i ) +Lreg (ps i , ys i ) , (2) Lu = X i Lcls \u0000 ˆpu i , ˜yu ij \u0001 + Lreg \u0000 ˆpu i , ˜yu ij \u0001 +wijLcls \u0000 ˆpu i , ˆyu ij \u0001 + wijLreg \u0000 ˆpu i , ˆyu ij \u0001 , (3) where Lcls is the classification loss, Lreg is the regression loss, ˆyu ij and ˜yu ij are the j-th ambiguous level pseudo la- bels and high-confidence level pseudo labels generated by the teacher detector in the i-th scene, and wij = rcls ij · robj ij denotes the soft-weight for ambiguous level pseudo labels ˆyu ij, which is determined by a combination of predicted confidence score and predicted objectness reliability score. Thanks to the clean scenes generated by the noise points re- moval operation and further obtaining complex scenes by GT sampling data augmentation [46], we do not force each training batch to contain a mixture of labeled scenes ps i and unlabeled scenes pu i with a certain ratio (e.g., 1 : 1in [41]), but randomly sample each batch from the entire dataset for training. Hence, the training loss is defined as follows: L = Ls + Lu. (4) Thus, we remove the hyper-parameter to trade-off between Ls and Lu as used in the common teacher-student frame- work [18, 36, 41]. 4. Experiments 4.1. Datasets and Evaluation Metrics KITTI Dataset. Following the state-of-the-art meth- ods [25, 41], we evaluate our HSSDA on the KITTI 3D de- tection benchmark [8], and we use the divided train split of 3,712 samples and val split of 3,769 samples as a com- mon practice [31]. Then we sample three different 1% and 2% labeled scenes over train split based on the released 3DIoUMatch [41] splits. The reported results are averaged over model training on three sampled splits and evaluated on the val split. In addition, the KITTI benchmark has three difficulty levels (easy, moderate, and hard) due to the oc- clusion and truncation levels of objects. For fair compar- isons, we report the mAP with 40 recall positions, with a 3D IoU threshold of 0.7, 0.5, and 0.5 for the three classes: car, pedestrian, and cyclist, respectively. Waymo Open Dataset.We also evaluate our HSSDA on the Waymo Open Dataset [37], which is one of the biggest autonomous driving datasets, containing 798 sequences (ap- proximately 158k point cloud scenes) for training and 202 sequences (approximately 40k point cloud scenes) for val- idation, whilst the view of annotations is in full 360 ◦ field. We find that even only 1% of the labeled Waymo scenes contain approximately 3 times as many object annotations as the full KITTItrain split. Thus, we sample 1% of the 798 training sequences (approximately 1.4k point cloud scenes) and report the standard mean average precision (mAP) as well as mAPH, which represent the heading angle factors. In addition, the prediction results are split into LEVEL 1 and LEVEL 2 for 3D objects including more than five Li- DAR points and one LiDAR point, respectively. 4.2. Implementation Details At the training stage, the student network of our HSSDA is end-to-end optimized with the ADAM optimizer and a cosine annealing learning rate [19]. As for the weak augmentation for the teacher network, we randomly flip each scene along X-axis and Y-axis with 0.5 probability, and then scale it with a uniformly sampled factor from [0.91, 1.12]. Finally, we rotate the point cloud around Z- axis with a random angle sampled from \u0002 −π 4 , π 4 \u0003 . For 6Model Modality 1% 2% 20% Car Ped. Cyc. Avg. Car Ped. Cyc. Avg. Car Ped. Cyc. Avg. PV-RCNN [31] LiDAR 73.5 28.7 28.4 43.5 76.6 40.8 45.5 54.3 77.9 47.1 58.9 61.3 3DIoUMatch [41] (PVR.-based) LiDAR 76.0 31.7 36.4 48.0 78.7 48.2 56.2 61.0 - - - - DetMatch [25] (PVR.&FR. [29]-based)LiDAR + RGB77.5 57.3 42.3 59.0 78.2 54.1 64.7 65.6 78.7 57.6 69.6 68.7 Our HSSDA (PVR.-based) LiDAR 80.9 51.9 45.7 59.5 81.9 58.2 65.8 68.6 82.5 59.1 73.2 71.6 Table 1. Experimental results on KITTI dataset compared with recent state-of-the-art methods. For fair comparison, the results are reported with 40 recall positions, under IoU thresholds 0.7, 0.5, and 0.5 for ‘Car’, ‘Pedestrian’, and ‘Cyclist’, respectively. 1% Data (∼1.4k scenes) Modality Veh. (LEVEL 1)Veh. (LEVEL 2)Ped. (LEVEL 1)Ped. (LEVEL 2)Cyc. (LEVEL 1)Cyc. (LEVEL 2) mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH PV-RCNN [31] LiDAR 47.3 45.6 43.6 42.0 28.9 15.6 26.2 14.1 - - - - DetMatch [25] (PVR.& FR. [29]-based)LiDAR+RGB52.2 51.1 48.1 47.2 39.5 18.9 35.8 17.1 - - - - Improvement - +4.9 +5.5 +4.5 +5.2 +10.6 +3.3 +9.6 +3.0 - - - - PV-RCNN [31] (Reproduced by us)LiDAR 48.5 46.2 45.5 43.3 30.1 15.7 27.3 15.9 4.5 3.0 4.3 2.9 Our HSSDA (PVR.-based) LiDAR 56.4 53.8 49.7 47.3 40.1 20.9 33.5 17.5 29.1 20.9 27.9 20.0 Improvement - +7.9 +7.6 +4.2 +4.0 +10.0 +5.2 +6.2 +1.6 +24.6 +17.9 +23.6 +17.1 Table 2. Performance comparison on the Waymo Open Dataset with 202 validation sequences for the 3D detection. 3D Detection (Car)3D Detection (Ped)3D Detection (Cyc)Model DataEasy Mod HardEasy Mod HardEasy Mod HardV oxel-RCNN [6]1% 87.9 74.0 67.123.7 19.0 17.444.8 37.0 25.5Ours (V oxel-RCNN-based)1% 92.5 81.7 77.550.7 43.9 42.465.2 48.3 42.5V oxel-RCNN [6]2% 89.2 76.5 71.544.2 40.2 34.456.7 39.9 37.4Ours (V oxel-RCNN-based)2% 91.6 82.0 77.964.9 58.3 50.988.0 65.7 60.9 Table 3. Experimental results on KITTI dataset based on the V oxel-RCNN detector, where the metrics are the same as Tab. 1. the KITTI Dataset, the X-axis and Y-axis are limited in [0, 70.4]m and [−40, 40]m in the shuffle data augmentation, and our HSSDA (PV-RCNN-based) is trained for 80 epochs with the batch size 50. For the Waymo Open Dataset, the point cloud scene is clipped into [−75.2, 75.2]m for X and Y axes, and training with the batch size 30 for 10 epochs. We set the value ofτpair to 0.5 in all experiments according to the evaluation metric in the public datasets. 4.3. Main Results KITTI Dataset. We first evaluate our proposed model on the popular KITTI dataset. Tab. 1 lists the results of dif- ferent methods. From this table, we can observe that our ap- proach significantly outperforms the state-of-the-art meth- ods. Specifically, our approach has a remarkable boost in the ‘Car’ class for all settings, which has improvements of 7.4, 5.3, and 4.6 points compared to the PV-RCNN baseline for 1%, 2%, and 20%, respectively. Even compared to the recently proposed DetMatch [25] which uses two modalities of LiDAR and RGB, our methods just with LiDAR still have better results for most of the settings. Besides, we replace the point-voxel-based PV-RCNN 3D detector with a repre- sentative voxel-based V oxel-RCNN [6] 3D detector. The similarly impressive experimental results in Tab. 3 demon- strate the effectiveness of our HSSDA. Waymo Dataset. For the more challenging Waymo Dataset, our approach still has a significant improvement in performance compared to the state-of-the-art methods. As shown in Tab. 2, our approach surpasses DetMatch [25]. It is worth mentioning that the proposed method achieves 29.1 mAP for ‘Cyclist’, which far exceeds the baseline. 4.4. Ablation Study In this section, we present a series of ablation studies to analyze the effect of our proposed strategies in HSSDA. All the experiments are conducted based on the V oxel-RCNN detector with the 2% KITTI split and evaluated on val split due to its fast training speed. Tab. 4 summarizes the abla- tion results on our shuffle data augmentation (SDA) and hi- erarchical supervision (HS) of the teacher network, which provides three levels of supervision: high-confidence level (H LEV) pseudo labels, ambiguous level (A LEV) pseudo labels, and low-confidence level (L LEV) pseudo labels. Effect of the hierarchical supervision. It can be found that only considering the high-confidence level pseudo la- bels will perform better than the baseline as shown in Exp.2 in Tab. 4, but the improvements are limited by the con- fused background supervision. The introduction of ambigu- ous level supervision information can lead to further per- formance improvements which can be seen in Exp.3. Fur- thermore, we can observe that from Exp.4 generating clean scenes via low-confidence supervision can significantly im- prove the detection accuracy, which indicates the effective- ness of the points removal operation. Besides, the collab- oration of three different levels of supervisions can greatly improve performance, as shown in Exp.5 of Tab. 4. Those results mean that all hierarchical supervisions have contri- butions to final performance when they work together. Effect of the shuffle data augmentation. Exp.5 in Tab. 4 shows the effect of our shuffle data augmentation strategy. The classes of ‘Pedestrian’ and ‘Cyclist’ have very weak original features due to their small sizes. Both of them usually are very hard to detect. However, our shuffle data augmentation strategy can significantly improve their per- formance. It can be also observed that a slight drop for ‘Car’ may be due to the shuffle data augmentation splitting the original objects, leading to blurred edges for locating. 7Exp. Hierarchical Supervision (HS)SDA 3D Detection mAPHLEV A LEV L LEV Car Ped. Cyc. 1 - - - - 76.5 40.2 39.9 52.2 2 ✓ - - - 75.9 51.8 51.5 59.7 3 ✓ ✓ - - 78.8 51.1 52.2 60.7 4 ✓ ✓ ✓ - 82.4 56.0 61.3 66.5 5 ✓ ✓ ✓ ✓ 82.0 58.3 65.7 68.6 Table 4. Ablation study of different components in HSSDA. Method R C 3D Detection mAPCar Ped. Cyc. HS 1 1 82.4 56.0 61.3 66.5 HS + SDA 1 2 82.5 57.1 64.3 67.9 2 2 82.0 58.3 65.7 68.6 2 4 81.2 56.8 65.5 67.8 4 4 81.1 54.8 65.8 67.2 Table 5. Results of various combinations of R and C. Our shuffle data augmentation has two hyperparameters: R and C, which decide the number of scene patches to shuf- fle. To evaluate the effect of two hyperparameters, we inves- tigate the performance of the proposed HSSDA with differ- ent combinations of R and C in Tab. 5. We can observe that the model achieves the best result when R = C = 2 (i.e., the scene grids are split into 4 patches and perform random shuffle). Hence, we set R and C to 2 in all our experiments. 4.5. Quality Analysis In this section, we analyze the quality of the generated pseudo labels which play a key role during model train- ing. First of all, if the 3D IoU between pseudo labels and ground-truth boxes of labeled scenes is bigger than 0.5 with the same class, we regard the pseudo-label as a correctly mined object. From Tab. 6, we can see the final precision of our high-confidence level pseudo labels for each class on the KITTI dataset is particularly accurate, which indicates the effectiveness of our dual-threshold strategy. Besides, we provide qualitative results of wrong high-confidence level pseudo labels in Fig. 4. For ease of viewing, we only show the object of one failure case in each scene. (a) and (b) in Fig. 4 show that the common failures for ‘Car’ usually occur with similar classes (such as vans and trucks). Interestingly, our method can reliably mine some real objects which were not annotated in the dataset (see Fig. 4 (c) and Fig. 4 (d)). Additionally, due to the small sizes of ‘Pedestrian’, most of the failure examples are caused by inaccurate localization, as shown in Fig. 4 (e) and (f) (the ground truth and pseudo- labeling 3D bounding box are drawn in red and cyan.) 5. Conclusion In this paper, we propose a novel teacher-student-based method for 3D semi-supervised object detection, called HSSDA. Through the dual-threshold strategy in the teacher network, we can provide hierarchical supervision to effec- Category Split setting on KITTI 1% 2% 20% Car 96.73 (4627/4783)98.69 (4476/4535)98.88 (4508/4559) Pedestrian 85.58 (204/239)92.29 (273/296) 93.67 (74/79) Cyclist 95.53 (107/112)95.00 (114/120)96.33 (105/109) Table 6. Final precision of high-confidence level pseudo labels on ‘Car’, ‘Pedestrian’ and ‘Cyclist’ classes with different SSL set- tings. The blue and red numbers represent the total and correctly number of mined pseudo labels, respectively. （a） （b） （c） （d） （e） （f） （a） （b） （c） （d） （e） （f） Figure 4. Qualitative analysis of pseudo labels on KITTI. For a better view, we only show the objects we are interested in and set the pseudo-labeling car, pseudo-labeling pedestrian, and ground truth bounding box in green, cyan, and red, respectively, whilst projecting boxes in point cloud back to RGB images. (a) and (b) show the case of category errors, (c) and (d) show the missing- annotated instance of the dataset, and (e) and (f) show the case of poorly localized pseudo labels. Best viewed in color. tively train the student network, while eliminating the neg- ative impact of missing-mined objects of unlabeled scenes. In addition, the shuffle data augmentation strategy shuffles the input and unshuffles the feature blocks to strengthen the feature representation ability of the student network. Ex- tensive experiments validate the superiority of our method in challenging datasets. Our HSSDA can train any 3D de- tector which consists of a backbone and detection head. Limitations. Our HSSDA designs a dynamic dual- threshold strategy that determines the optimal threshold in a global view. So, we need to use the teacher network for additional validation, which requires more training time. In addition, the shuffle data augmentation may split the com- plete objects resulting in blurred edges and locating. Acknowledgments. This work is supported in part by the National Key R&D Program of China (2022YFA1004100), and in part by the National Natural Science Foundation of China (No.62176035, 62201111, 62036007), the Sci- ence and Technology Research Program of Chongqing Mu- nicipal Education Commission under Grant (No.KJZD- K202100606), the Chongqing graduate Research Innova- tion Project (CYB22249), the CQUPT Ph.D. Innovative Talents Project (BYJS202105), and the Macao Science and Technology Development Fund (061/2020/A2). 8References [1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, pages 1–8. IEEE, 2020. 3 [2] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Ku- rakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remix- match: Semi-supervised learning with distribution matching and augmentation anchoring. In ICLR, 2019. 2, 3 [3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32, 2019. 3 [4] Alexey Bochkovskiy, Chien-Yao Wang, and Hong- Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 2, 3 [5] Yunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal Mettes, Pengwan Yang, and Cees GM Snoek. Pointmixup: Augmentation for point clouds. In ECCV, pages 330–345. Springer, 2020. 11 [6] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. V oxel r-cnn: Towards high performance voxel-based 3d object detection. In AAAI, pages 1201–1209, 2021. 3, 7, 11 [7] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. Int. J. Rob. Res., 32(11):1231–1237, 2013. 6 [9] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. V oxel set transformer: A set-to-set approach to 3d object detection from point clouds. In CVPR, pages 8417–8427, 2022. 2, 3 [10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detec- tion from point cloud. In CVPR, pages 11873–11882, 2020. 3 [11] Jordan SK Hu, Tianshu Kuai, and Steven L Waslander. Point density-aware voxels for lidar 3d object detection. In CVPR, pages 8469–8478, 2022. 1 [12] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In CVPR, pages 5070–5079, 2019. 3 [13] George F Jenks. Optimal data classification for choropleth maps. Department of Geographiy, University of Kansas Oc- casional Paper, 1977. 5 [14] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object de- tection. NeurIPS, 32, 2019. 2, 3 [15] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, pages 12697–12705, 2019. 2 [16] Zhaoqi Leng, Shuyang Cheng, Benjamin Caine, Weiyue Wang, Xiao Zhang, Jonathon Shlens, Mingxing Tan, and Dragomir Anguelov. Pseudoaugment: Learning to use unla- beled data for data augmentation in point clouds. In ECCV, pages 555–572. Springer, 2022. 3 [17] Chuandong Liu, Chenqiang Gao, Fangcen Liu, Jiang Liu, Deyu Meng, and Xinbo Gao. Ss3d: Sparsely-supervised 3d object detection from point cloud. In CVPR, pages 8428– 8437, 2022. 5 [18] Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors. In CVPR, pages 9819–9828, 2022. 2, 3, 6 [19] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In ICLR, 2017. 6 [20] Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid r-cnn: Towards bet- ter performance and adaptability for 3d object detection. In ICCV, pages 2723–2732, 2021. 3 [21] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel transformer for 3d object detection. In ICCV, pages 3164– 3173, 2021. 2 [22] Peng Mi, Jianghang Lin, Yiyi Zhou, Yunhang Shen, Gen Luo, Xiaoshuai Sun, Liujuan Cao, Rongrong Fu, Qiang Xu, and Rongrong Ji. Active teacher for semi-supervised object detection. In CVPR, pages 14482–14491, 2022. 2 [23] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence , 41(8):1979–1993, 2018. 3 [24] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3d: Out-of-context data aug- mentation for 3d scenes. In 3DV, pages 116–125. IEEE, 2021. 11 [25] Jinhyung Park, Chenfeng Xu, Yiyang Zhou, Masayoshi Tomizuka, and Wei Zhan. Detmatch: Two teachers are bet- ter than one for joint 2d and 3d semi-supervised object de- tection. In ECCV, pages 370–389. Springer, 2022. 2, 3, 6, 7 [26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277–9286, 2019. 2 [27] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652–660, 2017. 3 [28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, pages 5099–5108, 2017. 3 [29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS, 28, 2015. 7 [30] Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, and Min-Jian Zhao. Improving 3d object detection with channel-wise transformer. In ICCV, pages 2743–2752, 2021. 3 9[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point- voxel feature set abstraction for 3d object detection. In CVPR, pages 10529–10538, 2020. 3, 6, 7, 11 [32] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv- rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. International Journal of Computer Vision, 131(2):531–551, 2023. 1, 3 [33] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr- cnn: 3d object proposal generation and detection from point cloud. In CVPR, pages 770–779, 2019. 1, 2, 3, 11 [34] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net- work for 3d object detection in a point cloud. InCVPR, pages 1711–1719, 2020. 2 [35] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NeurIPS, 33:596–608, 2020. 2 [36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020. 2, 3, 6, 11 [37] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 2446–2454, 2020. 6, 11 [38] Yihe Tang, Weifeng Chen, Yijun Luo, and Yuting Zhang. Humble teachers teach better students for semi-supervised object detection. In CVPR, pages 3132–3141, 2021. 6 [39] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, pages 1195–1204, 2017. 4 [40] OpenPCDet Development Team. Openpcdet: An open- source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet , 2020. 4 [41] He Wang, Yezhen Cong, Or Litany, Yue Gao, and Leonidas J Guibas. 3dioumatch: Leveraging iou prediction for semi- supervised 3d object detection. In CVPR, pages 14615– 14624, 2021. 2, 3, 6, 7 [42] Zhenyu Wang, Yali Li, Ye Guo, Lu Fang, and Shengjin Wang. Data-uncertainty guided multi-phase learning for semi-supervised object detection. In CVPR, pages 4568– 4577, 2021. 2, 3 [43] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. NeurIPS, 33:6256–6268, 2020. 3 [44] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet clas- sification. In CVPR, pages 10687–10698, 2020. 3 [45] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to- end semi-supervised object detection with soft teacher. In ICCV, pages 3060–3069, 2021. 2 [46] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed- ded convolutional detection. Sensors, 18(10):3337, 2018. 2, 6 [47] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In CVPR, pages 11040–11048, 2020. 2 [48] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In ICCV, pages 1951–1960, 2019. 2 [49] Junbo Yin, Jin Fang, Dingfu Zhou, Liangjun Zhang, Cheng- Zhong Xu, Jianbing Shen, and Wenguan Wang. Semi- supervised 3d object detection with proficient teachers. In ECCV, pages 727–743. Springer, 2022. 2, 3 [50] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 3, 6, 11 [51] Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujin Chen, Yanmei Meng, and Danfeng Wu. Pointcut- mix: Regularization strategy for point cloud classification. Neurocomputing, 505:58–67, 2022. 11 [52] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian- wei Wan, and Yulan Guo. Not all points are equal: Learn- ing highly efficient point-based detectors for 3d lidar point clouds. In CVPR, pages 18953–18962, 2022. 3 [53] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian- wei Wan, and Yulan Guo. Not all points are equal: Learn- ing highly efficient point-based detectors for 3d lidar point clouds. In CVPR, pages 18953–18962, 2022. 1 [54] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point cloud completion and graph neural network for 3d object de- tection. In AAAI, number 4, pages 3430–3437, 2021. 2 [55] Na Zhao, Tat-Seng Chua, and Gim Hee Lee. Sess: Self- ensembling semi-supervised 3d object detection. In CVPR, pages 11079–11087, 2020. 2, 3 [56] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi- Wing Fu. Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In AAAI, number 4, pages 3555– 3562, 2021. 2 [57] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se- ssd: Self-ensembling single-stage object detector from point cloud. In CVPR, pages 14494–14503, 2021. 2 [58] Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, and Hao Li. Instant-teaching: An end-to-end semi-supervised object detection framework. In CVPR, pages 4081–4090, 2021. 2, 3 [59] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, pages 4490–4499, 2018. 3 101% Data(∼1.4k scenes)Veh.(LEVEL 1)Veh.(LEVEL 2)Ped.(LEVEL 1)Ped.(LEVEL 2)Cyc.(LEVEL 1)Cyc.(LEVEL 2)V oxel-RCNN [6]49.02/48.0342.36/41.5041.16/32.8134.73/27.665.84/5.615.62/5.40Ours (V oxel-RCNN-based)54.89/54.0648.28/47.5343.86/37.8436.59/31.5617.47/16.7316.72/16.01 Table 7. Results on the Waymo for the V oxel-RCNN detector. Model Data 3D Detection (Car)3D Detection (Ped.)3D Detection (Cyc.)Easy Mod HardEasy Mod HardEasy Mod HardPV-RCNN [31]100%92.1084.3682.4863.1254.8451.7889.10 70.38 66.01PV-RCNN [31] with SDA100%91.9184.5782.3162.8355.4951.0489.68 71.09 66.71 Table 8. Ablation study of SDA in the fully supervised framework. A. Additional Experimental Results (1) Additional experiments on the Waymo Dataset. We additionally test the V oxel-RCNN [6] on 1% of the Waymo [37] dataset, and the results in Tab. 7 still show the superiority of our method, which validates its generaliza- tion. (2) If the shuffle data augmentation (SDA) strategy is also effective for full supervision training ? To verify the effect of the SDA on fully-supervised 3D object de- tector, we inset the SDA into the PV-RCNN [31] and the results are listed in Tab. 8, which shows that the superior- ity of SDA in the supervised framework is not as obvious as in the semi-supervised framework. This is due to that the design of the strong augmentation in the student branch module has two main purposes: (1) strong enough to make a significant difference with weakly augmented samples of the teacher branch and (2) not too strong to ensure effective supervision information transmission. B. Visualization of Dynamic Dual-Threshold To better understand the dual-threshold hierarchical su- pervision in intuitive, we visualize the dynamic threshold changes during the training process in Fig. 5, where a solid line of a certain color represents a high threshold, and the dotted line of the same color represents a low threshold. C. Discuss on Other Augmentation Methods for Point Cloud The Mixup-based [50] augmentation methods have been extensively studied in the field of image classification and widely applied in 2D semi-supervised object detection [36] task. Following this idea, there have been several explo- rations in point cloud tasks as well. PointMixup [5] first ap- plied the idea of Mixup to point cloud and achieved linear interpolation through the optimal allocation. Mix3D [24] balances global contextual information and local geomet- ric information to achieve high-performance models. In addition, PointCutMix [51] proposes two different ways of replacing points to mix two point clouds. The latest SageMix explores salient regions in two point clouds and Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene ( a) RGB ImageFull labeled scene (a) (b) Ours Teacher Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR of cyc of car of car of car of car of cyc of cyc of cyc 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 10 20 30 40 50 60 70 80 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Epoch  Ratio of label data (%) 8060 10040200 50 60 70 80 Moderate mAP Car Pedestrian Cyclist (a) (b) of cyc of car of car of car of car of cyc of cyc of cyc 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 10 20 30 40 50 60 70 80 0.6 0.7 0.8 0.9 1.0 Threshold Epoch Figure 5. Visualization curve of the dynamic dual-threshold during training. smoothly combines them into a continuous shape. How- ever, these methods mainly focus on point cloud classifica- tion and segmentation tasks. For outdoor 3D object detec- tion task, objects are usually naturally separated [33], and merging two point cloud scenes will cause overlaps between objects (e.g., two vehicles are rarely overlapped in 3D real- ity). Therefore, to the best of our knowledge, the above Mixup-based point cloud augmentation methods cannot be directly applied to detection tasks, which is the direction for our future research. 11",
      "references": [
        "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
        "Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Yolov4: Optimal speed and accuracy of object detection",
        "Pointmixup: Augmentation for point clouds",
        "Voxel r-cnn: Towards high performance voxel-based 3d object detection",
        "Improved regularization of convolutional neural networks with cutout",
        "Vision meets robotics: The kitti dataset",
        "Voxel set transformer: A set-to-set approach to 3d object detection from point clouds",
        "Structure aware single-stage 3d object detection from point cloud",
        "Point density-aware voxels for lidar 3d object detection",
        "Label propagation for deep semi-supervised learning",
        "Optimal data classification for choropleth maps",
        "Consistency-based semi-supervised learning for object detection",
        "Pointpillars: Fast encoders for object detection from point clouds",
        "Pseudoaugment: Learning to use unlabeled data for data augmentation in point clouds",
        "Ss3d: Sparsely-supervised 3d object detection from point cloud",
        "Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors",
        "SGDR: stochastic gradient descent with warm restarts",
        "Pyramid r-cnn: Towards better performance and adaptability for 3d object detection",
        "Voxel transformer for 3d object detection",
        "Active teacher for semi-supervised object detection",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "Mix3d: Out-of-context data augmentation for 3d scenes",
        "Detmatch: Two teachers are better than one for joint 2d and 3d semi-supervised object detection",
        "Deep hough voting for 3d object detection in point clouds",
        "Pointnet: Deep learning on point sets for 3d classification and segmentation",
        "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
        "Faster r-cnn: Towards real-time object detection with region proposal networks",
        "Improving 3d object detection with channel-wise transformer",
        "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection",
        "Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection",
        "Pointrcnn: 3d object proposal generation and detection from point cloud",
        "Point-gnn: Graph neural network for 3d object detection in a point cloud",
        "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
        "A simple semi-supervised learning framework for object detection",
        "Scalability in perception for autonomous driving: Waymo open dataset",
        "Humble teachers teach better students for semi-supervised object detection",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "Openpcdet: An open-source toolbox for 3d object detection from point clouds",
        "3dioumatch: Leveraging iou prediction for semi-supervised 3d object detection",
        "Data-uncertainty guided multi-phase learning for semi-supervised object detection",
        "Unsupervised data augmentation for consistency training",
        "Self-training with noisy student improves imagenet classification",
        "End-to-end semi-supervised object detection with soft teacher",
        "Second: Sparsely embedded convolutional detection",
        "3dssd: Point-based 3d single stage object detector",
        "Std: Sparse-to-dense 3d object detector for point cloud",
        "Semi-supervised 3d object detection with proficient teachers",
        "mixup: Beyond empirical risk minimization",
        "Pointcutmix: Regularization strategy for point cloud classification",
        "Not all points are equal: Learning highly efficient point-based detectors for 3d lidar point clouds",
        "Pc-rgnn: Point cloud completion and graph neural network for 3d object detection",
        "Sess: Self-ensembling semi-supervised 3d object detection",
        "Cia-ssd: Confident iou-aware single-stage object detector from point cloud",
        "Se-ssd: Self-ensembling single-stage object detector from point cloud",
        "Instant-teaching: An end-to-end semi-supervised object detection framework",
        "Voxelnet: End-to-end learning for point cloud based 3d object detection"
      ],
      "meta_data": {
        "arxiv_id": "2304.01464v1",
        "authors": [
          "Chuandong Liu",
          "Chenqiang Gao",
          "Fangcen Liu",
          "Pengcheng Li",
          "Deyu Meng",
          "Xinbo Gao"
        ],
        "published_date": "2023-04-04T02:09:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces HSSDA, a teacher–student framework for 3-D semi-supervised object detection that: (1) develops a dynamic dual-threshold scheme to create hierarchical pseudo-labels (high-confidence, ambiguous, low-confidence) improving supervision quality, and (2) proposes Shuffle Data Augmentation that splits, shuffles and later unshuffles BEV point-cloud patches to strengthen the student’s feature learning. The method is detector-agnostic and achieves state-of-the-art results on KITTI and Waymo with as little as 1 % labeled data.",
        "methodology": "Uses a teacher network (EMA of student) with weak augmentations to predict on labeled + unlabeled data. Per class, three dual thresholds (for confidence, objectness and teacher–student IoU consistency) are computed each epoch via Jenks Natural Breaks on a growing set of confident scenes, dividing predictions into the three supervision levels. High-confidence labels are used as hard targets, ambiguous labels are weighted by confidence, and low-confidence boxes trigger point removal. Student receives Shuffle Data Augmentation: BEV grid split into R×C patches (default 2×2), patches randomly permuted before backbone, then features are unshuffled before the head. Loss combines supervised and unsupervised terms; teacher updated by EMA.",
        "experimental_setup": "Datasets: KITTI 3D detection (train/val split 3712/3769). Semi-supervised settings with 1 %, 2 %, 20 % labeled scenes (three random seeds). Waymo Open Dataset: 798 training, 202 validation sequences, using 1 % labeled data. Base detectors: point-voxel PV-RCNN and voxel-based Voxel-RCNN. Metrics: KITTI—mAP40 with IoU 0.7(Car) / 0.5(Ped,Cyc) for Easy/Moderate/Hard; Waymo—mAP/mAPH on LEVEL 1 & 2. Training: 80 epochs on KITTI (batch 50), 10 epochs on Waymo (batch 30); SGD with Adam, cosine LR. Comparisons with PV-RCNN, 3DIoUMatch, DetMatch, Proficient Teachers, etc., plus ablations on threshold strategy, augmentation, R×C values.",
        "limitations": "Dynamic thresholding requires additional teacher inference on a confidence scene set each epoch, increasing training time; global thresholds may not adapt to scene-specific statistics. Shuffle augmentation can split large objects, slightly degrading car localization. Method still depends on some labeled data and manually chosen hyper-parameters (IoU pairing τ_pair, patch grid size R,C, EMA decay). Tested only on outdoor LiDAR data; performance on indoor or multi-modal settings unverified.",
        "future_research_directions": "1) Learn per-scene or per-instance adaptive thresholding to reduce extra passes and improve flexibility. 2) Design learnable or content-aware shuffling/augmentation that preserves object integrity. 3) Extend framework to multi-modal (LiDAR+RGB) or indoor datasets. 4) Reduce computational overhead via lightweight teacher or periodic update. 5) Explore Mixup/CutMix style augmentations for point clouds within hierarchical supervision.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Robust Image Denoising through Adversarial Frequency Mixup",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model",
      "full_text": "Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model Zhicai Wang1*, Longhui Wei2†, Tan Wang3, Heyu Chen1, Yanbin Hao1, Xiang Wang1†, Xiangnan He1, Qi Tian2 1University of Science and Technology of China, 2 Huawei Inc., 3 Nanyang Technological University Abstract Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo- realistic images and giving rise to a multitude of appli- cations. However, the effective integration of T2I mod- els into fundamental image classification tasks remains an open question. A prevalent strategy to bolster image clas- sification performance is through augmenting the training set with synthetic images generated by T2I models. In this study, we scrutinize the shortcomings of both current gener- ative and conventional data augmentation techniques. Our analysis reveals that these methods struggle to produce im- ages that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain- specific concepts. To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix 1, which enriches the dataset by performing image translations between classes. Our empirical results demon- strate that Diff-Mix achieves a better balance between faith- fulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifica- tions for domain-specific datasets. 1. Introduction In comparison to GAN-based models [7, 17, 25], contem- porary state-of-the-art text-to-image (T2I) diffusion models exhibit enhanced capabilities in producing high-fidelity im- ages [12, 37, 44, 49]. With the remarkable cross-modality alignment capabilities of T2I models, there is significant po- tential for generative techniques to enhance image classifi- cation [2, 4]. For instance, a straightforward approach en- tails augmenting the existing training dataset with synthetic images generated by feeding categorical textual prompts to a T2I diffusion model. However, upon reviewing prior ap- *This work was done during the internship in Huawei Inc.. †Xiang Wang and Longhui Wei are both the corresponding authors. 1https://github.com/Zhicaiwww/Diff-Mix domain-specific  dataset “Red winged Blackbird” reference image T2I “Red winged Blackbird” Intra-class Augmentation T2I Inter-class Augmentation T2I T2I T2I Vanilla T2I Fine-tuned T2I synthetic image Diversity Faithfulness Diversity Faithfulness Diversity Faithfulness Figure 1. Strategies to expand domain-specific datasets for im- proved classification are varied. Row 1 illustrates vanilla distilla- tion from a pretrained text-to-image (T2I) model, which carries the risk of generating outputs with reduced faithfulness. Intra-class augmentation, depicted in Row 2, tends to yield samples with lim- ited diversity to maintain high fidelity to the original class. Our proposed method, showcased in Rows 3 and 4, adopts an inter- class augmentation strategy. This involves introducing edits to a reference image using guidance of another class, which signifi- cantly enriches the dataset with a greater diversity of samples. proaches employing T2I diffusion models for image classi- fication, it becomes evident that the challenge in generative data augmentation for domain-specific datasets is produc- ing samples with both a faithful foreground and a diverse background. Depending on whether a reference image is used in the generative process, we divide these methods into two groups: • Text-guided knowledge distillation [52, 57] involves gen- erating new images from scratch using category-related prompts to expand the dataset. For the off-the-shelf T2I models, such vanilla distillation presume these models have comprehensive knowledge of target domain, which can be problematic for domain-specific datasets. Insuf- ficient domain knowledge easily makes the distillation process less effective. For example, vanilla T2I models arXiv:2403.19600v1  [cs.CV]  28 Mar 2024struggle to generate images that accurately represent spe- cific bird species based solely on their names (see Row 1 of Fig. 1). • Generative data augmentation [1, 69] employs genera- tive models to enhance existing images. Da-fusion [58], for instance, translates the source image into multiple edited versions within the same class . This strategy, termed intra-class augmentation, primarily introduces intra-class variations. While intra-class augmentation re- tains much of the original image’s layout and visual de- tails, it results in limited background diversity (see Row 2 of Fig. 1). However, synthetic images with constrained diversity may not sufficiently enhance the model’s ability to discern foreground concepts. Based on these observations, a fundamental question emerges: ‘ Is it feasible to develop a method that optimizes both the diversity and faithfulness of synthesized data si- multaneously?’ In this work, we introduce Diff-Mix, a simple yet ef- fective data augmentation method that harnesses diffusion models to perform inter-class image interpolation, tailored for enhancing domain-specific datasets. The method en- compasses two pivotal operations: personalized fine-tuning and inter-class image translation. Personalized fine-tuning [15, 46] is originally designed for customizing T2I mod- els and enabling them to generate user-specific contents or styles. In our case, we implement the technique to tailor the model, enabling it to generate images with faithful fore- ground concepts. Inter-class image translation in Diff-Mix entails transforming a reference image into an edited ver- sion that incorporates prompts from different classes. This translation strategy is designed to retain the original back- ground context while editing the foreground to align with the target concept. For instance, as depicted in the bottom rows of Fig. 1, Diff-Mix can generate images of land birds in diverse settings, such as maritime environments, enrich- ing the dataset with a variety of counterfactual samples. Unlike previous non-generative augmentation methods, such as Mixup [68] and CutMix [66], Diff-Mix works in a foreground-perceivable inter-class interpolation manner and shares a different mechanism with the non-generative approaches. Our experiment under the conventional clas- sification setting indicates that incorporating both CutMix and Diff-Mix could further enhance performance. Addi- tionally, when compared with other generative approaches, we conduct experiments under few-shot and long-tail sce- narios and observe consistent performance improvements. Our contributions can be summarized as follows: • We pinpoint the critical factors that affect the efficacy of generative data augmentation in domain-specific image classification: namely, faithfulness and diversity. • We introduce Diff-Mix, a simple yet effective generative data augmentation strategy that leverages fine-tuned dif- fusion models for inter-class image interpolation. • We conduct a comparative analysis of Diff-Mix with other distillation-based and intra-class augmentation methods, as well as non-generative approaches, highlight- ing its unique features and benefits. 2. Related Works Text-to-image diffusion models. Following pretraining on web-scale data, the T2I diffusion model has demon- strated robust capabilities in generating text-controlled im- ages [37, 49, 53, 56]. Its versatility has led to diverse applications, including novel view synthesis [6, 63], con- cept learning [28, 46], and text-to-video generation [22, 55], among others. Recent advancements [29] also highlight the cross-modality features of such generative models, show- casing their ability to serve as zero-shot classifiers. Synthetic data for image classification.There are two per- spectives on the utilization of synthetic data for image clas- sification: knowledge distillation [2] and data augmentation [5, 51, 54, 62]. From the knowledge distillation perspective, SyntheticData [19] reports significant performance gains in both zero-shot and few-shot settings by leveraging off-the- shelf T2I models to obtain synthetic data. The work of [2] has indicated that fine-tuning the T2I model on ImageNet [47] yields improved classification accuracy by narrowing the domain gap. Some works also find that learning from the synthetic data presents strong transferability [19, 57] and robustness [4, 30, 67]. From the data augmentation standpoint, Da-fusion [58] achieves stable performance im- provements on few-shot datasets by augmenting from ref- erence images. In a related study [3], the use of StyleGAN [25] for generating interpolated images between two differ- ent domains has been shown to enhance classifier robust- ness for out-of-distribution data. Our work shares similar- ities with AMR [5], which generates realistic novel exam- ples by interpolating between two images using GAN [16]. The distinction lies in our discussion of interpolation using the T2I diffusion model, where its noise-adding and denois- ing characteristics enable a smoother implementation of in- terpolation. Non-generative data augmentation. Mixup [68] and Cut- Mix [66] stand out as two prominent non-generative data augmentation methods, serving as effective regularization techniques during training. While Mixup achieves aug- mented samples through a convex combination of two im- ages, CutMix achieves augmentation by cutting and pasting parts of images. However, both methods are constrained in their ability to produce realistic images. In addressing this limitation, the utilization of generative models emerges as a potential solution to alleviate this issue.3. Method 3.1. Preliminary Text-to-Image diffusion model. Diffusion models gener- ate images by gradually removing noise from a Gaussian noise source [21]. In a diffusion process with a total of T steps, its forward process, which gradually adds noise, is represented as a Markov chain with a Gaussian transition kernel, where q(xt|xt−1) = N \u0000 xt; √αtxt−1, (1 − αt)I \u0001 , where xt represents the noisy image at step t. The train- ing objective at step t is to predict the noise to reconstruct xt−1. When training a text-conditioned diffusion model, the simplified training objective can be summarized as fol- lows: Eϵ,x,c,t h ∥ϵ − ϵθ (xt, c, t)∥2 2 i , (1) where ϵθ represents the predicted noise, and c is the en- coded text caption associated with the image x. T2I personalization. T2I personalization aims to personal- ize a diffusion model for generating specific concepts using a limited number of concept-oriented images [28, 40, 42]. These concepts are typically represented using identifiers (e.g., “ [V]”). As a result, we formalize the constructed image-caption set as x, “ photo of a [V]”. Various personalization methods differ in their fine-tuning strate- gies. For instance, Textual Inversion (TI) [15] makes the identifier learnable, but other modules are not fine-tuned, potentially sacrificing some faithfulness in image genera- tion. On the other hand, Dreambooth (DB) [46] fine-tunes the U-Net [45] for more refined personalized generation but faces the challenge of increased computational cost. Image-to-image translation. Image translation enables image synthesis and editing using a reference image as guidance [24, 64, 71]. Diffusion-based image translation methods can generate fine edits, which refer to subtle mod- ifications, with varying degrees of shift relative to the ref- erence image [8, 35, 59]. Here, we draw inspiration from SDEdit [35] to perform edits on the reference image, where the target image xtar is translated from a reference image xref. During translation, the reverse process does not tra- verse the full process but starts from a certain step ⌊sT⌋, where s ∈ [0, 1] controls the insertion position of the refer- ence image with noise, as follows, x⌊sT⌋ = q ˜α⌊sT⌋xref 0 + q 1 − ˜α⌊sT⌋ϵ. (2) By adjusting the strength parameter s, one can strike a bal- ance between the diversity of the generated images and their faithfulness to the reference image. 3.2. General Framework The Diff-Mix pipeline consists of two key steps. Firstly, to produce more faithful images for domain-specific datasets, (a) Real images (b) Synthetic images (vanilla SD) (c) Synthetic images (SD fine-tuned via DB) (d) Synthetic images (SD fine-tuned via TI+DB) Figure 2. Examples of “ American Three toed Woodpecker”. (a) Real images from the training set. (b- d) synthetic images generated using different fine-tuned models with the same number of fine-tuning steps. TI+DB indicates both text embedding and U-Net are fine-tuned. TI+DB achieves a more faithful output compared to DB alone (check the head and wing patterns of the birds). we propose treating it as a T2I personalization problem and fine-tuning the Stable Diffusion (SD). Subsequently, to en- hance the diversity of synthetic data beyond the well-fitted training distribution, we employ inter-class image transla- tion. This process produces interpolated images with in- creased background diversity for each class. 3.3. Fine-tune Diffusion Model Vanilla distillation tends to be less effective, especially as the number of training shots increases (refer to Sec. 4.1). In order to mitigate the distribution gap, we propose fine- tuning Stable Diffusion in conjunction with current widely- used T2I personalization strategies. Dreambooth meets Textual Inversion. Many fine-grained datasets provide terminological names for their cate- gories, like “ American Three toed Woodpecker” and “ Pileated Woodpecker”. We could construct image-text pairs using category-related prompts and fine- tune the denoising network of SD using Eq. 1, which is analogous to Dreambooth. However, we observe that di- rectly incorporating these specialized terms into the text during fine-tuning can impede convergence and hinder the generation of faithful images. We attribute this challenge to the semantic proximity of terminology within a fine-grained domain, where fine-tuning the vision module alone tends to be less effective at distinguishing two similar classes within the same family, like “Woodpecker”. Inspired by Textual Inversion [15], we opt to replace the terminological name in the dataset with “[Vi] [metaclass]” where “[Vi]” is a learnable identifier, and i varies from 1 to N, represent-American GoldfinchAmerican Crow  Acadian Flycatcher [V1] bird [V3] bird[V2] bird a ... bird [V2] [V1] ... “photo of a [V1] bird” CLIP-T LoRA UNet Frozen Tunable Figure 3. Fine-tuning framework of Diff-Mix operates as follows: Initially, we replace the class name with a structured identifier for- matted as “[Vi] [metaclass]”, thereby sidestepping the need for specific terminological expressions. Next, we engage in joint fine-tuning of these identifiers and the low-rank residues (LoRA) of U-Net to capture the domain-specific distribution. ing the category index. The illustration of our fine-tuning strategy is presented in Fig. 3. The term “[metaclass]” is determined by the theme of the current dataset, such as “bird” for a fine-grained bird dataset. By concurrently fine-tuning the identifier and the U-Net, we empower the model to quickly adapting to the fine-grained domain, al- lowing it to generate faithful images using the identifier (see comparison between Row 3 and Row 4 in Fig. 2). Parameter efficient fine-tuning. In this context, we em- brace the parameter-efficient fine-tuning strategy known as LoRA [23]. LoRA distinguishes itself by fine-tuning the residue of low-rank matrices instead of directly fine-tuning the pre-trained weight matrices. To elaborate, consider a weight matrix W ∈ Rm×n. The tunable residual matrix ∆W comprises two low-rank matrices: A ∈ Rm×d and B ∈ Rn×d, defined as ∆W = AB⊤. As a default config- uration, we set the rank d to 10. 3.4. Data Synthesis Using Diffusion Model In generating pseudo data, three strategies can be used with our fine-tuned diffusion model 2: (1) distillation-based method Diff-Gen, (2) intra-class augmentation Diff-Aug, and (3) inter-class augmentation Diff-Mix. Diff-Gen and Diff-Aug. For a target class yi and its textual condition, “ photo of a [Vi] [metaclass]”, both 2We use the prefix “Diff-” denotes the T2I model is fine-tuned and “Real-” denotes the vanilla T2I model. 0.1 0.3 0.5 0.7 0.9 Diff-Mix Translation strength Diff-Aug Target class Figure 4. Examples of images translated using Diff-Mix and Diff- Aug across various strengths. Diff-Aug employs the same target and reference image classes, typically resulting in subtle modifica- tions. Diff-Mix progressively adjusts the foreground to align with the target class as the translation strength increases, while preserv- ing the background layout from the reference image. methods generate synthetic samples annotated with class i. Specifically, Diff-Gen generates samples from scratch by initializing with random Gaussian noise and proceed- ing through the full reverse diffusion process with T steps. Diff-Gen can produce images aligned with its fine-tuned distribution. In contrast, Diff-Aug sacrifices a portion of diversity and generate images by editing on a reference im- age. Specifically, it randomly sample a image from the intra-class training set and enhances the image through im- age translation using Eq. 2. The term “intra-class” means that the conditioning prompts are constructed based on the ground truth categories of images, and such a denoising pro- cess tends to introduce less variation, particularly for the foreground concepts (see top rows of Fig. 1). Diff-Mix. Diff-Mix employs the same translation process as Diff-Aug, but the reference image is sampled from the full training set rather than intra-class set to enable inter- class interpolation. The key difference is that Diff-Mix can generate numerous counterfactual examples, such as a blackbird in the sea (see fourth row of Fig. 1). This ne- cessitates that downstream models make a more refined dif- ferentiation of category attributes, thereby reducing the im- pact of spurious correlations introduced by variations in the background. Denoting the label of the reference image as yj, by inserting the reference image into the reverse process with the prompt “photo of a [Vi] [metaclass]”, we can obtain interpolated images between the ith and jth categories. By controlling the intensity s, we can precisely(a) (b) diverse  background faithful  foreground Figure 5. A schematic explanation of Diff-Mix’s effectiveness us- ing structural casual model [41]. xfg is the foreground that deter- mines the real class label, xbg denotes the background. xfg → z → y is the causal path that we are focusing and xfg ← I → xbg → z → y is the backdoor path that introduces spurious rela- tions between xfg and y. manage the interpolation process. When annotating the syn- thetic image, unlike Mixup and Cutmix, we take into ac- count the non-linear nature of diffusion translation. Thus the annotation function is given by ˜y = (1 − sγ)yi + sγyj, (3) where γ is a hyperparameter introducing non-linearity. Our empirical findings indicate that a γ smaller than 1 is fa- vored. Additionally, in low-shot cases, the samples with higher confidence in the target class are preferred (see de- tails in Sec. 4.5). Construct synthetic dataset. To construct the synthetic dataset using Diff-Mix, similar to Da-fusion [58], we adopt a randomized sampling strategy (s ∈ {0.5, 0.7, 0.9}) for the selection of translation strength. While applying the inter- class editing, we observe that Diff-Mix tends to produce more undesirable samples compared to Diff-Aug. These un- desirable samples have incomplete foreground such as frag- mented bird bodies. This is caused by the intrinsic shape and pose differences among classes. To mitigate this, we in- troduce a simple data-cleaning approach to reduce the pro- portion of such problematic images. We utilize the large vision language model CLIP [43] to assess the confidence in the content, serving as the filtering criterion. Further de- tails can be found in the supplementary materials (SMs). Analysis. We depict the core insight of Diff-Mix in Fig. 5. To eliminate the spurious correlation introduced by xbg, learning on the synthetic set with randomized xbg (back- ground) can cut off spurious correlation, forcing the classi- fication model to infer only from the foreground. The study in Fig. 7 (b) shows that the more diverse the background (larger the referable class number), the better the perfor- mance on the CUB test set. 4. Experiments In this section, we investigate the effectiveness of Diff-Mix in domain-specific datasets. The key questions we aim to address are as follows: Q1: Can generative inter-class augmentation lead to more significant performance gains in downstream tasks compared to those intra-class augmentation methods and distillation-based methods? Q2: Is improved background diversity the secret weapon of Diff-Mix for enhancing the performance? Q3: How to choose the fine-tuning strategy and annota- tion strategy to boost the performance gains for the inter- class augmentation? To address Q1, we separately discuss these questions in few-shot settings in Sec. 4.1, conventional classification in Sec. 4.2, as well as long-tail classification in Sec. 4.3. Ad- ditionally, to answer Q2, we conduct a test for background robustness in Sec. 4.4 and perform an ablation study fo- cusing on the size and diversity of synthetic data in Sec. 4.5. For Q3, we conduct an ablation study to empirically discover effective strategies for deployment in Sec. 4.5. 4.1. Few-shot Classification Experimental Setting. To investigate the impact of dif- ferent data expansion methods, we conduct few-shot exper- iments on a domain-specific dataset Caltech-UCSD Birds (CUB) [61], with shot numbers of 1, 5, 10, and all. For augmentation-based methods, the synthetic dataset is con- structed using various translation strengths (s), specifically, s ∈ {0.1, 0.3, ...,1.0}. We expand the original training set with a multiplier of 5 and cache the synthesized dataset locally for joint training. Real data are replaced by syn- thetic data proportionally during training, and the replace- ment probability p is set as 0.1, 0.2, 0.3, and 0.5 for all- shot, 10-shot, 5-shot, and 1-shot classification, respectively. All experiments use ResNet50 with an input resolution of 224 × 224. Additional details can be found in the SMs. Comparison methods. To unveil the trade-off between faithfulness and diversity resulting from different expan- sion strategies, we compared Diff-Mix with Diff-Gen and Diff-Aug. Furthermore, we conduct experiments on expan- sion strategies using vanilla SD: Real-Mix, Real-Gen, and Real-Aug, where ‘Real’ signifies that SD is not fine-tuned. Main results. To answer Q1 under the few-shot classifica- tion setting, we augment CUB using X-Mix, X-Aug, and X-Gen, where ‘X’ denotes ‘Diff/Real’ for simplicity. The results are shown in Fig. 6, and we can observe that: 1. Diff-Mix generally outperforms the intra-class competi- tor X-Aug and distillation competitor X-Gen in various few-shot scenarios. It tends to achieve higher gains when the strength s is relatively large, i.e., {0.5, 0.7, 0.9 }, where the foreground has been edited to match the tar- get class and the background retains similarities to the reference image. 2. Among the Real-X methods, distillation tends to be more effective than the augmentation method when the shot0 0.1 0.3 0.5 0.7 0.9 1.0 s 16 18 20 22 24 26Acc (%) Diff-Mix Diff-Aug Diff-Gen Real-Mix Real-Aug Real-Gen Baseline (a) 1-shot 0 0.1 0.3 0.5 0.7 0.9 1.0 s 50 52 54 56 58 60Acc (%)  (b) 5-shot 0 0.1 0.3 0.5 0.7 0.9 1.0 s 66 67 68 69 70 71Acc (%)  (c) 10-shot 0 0.1 0.3 0.5 0.7 0.9 1.0 s 80.0 80.5 81.0 81.5 82.0 82.5 83.0Acc (%)  (d) All-shot Figure 6. Few-shot classification results on CUB. number is low, but the trend reverses as the shot num- ber increases (compare Real-Gen with Real-Aug). Real- Gen’s samples even show less effective than Real-Aug (s = 1.0) under the all-shot case 3. This indicates that the importance of faithfulness in the trade-off between faithfulness and diversity increases with the shot num- ber. Additionally, Real-Mix exhibits consistent and sta- ble improvement over the other two methods. 3. Diff-Gen consistently outperforms Real-Gen under four scenarios. Notably, Real-Gen’s performance declines below that of the baseline as the shot numbers reach 10, showcasing the importance of the fine-tuning process which increases the faithfulness of synthetic samples. 4.2. Conventional Classification Experimental setting. To test whether Diff-Mix can fur- ther boost performance in a more challenging setting, i.e., under the all-shot scenario with high input resolution, we conduct conventional classification on five domain-specific datasets: CUB [61], Stanford Cars [27], Oxford Flowers [38], Stanford Dogs [26], and FGVC Aircraft [34]. Two backbones are employed: pretrained (ImageNet1K [49]) ResNet50 [18] with input resolution 448 × 448, and pre- trained (ImageNet21K) ViT-B/16 [13] with input resolu- tion 384 × 384. Label smoothing [36] is applied across all datasets with a confidence level of 0.9. For all expansion strategies, the expansion multiplier is 5 and the replacement probability p is 0.1. Besides, we use a randomized sampling strategy (s ∈ {0.5, 0.7, 0.9}) and a fixed γ (0.5, and this is specific to Diff-Mix) for all datasets. Comparison methods. We compare Diff-Mix with (1) Real-Filtering (RF) [19], a variation of Real-Gen that in- corporates clip filtering, (2) Real-Guidance (RG) [19], which augments the dataset using intra-class image trans- lation at low strength ( s = 0 .1), (3) Da-Fusion [58], a method that solely fine-tunes the identifier to personal- ize each class and employs randomized sampling strategy 3Real-Aug ( s = 1 .0) remains analogous to the reference image (slightly higher faithfulness compared to Real-Gen) because the discrete forward process cannot approximate the ideal normal distribution within a limited number of steps (T = 25). (s ∈ {0.25, 0.5, 0.75, 1.0}), and non-generative augmenta- tion methods (4) CutMix [66] and (5) Mixup [68]. Main results. We show the classification accuracy for dif- ferent data expansion strategies in Table 1, our observa- tions can be summarized: (1) Diff-Mix consistently demon- strates stable improvements across the majority of settings. Its average performance gain across the five datasets ex- ceeds that of baselines employing intra-class augmentation methods (RG and Da-fusion), distillation method (RF), and non-generative data augmentation techniques (CutMix and Mixup). (2) Real-filtering, analogous to the discussion of Real-Gen, exhibits performance degradation on most datasets due to the distribution gap. (3) The combined use of Diff-Mix and CutMix often yields better performance gains. This is attributed to the distinct enhancement mech- anisms of the two methods, i.e., vicinal risk minimization [11, 68] and foreground-background disentanglement. (4) Diff-Mix does not exhibit significant performance improve- ment in the dog dataset. We attribute this lack of improve- ment to the complexity of the dog dataset, which often con- tains multiple subjects in a single image, impeding effective foreground editing (refer to the SMs for visual examples). 4.3. Long-Tail Classification Experiment setting. Following the settings of previ- ous long-tail dataset constructions [9, 32, 39], we create two domain-specific long-tail datasets, CUB-LT [50] and Flower-LT. The imbalance factor controls the exponential distribution of the imbalanced data, where a larger value in- dicates a more imbalanced distribution. To leverage gen- erative models for long-tail classification, we adopt the approach of SYNAuG [65], which uniformize the imbal- anced real data distribution using synthetic data. Transla- tion strength s (0.7) and γ (0.5) are fixed for both Diff-Mix and Real-Mix. Comparison methods. We compare Diff-Mix with Real- Mix, Real-Gen, the non-generative CutMix-based oversam- pling approach CMO [39], and its enhanced variant with two-stage deferred re-weighting [9] (CMO+DRW). Main results. We present the long-tail classification re-Backbone Aug. Method FT Strategy Dataset CUB Aircraft Flower Car Dog Avg ResNet50@448 - - 86.64 89.09 99.27 94.54 87.48 91.40 Cutmix[66] - 87.23 89.44 99.25 94.73 87.59 91.65 +0.25 Mixup[68] - 86.68 89.41 99.40 94.49 87.42 91.48 +0.08 Real-filtering [19] † % 85.60 88.54 99.09 94.59 87.30 91.22 −0.18 Real-guidance [19] † % 86.71 89.07 99.25 94.55 87.40 91.59 +0.19 Da-fusion [58] † TI 86.30 87.64 99.37 94.69 87.33 91.07 −0.58 Diff-Mix TI + DB 87.16 90.25 99.54 95.12 87.74 91.96+0.56 Diff-Mix + Cutmix TI + DB 87.56 90.01 99.47 95.21 87.89 92.03+0.63 ViT-B/16@384 - - 89.37 83.50 99.56 94.21 92.06 91.74 Cutmix[66] - 90.52 83.50 99.64 94.83 92.13 92.12+0.38 Mixup[68] - 90.32 84.31 99.73 94.98 92.02 92.27 +0.53 Real-filtering [19] † % 89.49 83.07 99.36 94.66 91.91 91.69 −0.05 Real-guidance [19] † % 89.54 83.17 99.59 94.65 92.05 91.80 +0.06 Da-fusion [58] † TI 89.40 81.88 99.61 94.53 92.07 91.50 −0.24 Diff-Mix TI + DB 90.05 84.33 99.64 95.09 91.99 92.22+0.48 Diff-Mix + Cutmix TI + DB 90.35 85.12 99.68 95.26 91.89 92.46+0.72 Table 1. Conventional classification in six fine-grained datasets. ‘†’ indicates our reproduced results using SD. Method IF=100 50 10Many Medium Few All CE 79.11 64.28 13.48 33.65 44.82 58.13 CMO [39] 78.32 58.57 14.78 32.94 44.08 57.62 CMO + DRW [10] 78.97 56.36 14.66 32.57 46.43 59.25 Real-Gen 84.88 65.23 30.68 45.86 53.43 61.42 Real-Mix (s=0.7)84.63 66.34 34.44 47.75 55.67 62.27 Diff-Mix (s=0.7)84.07 67.79 36.55 50.35 58.19 64.48 Table 2. Long-tail classification in CUB-LT. Method IF=100 50 10Many Medium Few All CE 99.19 94.95 58.18 80.43 90.87 95.07 CMO [39] 99.25 95.19 67.45 83.95 91.43 95.19 CMO+ DRW [10]99.97 95.06 67.31 84.07 92.06 95.92 Real-Gen 98.64 95.55 66.10 83.56 91.84 95.22 Real-Mix (s=0.7)99.87 96.26 68.53 85.19 92.96 96.04 Diff-Mix (s=0.7)99.25 96.98 78.41 89.46 93.86 96.63 Table 3. Long-tail classification in Flower-LT. sults for CUB-LT in Table 2 and Flower-LT in 3. The ob- servations are as follows: (1) Generative approaches ex- hibit superior performance in tackling imbalanced classi- fication issues compared to CutMix-based methods (CMO and CMO+DRW). (2) Real-Mix surpasses Real-Gen in per- formance across various imbalance factors in two datasets. This indicates that tail classes can benefit from the en- hanced diversity by leveraging the visual context of major- ity classes. (3) Diff-Mix generally achieves the best per- formance among the compared strategies, especially at the low-shot case, highlighting the importance of fine-tuning. 4.4. Background Robustness In this section, we aim to evaluate Diff-Mix’s robustness to background shifts, specifically, whether synthesizing more diverse samples can improve the classification model’s gen- eralizability when the background is altered. To achieve Group Base. CutMix DA-fusion Diff-AugDiff-Mix (waterbird, water)59.50 62.46 60.90 61.83 63.83 (waterbird, land)56.70 60.12 58.10 60.12 63.24 (landbird, land)73.48 73.39 72.94 73.04 75.64 (landbird, water)73.97 74.72 72.77 73.52 74.36 Avg. 70.19 71.23 69.90 70.28 72.47 Table 4. Classification results across four groups in Waterbird [48]. Waterbird is an out-of-distribution dataset for CUB, crafted by segmenting CUB’s foregrounds and paste them into the scene images from Places [70]. The constructed dataset can be divided into four groups based on the composition of foregrounds (water- bird and landbird) and backgrounds (water and land) . this, we utilize an out-of-distribution test set for CUB, namely Waterbird [48]. We then perform inference on the whole Waterbird set using classifiers that have been trained on either the original CUB dataset or its expanded varia- tions. In Table 4, we present the classification accuracies across the four groups and compare Diff-Mix with other intra-class methods (Da-fusion and Diff-Aug) as well as CutMix. We observe that Diff-Mix generally outperforms its counterparts and achieves a significant performance im- provement (+6.5%) in the challenging counterfactual group (waterbirds with land backgrounds). It is important to high- light that the background scenes in the Waterbird dataset are novel to CUB, requiring the classification model to have a stronger perceptual capability for the images’ foregrounds. 4.5. Discussion In this section, we address Q2 by examining the impact of size and diversity on synthetic data. Furthermore, we per- form an ablation study to assess the effects of fine-tuning strategies and training hyperparameters of Diff-Mix, which is aimed at answering Q3. Unless specified otherwise, our discussions are based on experiments conducted using CUB2X 4X 6X 8X 10X Synthetic data size 81 82 83Acc (%) +1.14 +1.33 +1.48 +1.55 +1.87 Baseline (a) 1 5 10 50 200 Reference class number 81 82 83Acc (%) +0.14 +0.70 +1.08 +1.09 +1.29 Baseline (b) Figure 7. Comparison of results across various (a) synthetic data sizes and (b) numbers of referable classes for each target class. with ResNet50, where inputs are resized to 224 × 224. Size and diversity of synthetic Data. The relationship be- tween performance gain and the size of synthetic data is depicted in Fig. 7 (a), where a classification model was trained with synthetic data of varying sizes. We observe a monotonically increasing trend as the multiplier for the syn- thetic dataset ranges from 2 to 10. Ideally, the combination choices of (xi, yj) are in the order of N|Dtrain| (N = 200 for CUB). Furthermore, we limit the number of referable classes for each target class, which means the number of referable backgrounds decreases, resulting in a synthetic dataset of relatively lower diversity. The results are shown in Fig. 7 (b), and we observe a consistent improvement in performance as the number of referable classes increases. These results consistently underscore the critical role of background diversity introduced by Diff-Mix. Impact of fine-tuning strategy. Here we compare three different fine-tuning strategies: TI, DB, and the combined TI+DB. All strategies share the same fine-tuning hyperpa- rameters and training steps (35000). To evaluate the distri- bution gap, we compute the FID score [20] of synthesized images (Diff-Gen) with the training set. As illustrated in Table 5, we observe that both TI+DB and TI have lower FID scores than DB. This can be attributed to the fact that semantic proximity impedes the convergence process. Ad- ditionally, while using TI alone results in a relatively low FID score, the improvements in performance are limited. This limitation stems from TI’s inability to accurately re- construct detailed concept (foreground) information, as it is primarily fine-tuned at the semantic level [28]. Annotation function. In this section, we discuss the im- pact of the choices of the translation strength s and the non-linear factor γ in Eq. 3. As shown in Table 6, we observe that as the translation strength decreases, the opti- mal value for γ also decreases, which underscores the non- linearity of Diff-Mix. The comparison between the 5-shot and all-shot settings indicates that the model tends to pre- fer a more diverse synthetic dataset when the number of training shots is small ( s = 0.9 for 5-shot, s = 0.7 for all- shot). Besides, a larger confidence in the target class is pre- Baseline TI DB TI + DB 5-shot FID (Diff-Gen) - 18.26 19.55 18.43 Acc. (Diff-Mix) 50.90 57.64 56.11 59.41 All-shot FID (Diff-Gen) - 14.13 14.64 13.99 Acc. (Diff-Mix) 81.60 81.86 81.99 82.85 Table 5. Comparison of distribution gap and classification accu- racy across three fine-tuning strategies. TI solely fine-tunes the identifier, and DB solely fine-tunes the U-Net, and TI+DB. γ 5-shot All-shot s = 0.5 s = 0.7 s = 0.9 s = 0.5 s = 0.7 s = 0.9 1.5 -4.50 -0.31 +10.30 -1.08 +0.92 +0.90 1.0 -2.31 +2.99 +10.79 +0.25 +1.14 +0.90 0.5 +2.35 +8.44 +11.01 +0.92 +1.30 +0.86 0.3 +3.94 +9.41 +11.15 +0.97 +1.24 +0.69 0.1 +6.18 +9.86 +10.84 +0.50 +0.88 +0.84 0.0 +5.25 +9.41 +11.06 +0.38 +0.63 +0.54 Table 6. Comparison of performance gain across various γ and translation strength s. Lower γ indicates a higher confidence over target class, e.g. (γ = 0.1, s= 0.7) results in 0.04yi + 0.96yj and (γ = 0.5, s= 0.7) results in 0.16yi + 0.84yj. ferred when the shot number is small ( γ = 0.1 for 5-shot, γ = 0.5 for all-shot). A possible explanation is that the all- shot setting is less tolerant towards unrealistic images, as discussed in Section 6. Empirically, we recommend choos- ing a higher translation strength ( s ∈ 0.5, 0.7, 0.9) and a smaller γ (γ ∈ 0.1, 0.3, 0.5) as a conservative option. 5. Conclusion In this work, we investigate two pivotal aspects, faithfulness and diversity, that are critical for the current state-of-the-art text-to-image generative models to enhance image classifi- cation tasks. To achieve a more effective balance between these two aspects, we propose an inter-class augmentation strategy that leverages Stable Diffusion. This method en- ables generative models to produce a greater diversity of samples by editing images from other classes and shows consistent performance improvement across various classi- fication tasks. 6. Acknowledgement This research is mainly supported by the National Natural Science Foundation of China (92270114). This work is also partially supported by the National Key R&D Program of China under Grant 2021ZD0112801.References [1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. 2 [2] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo- hammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. 1, 2 [3] Haoyue Bai, Ceyuan Yang, Yinghao Xu, S-H Gary Chan, and Bolei Zhou. Improving out-of-distribution robustness of classifiers via generative interpolation. arXiv preprint arXiv:2307.12219, 2023. 2 [4] Hritik Bansal and Aditya Grover. Leaving reality to imag- ination: Robust classification via generated datasets. arXiv preprint arXiv:2302.02503, 2023. 1, 2 [5] Christopher Beckham, Sina Honari, Vikas Verma, Alex M Lamb, Farnoosh Ghadiri, R Devon Hjelm, Yoshua Bengio, and Chris Pal. On adversarial mixup resynthesis. Advances in neural information processing systems, 32, 2019. 2 [6] Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, and Fahad Shahbaz Khan. Person image synthesis via de- noising diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5968–5976, 2023. 2 [7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 1 [8] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In- structpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392–18402, 2023. 3 [9] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label- distribution-aware margin loss. Advances in neural informa- tion processing systems, 32, 2019. 6 [10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label- distribution-aware margin loss. Advances in neural informa- tion processing systems, 32, 2019. 7 [11] Olivier Chapelle, Jason Weston, L ´eon Bottou, and Vladimir Vapnik. Vicinal risk minimization. Advances in neural in- formation processing systems, 13, 2000. 6 [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural informa- tion processing systems, 34:8780–8794, 2021. 1 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6 [14] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual ob- ject classes challenge: A retrospective.International Journal of Computer Vision, 111(1):98–136, 2015. 1, 2, 5 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3 [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 6 [19] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. 2, 6, 7, 3 [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. Advances in neural information processing systems , 30, 2017. 8 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. 3 [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion mod- els. arXiv preprint arXiv:2210.02303, 2022. 2 [23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 4 [24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017. 3 [25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 4401–4410, 2019. 1, 2 [26] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, 2011. 6, 2 [27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on com- puter vision workshops, pages 554–561, 2013. 6, 2 [28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customizationof text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1931–1941, 2023. 2, 3, 8 [29] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203 , 2023. 2 [30] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat seng Chua. Invariant grounding for video question answering. In CVPR, 2022. 2 [31] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. arXiv preprint arXiv:2305.08891, 2023. 1 [32] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2537–2546, 2019. 6 [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems , 35:5775–5787, 2022. 3 [34] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classi- fication of aircraft. arXiv preprint arXiv:1306.5151 , 2013. 6, 2 [35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equa- tions. arXiv preprint arXiv:2108.01073, 2021. 3 [36] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural in- formation processing systems, 32, 2019. 6 [37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.arXiv preprint arXiv:2112.10741, 2021. 1, 2 [38] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & im- age processing, pages 722–729. IEEE, 2008. 6, 2 [39] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The majority can help the minor- ity: Context-rich minority oversampling for long-tailed clas- sification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6887– 6896, 2022. 6, 7, 3 [40] Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. Conceptbed: Evaluating concept learning abili- ties of text-to-image diffusion models. arXiv preprint arXiv:2306.04695, 2023. 3 [41] Judea Pearl. Causal inference in statistics: An overview. 2009. 5 [42] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch¨olkopf. Controlling text-to-image diffusion by orthogo- nal finetuning. Advances in Neural Information Processing Systems, 36, 2024. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 5, 1 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image syn- thesis with latent diffusion models, 2021. 1 [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. 3 [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500– 22510, 2023. 2, 3 [47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015. 2 [48] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. arXiv preprint arXiv:1911.08731, 2019. 7 [49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 1, 2, 6 [50] Dvir Samuel, Yuval Atzmon, and Gal Chechik. From gener- alized zero-shot learning to long-tail with class descriptors. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 286–295, 2021. 6 [51] Veit Sandfort, Ke Yan, Perry J Pickhardt, and Ronald M Summers. Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct seg- mentation tasks. Scientific reports, 9(1):16884, 2019. 2 [52] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning trans- ferable representations from synthetic imagenet clones. In CVPR 2023–IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1 [53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. Laion-5b: An open large-scale dataset for trainingnext generation image-text models. Advances in Neural In- formation Processing Systems, 35:25278–25294, 2022. 2 [54] Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, and Clinton Fookes. Boosting zero-shot classifica- tion with synthetic data diversity via stable diffusion. arXiv preprint arXiv:2302.03298, 2023. 2 [55] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 , 2022. 2 [56] Shikun Sun, Longhui Wei, Zhicai Wang, Zixuan Wang, Jun- liang Xing, Jia Jia, and Qi Tian. Inner classifier-free guid- ance and its taylor expansion for diffusion models. In The Twelfth International Conference on Learning Representa- tions, 2023. 2 [57] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to- image models make strong visual representation learners. arXiv preprint arXiv:2306.00984, 2023. 1, 2 [58] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023. 2, 5, 6, 7, 3 [59] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1921–1930, 2023. 3 [60] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. 3 [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per- ona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 5, 6, 2 [62] Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, and Xiangnan He. Bi-directional distribution align- ment for transductive zero-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19893–19902, 2023. 2 [63] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022. 2 [64] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls for stylegan image genera- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pages 12863–12872, 2021. 3 [65] Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Nayeong Kim, Suha Kwak, and Tae-Hyun Oh. Exploiting synthetic data for data imbalance problems: Baselines from a data per- spective. arXiv preprint arXiv:2308.00994, 2023. 6 [66] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international con- ference on computer vision, pages 6023–6032, 2019. 2, 6, 7 [67] An Zhang, Wenchang Ma, Xiang Wang, and Tat seng Chua. Incorporating bias-aware margins into contrastive loss for collaborative filtering. In NeurIPS, 2022. 2 [68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 6, 7 [69] Chenyu Zheng, Guoqiang Wu, and Chongxuan Li. Toward understanding generative data augmentation. Advances in Neural Information Processing Systems, 36, 2024. 2 [70] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017. 7 [71] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision , pages 2223– 2232, 2017. 3Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model Supplementary Material (a) Filtered samples  (b) Preserved samples  Figure 8. Examples of (a) filtered samples with the lowest 10% confidence scores and (b) preserved samples in the CUB dataset. 7. Appendix This appendix is organized as follows: • In Sec. 7.1, we elaborate on the details of data cleaning design for Diff-Mix. • In Sec. 7.2, additional visualizations are presented, in- cluding the visualization of attention maps and failure ex- amples of complex datasets. • In Sec. 7.3, few-shot classification results on a general dataset Pascal is provided [14]. • In Sec. 7.4 and Sec. 7.5, implementation details and la- tency considerations are presented, respectively. 7.1. Data-Cleaning Strategy Due to the inherent differences in contour and size between the two classes, there is a higher risk of producing less real- istic images during inter-class editing. We employ a simple data cleaning strategy that utilizes CLIP [43] 4 as the filter- ing criterion. Specifically, we construct a positive caption, ”a photo with a [metaclass] on it”, and a negative caption, ”a photo without a [metaclass] on it”, and evaluate the syn- thetic data’s confidence score towards the positive caption. We filter out the 10% of samples with the lowest confidence scores (we do not synthesize an additional 10% samples af- ter data cleaning), and a subset of the filtered samples is displayed in Fig. 8. The preserved samples constitute the 4https://huggingface.co/openai/clip-vit-base-patch32 synthetic dataset that participates in the training process of downstream classification tasks. 7.2. Visualizations Visualizations of attention maps. In Section 3.4, we have shown that Diff-Mix can perform foreground editing while preserving most of the layout of the reference im- age. To support the claim, we provide evidence that SD can offer weak segmentation through textual conditions and achieve realistic foreground editing. We present visualiza- tions of attention maps in Figure 9 for different datasets. The identifier, class descriptor (e.g., “ bird”, “car”) and the “<eot>” token, which contains the global semantic in- formation, tend to attend to the foreground part in the refer- ence image. For example, the mentioned tokens primarily emphasize the bird rather than the tree branches (refer to Row 1 in the figure). This suggests that textual guidance, can offer a robust foreground prior, aiding in effective fore- ground editing at intermediate strengths. We posit that this characteristic ensures the generation of challenging samples where the foreground is replaced by the target concept when employing Diff-Mix for inter-class editing. Visualizations on more datasets. In Fig. 12, we illus- trate the editing process of Diff-Mix with varying strength s ∈ {0.1, 0.3, 0.5, 0.7, 0.9, 1.0} across five datasets. It is worth noting that for strength s = 1 , the translated im- ages still exhibit a certain degree of similarity to the ref- erence images. This phenomenon may be attributed to the last time-step not guaranteeing a zero signal-to-noise ratio, preserving the style and layout of the reference images [31]. Particularly, when the foreground is distinct against a sim- ple background, Diff-Mix tends to generate high-quality in- terpolated images. We also observe that for more complex datasets, such as Stanford Dogs, where the foreground is less clear, and there are multiple concepts in a single im- age, unrealistic images tend to be generated, as seen in Fig. 13 (a) and (b). For general dataset Pascal [14], the dramatic differences in contour and size between two dis- tinct classes lead to the generation of more unrealistic im- ages (e.g., “ bus” −→ “cat”), especially at intermediate strengths (e.g., 0.7), as seen in Fig 13 (c) and (d). Real-Gen versus Diff-Gen. To illustrate the distribution gap between domain-specific datasets and the pre-trained T2I model, as well as to demonstrate how fine-tuning can significantly mitigate this gap, we present a comparison in Fig. 14. It is worth noting that Real-Gen sometimes fails to generate correct concepts based on the terminol- ogy name of the target class (see “ photo of a chuck<sot> a photo of a [Vtar] bird <eot> <pad> <pad> <sot> a photo of a [Vtar] car <eot> <pad> <pad> <sot> a photo of a [Vtar] aircraft <eot> <pad> <pad> <sot> a photo of a [Vtar] dog <eot> <pad> <pad> <sot> a photo of a [Vtar] flower <eot> <pad> <pad> Reference image Figure 9. Visualizations of attention maps are shown in different rows for various datasets: CUB (Row 1) [61], Stanford Cars (Row 2) [27], FGVC Aircraft (Row 3) [34], Stanford Dogs (Row 4) [26], and Oxford Flowers (Row 5) [38]. These attention maps were generated during inter-class editing using Diff-Mix. will widow” in panel (a)). Diff-Gen tends to generate more faithful outputs, it is noted that the majority of the generated images exhibit a similar layout and closely re- semble the training samples. This resemblance is especially pronounced for those training samples characterized by a prominent foreground and a simple background. Real-Mix versus Diff-Mix. In Fig. 15, we compare the generated samples between Real-Mix and Diff-Mix. We observe that, by conditioning on the reference image, Real- Mix accurately captures the semantic meaning of the termi- nology name (see “ chuck will widow” in panel (a)). This feature of Real-Mix is consistent with its superior per- formance in few-shot classification, as depicted in Fig. 6. Additionally, we observe that Diff-Mix achieves more pre- cise foreground editing (refer to Fig. 15 (c) and (d)). This enhanced accuracy can be attributed to the class descriptor maintaining its focus on the semantic content without being diverted to other extraneous information. 7.3. Experiments Few-shot classification in Pascal. In Sec. 7.2, We have demonstrated that inter-class edit- ing for general datasets tends to produce unrealistic im- 0 0.1 0.3 0.5 0.7 0.9 1.0 s 67 68 69 70 71 72 73 74 75Acc (%) Diff-Mix Diff-Aug Diff-Gen Real-Mix Real-Aug Real-Gen Baseline (a) 5-shot 0 0.1 0.3 0.5 0.7 0.9 1.0 s 84.8 85.0 85.2 85.4 85.6 85.8 86.0 86.2Acc (%)  (b) all-shot Figure 10. 5-shot and all-shot classification results in Pascal. ages due to the visual gaps between two classes. Here, we present 5-shot and all-shot classification results in Figure 10 for different expansion strategies on the general dataset Pascal [14]. Originally, Pascal is an object class recognition dataset containing 11,530 images and 6,929 object segmen- tation masks. We construct it into a classification dataset, following the setting of Da-fusion [58], resulting in a train- ing split of 1,464 and a validation split of 1,449 for 20 gen- eral classes ( e.g., cat and boat). The main observation is that inter-class augmentation tends to be less effective for this general dataset, especially as the shot number increases (compare X-Aug and X-Mix in the figure). The effective-hyperparameter DB TI TI+DB Base Model Stable Diffusion-v1.5 Stable Diffusion-v1.5 Stable Diffusion-v1.5 Optimized U-Net(LORA) [Vi] U-Net(LORA) + [Vi] Optimization Steps 35000 35000 35000 Batchsize 8 8 8 Input Resolution 512× 512 512 × 512 512 × 512 Learning Rate 5e-5 5e-5 5e-5 Placeholder Token - [Vi] [V i] LORA Rank 10 - 10 # if inference steps (T) 25 25 25 Guidance Scale 7.5 7.5 7.5 Noise Scheduler DPMsolver++[33] DPMsolver++[33] DPMsolver++[33] Table 7. Hyperparameters. This tables summarizes the hyperparameter settings of different fine-tuning strategies. Dataset # of classes # of training # of val. Source CUB 200 5994 5794 Huggingface.co FGVC Aircraft 100 3334 3333 Huggingface.co Oxford Flowers102 4070 4119 Huggingface.co Stanford Dogs 120 12000 8580 vision.stanford.edu Stanford Cars 196 8144 8041 Huggingface.co Table 8. Statistics of datasets. ness of fine-tuning also decreases, with Real-Gen consis- tently outperforming Diff-Gen. This suggests that the pre- trained SD is capable of generating sufficiently diverse and faithful samples for these coarse concepts. Diff-Mix ex- cels in handling domain-specific scenarios, where smaller differences in contour and layout between two classes are presented. 7.4. Implementation Details. Diff-Mix. Diff-Mix comprises two stages: the fine-tuning stage and the sampling stage. The implementation details of Diff-Mix for three different fine-tuning strategies are de- picted in Table 7. Note that our fine-tuning strategy heavily relies on the diffuser [60] repository. For DB and TI+DB, we only fine-tune the residual LORA matrices in atten- tion modules in the U-Net. Please note that in the original Dreambooth [46] paper, an unlearnable identifier was intro- duced to represent user-specific concepts in concept learn- ing. However, in our implementation, we have opted not to use the identifier and have implemented it as a straightfor- ward fine-tuning of text-to-image models. All fine-tuning and sampling processes are conduct on 4 RTX3090 GPUs. Datasets. We list the statistics of the datasets involved in our experiments in Table 8. Conventional classification. The hyperparameter settings for the CUB dataset are presented in Table 9. To repro- duce Real-filtering (RF) [19], a subset is derived from Real- Gen through data cleaning, as detailed in Section 7.1. Real- guidance (RG) [19] augments the dataset with low-strength intra-class editing, akin to Real-Aug with a strength param- hyperparameters ResNet50 ViT-B/16 Source torchvision torchvision # of parameters 25.5M 86.6M Pre-trained ImageNet1K ImageNet21K Fine-tuned - ImageNet1K Input Resolution 448 × 448 384 × 384 Batchsize 64 32 Epochs 128 100 Optimizer SGD SGD Learning Rate 0.02 0.001 Momentum 0.9 0.9 Weight Decay 5e-5 5e-5 Label Smoothing 0.9 0.9 Table 9. hyperparameters. This table summarizes the hyperparam- eter settings for CUB using two visual backbones in our conven- tional classification task. eter s = 0 .1. For the replication of Da-fusion [58], we fine-tune the synthetic data (SD) using our TI strategy over 35,000 steps, with translation strengths randomly selected from the set 0.25, 0.5, 0.75, 1.0. For CutMix and Mixup, the weight decay is 1 × 10−5, and the mixup ratios are set to 0.1 and 0.3, respectively. Few-shot classification. The few-shot classification is con- ducted on CUB with varying shot numbers: 1, 5, 10, and all. The comparison methods encompass: (1) inter-class aug- mentation strategies, namely Diff-Mix and Real-Mix, (2) intra-class augmentation strategies, namely Diff-Aug and Real-Aug, and (3) distillation-based methods, Diff-Gen and Real-Gen. The backbone model used is ResNet50 with an input resolution of 2242. We employ the same hyperparam- eters as in the conventional setting, as detailed in Table 9, albeit with a larger batch size (256) and a higher learning rate (0.05). All experiments are conducted with three trials, and the average results are reported. Long-tail classification. Thanks to the authors of CMO [39], the reproduced long-tail results are built upon its open-Dataset # of classes # of training Imbalance Factor (IF) CUB-LT 200 {1242, 1798, 2238} { 100,50,10} Flower-LT 102 {847, 1238, 1532} { 100,50,10} Table 10. Statistics of long-tail datasets CUB-LT and Flower-LT. source git repository 5. To construct the imbalanced dataset, an imbalance factor is introduced to control the imbalance level. The imbalance factor ρ is defined as ρ = maxk{nk} mink{nk} , where nk is the number of samples in thek-th class. Specif- ically, given a normal dataset, we first sort the classes based on the number of images within classes in descending or- der, and use k′ to denote the sorted class index. A subset of images is randomly sampled from each class to achieve the desired imbalance, ensuring that the number of images for each class corresponds to the calculated target. The number of sampled images is determined by, nk′ = max   ¯n × \u00121 ρ \u0013 k N−1 , 1 ! (4) where ¯n is the averaged number of images for each class, and N is the total number of classes. The statistics of con- structed CUB-LT and Flower-LT datasets can be found in Table 10. To uniformize the distribution of imbalanced real data, we first fix the number of iterative samples within each epoch and replace real samples with synthetic data with a 50% probability. Note that the synthetic data conforms to the distribution specified by Eq. 4 with reversed class indices, thereby generating more synthetic images for tail classes. We maintain a constant number of training epochs and learning rate for both synthesis-free and synthesis- based approaches to ensure a fair comparison. We further present accuracy results for three distinct subsets when IF is 100: Many-shot classes (classes with over 20/30 train- ing samples for CUB-LT/Flower-LT), medium-shot classes (classes with 5-20/10-30 samples for CUB-LT and Flower- LT), and few-shot classes (classes with fewer than 5/10 sam- ples for CUB-LT/Flower-LT). 7.5. Latency Compared to non-generative augmentation methods, Diff- Mix’s implementation incurs additional computational overhead during fine-tuning and data sampling. For in- stance, when working with the CUB dataset, which con- tains approximately 6,000 training samples, the fine-tuning process is completed in about 3 hours. This duration is achieved using an input resolution of 512 × 512 on 4 NVIDIA RTX 3090 GPUs with a total batch size of 8. During sampling, synthetic samples are generated at the same resolution with a total of T = 25 reverse steps. The throughput across various translation strengths is evaluated 5https://github.com/naver-ai/cmo 0.2 0.4 0.6 0.8 1.0 Translation strength (s) 2500 5000 7500 10000 12500 15000 17500 20000Images / GPU-hour  Throughput vs Strength Real-Gen/Diff-Gen Figure 11. Sampling throughput across various translation strengths in a single RTX 3090 GPU. Method Translation strength s Images per GPU-hour Real-filtering ∈{1.0} 2,957 Real-guidance ∈{0.1} 20,502 Da-fusion ∈{0.25,0.5,0.75,1.0 } 4,952 Diff-Mix ∈{0.5,0.7,0.9 } 4,179 Table 11. Comparison of sampling throughput of different expansion strategies. in Fig. 11, and a throughput comparison with other syn- thesis strategies is provided in Table 11. While Diff-Mix is more efficient than generating data from scratch, it is less so than low-strength editing (e.g., Real-guidance). For gener- ating synthetic data for the CUB dataset with a 5x multiplier (resulting in approximately 30,000 images), the process re- quires roughly 2.5 hours using 4 NVIDIA RTX 3090 GPUs. 8. Limitations Our inter-class augmentation method shows less effective when applied to general datasets that encompass a broad spectrum of concepts. We are optimistic, however, that inte- grating an image inpainting strategy or confining Diff-Mix to operate among adjacent classes could address this lim- itation. Moreover, the current annotation strategy is deter- mined empirically and lacks a robust theoretical foundation, which may limit the generalizability of the strategy.0.1 0.3 0.5 0.7 0.9 1.0 Target class Diff-Mix Figure 12. Examples of image generated using Diff-Mix with varying translation strengths in CUB (Row 1), Stanford Cars (Row 2), FGVC Aircraft (Row 3), Stanford Dogs (Row 4) , Oxford Flowers (Row 5). 0.1 0.3 0.5 0.7 0.9 1.0 (a) “Maltese dog ” -> “Irish wolfhound ”   (b) “Saluki ” -> “bloodhound ”   (c) “boat ” -> “car ”   (d) “bus ” -> “cat ”       Target class Diff-Mix Figure 13. Failure examples generated using Diff-Mix with varying translation strengths are shown in panels (a) and (b) for the complex dataset Stanford Dogs, and in panels (c) and (d) for the general dataset Pascal [14].(a) “photo of a chuck will widow ”  Real-Gen (pre-trained)     Diff-Gen (fine-tuned)             (b) “photo of a Pied billed Grebe ”  (c) “photo of a Challenger 600 ”  (d) “photo of a A330-200 ”  Target class Figure 14. Examples of image generated using Real-Gen and Diff-Gen. The prompts are formatted as “photo of a [terminology name]” for Real-Gen and “photo of a [Vi] [metaclass]” for Diff-Gen. Panels (a) and (b) depict the samples of CUB dataset, while panels (c) and (d) depict the samples of FGVC Aircraft dataset.        Real-Mix (pre-trained)              Diff-Mix (fine-tuned)                   (a) “Bronzed Cowbird ” -> “chuck will widow ”   (b) “Bronzed Cowbird ” -> “Pied billed Grebe ”   (c) “acura tl type-s 2008 ” -> “bmw activehybrid 5 sedan 2012 ”   (d) “acura tl type-s 2008 ” -> “chevrolet tahoe hybrid suv 2012 ”   0.1 0.3 0.5 0.7 0.9 1.0 0.1 0.3 0.5 0.7 0.9 1.0 Target class Figure 15. Examples of image generated using Real-Mix and Diff-Mix with varying translation strengths. The prompts are formatted as “photo of a [terminology name]” for Real-Mix and “photo of a [Vi] [metaclass]” for Diff-Mix. Panels (a) and (b) depict the samples of CUB dataset, while panels (c) and (d) depict the samples of Stanford Car dataset.",
      "references": [
        "Data augmentation generative adversarial networks",
        "Synthetic data from diffusion models improves imagenet classification",
        "Improving out-of-distribution robustness of classifiers via generative interpolation",
        "Leaving reality to imagination: Robust classification via generated datasets",
        "On adversarial mixup resynthesis",
        "Person image synthesis via denoising diffusion model",
        "Large scale gan training for high fidelity natural image synthesis",
        "Instructpix2pix: Learning to follow image editing instructions",
        "Learning imbalanced datasets with label-distribution-aware margin loss",
        "Vicinal risk minimization",
        "Diffusion models beat gans on image synthesis",
        "An image is worth 16x16 words: Transformers for image recognition at scale",
        "The pascal visual object classes challenge: A retrospective",
        "An image is worth one word: Personalizing text-to-image generation using textual inversion",
        "Generative adversarial nets",
        "Deep residual learning for image recognition",
        "Is synthetic data from generative models ready for image recognition?",
        "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
        "Denoising diffusion probabilistic models",
        "Imagen video: High definition video generation with diffusion models",
        "Lora: Low-rank adaptation of large language models",
        "Image-to-image translation with conditional adversarial networks",
        "A style-based generator architecture for generative adversarial networks",
        "Novel dataset for fine-grained image categorization",
        "3d object representations for fine-grained categorization",
        "Multi-concept customization of text-to-image diffusion",
        "Your diffusion model is secretly a zero-shot classifier",
        "Invariant grounding for video question answering",
        "Common diffusion noise schedules and sample steps are flawed",
        "Large-scale long-tailed recognition in an open world",
        "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
        "Fine-grained visual classification of aircraft",
        "Sdedit: Guided image synthesis and editing with stochastic differential equations",
        "When does label smoothing help?",
        "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
        "Automated flower classification over a large number of classes",
        "The majority can help the minority: Context-rich minority oversampling for long-tailed classification",
        "Conceptbed: Evaluating concept learning abilities of text-to-image diffusion models",
        "Causal inference in statistics: An overview",
        "Controlling text-to-image diffusion by orthogonal finetuning",
        "Learning transferable visual models from natural language supervision",
        "High-resolution image synthesis with latent diffusion models",
        "U-net: Convolutional networks for biomedical image segmentation",
        "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
        "Imagenet large scale visual recognition challenge",
        "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
        "Photorealistic text-to-image diffusion models with deep language understanding",
        "From generalized zero-shot learning to long-tail with class descriptors",
        "Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks",
        "Fake it till you make it: Learning transferable representations from synthetic imagenet clones",
        "Laion-5b: An open large-scale dataset for training next generation image-text models",
        "Boosting zero-shot classification with synthetic data diversity via stable diffusion",
        "Make-a-video: Text-to-video generation without text-video data",
        "Inner classifier-free guidance and its taylor expansion for diffusion models",
        "Stablerep: Synthetic images from text-to-image models make strong visual representation learners",
        "Effective data augmentation with diffusion models",
        "Plug-and-play diffusion features for text-driven image-to-image translation",
        "Diffusers: State-of-the-art diffusion models",
        "The caltech-ucsd birds-200-2011 dataset",
        "Bi-directional distribution alignment for transductive zero-shot learning",
        "Novel view synthesis with diffusion models",
        "Stylespace analysis: Disentangled controls for stylegan image generation",
        "Exploiting synthetic data for data imbalance problems: Baselines from a data perspective",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "Incorporating bias-aware margins into contrastive loss for collaborative filtering",
        "Mixup: Beyond empirical risk minimization",
        "Toward understanding generative data augmentation",
        "Places: A 10 million image database for scene recognition",
        "Unpaired image-to-image translation using cycle-consistent adversarial networks"
      ],
      "meta_data": {
        "arxiv_id": "2403.19600v1",
        "authors": [
          "Zhicai Wang",
          "Longhui Wei",
          "Tan Wang",
          "Heyu Chen",
          "Yanbin Hao",
          "Xiang Wang",
          "Xiangnan He",
          "Qi Tian"
        ],
        "published_date": "2024-03-28T17:23:45Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies that existing generative and conventional data-augmentation techniques cannot simultaneously guarantee foreground faithfulness and background diversity for domain-specific image classification. Introduces Diff-Mix, an inter-class data-augmentation pipeline that (1) personalises a text-to-image diffusion model with combined Textual-Inversion and LoRA-based DreamBooth fine-tuning, and (2) performs controllable image-to-image translation between different classes to create counterfactual samples with mixed foregrounds and backgrounds. Demonstrates consistent accuracy gains over intra-class augmentation, pure distillation, and non-generative methods across few-shot, conventional, long-tail and background-shift scenarios.",
        "methodology": "1) Fine-tune Stable Diffusion v1.5 on each target dataset: replace class names with learnable identifiers [Vi] plus a meta-class token (e.g., \"bird\"), and jointly optimise these embeddings with low-rank (LoRA) updates of the U-Net (DreamBooth style) for 35k steps. 2) Inter-class image translation (Diff-Mix): insert a noisy reference image from class j at diffusion step ⌊sT⌋ and condition denoising with the prompt of class i to gradually morph the foreground while retaining background; strength s∈[0.1,1.0] controls the mix ratio. 3) Label the synthetic image with a non-linear mixture ỹ=(1−s^γ)yi+s^γ yj (γ<1). 4) Build a synthetic set (5× real set) with random strengths {0.5,0.7,0.9}, filter low-confidence samples using CLIP, and train classifiers jointly on real and synthetic images. 5) Diff-Mix can be combined with CutMix or other augmentation during classifier training.",
        "experimental_setup": "Datasets: CUB-200-2011 (fine-grained birds), Stanford Cars, FGVC Aircraft, Oxford Flowers-102, Stanford Dogs; few-shot variants (1/5/10-shot), long-tail variants CUB-LT & Flower-LT with imbalance factors 10,50,100; background robustness test on Waterbirds (OOD); additional test on Pascal VOC classes. Classifiers: ResNet-50 (ImageNet1K pre-train, 448 or 224 px) and ViT-B/16 (ImageNet-21K pre-train, 384 px). Baselines: vanilla diffusion distillation (Real-Gen), intra-class editing (Real-Aug, Diff-Aug), Real-Filtering & Real-Guidance, Da-Fusion, non-generative Mixup & CutMix, CMO/CMO+DRW for long-tail. Metrics: top-1 accuracy on validation/test splits, group accuracies (many/medium/few), FID between synthetic and real images, background-shift accuracy on four Waterbird groups. Implementation: 4×RTX3090, 512×512 synthesis with 25-step DPM-Solver++, throughput ≈4k images/GPU-hour for Diff-Mix.",
        "limitations": "1) Inter-class editing can generate unrealistic samples when foreground shapes differ greatly (e.g., general Pascal or multi-object dog images). 2) Annotation function hyper-parameters (s, γ) and data-cleaning thresholds are empirically chosen without theoretical justification. 3) Additional computation for diffusion fine-tuning and image generation (≈3 h fine-tune + 2.5 h sampling for CUB on 4×3090). 4) Method currently tuned for domain-specific, fine-grained datasets and shows limited gains on broad coarse-grained datasets. 5) Relies on pre-trained Stable Diffusion; performance depends on its coverage and may inherit its biases.",
        "future_research_directions": "1) Develop principled label-mixing and strength-scheduling strategies grounded in diffusion dynamics or causal theory. 2) Integrate object-mask guidance or inpainting to better handle classes with large shape disparity or multiple instances. 3) Explore class-graph or semantic similarity constraints to select suitable source–target pairs automatically. 4) Optimise efficiency via distillation or latent-space diffusion to cut generation cost. 5) Extend Diff-Mix to other tasks such as detection, segmentation, or multimodal retrieval, and to 3D or video domains.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks",
      "full_text": "Published as a conference paper at ICLR 2020 MIXUP INFERENCE : B ETTER EXPLOITING MIXUP TO DEFEND ADVERSARIAL ATTACKS Tianyu Pang∗, Kun Xu∗, Jun Zhu† Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua University; RealAI {pty17,xu-k16}@mails.tsinghua.edu.cn, dcszj@tsinghua.edu.cn ABSTRACT It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally unreasonable behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efﬁcient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants. 1 I NTRODUCTION Deep neural networks (DNNs) have achieved state-of-the-art performance on various tasks (Good- fellow et al., 2016). However, counter-intuitive adversarial examples generally exist in different domains, including computer vision (Szegedy et al., 2014), natural language processing (Jin et al., 2019), reinforcement learning (Huang et al., 2017), speech (Carlini & Wagner, 2018) and graph data (Dai et al., 2018). As DNNs are being widely deployed, it is imperative to improve model robustness and defend adversarial attacks, especially in safety-critical cases. Previous work shows that adversarial examples mainly root from the locally unstable behavior of classiﬁers on the data manifolds (Goodfellow et al., 2015; Fawzi et al., 2016; 2018; Pang et al., 2018b), where a small adversarial perturbation in the input space can lead to an unreasonable shift in the feature space. On the one hand, many previous methods try to solve this problem in the inference phase, by introducing transformations on the input images. These attempts include performing local linear transformation like adding Gaussian noise (Tabacof & Valle, 2016), where the processed inputs are kept nearby the original ones, such that the classiﬁers can maintain high performance on the clean inputs. However, as shown in Fig. 1(a), the equivalent perturbation, i.e., the crafted adversarial perturbation, is still δand this strategy is easy to be adaptively evaded since the randomness ofx0 w.r.t x0 is local (Athalye et al., 2018). Another category of these attempts is to apply various non-linear transformations, e.g., different operations of image processing (Guo et al., 2018; Xie et al., 2018; Raff et al., 2019). They are usually off-the-shelf for different classiﬁers, and generally aim to disturb the adversarial perturbations, as shown in Fig. 1(b). Yet these methods are not quite reliable since there is no illustration or guarantee on to what extent they can work. On the other hand, many efforts have been devoted to improving adversarial robustness in the training phase. For examples, the adversarial training (AT) methods (Madry et al., 2018; Zhang et al., 2019; Shafahi et al., 2019) induce locally stable behavior via data augmentation on adversarial examples. However, AT methods are usually computationally expensive, and will often degenerate model ∗Equal contribution. †Corresponding author. 1 arXiv:1909.11515v2  [cs.LG]  20 Feb 2020Published as a conference paper at ICLR 2020 !\"!\" #−\" %&% %' %(&%(Mixup Inference(Global Linear Transformation) !% %()%!)%(Local Linear Transformation !% %( %∗ !∗ %(∗ Non-Linear TransformationObserved inputsVirtual inputsThe processedinputs fed into classifiers (a) (b) (c) Figure 1: Intuitive mechanisms in the input space of different input-processing based defenses. xis the crafted adversarial example, x0 is the original clean example, which is virtual and unknown for the classiﬁers. δis the adversarial perturbation. performance on the clean inputs or under general-purpose transformations like rotation (Engstrom et al., 2019). In contrast, the mixup training method (Zhang et al., 2018) introduces globally linear behavior in-between the data manifolds, which can also improve adversarial robustness (Zhang et al., 2018; Verma et al., 2019a). Although this improvement is usually less signiﬁcant than it resulted by AT methods, mixup-trained models can keep state-of-the-art performance on the clean inputs; meanwhile, the mixup training is computationally more efﬁcient than AT. The interpolated AT method (Lamb et al., 2019) also shows that the mixup mechanism can further beneﬁt the AT methods. However, most of the previous work only focuses on embedding the mixup mechanism in the training phase, while the induced global linearity of the model predictions is not well exploited in the inference phase. Compared to passive defense by directly classifying the inputs (Zhang et al., 2018; Lamb et al., 2019), it would be more effective to actively defend adversarial attacks by breaking their locality via the globally linear behavior of the mixup-trained models. In this paper, we develop an inference principle for mixup-trained models, named mixup inference (MI). In each execution, MI performs a global linear transformation on the inputs, which mixups the input xwith a sampled clean example xs, i.e., ˜x= λx+ (1 −λ)xs (detailed in Alg. 1), and feed ˜xinto the classiﬁer as the processed input. There are two basic mechanisms for robustness improving under the MI operation (detailed in Sec. 3.2.1), which can be illustrated by simple geometric intuition in Fig. 1(c). One is perturbation shrinkage: if the input is adversarial, i.e., x= x0 + δ, the perturbation δwill shrink by a factor λafter performing MI, which is exactly the mixup ratio of MI according to the similarity between triangles. Another one is input transfer: after the MI operation, the reduced perturbation λδacts on random ˜x0. Comparing to the spatially or semantically local randomness introduced by Gaussian noise or image processing, ˜x0 introduces spatially global and semantically diverse randomness w.r.tx0. This makes it less effective to perform adaptive attacks against MI (Athalye et al., 2018). Furthermore, the global linearity of the mixup-trained models ensures that the information of x0 remained in ˜x0 is proportional to λ, such that the identity of x0 can be recovered from the statistics of ˜x0. In experiments, we evaluate MI on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) under the oblivious attacks (Carlini & Wagner, 2017) and the adaptive attacks (Athalye et al., 2018). The results demonstrate that our MI method is efﬁcient in defending adversarial attacks in inference, and is also compatible with other variants of mixup, e.g., the interpolated AT method (Lamb et al., 2019). Note that Shimada et al. (2019) also propose to mixup the input points in the test phase, but they do not consider their method from the aspect of adversarial robustness. 2 P RELIMINARIES In this section, we ﬁrst introduce the notations applied in this paper, then we provide the formula of mixup in training. We introduce the adversarial attacks and threat models in Appendix A.1. 2.1 N OTATIONS Given an input-label pair (x,y), a classiﬁer F returns the softmax prediction vector F(x) and the predicted label ˆy = arg maxj∈[L] Fj(x), where Lis the number of classes and [L] = {1,··· ,L}. The classiﬁer F makes a correct prediction on xif y= ˆy. In the adversarial setting, we augment the 2Published as a conference paper at ICLR 2020 data pair (x,y) to a triplet (x,y,z ) with an extra binary variable z, i.e., z= {1, if xis adversarial, 0, if xis clean. (1) The variable z is usually considered as hidden in the inference phase, so an input x(either clean or adversarially corrupted) can be generally denoted as x = x0 + δ·1z=1. Here x0 is a clean sample from the data manifold p(x) with label y0, 1z=1 is the indicator function, and δis a potential perturbation crafted by adversaries. It is worthy to note that the perturbation δshould not change the true label of the input, i.e., y= y0. For ℓp-norm adversarial attacks (Kurakin et al., 2017; Madry et al., 2018), we have ∥δ∥p ≤ϵ, where ϵis a preset threshold. Based on the assumption that adversarial examples are off the data manifolds, we formally have x0 + δ /∈supp(p(x)) (Pang et al., 2018a). 2.2 M IXUP IN TRAINING In supervised learning, the most commonly used training mechanism is the empirical risk minimiza- tion (ERM) principle (Vapnik, 2013), which minimizes 1 n ∑n i=1 L(F(xi),yi) on the training dataset D= {(xi,yi)}n i=1 with the loss function L. While computationally efﬁcient, ERM could lead to memorization of data (Zhang et al., 2017) and weak adversarial robustness (Szegedy et al., 2014). As an alternative, Zhang et al. (2018) introduce the mixup training mechanism, which minimizes 1 m ∑m j=1 L(F(˜xj),˜yj). Here ˜xj = λxj0 + (1−λ)xj1; ˜yj = λyj0 + (1−λ)yj1, the input-label pairs (xj0,yj0) and (xj1,yj1) are randomly sampled from the training dataset, λ∼Beta(α,α) and αis a hyperparameter. Training by mixup will induce globally linear behavior of models in-between data manifolds, which can empirically improve generalization performance and adversarial robustness (Zhang et al., 2018; Tokozume et al., 2018a;b; Verma et al., 2019a;b). Compared to the adversarial training (AT) methods (Goodfellow et al., 2015; Madry et al., 2018), trained by mixup requires much less computation and can keep state-of-the-art performance on the clean inputs. 3 M ETHODOLOGY Although the mixup mechanism has been widely shown to be effective in different domains (Berthelot et al., 2019; Beckham et al., 2019; Verma et al., 2019a;b), most of the previous work only focuses on embedding the mixup mechanism in the training phase, while in the inference phase the global linearity of the trained model is not well exploited. Compared to passively defending adversarial examples by directly classifying them, it would be more effective to actively utilize the globality of mixup-trained models in the inference phase to break the locality of adversarial perturbations. 3.1 M IXUP INFERENCE The above insight inspires us to propose the mixup inference (MI)method, which is a specialized inference principle for the mixup-trained models. In the following, we apply colored y, ˆyand ys to visually distinguish different notations. Consider an input triplet (x,y,z ), where zis unknown in advance. When directly feeding xinto the classiﬁer F, we can obtain the predicted label ˆy. In the adversarial setting, we are only interested in the cases where xis correctly classiﬁed by F if it is clean, or wrongly classiﬁed if it is adversarial (Kurakin et al., 2018). This can be formally denoted as 1y̸=ˆy = 1z=1. (2) The general mechanism of MI works as follows. Every time we execute MI, we ﬁrst sample a label ys ∼ps(y), then we sample xs from ps(x|ys) and mixup it with xas ˜x= λx+ (1 −λ)xs. ps(x,y) denotes the sample distribution, which is constrained to be on the data manifold, i.e., supp(ps(x)) ⊂ supp(p(x)). In practice, we execute MI for N times and average the output predictions to obtain FMI(x), as described in Alg. 1. Here we ﬁx the mixup ratio λin MI as a hyperparameter, while similar properties hold if λcomes from certain distribution. 3.2 T HEORETICAL ANALYSES Theoretically, with unlimited capability and sufﬁcient clean samples, a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples (Hornik et al., 1989; Guo et al., 2019), i.e., ∀xi,xj ∼p(x) and λ∈[0,1], there is H(λxi + (1 −λ)xj) = λH(xi) + (1−λ)H(xj). (3) 3Published as a conference paper at ICLR 2020 Algorithm 1Mixup Inference (MI) Input: The mixup-trained classiﬁer F; the input x. Hyperparameters: The sample distribution ps; the mixup ratio λ; the number of execution N. Initialize FMI(x) = 0; for k= 1 to N do Sample ys,k ∼ps(ys), xs,k ∼ps(xs|ys,k); Mixup xwith xs,k as ˜xk = λx+ (1 −λ)xs,k; Update FMI(x) = FMI(x) + 1 NF(˜xk); end for Return: The prediction FMI(x) of input x. Specially, we consider the case where the training objective Lis the cross-entropy loss, then H(xi) should predict the one-hot vector of label yi, i.e., Hy(xi) = 1y=yi. If the input x = x0 + δ is adversarial, then there should be an extra non-linear part G(δ; x0) of F, since x is off the data manifolds. Thus for any input x, the prediction vector can be compactly denoted as F(x) = F(x0 + δ·1z=1) = H(x0) + G(δ; x0) ·1z=1. (4) According to Eq. (3) and Eq. (4), the output of ˜xin MI is given by: F(˜x) = H(˜x0) + G(λδ; ˜x0) ·1z=1 = λH(x0) + (1−λ)H(xs) + G(λδ; ˜x0) ·1z=1, (5) where ˜x0 = λx0 + (1 −λ)xs is a virtual unperturbed counterpart of ˜xas shown in Fig. 1(c). Note that FMI(x) in Alg. 1 is a Monte Carlo approximation of Eps[F(˜x)] as FMI(x) = 1 N N∑ i=1 F(˜xi) ∞ −→Eps[F(˜x)], (6) where ∞ −→represents the limitation when the execution timesN →∞. Now we separately investigate the y-th and ˆy-th (could be the same one) components of F(˜x) according to Eq. (5), and see how these two components differ from those of F(x). These two components are critical because they decide whether we can correctly classify or detect adversarial examples (Goodfellow et al., 2016). Note that there is Hy(x0) = 1 and Hys(xs) = 1, thus we have the y-th components as Fy(x) = 1 + Gy(δ; x0) ·1z=1; Fy(˜x) = λ+ (1 −λ) ·1y=ys + Gy(λδ; ˜x0) ·1z=1. (7) Furthermore, according to Eq. (2), there is 1y=ˆy = 1z=0. We can represent the ˆy-th components as Fˆy(x) = 1z=0 + Gˆy(δ; x0) ·1z=1; Fˆy(˜x) = λ·1z=0 + (1 −λ) ·1ˆy=ys + Gˆy(λδ; ˜x0) ·1z=1. (8) From the above formulas we can ﬁnd that, except for the hidden variable z, the sampling label ys is another variable which controls the MI output F(˜x) for each execution. Different distributions of sampling ys result in different versions of MI. Here we consider two easy-to-implement cases: MI with predicted label (MI-PL): In this case, the sampling label ys is the same as the predicted label ˆy, i.e., ps(y) = 1y=ˆy is a Dirac distribution on ˆy. MI with other labels (MI-OL): In this case, the label ys is uniformly sampled from the labels other than ˆy, i.e., ps(y) = Uˆy(y) is a discrete uniform distribution on the set {y∈[L]|y̸= ˆy}. We list the simpliﬁed formulas of Eq. (7) and Eq. (8) under different cases in Table 1 for clear representation. With the above formulas, we can evaluate how the model performance changes with and without MI by focusing on the formula of ∆F(x; ps) = FMI(x) −F(x) ∞ −→Eps[F(˜x)] −F(x). (9) Speciﬁcally, in the general-purpose setting where we aim to correctly classify adversarial exam- ples (Madry et al., 2018), we claim that the MI method improves the robustness if the prediction 4Published as a conference paper at ICLR 2020 Table 1: The the simpliﬁed formulas of Eq. (7) and Eq. (8) in different versions of MI. Here MI-PL indicates mixup inference with predicted label; MI-OL indicates mixup inference with other labels. MI-PL MI-OL z= 0 z= 1 z= 0 z= 1 Fy(x) 1 1 +Gy(δ; x0) 1 1 +Gy(δ; x0) Fy(˜x) 1 λ+ Gy(λδ; ˜x0) λ λ+ (1−λ) ·1y=ys + Gy(λδ; ˜x0) Fˆy(x) 1 Gˆy(δ; x0) 1 Gˆy(δ; x0) Fˆy(˜x) 1 (1 −λ) +Gˆy(λδ; ˜x0) λ Gˆy(λδ; ˜x0) value on the true label yincreases while it on the adversarial label ˆydecreases after performing MI when the input is adversarial (z= 1). This can be formally denoted as ∆Fy(x; ps)|z=1 >0; ∆Fˆy(x; ps)|z=1 <0. (10) We refer to this condition in Eq. (10) as robustness improving condition (RIC). Further, in the detection-purpose setting where we want to detect the hidden variable zand ﬁlter out adversarial inputs, we can take the gap of the ˆy-th component of predictions before and after the MI operation, i.e., ∆Fˆy(x; ps) as the detection metric (Pang et al., 2018a). To formally measure the detection ability on z, we use the detection gap (DG), denoted as DG = ∆Fˆy(x; ps)|z=1 −∆Fˆy(x; ps)|z=0. (11) A higher value of DG indicates that ∆Fˆy(x; ps) is better as a detection metric. In the following sections, we speciﬁcally analyze the properties of different versions of MI according to Table 1, and we will see that the MI methods can be used and beneﬁt in different defense strategies. 3.2.1 M IXUP INFERENCE WITH PREDICTED LABEL In the MI-PL case, when the input is clean (i.e., z = 0 ), there is F(x) = F(˜x), which means ideally the MI-PL operation does not inﬂuence the predictions on the clean inputs. When the input is adversarial (i.e., z= 1), MI-PL can be applied as a general-purpose defense or a detection-purpose defense, as we separately introduce below: General-purpose defense:If MI-PL can improve the general-purpose robustness, it should satisfy RIC in Eq. (10). By simple derivation and the results of Table 1, this means that Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(λδ; ˜x0)] {>1 −λ, if k= ˆy, <λ −1, if k= y. (12) Since an adversarial perturbation usually suppress the predicted conﬁdence on the true label and pro- mote it on the target label (Goodfellow et al., 2015), there should beGˆy(δ; ˜x0) >0 and Gy(δ; ˜x0) <0. Note that the left part of Eq. (12) can be decomposed into Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(δ; ˜x0)]   input transfer + Exs∼ps(x|ˆy) [Gk(δ; ˜x0) −Gk(λδ; ˜x0)]   perturbation shrinkage . (13) Here Eq. (13) indicates the two basic mechanisms of the MI operations defending adversarial attacks, as shown in Fig. 1(c). The ﬁrst mechanism is input transfer, i.e., the clean input that the adversarial perturbation acts on transfers from the deterministic x0 to stochastic ˜x0. Compared to the Gaussian noise or different image processing methods which introduce spatially or semantically local randomness, the stochastic ˜x0 induces spatially global and semantically diverse randomness. This will make it harder to perform an adaptive attack in the white-box setting (Athalye et al., 2018). The second mechanism isperturbation shrinkage, where the original perturbationδshrinks by a factor λ. This equivalently shrinks the perturbation threshold since ∥λδ∥p = λ∥δ∥p ≤λϵ, which means that MI generally imposes a tighter upper bound on the potential attack ability for a crafted perturbation. Besides, empirical results in previous work also show that a smaller perturbation threshold largely weakens the effect of attacks (Kurakin et al., 2018). Therefore, if an adversarial attack defended by these two mechanisms leads to a prediction degradation as in Eq. (12), then applying MI-PL would improve the robustness against this adversarial attack. Similar properties also hold for MI-OL as described in Sec. 3.2.2. In Fig. 2, we empirically demonstrate that most of the existing adversarial attacks, e.g., the PGD attack (Madry et al., 2018) satisﬁes these properties. 5Published as a conference paper at ICLR 2020 MI-OL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 !\"#$ !\"%#$ !\"%# !\"# −'\"(;#$*−1,−1−'\"*(;%#$ * !\" 1,−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# 1,−1 '-\"(;#$ !-\"%#$ !-\"%#*−1,−1+'-\"*(;%#$ MI-PL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !\" !\"#$ !\"# −'\"(;#$ !\"%#$ !\"%# 1−*−'\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# '-\"(;#$ !-\"%#$ !-\"%# 1−*+'-\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' (1−*)(,−2),−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' 1−* Clean inputsAdversarial inputs∆'\"='\"*(;%#$−'\"(;#$ ∆'-\"='-\"(;#$−'-\"*(;%#$ Figure 2: The results are averaged on 100 randomly test clean samples of CIFAR-10. The adversarial attack is untargeted PGD-10. Note that the ∆Gy calculated here is the minus value of it in Eq. (12) and Eq. (15). Detection-purpose defense:According to Eq. (11), the formula of DG for MI-PL is DGMI-PL = Exs∼ps(x|ˆy)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (14) By comparing Eq. (12) and Eq. (14), we can ﬁnd that they are consistent with each other, which means that for a given adversarial attack, if MI-PL can better defend it in general-purpose, then ideally MI-PL can also better detect the crafted adversarial examples. 3.2.2 M IXUP INFERENCE WITH OTHER LABELS As to MI-OL, when the input is clean (z= 0), there would be a degeneration on the optimal clean prediction as Fy(˜x) = Fˆy(˜x) = λ, since the sampled xs does not come from the true label y. As compensation, MI-OL can better improve robustness compared to MI-PL when the input is adversarial (z= 1), since the sampled xs also does not come from the adversarial label ˆyin this case. General-purpose defense:Note that in the MI-OL formulas of Table 1, there is a term of 1y=ys. Since we uniformly select ys from the set [L] \\{ˆy}, there is E(1y=ys) = 1 L−1 . According to the RIC, MI-OL can improve robustness against the adversarial attacks if there satisﬁes Eys∼Uˆy(y)Exs∼ps(x|ys) [Gk(δ; x0) −Gk(λδ; ˜x0)] { >0, if k= ˆy, < (λ−1)(L−2) L−1 , if k= y. (15) Note that the conditions in Eq. (15) is strictly looser than Eq. (12), which means MI-OL can defend broader range of attacks than MI-PL, as veriﬁed in Fig. 2. Detection-purpose defense:According to Eq. (11) and Table 1, the DG for MI-OL is DGMI-OL = Eys∼Uˆy(y)Exs∼ps(x|ys)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (16) It is interesting to note thatDGMI-PL = DGMI-OL, thus the two variants of MI have the same theoretical performance in the detection-purpose defenses. However, in practice we ﬁnd that MI-PL performs better than MI-OL in detection, since empirically mixup-trained models cannot induce ideal global linearity (cf. Fig. 2 in Zhang et al. (2018)). Besides, according to Eq. (6), to statistically make sure that the clean inputs will be correctly classiﬁed after MI-OL, there should be ∀k∈[L] \\{y}, Eys∼Uˆy(y)Exs∼ps(x|ys)[Fy −Fk] >0 =⇒λ>L −1. (17) 4 E XPERIMENTS In this section, we provide the experimental results on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) to demonstrate the effectiveness of our MI methods on defending adversarial attacks. Our codes are available at https://github.com/P2333/Mixup-Inference. 6Published as a conference paper at ICLR 2020 Table 2: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-10. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 4. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 93.8 3.6 3.2 3.1 ≤1 ≤1 ≤1 Mixup + Gaussian noise 84.4 13.5 9.6 8.8 37.7 28.6 27.9 Mixup + Random rotation 82.0 21.8 18.7 18.2 38.9 32.5 26.5 Mixup + Xie et al. (2018) 82.1 23.0 19.6 19.1 38.4 31.1 25.2 Mixup + Guo et al. (2018) 83.3 31.2 28.8 28.3 57.8 49.1 48.9 ERM + MI-OL (ablation study) 81.6 7.4 6.4 6.1 33.0 26.7 23.2 Mixup + MI-OL 83.9 26.1 18.8 18.3 55.6 51.2 50.8 Mixup + MI-Combined 82.9 33.7 31.0 30.7 56.1 49.7 49.4 Interpolated AT 89.7 46.7 43.5 42.5 65.6 62.5 61.9 Interpolated AT + Gaussian noise 84.7 55.6 53.7 53.5 70.1 69.1 69.0 Interpolated AT + Random rotation 83.4 57.8 56.7 55.9 69.8 68.2 67.4 Interpolated AT + Xie et al. (2018) 82.1 59.7 58.4 57.9 71.1 69.7 69.3 Interpolated AT + Guo et al. (2018) 83.9 60.9 60.7 60.3 73.2 72.1 71.6 AT + MI-OL (ablation study) 81.2 56.2 55.8 55.1 67.7 67.2 66.4 Interpolated AT + MI-OL 84.2 64.5 63.8 63.3 75.3 74.7 74.5 0.23 0.78 0.08 0.72 0.05 0.88 0.01 0.84 00.10.20.30.40.50.60.70.80.91 MixupMixup + MI-PLPGD-10 (untargeted)PGD-50 (untargeted)PGD-10 (targeted)PGD-50 (targeted) Random guess (a) AUC scores 0 10 20 30 40 50 60 70 80 90 100 Accuracy on clean examples (%) 0 5 10 15 20 25 30 35 40 45 50Accuracy on adversarial examples (%) mixup + Gaussian noise mixup + Rotation mixup + Xie et al. (2018) mixup + Guo et al. (2018) ERM + MI-OL mixup + MI-OL mixup + MI-Combined (b) Adversarial accuracy w.r.t clean accuracy Figure 3: Results on CIFAR-10. (a) AUC scores on 1,000 randomly selected test clean samples and 1,000 adversarial counterparts crafted on these clean samples. (b) The adversarial accuracy w.r.t clean accuracy on 1,000 randomly selected test samples. The adversarial attack is untargeted PGD-10, with ϵ= 8/255 and step size 2/255. Each point for a certain method corresponds to a set of hyperparameters. 4.1 S ETUP In training, we use ResNet-50 (He et al., 2016) and apply the momentum SGD optimizer (Qian, 1999) on both CIFAR-10 and CIFAR-100. We run the training for200 epochs with the batch size of 64. The initial learning rate is 0.01 for ERM, mixup and AT; 0.1 for interpolated AT (Lamb et al., 2019). The learning rate decays with a factor of 0.1 at 100 and 150 epochs. The attack method for AT and interpolated AT is untargeted PGD-10 with ϵ = 8/255 and step size 2/255 (Madry et al., 2018), and the ratio of the clean examples and the adversarial ones in each mini-batch is 1 : 1 (Lamb et al., 2019). The hyperparameter αfor mixup and interpolated AT is1.0 (Zhang et al., 2018). All defenses with randomness are executed 30 times to obtain the averaged predictions (Xie et al., 2018). 4.2 E MPIRICAL VERIFICATION OF THEORETICAL ANALYSES To verify and illustrate our theoretical analyses in Sec. 3, we provide the empirical relationship between the output predictions of MI and the hyperparameter λin Fig. 2. The notations and formulas annotated in Fig. 2 correspond to those introduced in Sec. 3. We can see that the results follow our theoretical conclusions under the assumption of ideal global linearity. Besides, both MI-PL and MI-OL empirically satisfy RIC in this case, which indicates that they can improve robustness under the untargeted PGD-10 attack on CIFAR-10, as quantitatively demonstrated in the following sections. 7Published as a conference paper at ICLR 2020 Table 3: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-100. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 5. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 74.2 5.5 5.3 5.2 ≤1 ≤1 ≤1 Mixup + Gaussian noise 65.0 5.5 5.3 5.3 10.0 4.3 4.1 Mixup + Random rotation 66.2 7.8 6.7 6.3 21.4 15.5 15.2 Mixup + Xie et al. (2018) 66.3 9.6 7.6 7.4 30.2 22.5 22.3 Mixup + Guo et al. (2018) 66.1 13.1 10.8 10.5 33.3 26.3 26.1 Mixup + MI-OL 68.8 12.6 9.4 9.1 37.0 29.0 28.7 Mixup + MI-Combined 67.0 14.8 11.7 11.3 31.4 26.9 26.7 Interpolated AT 64.7 26.6 24.1 24.0 52.0 50.1 49.8 Interpolated AT + Gaussian noise 60.4 32.6 31.6 31.4 50.1 50.0 49.6 Interpolated AT + Random rotation 62.6 34.5 32.4 32.1 51.0 49.9 49.7 Interpolated AT + Xie et al. (2018) 62.1 42.2 41.5 41.3 57.1 56.3 55.8 Interpolated AT + Guo et al. (2018) 61.5 36.2 33.7 33.3 53.8 52.4 52.2 Interpolated AT + MI-OL 62.0 43.8 42.8 42.5 58.1 56.7 56.5 4.3 P ERFORMANCE UNDER OBLIVIOUS ATTACKS In this subsection, we evaluate the performance of our method under the oblivious-box attacks (Carlini & Wagner, 2017). The oblivious threat model assumes that the adversary is not aware of the existence of the defense mechanism, e.g., MI, and generate adversarial examples based on the unsecured classiﬁcation model. We separately apply the model trained by mixup and interpolated AT as the classiﬁcation model. The AUC scores for the detection-purpose defense are given in Fig. 3(a). The results show that applying MI-PL in inference can better detect adversarial attacks, while directly detecting by the returned conﬁdence without MI-PL performs even worse than a random guess. We also compare MI with previous general-purpose defenses applied in the inference phase, e.g., adding Gaussian noise or random rotation (Tabacof & Valle, 2016); performing random padding or resizing after random cropping (Guo et al., 2018; Xie et al., 2018). The performance of our method and baselines on CIFAR-10 and CIFAR-100 are reported in Table 2 and Table 3, respectively. Since for each defense method, there is a trade-off between the accuracy on clean samples and adversarial samples depending on the hyperparameters, e.g., the standard deviation for Gaussian noise, we carefully select the hyperparameters to ensure both our method and baselineskeep a similar performance on clean data for fair comparisons. The hyperparameters used in our method and baselines are reported in Table 4 and Table 5. In Fig. 3(b), we further explore this trade-off by grid searching the hyperparameter space for each defense to demonstrate the superiority of our method. As shown in these results, our MI method can signiﬁcantly improve the robustness for the trained mod- els with induced global linearity, and is compatible with training-phase defenses like the interpolated AT method. As a practical strategy, we also evaluate a variant of MI, calledMI-Combined, which applies MI-OL if the input is detected as adversarial by MI-PL with a default detection threshold; otherwise returns the prediction on the original input. We also perform ablation studies of ERM / AT + MI-OL in Table 2, where no global linearity is induced. The results verify that our MI methods indeed exploit the global linearity of the mixup-trained models, rather than simply introduce randomness. 4.4 P ERFORMANCE UNDER WHITE -BOX ADAPTIVE ATTACKS Following Athalye et al. (2018), we test our method under the white-box adaptive attacks (detailed in Appendix B.2). Since we mainly adopt the PGD attack framework, which synthesizes adversarial examples iteratively, the adversarial noise will be clipped to make the input image stay within the valid range. It results in the fact that with mixup on different training examples, the adversarial perturbation will be clipped differently. To address this issue, we average the generated perturbations over the adaptive samples as the ﬁnal perturbation. The results of the adversarial accuracy w.r.t the number of adaptive samples are shown in Fig. 4. We can see that even under a strong adaptive attack, equipped with MI can still improve the robustness for the classiﬁcation models. 8Published as a conference paper at ICLR 2020 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Mixup + MI-OL Interpolated AT + MI-OL Adversarial accuracyNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samples Untargetedmode UntargetedmodeTargetedmode 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Targetedmode Adaptive PGD-10Adaptive PGD-50 Adaptive PGD-200 Figure 4: Classiﬁcation accuracy under the adaptive PGD attacks on CIFAR-10. The number of adaptive samples refers to the execution times of sampling xs in each iteration step of adaptive PGD. The dash lines are the accuracy of trained models without MI-OL under PGD attacks. 5 C ONCLUSION In this paper, we propose the MI method, which is specialized for the trained models with globally linear behaviors induced by, e.g., mixup or interpolated AT. As analyzed in Sec. 3, MI can exploit this induced global linearity in the inference phase to shrink and transfer the adversarial perturbation, which breaks the locality of adversarial attacks and alleviate their aggressivity. In experiments, we empirically verify that applying MI can return more reliable predictions under different threat models. ACKNOWLEDGEMENTS This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, U19B2034, U1811461), Beijing NSF Project (No. L172037), Beijing Academy of Artiﬁcial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program and the NVIDIA NV AIL Program with GPU/DGX Acceleration. REFERENCES Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning (ICML), 2018. Christopher Beckham, Sina Honari, Alex Lamb, Vikas Verma, Farnoosh Ghadiri, R Devon Hjelm, and Christopher Pal. Adversarial mixup resynthesizers. In Advances in Neural Information Processing Systems (NeurIPS), 2019. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artiﬁcial Intelligence and Security (AISec), 2017. Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 1–7. IEEE, 2018. Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In International Conference on Machine Learning (ICML), 2018. Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 9Published as a conference paper at ICLR 2020 Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation sufﬁce: Fooling cnns with simple transformations. In International Conference on Machine Learning (ICML), 2019. Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classi- ﬁers: from adversarial to random noise. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1632–1640, 2016. Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015. Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations (ICLR), 2018. Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regular- ization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 33, pp. 3714–3722, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pp. 630–645. Springer, 2016. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989. Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Di Jin, Zhijing Jin, Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language attack on text classiﬁcation and entailment. arXiv preprint arXiv:1907.11932, 2019. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In The International Conference on Learning Representations (ICLR) Workshops, 2017. Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, et al. Adversarial attacks and defences competition. arXiv preprint arXiv:1804.00097, 2018. Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving robust neural networks without sacriﬁcing accuracy. arXiv preprint arXiv:1906.06784, 2019. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 427–436, 2015. Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS) , pp. 4579–4589, 2018a. 10Published as a conference paper at ICLR 2020 Tianyu Pang, Chao Du, and Jun Zhu. Max-mahalanobis linear discriminant analysis networks. In International Conference on Machine Learning (ICML), 2018b. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145–151, 1999. Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for adversarially robust defense. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6528–6537, 2019. Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information Processing Systems (NeurIPS), 2019. Takuya Shimada, Shoichiro Yamaguchi, Kohei Hayashi, and Sosuke Kobayashi. Data interpolating prediction: Alternative interpretation of mixup. arXiv preprint arXiv:1906.08412, 2019. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In 2016 International Joint Conference on Neural Networks (IJCNN), pp. 426–433. IEEE, 2016. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Learning from between-class examples for deep sound recognition. International Conference on Learning Representations (ICLR), 2018a. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classi- ﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5486–5494, 2018b. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. In International Conference on Machine Learning (ICML), 2019a. Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019b. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In International Conference on Learning Representations (ICLR), 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning (ICML), 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018. 11Published as a conference paper at ICLR 2020 A M ORE BACKGROUNDS In this section, we provide more backgrounds which are related to our work in the main text. A.1 A DVERSARIAL ATTACKS AND THREAT MODELS Adversarial attacks.Although deep learning methods have achieved substantial success in different domains (Goodfellow et al., 2016), human imperceptible adversarial perturbations can be easily crafted to fool high-performance models, e.g., deep neural networks (DNNs) (Nguyen et al., 2015). One of the most commonly studied adversarial attack is the projected gradient descent (PGD) method (Madry et al., 2018). Let rbe the number of iteration steps, x0 be the original clean example, then PGD iteratively crafts the adversarial example as x∗ i = clipx,ϵ(x∗ i−1 + ϵi ·sign(∇x∗ i−1L(x∗ i−1,y))), (18) where clipx,ϵ(·) is the clipping function. Here x∗ 0 is a randomly perturbed image in the neighborhood of x0, i.e., ˚U(x0,ϵ), and the ﬁnally returned adversarial example is x= x∗ r = x0 + δ, following our notations in the main text. Threat models.Here we introduce different threat models in the adversarial setting. As suggested in Carlini et al. (2019), a threat model includes a set of assumptions about the adversarys goals, capabilities, and knowledge. Adversary’s goals could be simply fooling the classiﬁers to misclassify, which is referred to as untargeted mode. Alternatively, the goals can be more speciﬁc to make the model misclassify certain examples from a source class into a target class, which is referred to as targeted mode. In our experiments, we evaluate under both modes, as shown in Table 2 and Table 3. Adversary’s capabilities describe the constraints imposed on the attackers. Adversarial examples require the perturbation δto be bounded by a small threshold ϵunder ℓp-norm, i.e., ∥δ∥p ≤ϵ. For example, in the PGD attack, we consider under the ℓ∞-norm. Adversary’s knowledge describes what knowledge the adversary is assumed to have. Typically, there are three settings when evaluating a defense method: •Oblivious adversaries are not aware of the existence of the defense Dand generate adver- sarial examples based on the unsecured classiﬁcation model F (Carlini & Wagner, 2017). •White-box adversaries know the scheme and parameters of D, and can design adaptive methods to attack both the model F and the defense Dsimultaneously (Athalye et al., 2018). •Black-box adversaries have no access to the parameters of the defense Dor the model F with varying degrees of black-box access (Dong et al., 2018). In our experiments, we mainly test under the oblivious setting (Sec. 4.3) and white-box setting (Sec. 4.4), since previous work has already demonstrated that randomness itself is efﬁcient on defending black-box attacks (Guo et al., 2018; Xie et al., 2018). A.2 I NTERPOLATED ADVERSARIAL TRAINING To date, the most widely applied framework for adversarial training (AT) methods is the saddle point framework introduced in Madry et al. (2018): min θ ρ(θ), where ρ(θ) = E(x,y)∼p[max δ∈S L(x+ δ,y; θ)]. (19) Here θrepresents the trainable parameters in the classiﬁer F, and Sis a set of allowed perturbations. In implementation, the inner maximization problem for each input-label pair (x,y) is approximately solved by, e.g., the PGD method with different random initialization (Madry et al., 2018). As a variant of the AT method, Lamb et al. (2019) propose the interpolated AT method, which combines AT with mixup. Interpolated AT trains on interpolations of adversarial examples along with interpolations of unperturbed examples (cf. Alg. 1 in Lamb et al. (2019)). Previous empirical results demonstrate that interpolated AT can obtain higher accuracy on the clean inputs compared to the AT method without mixup, while keeping the similar performance of robustness. 12Published as a conference paper at ICLR 2020 Table 4: The parameter settings for the methods in Table 2. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.04 Mixup + Random rotation Rotation degree range [−40◦,40◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [16,24] Mixup + Guo et al. (2018) The random crop size is randomly selected from [22,30] ERM + MI-OL (ablation study) The λOL = 0.6 Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.075 Interpolated AT + Random rotation Rotation degree range [−30◦,30◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [20,28] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [20,28] AT + MI-OL (ablation study) The λOL = 0.8 Interpolated AT + MI-OL The λOL = 0.6 B T ECHNICAL DETAILS We provide more technical details about our method and the implementation of the experiments. B.1 M ORE DISCUSSION ON THE MI METHOD Generality. According to Sec. 3, except for the mixup-trained models, the MI method is generally compatible with any trained model with induced global linearity. These models could be trained by other methods, e.g., manifold mixup (Verma et al., 2019a; Inoue, 2018; Lamb et al., 2019). Besides, to better defend white-box adaptive attacks, the mixup ratio λin MI could also be sampled from certain distribution to put in additional randomness. Empirical gap.As demonstrated in Fig. 2, there is a gap between the empirical results and the theo- retical formulas in Table 1. This is because that the mixup mechanism mainly acts as a regularization in training, which means the induced global linearity may not satisfy the expected behaviors. To improve the performance of MI, a stronger regularization can be imposed, e.g., training with mixup for more epochs, or applying matched λboth in training and inference. B.2 A DAPTIVE ATTACKS FOR MIXUP INFERENCE Following Athalye et al. (2018), we design the adaptive attacks for our MI method. Speciﬁcally, according to Eq. (6), the expected model prediction returned by MI is: FMI(x) = Eps[F(λx+ (1 −λ)xs)]. (20) Note that generally the λ in MI comes from certain distribution. For simplicity, we ﬁx λ as a hyperparameter in our implementation. Therefore, the gradients of the prediction w.r.t. the input xis: ∂FMI(x) ∂x = Eps [∂F(λx+ (1 −λ)xs) ∂x ] (21) = Eps [∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs ·∂λx+ (1 −λ)xs ∂x ] (22) = λEps [∂F(u) ∂u |u=λx+(1−λ)xs ] . (23) 13Published as a conference paper at ICLR 2020 Table 5: The parameter settings for the methods in Table 3. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.025 Mixup + Random rotation Rotation degree range [−20◦,20◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [18,26] Mixup + Guo et al. (2018) The random crop size is randomly selected from [24,32] Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.06 Interpolated AT + Random rotation Rotation degree range [−20◦,20◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [22,30] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [24,32] Interpolated AT + MI-OL The λOL = 0.6 Clean Adversarial Figure 5: Adversarial examples crafted by adaptive attacks with ϵ= 16/255 on CIFAR-10, against the defense of Interpolated AT + MI-OL. In the implementation of adaptive PGD attacks, we ﬁrst sample a series of examples {xs,k}NA k=1, where NA is the number of adaptive samples in Fig. 3. Then according to Eq. (18), the sign of gradients used in adaptive PGD can be approximated by sign (∂FMI(x) ∂x ) ≈sign (NA∑ k=1 ∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs,k ) . (24) B.3 H YPERPARAMETER SETTINGS The hyperparameter settings of the experiments shown in Table 2 and Table 3 are provided in Table 4 and Table 5, respectively. Since the original methods in Xie et al. (2018) and Guo et al. (2018) are both designed for the models on ImageNet, we adapt them for CIFAR-10 and CIFAR-100. Most of our experiments are conducted on the NVIDIA DGX-1 server with eight Tesla P100 GPUs. 14",
      "references": [
        "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
        "Adversarial mixup resynthesizers",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Adversarial examples are not easily detected: Bypassing ten detection methods",
        "Audio adversarial examples: Targeted attacks on speech-to-text",
        "On evaluating adversarial robustness",
        "Adversarial attack on graph structured data",
        "Boosting adversarial attacks with momentum",
        "A rotation and a translation suffice: Fooling cnns with simple transformations",
        "Robustness of classifiers: from adversarial to random noise",
        "Adversarial vulnerability for any classifier",
        "Deep Learning",
        "Explaining and harnessing adversarial examples",
        "Countering adversarial images using input transformations",
        "Mixup as locally linear out-of-manifold regularization",
        "Identity mappings in deep residual networks",
        "Multilayer feedforward networks are universal approximators",
        "Adversarial attacks on neural network policies",
        "Data augmentation by pairing samples for images classification",
        "Is bert really robust? natural language attack on text classification and entailment",
        "Learning multiple layers of features from tiny images",
        "Adversarial examples in the physical world",
        "Adversarial attacks and defences competition",
        "Interpolated adversarial training: Achieving robust neural networks without sacrificing accuracy",
        "Towards deep learning models resistant to adversarial attacks",
        "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "Towards robust detection of adversarial examples",
        "Max-mahalanobis linear discriminant analysis networks",
        "On the momentum term in gradient descent learning algorithms",
        "Barrage of random transforms for adversarially robust defense",
        "Adversarial training for free!",
        "Data interpolating prediction: Alternative interpretation of mixup",
        "Intriguing properties of neural networks",
        "Exploring the space of adversarial images",
        "Learning from between-class examples for deep sound recognition",
        "Between-class learning for image classification",
        "The nature of statistical learning theory",
        "Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer",
        "Interpolation consistency training for semi-supervised learning",
        "Mitigating adversarial effects through randomization",
        "Understanding deep learning requires rethinking generalization",
        "Theoretically principled trade-off between robustness and accuracy",
        "mixup: Beyond empirical risk minimization"
      ],
      "meta_data": {
        "arxiv_id": "1909.11515v2",
        "authors": [
          "Tianyu Pang",
          "Kun Xu",
          "Jun Zhu"
        ],
        "published_date": "2019-09-25T14:21:55Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Mixup Inference (MI), a test-time defense that actively leverages the global linearity induced by mixup-trained (or similar) models to improve robustness and adversarial example detection. MI mixes each input with randomly selected clean samples during inference, theoretically shrinks and transfers potential adversarial perturbations, provides two variants (MI-PL, MI-OL) and a combined strategy, offers formal robustness and detection conditions, and demonstrates significant gains over prior input-processing defenses and compatibility with interpolated adversarial training.",
        "methodology": "1. Assume a classifier trained with mixup, inducing approximately linear behaviour between data points.\n2. During inference, for N Monte-Carlo runs, sample a clean example xs according to a chosen label distribution, build a mixed input x̃ = λx + (1−λ)xs (λ fixed or sampled), feed x̃ to the classifier, and average predictions.\n3. Two sampling schemes:\n   • MI-PL: xs drawn from the class predicted for x.\n   • MI-OL: xs drawn uniformly from classes other than the prediction.\n4. Perturbation shrinkage (scale by λ) and input transfer (perturbation applied to random xs) reduce attack efficacy; theoretical robustness-improving condition (RIC) and detection metric derived.\n5. Compatible with existing defenses (e.g., interpolated adversarial training) and operates without retraining.",
        "experimental_setup": "Models: ResNet-50 trained 200 epochs on CIFAR-10 and CIFAR-100 using SGD (batch 64, initial LR 0.01 for ERM/mixup/AT, 0.1 for interpolated AT; LR×0.1 at epochs 100 and 150). Mixup α=1.0; adversarial training uses PGD-10 (ε=8/255, step 2/255).\nBaselines: input Gaussian noise, random rotation, random resize-padding (Xie et al. 2018), random cropping (Guo et al. 2018).\nEvaluation: 1,000 random test images; craft adversarial examples with untargeted and targeted PGD (10/50/200 steps, ε=8/255); test oblivious attacks and white-box adaptive PGD with varying adaptive samples.\nMetrics: classification accuracy on clean and adversarial inputs, AUC for detection; repeated 30 times for stochastic defenses; hyperparameters (λ, noise σ, rotation angle, crop sizes) tuned to keep similar clean accuracy.",
        "limitations": "• Experiments limited to small image datasets (CIFAR-10/100) and ℓ∞ perturbations; generality to large-scale or other modalities untested.\n• Requires models that exhibit global linearity (e.g., mixup-trained); effectiveness drops without such training.\n• Additional inference cost due to multiple forward passes and sampling.\n• Defense hyperparameters (λ, number of samples) need tuning to balance clean and adversarial accuracy.\n• Theoretical analysis assumes ideal linearity; real networks deviate, leaving potential gaps exploitable by stronger adaptive attacks.",
        "future_research_directions": "1. Extend MI evaluation to large datasets (ImageNet) and other data domains (text, audio, graphs).\n2. Explore adaptive or learned λ scheduling and smarter sample selection to reduce inference overhead.\n3. Jointly train models with objectives explicitly optimizing for MI robustness, integrating with adversarial or certified training methods.\n4. Theoretically quantify robustness guarantees under partial linearity and different norm constraints or physical attacks.\n5. Investigate distillation or approximation techniques to retain MI benefits with fewer Monte-Carlo passes, enabling real-time deployment.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Single-Photon Image Classification",
      "full_text": "Published as a conference paper at ICLR 2021 SINGLE -PHOTON IMAGE CLASSIFICATION Thomas Fischbacher Google Research tfish@google.com Luciano Sbaiz Google Research sbaiz@google.com ABSTRACT Quantum Computing based Machine Learning mainly focuses on quantum comput- ing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. We demonstrate the existence of a “quantum computing toy model” that illustrates key aspects of quantum information processing while being experimentally accessible with room temperature optics. Pondering the question of the theoretical classiﬁcation accuracy performance limit for MNIST (respectively “Fashion-MNIST”) classiﬁers, subject to the constraint that a decision has to be made after detection of the very ﬁrst photon that passed through an image-ﬁlter, we show that a machine learning system that is permitted to use quantum interference on the photon’s state can substantially outperform any machine learning system that can not. Speciﬁcally, we prove that a “classical” MNIST (respectively “Fashion-MNIST”) classiﬁer cannot achieve an accuracy of better than 22.96% (respectively 21.38% for “Fashion-MNIST”) if it must make a decision after seeing a single photon falling on one of the 28 ×28 image pixels of a detector array. We further demonstrate that a classiﬁer that is permitted to employ quantum interference by optically transforming the photon state prior to detection can achieve a classiﬁcation accuracy of at least 41.27% for MNIST (respectively 36.14% for “Fashion-MNIST”). We show in detail how to train the corresponding quantum state transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics. 1 I NTRODUCTION Both quantum mechanics and machine learning play a major role in modern technology, and the emerging ﬁeld of AI applications of quantum computing may well enable major breakthroughs across many scientiﬁc disciplines. Yet, as the majority of current machine learning practitioners do not have a thorough understanding of quantum mechanics, while the majority of quantum physicists only have an equally limited understanding of machine learning, it is interesting to look for “Rosetta Stone” problems where simple and widely understood machine learning ideas meet simple and widely understood quantum mechanics ideas. It is the intent of this article to present a setting in which textbook quantum mechanics sheds a new light on a textbook machine learning problem, and vice versa, conceptually somewhat along the lines of Google’s TensorFlow Playground (Smilkov et al. (2017),) which was introduced as a teaching device to illustrate key concepts from Deep Learning to a wider audience. Speciﬁcally, we want to consider the question what the maximal achievable accuracy on common one-out-of-many image classiﬁcation tasks is if one must make a decision after the detection of the very ﬁrst quantum of light (i.e. photon) that passed a ﬁlter showing an example image from the test set. In this setting, we do not have a one-to-one correspondence between example images from the training (respectively test) set and classiﬁcation problems. Instead, every example image deﬁnes a probability distribution for the (x,y) detector pixel location on which the ﬁrst photon passing an image ﬁlter lands, the per-pixel probability being the pixel’s brightness relative to the accumulated (across all pixels) image brightness. So, from every (28 ×28 pixels) example image, we can sample arbitrarily many photon-detection-event classiﬁer examples, where the features are a pair of integer pixel coordinates, and the label is the digit class. On the MNIST handwritten digit dataset (LeCun and Cortes (2010)), any machine learning sys- tem that only gets to see a single such “photon detected at coordinates (x,y)” event as its in- 1 arXiv:2008.05859v2  [cs.LG]  12 Mar 2021Published as a conference paper at ICLR 2021 put features, of the pixel that ﬂashed up are the only input features, is limited in accuracy by the maximum likelihood estimate, since we have: P(Image class C|Photon detected at (x,y)) =∑ E P(Image class C|Example E)P(Example E|Photon detected at (x,y)). On photon detection events generated each by ﬁrst randomly picking an example image, and then randomly picking a brightness-weighted pixel from that, we cannot do any better than predicting the most likely digit class given these input features – the two pixel coordinates. As performance is measured on the test set, no classiﬁer could possibly ever outperform one that is built to achieve maximal performance on the test set. This is obtained by determining, for each pixel, what the most likely class is, where examples from the test set are weighted by the fraction of total example-image brightness that comes from the pixel in question. Figure 2(b) shows the most likely image-class per pixel. (For MNIST, some pixels are dark in every test set example.) No classiﬁer can outperform one that simply looks up the pixel-coordinates at which a photon was detected in Figure 2(b) and returns the corresponding class, and this optimal classiﬁer’s accuracy is22.96% for the the MNIST dataset – substantially higher than random guessing (10%). Appendix A.2 provides a detailed (but mostly straightforward) optimality proof of this accuracy threshold. We cannot, for example, outperform it by redistributing light intensity between pixels, since any such redistribution could only destroy some of the available useful information, not magically create extra useful information. An entirely different situation arises when we allow quantum mechanics to enter the stage: For a single photon passing through a coherently illuminated image ﬁlter, with all pixels at the same optical phase on the incoming wave, we can imagine putting some precision optical device between the image ﬁlter and the detector array that redistributes not the probabilities (which correspond to light intensity when aggregating over many photons), but the amplitudes that make up the spatial part of the photon wave-function. Illuminating such a set-up with many photons would show a hologram-like interference pattern on the detector array. This transformation of the (single-)photon wave function by linear optical elements then has tuneable parameters which we can adjust to improve classiﬁer accuracy. Quantum mechanics tells us that every (lossless) linear optical device can be represented by a linear unitary transform on the photon state: The action of any complex optical device consisting of (potentially very many) components which transforms a N-component photon state (in our case, N = 282 amplitudes in the spatial part of the photon wave function) can be described by an element of the N2-dimensional unitary matrix Lie group U(N). Vice versa, Reck et al. (1994) describes a constructive algorithm by which any U(N) transformation matrix can be translated back to a network of optical beam splitters and phase shifters. 1.1 R ELATED WORK Conceptually, exploiting interference to enhance the probability of a quantum experiment producing the sought outcome is the essential idea underlying all quantum computing. The main difference between this problem and modern quantum computing is that the latter tries to perform calculations by manipulating quantum states of multiple “entangled” constituents, typically coupled two-state quantum systems called “qubits,” via “quantum gates” that are controlled by parts of the total quantum system’s quantum state. Building a many-qubit quantum computer hence requires delicate control over the interactions between constituent qubits. This usually requires eliminating thermal noise by going to millikelvin temperatures. For the problem studied here, the quantum state can be transformed with conventional optics at room temperature: the energy of a green photon is 2.5 eV , way above the typical room temperature thermal radiation energy of kT ≃25 meV . The price to pay is that it is challenging to build a device that allows multiple photons to interact in the way needed to build a many-qubit quantum computer. Nevertheless, Knill, Laﬂamme, and Milburn (Knill et al. (2001)) devised a protocol to make this feasible in principle, avoiding the need for coherency-preserving nonlinear optics (which may well be impossible to realize experimentally) by clever exploitation of ancillary photon qubits, boson statistics, and the measurement process. In all such applications, the basic idea is to employ coherent multiphoton quantum states to do computations with multiple qubits. In the problem studied here, there is only a single photon, the only relevant information that gets processed is encoded in the spatial part of its wave function (i.e. polarization is irrelevant), so the current work resembles the “optical simulation of quantum logic” proposed by Cerf et al. (1998) where a N-qubit system is represented by 2N spatial modes of a single photon. Related work studied similar “optical simulations of quantum computing” for implementing various algorithms, in particular (small) integer factorization (Clauser and Dowling (1996); Summhammer (1997)), but to the best of the present authors’ knowledge did not consider machine learning problems. 2Published as a conference paper at ICLR 2021 This work can be described as belonging to the category of machine learning methods on quantum non-scalable architectures. Alternatively, one can regard it as a quantum analogue of recent work that demonstrated digital circuit free MNIST digit classiﬁcation via classical nonlinear optics, for instance via saturable absorbers (Khoram et al. (2019).) Apart from providing an accessible and commonly understandable toy problem for both quantum and ML research communities, this simple- quantum/simple-ML corner also may be of interest for teaching the physics of the measurement process (“the collapse of the wave function”) in a more accessible setting. Whereas explanations of the measurement process are forced to remain vague where they try to model “the quantum states of the observer” (typically unfathomably many states that one would never hope to be able to model in terms of actual numbers), using machine learning as a sort-of cartoon substitute for high level mental processes actually allows us to come up with fully concrete toy models of the measurement process on low-dimensional (such as: D< 1000) Hilbert spaces that nevertheless capture many of the essential aspects – to the extent that “ML classiﬁes the measurement as showing the image of a shoe” can be regarded as a crude approximation to “observer sees a shoe”. Looking closer at the relation between the present article and Khoram et al. (2019), both articles study the general feasibility of realizing Machine Learning classiﬁers in the form of an analog optical computer at the theoretical level, using numerical optimization to produce a blueprint of a device that can perform inference for a speciﬁc problem. In both articles, the primary problem under study is MNIST handwritten digit classiﬁcation, the input is encoded as spatial dependency of a (monochromatic) laser beam’s light intensity, and classiﬁcation happens by using interference to funnel optical energy onto a detector array. In both cases, stochastic gradient descent is used to shape how this funneling of optical energy happens. Indeed, even the loss function used for training (cross entropy) is essentially equivalent. The key differences are that Khoram et al. (2019) only considers the many-photon limit of classical wave optics, which allows one the luxury of using non-linear optical components, speciﬁcally saturable absorbers, to implement non-linearities. This has no analog for the single photon case. Also, having many photons available allows identifying the target class that receives most laser light and calling this the prediction of the model. This is clearly not possible when a decision has to be made after seeing only a single photon. If one sent many photons through an interference device as described in this article and picked the target class with the highest photon count, one would observe classiﬁcation accuracies of about 90% rather than the claimed about-40% for a single photon. This is considerably higher than the accuracies of about 80% presented inKhoram et al. (2019) as the focus of that article is on manufacturability, running gradient backpropagation directly on a Finite Difference Frequency Domain PDE simulation of Maxwell’s equations and taking materials engineering constraints into account, whereas our work focuses on upper and lower bounds for achievable accuracy, exploiting the one-to-one equivalence between linear optical devices and unitary transforms. Our work directly trains the parameters of the unitary transform, which only afterwards get mapped to a blueprint for an experimental device realization. Speculatively, if a device were built experimentally that was designed by the methods inKhoram et al. (2019), subject to the extra constraint that no non-linear elements can be used, and then deployed in a low-light-intensity single-photon setting, using a suitable detector such as a SPAD array, it may manage to realize better-than-classically-achievable classiﬁer performance, for reasons explained in the current work. 1.2 T HE MEASUREMENT PROCESS How well one can one solve a mental processing task, such as identifying a handwritten digit, if one is permitted to only measure a single quantum? This question leads to a Hilbert space basis factorization that parallels the factorization needed to study the quantum mechanical measurement process. Let us consider a gedankenexperiment where our quantum system (see Feynman et al. (2010); Landau and Lifshitz (1981) for an introduction to quantum mechanics) is a single atom that has two experiment-relevant quantum states, ‘spin-up’ and ‘spin-down’, |ψAtom⟩= c0|ψAtom=↑⟩+ c1|ψAtom=↓⟩. (1) This atom undergoes a measurement by interacting, over a limited time period, with an apparatus. The measurement process may involve for instance an atom emitting a photon that is detected by a camera, and it may include a human observing the result. We describe a quantum state in the potentially enormous Hilbert space of apparatus states with the vector |ψApparatus⟩.If, in this gedankenexperiment, we actually assume that we have maximal information about the (quantum) 3Published as a conference paper at ICLR 2021 state of the measurement apparatus (which, however, in practical terms would be unfathomably complicated) at the beginning of the experiment, then the full quantum state of the initial system is the tensor product |ψSystem, initial⟩= |ψAtom, initial⟩⊗|ψApparatus, initial⟩. (2) This factorization implies that atom and apparatus states are independent before the interaction. Without interaction between the apparatus and the atom, the time-evolution of the total system factorizes. A measurement requires an interaction between the apparatus and the atom, the solution of the Schrödinger equation is equivalent to the application of a unitary operator U to the state |ψSystem, initial⟩. This has the effect of combining the state components of the atom and the apparatus and, as a consequence, the joint time evolution no longer can be factorized. The overall state is |ψSystem, ﬁnal⟩= U|ψSystem, initial⟩and can be always decomposed in the sum: |ψSystem, ﬁnal⟩= α|ψAtom=↑⟩⊗|ψApparatus, ﬁnal=↑⟩+ β|ψAtom=↓⟩⊗|ψApparatus, ﬁnal=↓⟩, (3) where the apparatus states |ψApparatus, ﬁnal=↑⟩and |ψApparatus, ﬁnal=↓⟩represent the state of the apparatus after the measurement for the two basis states of the atom. Therefore, the apparatus is in a different state for the two cases, which leads to the apparent “collapse” of the wave function. The apparatus in the state |ψApparatus, ﬁnal=↑⟩perceives the “collapse” because the atom seems to have taken the state |ψAtom=↑⟩.The state of the apparatus includes also the representation of the thought process of a possible human observer, for instance asking herself at what instant the atom took a well determined state. This thought process disregards the superposed state |ψApparatus, ﬁnal=↓⟩which represents the alternative reality, where the apparatus observed a different outcome. Considering that a mental process could be seen as a measurement on the environment, one would naturally be inclined to think that high level mental concepts never would naturally lend themselves to a description in terms of some Hilbert space basis that has tensor product structure|ψgeneral concept⟩⊗ |ψdetails⟩.Machine learning is now making the question to what extent this may nevertheless work quantitatively testable for some simple cases, if we consider it as providing reasonably good (for this purpose) models for mental concepts. Let us consider the spatial part of a single photon’s quantum state as it traveled through a mask that has the shape of a complicated object. For instance, let’s assume that the mask is obtained from a random sample of the “Fashion-MNIST” dataset, Xiao et al. (2017), where each sample represents an object such as a shirt, a trouser, etc. One would generally expect that any sort of transformation that connects a highly regular and mathematically simple description of such a quantum system, such as in terms of per-picture-cell (“pixel”) amplitudes, with a description in human-interpretable terms, such as “the overall intensity pattern resembles a shirt,” would unavoidably involve very complicated entanglement, and one should not even remotely hope to be able to even only approximately express such photon states in terms of some factorization |ψphoton⟩≈ ∑ shape classC∈{shirt,trouser,...} ∑ style S cCS |ψshape class C⟩⊗|ψstyle S⟩, (4) since one would not expect the existence of a basis of orthonormal quantum states that can be (approxi- mately) labeled |ψshirt⟩, |ψshoe⟩, etc. Using machine learning, we can quantitatively demonstrate that, at least for some simple examples, precisely such a factorization does indeed work remarkably well, at least if we content ourselves with the concept of a “shirt shape” being that of a one-out-of-many machine learning classiﬁer, so not quite that of a human. In any case, it is reassuring to see that even near-future few-qbits quantum computers might be able to model high level concepts rather well. 2 S INGLE -QUANTUM OBJECT CLASSIFICATION Our gedankenexperiment starts with a single photon passing from faraway through a programmable LCD screen, which we here consider to consist of N ×N pixels and show an image, where for both the MNIST handwritten digit dataset of LeCun and Cortes (2010) and the “Fashion-MNIST” dataset of Xiao et al. (2017), we have N = 28. The size of the screen shall be sufﬁciently small for the photon’s quantum state to be at the same phase as it reaches each individual pixel. This does not mean that the screen has to be small in comparison to the wavelength. Rather, the light source must provide highly collimated illumination. 4Published as a conference paper at ICLR 2021 The relevant spatial part of the photon’s quantum state is described by an element of a N ×N- dimensional complex vector space. We can choose a basis for this Hilbert space such that the quantum state of a photon that managed to pass through the screen (rather than getting absorbed) has the form |ψPhoton⟩= ∑ row j, column k cjk|ψjk⟩ (5) where the |ψjk⟩basis functions correspond to a photon that went through pixel (j,k), and the coefﬁ- cients cjk are real, non-negative, proportional to the square roots of the image’s pixel-brightnesses, and are normalized according to ∑ j,k |cj,k|2 = 1. As we want to perform a rotation on this Hilbert space that maximizes alignment with a tensor product Hilbert space where one factor describes an image class, we pad this N2-dimensional Hilbert space into a larger Hilbert space with dimensionality M divisible by the number of object classes C, i.e. M = C ·S. This amounts to adding always-dark pixels (that may not form a complete row) to the image. The problem then amounts to engineering, for a problem P such as handwritten digit recognition, a single problem-speciﬁc unitary transform UP of the photon state, |ψPhoton⟩→ UP |ψPhoton⟩, such that we can meaningfully claim: UP |ψPhoton⟩= |ψPhoton∗ ⟩≈ ∑ example classc ∑ style s ccs|ψclass isc⟩⊗|ψstyle variant iss⟩ (6) Speciﬁcally, for each individual example image E, we would like to have UP |ψPhoton,E⟩≈| ψC(E)⟩⊗ ∑ style s cs|ψstyle variant iss(E)⟩, (7) where C(E) is the ground truth label of the example in a supervised learning setting. Using the method described in Reck et al. (1994), this trained matrix then can be translated to an optical network blueprint. The transformed quantum state at the output side of the network of beam splitters and phase shifters then gets measured by a detector array that can discriminate M = C·S quantum states which are labeled |ψdigit is a 0⟩⊗|ψstyle variant 1⟩, |ψdigit is a 0⟩⊗|ψstyle variant 2⟩, . . . , |ψdigit is a 3⟩⊗| ψstyle variant 57⟩, . . . ,|ψdigit is a 9⟩⊗| ψstyle variantSmax ⟩. If we detect the photon in any of the |ψdigit is a 7⟩⊗... cells, the classiﬁer output is a “7”, and likewise for the other digits. From a machine learning perspective, the trainable parameters hence are the complex entries of the matrix UP , which according to quantum mechanics have to satisfy an unitarity constraint,UP U† P = I, and this search space automatically covers all experimentally realizable linear optical devices. For MNIST, where examples have 28 ×28 pixels, the most obvious choice is padding to a M = 790-dimensional input vector. While one could implement the unitarity constraint in terms of a (regularizer) loss-function contribution that measures the degree of violation of unitarity, it here makes more sense to instead use a parametrization of UP that automatically guarantees unitarity, using Lie group theory. If WP is a 790 ×790 matrix of trainable (real) weights, then the hermitean matrix HP = −i(WP −WT P ) + (WP + WT P ) parametrizes the Lie algebra u(790), and UP = exp(iHP ) covers all of the (compact) unitary group U(790). This approach slightly over-parametrizes the problem, since, in the tensor-product basis that we are transforming to, we can freely re-deﬁne the basis on each of the ten 790/10 = 79-dimensional style subspaces. This means that 10% of the parameters are redundant. Overall, with all the trainable weights being provided by the matrix WP , and the brightness of the pixel at coordinates (y,x) for example Ebeing bE;yx, we have this formula for the probability of a photon travelling through an optical device that was designed by training weights and landing on a detector cell that predicts class c: p(c|E) = ∑ s ⏐⏐⏐⏐⏐⏐ ∑ j,k,y,x expm ( WP −WT P + i(WP + WT P ) ) kj √ bE;yx∑ ˜y,˜x bE;˜y˜x δN·y+x,jδj,c·S+s ⏐⏐⏐⏐⏐⏐ 2 . (8) Here, yand xare image row- and column-indices (for MNIST, running from 0 to 27), j,k are matrix row- and column-indices (in our example, running from 0 to 789, inclusive) for the exponentiated unitary matrix UP = expm(···), sis a style-index (here, running from 0 to S−1 = 78), the term 5Published as a conference paper at ICLR 2021 c = 0  c = 1 (a) c = 0  c = 1 (b) Figure 1: (a) The two shapes of the toy example. The four gray pixels correspond to a photon arrival probability of 1/4, i.e. a probability amplitude of 1/2. (b) The per-pixel photon arrival probability after the orthogonal transformation is applied. The dark gray pixels correspond to a probability of 1/8 and the light gray pixels to 1/2. under the square root is the relative contribution of the (y,x)-pixel to the total brightness of example image E, and the δ-factors are used for translating a pair of row,column image-indices to a linear pixel index, respectively an index on the UP -rotated quantum state vector to a pair of (class, style)- indices. Technically speaking, from the viewpoint of mapping an optical amplitude that describes light intensity passing through the image-ﬁlter to the quantum amplitude of a particular (class, style)- combination, this is simply a linear model (since quantum mechanics is linear), whose linear weights are however speciﬁed in a slightly unusual way, underneath a complex matrix exponential (since quantum mechanics is unitary, i.e. probability-preserving). The probability to predict a given class c is then obtained by summing over the probabilities associated with the given class (but different style-index). Model accuracy has to be evaluated with caution: as we need to make a prediction after detection of a single photon, accuracy is the quantum probability of the correct label, averaged over all examples. Naturally, we can not determine which of the Coutput classes would receive the most photons (= has highest probability) if all we have is a single photon detection event. This accuracy, about40% for the problems considered here, differs substantially from the accuracy that would be obtainable by looking at many photons coming from the same example image, which here typically exceeds 90%, roughly in alignment with the expected achievable performance of a linear model on MNIST. In other words, probabilities are uncalibrated, and the (non-linear “deep learning”) transformation that would be required to calibrate them cannot be expressed as a unitary operator. Let us consider a radically simpliﬁed example that illustrates why this method works. We want to discriminate between only two different shapes (with no further shape variation) on a 2 ×4 pixel screen where each pixel is either “on” or “off”, using only one photon. Speciﬁcally, let us consider the two Tetris “T” shapes represented in ﬁgure 1(a). For both shapes, the probability that the single photon arrives on one of the “on” pixels is 1/4; therefore, taking into account that for two pixels the correct shape is identiﬁed exactly and for two with 50% probability, we conclude that the baseline accuracy is 1/2 + 1/4 = 75%.Instead, we can apply a unitary transformation to reshape the probability amplitudes. Let us now consider the simple but not optimal transformation of the photon amplitude that replaces the pair of amplitudes (a,b) in each 2-pixel column with ((a−b)/ √ 2,(a+ b)/ √ 2), i.e. creates destructive interference in the top row and constructive interference in the bottom row. This gives the detection probability patterns shown in ﬁgure 1 (b). Maximum likelihood estimation here gives an accuracy of 1/2 + 3/8 = 87.5%. Obtaining the maximum achievable accuracy will here require a more complicated all-pixel amplitude transformation, obtained as follows: The quantum amplitude transformation is angle-preserving, and the angle αbetween the two amplitude quantum states q1, q2 is given by cos α = ⟨q1|q2⟩= 0.5. Hence, we can rotate these two states to lie in the plane of the ﬁrst two Cartesian coordinate axes of the Hilbert space, and at the same angle from their bisector. Identifying these coordinate axes with the correct labels, the accuracy is the cosine-squared of the angle between the transformed state and the corresponding axis, i.e. cos2(π/4 −α/2) = ( √ 3 + 2)/4 ≈93.30%. 6Published as a conference paper at ICLR 2021 Table 1: Results for the Fashion-MNIST and MNIST datasets. The “classic” accuracy and information refer to the observation of a single photon, while the “quantum” quantities are obtained after applying the quantum transformation. Dataset Entropy [bits] Accuracy Bound (classic) Information (classic) [bits] Accuracy (quantum) Information (quantum) [bits] Fashion-MNIST 3.32 21.38% 1.10 36.14% 1.85 MNIST 3.32 22.96% 1.20 41.27% 2.04 While the performance measure that we care about here is the probability for a correct classiﬁcation, one observes that model training is nevertheless more effective when one instead minimizes cross- entropy, as one would when training a conventional machine learning model. Intuitively, this seems to make sense, as a gradient computed on cross-entropy loss is expected to transport more information about the particular way in which a classiﬁcation is off than a gradient that is based only on maximizing the correct classiﬁcation probability. Overall, this task is somewhat unusual as a machine learning problem for three reasons: First, it involves complex intermediate quantities, and gradient backpropagation has to correctly handle the transitioning from real to complex derivatives where the loss function is the magnitude-squared of a complex quantity. TensorFlow is at the time of this writing the only widely used machine learning framework that can handle this aspect nicely. Appendix A.3 provides details on numerical aspects. Second, (as explained above), we cannot simply pick the class for which the predicted probability is highest as the predicted class. Rather, the probability for the single-photon measurement to produce the ground truth label sets the accuracy. Third, while most machine learning architectures roughly follow a logistic regression architecture and accumulate per-class evidence which gets mapped to a vector of per-class probabilities, we here have the probabilities as the more readily available data, so the computation of cross-entropy loss will have to infer logits from probabilities. Due to this need to compute logarithms of probabilities, it is very important that the training process does not intermediately see invalid probabilities outside the range (0 ... 1), and this is ensured by parametrizing unitary transforms as matrix exponentials of anti-hermitean matrices. TensorFlow code to both train such a model and also evaluate its performance is included in the supplementary material. 3 R ESULTS Figure 2 shows the most probable image class for each pixel, for the “Fashion-MNIST” and MNIST datasets. A classiﬁer that looks up and predicts the most likely class in this table achieves maximal accuracy among all single photon classiﬁers that do not employ quantum interference. This includes classiﬁers that have had access to the test set during training. This accuracy is reported in the third column of table 1. (We note that, as pointed out by Sun et al. (2007), the “Fashion-MNIST” dataset contains many mislabeled instances, which affects both classical and quantum results.) We can compute the amount of information provided by the photon by computing the difference between the class entropy, i.e. −log2(0.1) = 3.32,since there are 10 classes, and the entropy associated to the classiﬁcation errors, i.e. the accuracy. The mutual information for the classical classiﬁer is given in the fourth column of table 1. Training a unitary U(790) quantum transformation that gets applied after the photon passed the image ﬁlter and before it hits a bank of 790 detectors allows boosting accuracy for both the “Fashion-MNIST” and MNIST datasets, as reported in the ﬁfth column of table 1. The observation of the photon after the transformation provides a higher amount of mutual information with respect to the classical case. The values of mutual information in this case are given in the last column of table 1. Explicit matrices to perform the transformation for the two data sets have been made available with the supplementary material. The quantum transformation UP allows us to deﬁne the pixel-space projection operators: Pclass C := U−1 P (|ψC⟩⟨ψC|⊗Istyle) UP (9) 7Published as a conference paper at ICLR 2021 4 8 0 0 6 6 6 6 6 1 1 1 1 1 1 1 1 1 1 1 0 9 5 5 9 9 5 9 8 8 8 0 6 6 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 9 8 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 9 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 5 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 8 8 8 8 0 0 0 2 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 2 0 4 1 1 1 1 1 3 1 1 1 1 9 9 9 9 9 9 9 5 5 8 8 8 8 8 2 2 6 4 1 1 1 1 1 7 1 7 7 7 9 9 9 9 9 9 7 7 5 8 8 8 8 8 2 2 2 4 1 1 1 1 7 7 7 7 7 7 7 5 9 5 5 7 7 7 5 8 8 8 8 8 2 2 4 4 0 1 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 8 8 8 8 8 8 2 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 5 5 5 7 7 7 7 7 7 7 1 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 5 5 5 5 5 5 7 9 9 9 9 5 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 9 9 9 9 5 9 7 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 1 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 3 8 9 9 9 9 9 9 9 9 9 9 9 9 9 2 2 9 0 3 1 1 1 3 3 3 1 1 3 0 8 9 9 9 9 9 9 9 5 5 9 9 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 9 9 9 5 5 9 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 5 9 9 5 5 5 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 9 9 5 5 8 2 2 2 2 0 0 0 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 5 9 8 8 8 2 2 2 2 8 0 0 3 1 1 3 3 3 1 1 0 0 0 2 2 2 2 5 5 9 (a) 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 3 2 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 5 5 5 5 5 5 3 3 4 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 5 5 5 5 5 5 5 7 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 5 5 5 5 5 5 5 5 8 7 7 7 3 3 7 7 3 3 7 7 9 9 1 1 1 1 1 0 5 5 5 5 5 5 5 8 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 0 5 5 5 5 8 7 7 7 7 7 7 7 7 7 9 9 9 7 1 1 1 1 7 7 7 0 0 0 0 0 5 5 5 7 7 7 7 7 7 7 7 9 9 9 5 5 1 1 1 1 7 7 7 0 0 0 0 0 5 5 8 4 7 7 7 7 4 4 9 9 5 5 3 1 1 1 1 7 7 7 0 0 0 0 0 0 5 7 7 7 4 0 0 4 4 4 5 5 3 1 1 1 9 9 7 7 0 0 0 0 0 6 6 7 4 4 0 0 4 4 4 4 4 8 1 1 1 9 9 4 6 0 0 0 0 0 6 7 5 4 0 0 0 4 4 4 4 4 4 1 1 1 4 4 4 6 0 0 0 0 0 2 7 4 0 3 2 0 0 0 0 4 4 4 4 1 1 1 1 7 4 4 6 6 0 0 0 0 2 2 2 3 2 0 0 0 0 0 6 6 2 1 1 1 7 7 4 3 6 6 0 0 0 2 2 2 2 2 3 3 2 0 0 0 0 6 6 1 1 1 1 7 7 6 6 6 6 0 2 2 2 2 2 2 2 3 3 2 0 0 0 0 2 6 1 1 1 1 7 6 6 6 6 2 2 2 2 2 2 2 2 2 3 3 2 2 2 0 2 2 2 1 1 1 1 6 6 3 3 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 1 1 1 1 1 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 8 9 9 4 4 2 2 2 2 2 2 3 3 3 3 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 2 3 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 4 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 (b) Figure 2: (a) Fashion-MNIST most likely class given detection of a single photon at the corresponding pixel coordinates. Here, the classes are: 0=T-shirt/top, 1=Trouser, 2=Pullover, 3=Dress, 4=Coat, 5=Sandal, 6=Shirt, 7=Sneaker, 8=Bag, 9=Ankle Boot. (b) Most likely digit-class given detection of a single photon for MNIST. A non-quantum classiﬁer cannot outperform one that looks up its answer on the corresponding table. 0 1 2 3 4 5 6 7 8 9 (a) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (b) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (c) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (d) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 Figure 3: The confusion matrices for the “Fashion-MNIST” and MNIST datasets when classic and quantum classiﬁers are used: (a) “Fashion-MNIST”/classic, (b) MNIST/classic, (c) “Fashion- MNIST”/quantum, (d) MNIST/quantum. with which we can decompose any example into contributions that are attributable to the different classes. Here, one must keep in mind that such separation is done at the level of probability amplitudes, so while we can compute intensities/probabilities from these components, which are mutually orthogonal as quantum states, summing these per-component per-pixel intensities will not reproduce the example’s per-pixel intensities. This shows most clearly when considering the decomposition of an example “Trouser” from the “Fashion-MNIST” dataset’s test set with the model we trained for this task, as shown in ﬁgure 4(a). The dark vertical line between the legs in the original image mostly comes from destructive interference between a bright line from the “Trousers” component and a matching bright line from the “Dress” component. Due to the intrinsic quantum nature of this set-up, care must be taken when interpreting confusion matrices. Naturally, we never can claim of any single-photon classiﬁer that it would ‘classify a particular image example correctly’, since re-running the experiment on the same example will not see the photon always being counted by the same detector! So, strictly speaking, for any single-photon classiﬁer realized as a device, the “confusion matrix” could be determined experimentally only in the statistical sense, leaving uncertainty in the entries that decreases with the number of passes over the test set. Confusion matrices are shown in ﬁgure 3. 8Published as a conference paper at ICLR 2021 Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (a) Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (b) Figure 4: Projection of probability amplitudes for some samples of the “Fashion-MNIST” (a) and the MNIST (b) datasets. The ﬁrst image shows the original sample probability and the following images show the probability amplitudes for each class. We visualize the complex amplitude by using brightness to represent magnitude and hue for phase (the colormap for the phase is shown on the right of each row.) Our factorization ansatz appears to contain a hidden constraint: we are forcing each image class to use the same number of style-states. One could imagine, for instance, that a classiﬁer might achieve even higher accuracy by treating image classes unevenly. Any such model that allows more style-space dimesions for some classes can always be embedded into a model that allows more style-space dimensions for all classes, so this question can be answered by padding to a larger Hilbert space. Numerical experiments, e.g. padding to 1000 rather than 790 dimensions, suggest that this has no appreciable impact on classiﬁcation accuracy. 4 D ISCUSSION In summary, we demonstrated that, at least for the considered datasets, the space of the single observed photon state can be factorized remarkably well by a product of the example class space and a space collecting the remaining variables, such as the style. This factorization can be obtained easily by the proposed method and is experimentally realizable with optical elements placed in front of the sensor. The supplementary material contains a blueprint for an example circuit outperforming the classical limit (at 36.05% accuracy) on 10 ×10 downsampled MNIST. An experimental implementation of the proposed system would be a demonstration of a high- temperature and low- effective-qubit quantum ML device. With respect to other experimental approaches to quantum computing, such a device would have the limitation that it is built for the speciﬁc classiﬁcation problem and cannot be reconﬁgured easily. It would be interesting to see whether an advanced quantum protocol along the lines of Knill et al. (2001) might enable the realization of more sophisticated intermediate-scale high temperature quantum machine learning in a way that mostly (like here) bypasses the need for quantum logic built from common quantum gates. 5 T ENSOR FLOW CODE TensorFlow2 code to reproduce the experiments of this work and all the ﬁgures is provided in the ancillary ﬁles together with the computed unitary transformations for MNIST and “Fashion-MNIST”. REFERENCES Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Viégas, and Martin Wattenberg. Direct-manipulation visualization of deep networks. CoRR, abs/1708.03788, 2017. URL http://arxiv.org/abs/1708. 03788. 9Published as a conference paper at ICLR 2021 Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/. E. Knill, Laﬂamme, R., and G. Milburn. A scheme for efﬁcient quantum computation with linear optics. Nature, 409:46–52, 2001. URL https://doi.org/10.1038/35051009. N. J. Cerf, C. Adami, and P. G. Kwiat. Optical simulation of quantum logic. Phys. Rev. A , 57:R1477– R1480, Mar 1998. doi: 10.1103/PhysRevA.57.R1477. URL https://link.aps.org/doi/10. 1103/PhysRevA.57.R1477. John F. Clauser and Jonathan P. Dowling. Factoring integers with young’s n-slit interferometer.Phys. Rev. A, 53:4587–4590, Jun 1996. doi: 10.1103/PhysRevA.53.4587. URL https://link.aps.org/doi/10. 1103/PhysRevA.53.4587. Johann Summhammer. Factoring and fourier transformation with a mach-zehnder interferometer. Phys. Rev. A, 56:4324–4326, Nov 1997. doi: 10.1103/PhysRevA.56.4324. URL https://link.aps.org/doi/10. 1103/PhysRevA.56.4324. Erfan Khoram, Ang Chen, Dianjing Liu, Lei Ying, Qiqi Wang, Ming Yuan, and Zongfu Yu. Nanophotonic media for artiﬁcial neural inference. Photon. Res., 7(8):823–827, Aug 2019. doi: 10.1364/PRJ.7.000823. URL http://www.osapublishing.org/prj/abstract.cfm?URI=prj-7-8-823 . Richard Phillips Feynman, Robert Benjamin Leighton, and Matthew Sands. The Feynman lectures on physics; New millennium ed. Basic Books, New York, NY , 2010. URL https://cds.cern.ch/record/ 1494701. Originally published 1963-1965. L. D. Landau and L. M. Lifshitz. Quantum Mechanics Non-Relativistic Theory, Third Edition: Volume 3 . Butterworth-Heinemann, 3 edition, January 1981. ISBN 0750635398. URL http://www.worldcat. org/isbn/0750635398. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017. Michael Reck, Anton Zeilinger, Herbert J. Bernstein, and Philip Bertani. Experimental realization of any discrete unitary operator. Phys. Rev. Lett., 73:58–61, Jul 1994. doi: 10.1103/PhysRevLett.73.58. URL https://link.aps.org/doi/10.1103/PhysRevLett.73.58. J. Sun, F. Zhao, C. Wang, and S. Chen. Identifying and correcting mislabeled training instances. In Future Generation Communication and Networking (FGCN 2007), volume 1, pages 244–250, 2007. A A PPENDIX A.1 E XPERIMENT SCHEMATICS A        B         C          D Figure 5: Schematics of the experimental set-up. Top: Classical Baseline, Bottom: Quantum Set-Up. Figure 5 shows the schematics of an exper- imental set-up: The lens (A) stylizes the last optical component of the monochro- matic, coherent, linear-polarized (i.e. laser) light source that emits photons en- tering from the left and traveling to the right. Light intensity is controlled (e.g. by means of an absorbing ﬁlter, not shown) to be so low that photons travel through the apparatus individually. Any interference effects are hence due to self-interference of a single photon’s wave function (just as in the double slit gedankenexperiment). The laser photons then hit a coherently il- luminated N ×N screen (B) (e.g. a LCD screen, in this diagram 10 ×10) which allows light to pass through a given pixel with coordinates (y,x) with a probability that is proportional to the ink density on the example image. The photon arrives at each pixel with the same optical phase (i.e. 10Published as a conference paper at ICLR 2021 having traveled the same (fractional) number of wavelengths as seen from the laser). The diagram shows an ex- emplary raster image of a digit zero with maximal brightness (maximal ink density on the digitized ML example) on 14 pixels (with zero-based row/column coordinates (y,x) = (2,4),(2,5),(2,6),(3,2),..., (5,7)), with 75% brightness on three pixels (coordinates (3,3),(6,3),(6,5)), and 50% brightness on one pixel (coordinates (7,6)). The ‘Classical Baseline’ set-up does not use interference and would work just as well in a world where photons are ‘Particles of Light’ that cannot self-interfere (as envisioned by Newton). The dimensions of the apparatus need to be such that, when the image-ﬁlter is brightly illuminated, it casts a sharp shadow on the detector-array. For the classical case, use of a coherent source of light is not necessary. At very low light levels, photons coming from the light source (A) will keep hitting the screen (B), frequently getting absorbed by a dark pixel. At some point, a photon will (by chance) manage to hit a non-dark pixel and not get absorbed (the more likely the brighter the speciﬁc pixel) and travel on to the single-photon detector array (D) (such as: a SPAD array) and be detected as having passed through a speciﬁc pixel. Ignoring experimental imperfections that could in principle be made small such as optical losses, the only possible transform on the photon state one could perform with a passive-linear optical device at (C) would be equivalent to coupling the photon into an array of optical ﬁbers, routing each ﬁber from one pixel to some other pixel, and coupling out the photon at the other side of the device. This is equivalent to re-shufﬂing the pixels, which can always be un-done by re-shufﬂing the addresses of the cells of the detector array (D) and so does not affect classiﬁcation accuracy – the diagram hence omits such transformations that cannot affect performance. The quantum set-up (bottom) is perhaps easiest to analyze via Feynman’s path integral interpretation of Quantum Mechanics: There are different ‘histories’ which lead from the same initial point (a photon coming from the laser) to the same ﬁnal result (the photon being detected at a speciﬁc pixel, such as: ‘at coordinates (4, 5)’), and the prescription is that we have to attribute a complex quantum amplitude to each such ‘history’, summing over all amplitudes that connect the same initial and ﬁnal state to get the resultant amplitude, and obtaining the associated probability as the magnitude-square of the (complex) resultant probability. We can also consider the resultant per-pixel quantum amplitudes for a photon having traveled from the light source (A) not all the way to the detector but to some intermediate point, such as just after passing the screen (B). These are described by vectors with N2 entries, one per pixel, whose absolute-magnitude-squares sum to 1. In the example, the photon-state-vector on any ﬂat, screen-parallel surface between B and C, |ψBC ⟩, has zero entries for all but the 14+3+1=18 non-dark pixels. The entries ψBC [24],ψBC [25],... that correspond the 14 maximally-bright pixels at (2,4),(2,5),... are identical, as the photon reached each pixel at the same optical phase. Calling this amplitude cB (B for ‘bright’), the amplitudes for the three moderately-bright pixels cM , and the single dim-but-not-dark pixel’s amplitudecD, the amplitude magnitude-squares must be proportional to pixel brightness (probability for a photon to pass through the image) and sum to 1 (total probability for the photon that passed through the screen to have passed through a pixel). As the complex quantum amplitude phase matches the optical phase, these constraints ﬁx cB = u· √ 1/Z≈0.244, cM = u· √ 0.75/Z≈0.212, cD = u· √ 0.5/Z≈0.173, with Z being the overall normalization factor that makes the sum of magnitude-squares of all amplitudes 1, i.e. Z = 14·1 + 3·0.75 + 1·0.5, and ubeing some complex number of magnitude 1, the non-observable overall quantum phase factor. In the ‘Quantum’ set-up, we can employ a linear optical device, built from many beam-splitters and phase shifters (= ‘delay lines’), to adjust self-interference of the photon wave function. The diagram shows two example paths out of the total 10 ×10 different paths that the photon can take before it reaches the detector array. Quantum Mechanics tells us that ‘the single photon(!) travels along all these paths simultaneously’ – this is just Young’s double slit experiment in a slightly more complicated setting. There is a 1-to-1 correspondence between physically realizable linear optical devices and probability-preserving (generalized) rotations of the quantum state vector. The (in this downsampled example: 100 ×100) components of such a transformation form a unitary matrix. If we used a linear optical device that implemented a random such transformation, and sent many photons through the apparatus, they collectively would produce an image conceptually resembling the interference pattern on photo ﬁlm that codiﬁes a hologram. Using a basic Machine Learning procedure, stochastic gradient descent, we can train the parameters of the transform such that photons coming from an image that shows a digit ‘0’ preferentially land on the 0-th row of the detector array, photons coming from an image that shows a digit ‘4’ preferentially land on the 4-th row, etc. The supplementary material describes a speciﬁc set-up in terms of optical components that reaches >36% accuracy for MNIST downsampled to 10 ×10: If this were manufactured from ideal-quality optical components, the probability for a photon that passed through the screen (B) to land on the detector row matching its digit-class is better than 36%. A.2 C LASSICAL BASELINE ACCURACY THRESHOLD : M AXIMALITY PROOF Elementary statistical considerations allow us to obtain a stringent upper bound for the maximum accuracy that cannot be exceeded by any classiﬁer which satisﬁes these two properties: • P1: The classiﬁer must make a prediction using as its input only the index of the one detector in the detector-array that received the ﬁrst photon. It can not use any additional information about the 11Published as a conference paper at ICLR 2021 example (but may have been trained with arbitrary information about the example set, even including full knowledge of the training and test set). • P2: There is a one-to-one correspondence between image pixels and detector-cells: For each image- pixel, there is exactly one detector-cell such that when the example image Eis presented, then the probability for the ﬁrst photon to land on the detector cell kis proportional to the brightness of the associated pixel in image E. This bound is what we call the ‘classical accuracy bound’. The ‘quantum’ classiﬁer violates P2 by employing photon self-interference: The probability for the k-th detector to observe the ﬁrst photon depends on collective information which the photon ‘holographically’ transports about the input image once it passed the image-ﬁlter, rather than on a single pixel. The protocol for evaluating classiﬁer accuracy is as follows: We pick a random example from the dataset’s test set, set up the device to present this example as a problem, send light towards the ﬁlter-screen, and look at the ﬁrst photon that managed to pass the ﬁlter-screen and get counted by the detector array, then map the index of the detector that counted the photon to a predicted class. We register ‘successful prediction’ if the predicted class matches the example’s label, otherwise we register an ‘unsuccessful prediction’. The accuracy is the probability of the prediction to be successful. Somewhat unusually, this means that there is no such thing as ‘the predicted class of a given image’, as one normally would have it in a Machine Learning problem. This is due to the inherent randomness of quantum mechanics: Even repeating classiﬁcation for the same image multiple times, we will see photons land on detectors that correspond to different classiﬁcations. If we were to experimentally determine accuracy, this would then suffer from the usual problems of determining a probability via a statistical experiments: one can make the likelihood to be way off arbitrarily small, but never shrink it to zero. However, the aforementioned protocol makes it possible to directly compute the maximal achievable probability for any classiﬁer, without resorting to a statistical experiment. The gist of our argument parallels the reasoning behind the claim that we can show with simple statistics that no ML classiﬁer can possibly outperform an accuracy threshold of 29/36 for predicting from the eye total of rolling two dice whether any of the two dice showed a six. Here, the reason is that we can get maximal accuracy by looking look at all the possible realizations of any given eye total and make the best possible guess given the situation. For eye totals 2 −6 (1 + 2 + 3 + 4 + 5 = 15of 36 cases), we would predict ‘No’ and always be correct. For eye totals 11 and 12 (2 + 1additional cases), we would predict ‘Yes’ and also always be correct. For each other eye total, there are two realizations where one die shows a ‘six’, so we would want to predict ‘Yes’ for eye totals having at most four realizations, i.e. where we have at least a 50% chance of being correct, and ‘No’ otherwise. Using this approach, we would incorrectly classify three cases as ‘Yes’ (5 + 5,4 + 5,5 + 4), and incorrectly classify four cases as ‘No’ (6 + 1,1 + 6,6 + 2,2 + 6). Except for these 7/36 cases, we would make a correct prediction, so optimal accuracy is 29/36. Given the perhaps somewhat unfamiliar ‘quantum ML’ setting, and the need to rigorously justify the optimality claim, we prove it below. The only material difference to the dice-sum example is that relative weights of realizations are not determined by counting, but by looking at pixel brightnesses. In analogy to the the dice example, the key observation is that the classiﬁer’s input is a single pixel-index, and its output is an image-class. So, we can completely specify any (deterministic or not) classiﬁer’s action by tabulating, per-pixel-index, what the probability is for this classiﬁer to map the given input pixel-index kto each of the possible output classes c. The resulting matrix Kkc would, for a deterministic classiﬁer, simply be a matrix with one-hot encoded image class, one row per pixel-index. The classiﬁer’s accuracy is then given by Accuracy = P(Classiﬁcation is correct) =∑ E P(E) ∑ k ·P(γk|E) ·P(yE = C(γk)) = = ∑ E,k P(E) ·P(γk|E) ·Kk,c=yE. (10) Here, P(E) is the probability to pick example E from the test set (i.e. 1/{test set size}), P(γk|E) is the probability to detect the photon in the detector cell with index k, given the example E, and P(yE = C(γk)) is the probability that example E’s labelyE matches the classiﬁer’s output on the input “the photon was detected in cell k”. The probability for detecting a photon in cell kwhen randomly drawing an example image from the test set is P(γk) =∑ E P(E) ·P(γk|E). The probability for a fairly drawn example’s label to beyc when a photon was detected at cell kis P(yc|γk) =∑ E P(E) ·P(yE = yc) ·P(γk|E). Let us tabulate these P(yc|γk) in the {#pixels}×{#classes}matrix Rkc := P(yc|γk). We then have: Accuracy = ∑ Detector cellk ∑ Class c P(γk) ·P(yc|γk) ·Kkc = ∑ k,c P(γk)RkcKkc. (11) In words: We can compute accuracy by looking at each detector cell k and each class c, determining the probability P(γk) that, when fairly drawing examples from the test set, a photon gets detected at cell k, and 12Published as a conference paper at ICLR 2021 splitting up this probability into contributions from examples where the target class was 0, 1, 2, etc. These contributions are P(γk) ·P(yc|γk). We make a correct classiﬁcation when the classiﬁer also predicts class c given the input k. The classiﬁer’s behavior when given the inputkis speciﬁed by row kof the K-matrix, so this probability is Kkc. Here, P(γk) and Rkc are determined by the test set. Each admissible matrix Kkc that has ∑ c Kkc = 1speciﬁes a different classiﬁer, and the accuracy is a function of this matrix Konly. The question is now which admissible matrix Kmaximizes accuracy. Total classiﬁcation performance (accuracy) is a weighted sum over per-detector- cell performances (the weights being the probabilities to observe a photon in cell k when doing detection experiments on samples drawn fairly from the test set). Let K1 be a matrix that maximizes accuracy, and K2 be a matrix obtained by picking, for each cell-index k, a probability row-vector that maximizes ∑ c RkcK2,kc. We have Accuracy(K1) ≥Accuracy(K2) (since K1 is optimal), and also ∑ c Rkc(K2,kc −K1,kc) ≥0 (since K2 maximizes this value on each row k), so, taking a weighted sum with weights P(γk) ≥0, we ﬁnd ∑ k P(γk) ∑ c Rkc(K2,kc −K1,kc) ≥0, i.e. Accuracy(K2) ≥Accuracy(K1), hence Accuracy(K1) = Accuracy(K2). In words, we achieve maximal accuracy if we individually look at each “photon detected in cell k” case and make the optimal prediction there. Now, for a ﬁxed cell-index k, ∑ c RkcK2,kc is maximal if the matrix-row K2,kc has an entry 1 for the index cfor which Rkc is maximal, and is zero otherwise. To see this, let us assume K2,kc >0 for some index cfor which there is another class index dwith Rkd >Rkc. Then, incrementing K2,kd by K2,kc and subsequently setting K2,kc to zero increases ∑ c RkcK2,kc, which is a contradiction. So, optimal choices of K2,kc are zero for classes cfor which Rkc is not maximal. Also, we always attain the maximum when choosing each row-vector of Kto be one-hot and have its 1-entry in a place that maximizes Rkc, i.e. maximal achievable accuracy is obtained by a classiﬁer which, for every cell-index k, predicts the most likely digit-class subject to the constraint that a randomly drawn example from the test set had its ﬁrst photon-detection occur at detector cell k. This theoretical upper bound on classiﬁer accuracy hence is given by: Accuracy ≤ ∑ k P(γk)maxcRkc. (12) For the MNIST dataset, this is found to be 22.957% (rounded up to 22.96%), while for Fashion-“MNIST”, we get 21.375% (rounded up to 21.38%). Code that implements this calculation is available in the supplementary material. We should emphasize that the constructive procedure described here that yields a classiﬁer attaining this stringent upper bound does inspect the test set, and relevant deviations in statistical properties between training and test set would manifest in the form of lowering attainable accuracy for a classiﬁer that is trained on the training set only. A.3 B ACKPROPAGATION WITH COMPLEX INTERMEDIATE QUANTITIES As explained in the main text, the per-class probabilities deﬁned by Eq. (8), when used as input to a conventional softmax loss function, make training the real weight-parameters matrixWP in terms of which the unitary rotation is expressed a straightforward procedure. Nevertheless, this approach utilizes some capabilities which at the time of this writing are likely TensorFlow- speciﬁc. It hence may make sense to describe the training procedure in sufﬁcient detail to allow straightforward re-implementation on top of some other Machine Learning framework, or perhaps even directly without use of any such library. The loss function is deﬁned in terms of the magnitude-squared of a complex intermediate quantity, which here is the vector of complex quantum amplitudes, one entry per class/style combination. In this appendix, we henceforth consider the simpliﬁed 10 ×10 problem described in detail in appendix A.1. We can perform the calculation entirely in terms of real quantities by replacing every complex number C+ iD by a real 2 ×2 matrix block of the form C+ iD→ ( C −D D C ) . (13) This means in particular that a 100-dimensional (complex) amplitude-vector aj gets replaced by a 200 ×2- matrix Amn. If we interpret the a-index j as encoding class c and style s, i.e. j = c·S + s, the total probability for class c is p(c) = ∑ s |ac·S+s|2 = (Re ac·S+s)2 + (Imac·S+s)2, and this gets replaced by p(c) =∑ s ( A2 (c·S+s)·2,0 + A2 (c·S+s)·2,1 ) (reading off the real and imaginary part from the 1st column of the 2 ×2 block that represents aj). As the square root of the relative per-pixel intensity is real, the input-image amplitudes in this approach likewise get represented by a 200 ×2-matrix B. Speciﬁcally, if e.g. Q2,5 is the contribution of pixel (y= 2,x = 5)’s 13Published as a conference paper at ICLR 2021 brightness to the total image-brightness, this gets represented as: ( B(2·10+5)·2,0 B(2·10+5)·2+1,0 B(2·10+5)·2,1 B(2·10+5)·2+1,1 ) = ( B50,0 B51,0 B50,1 B51,1 ) = ( √ Q2,5 0 0 √ Q2,5 ) . (14) The off-diagonal part, which would correspond to the imaginary part of the amplitude, is zero here. The matrix that gets exponentiated is a real200×200 matrix, and its exponential, which also is a real200×200 matrix, gets multiplied from the right with the 200 ×2 matrix of input-image amplitudes and gives the real200 ×2-matrix A from above that contains the real and imaginary parts of class- and style-amplitudes. The real 200 ×200 matrix under the exponential only depends on 100 ×100 real parameters WP . Calling the 200 ×200-matrix M, the “2 ×2-blocking” prescription to obtain its entries from WP is: ( Mi·2 ,j·2 Mi·2 ,j·2+1 Mi·2+1,j·2 Mi·2+1,j·2+1 ) = ( (Wij −Wji) −(Wij + Wji) (Wij + Wji) ( Wij −Wji) ) . (15) Finally, we need a backpropagation-friendly prescription for computing a good approximation to the matrix exponential. The theory of compact Lie groups tells us that we can reach every ‘generalized’ (since complex) rotation matrix by exponentiating matrices where each entry is from some not too large interval. For matrices with small entries only, we can use a truncated Taylor polynomial to get a good numerical approximation of its exponential, using expm(M) ≈I+ M + 1 2M ·M + 1 6M ·M ·M + ..., (16) and we can reduce the problem of ﬁnding the matrix exponential of a matrix where this series requires many terms to give a good approximation by repeated halving and squaring, repeatedly using the property expm(M) = expm(M/2)2 = expm(M/2) ·expm(M/2). For the problem discussed here, the angle-ranges for rotations that need to be considered are limited, and this makes it feasible to in-advance pick both a number of squarings (such as: 8) and a maximal term in the Taylor expansion (such as: 10th power), and get very good results. The numerical computation implemented in the supplementary material, being based on TensorFlow, deviates from the procedure described here in two relevant ways. First, while TensorFlow’s differentiable matrix exponentiation algorithm employs repeated halving/squaring, it uses a Padé rather than Taylor approximation to compute the matrix exponential of a matrix with small entries. Second, TensorFlow can directly backpropagate through complex intermediate quantities, and handles the transition between real and complex gradients in just the way that one also obtains when expanding complex numbers to real 2 ×2 blocks as described above. It can however avoid the inefﬁciency associated with using actual real2 ×2 matrix blocks that make every real and every imaginary part show up in memory not once, but twice. 14",
      "references": [
        "Direct-manipulation visualization of deep networks",
        "MNIST handwritten digit database",
        "A scheme for efficient quantum computation with linear optics",
        "Optical simulation of quantum logic",
        "Factoring integers with young’s n-slit interferometer",
        "Factoring and fourier transformation with a mach-zehnder interferometer",
        "Nanophotonic media for artificial neural inference",
        "The Feynman lectures on physics; New millennium ed.",
        "Quantum Mechanics Non-Relativistic Theory, Third Edition: Volume 3",
        "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
        "Experimental realization of any discrete unitary operator",
        "Identifying and correcting mislabeled training instances"
      ],
      "meta_data": {
        "arxiv_id": "2008.05859v2",
        "authors": [
          "Thomas Fischbacher",
          "Luciano Sbaiz"
        ],
        "published_date": "2020-08-13T12:37:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces a room-temperature, single-photon \"toy\" quantum computing model for image classification and proves that allowing quantum interference before the first-photon detection can substantially beat any classical classifier constrained to the same information. Provides tight classical accuracy upper bounds (22.96% for MNIST, 21.38% for Fashion-MNIST) and trains a physically-realizable unitary optical transformation that attains 41.27% and 36.14% accuracy respectively. Supplies TensorFlow code and optical blueprints, positioning the setup as both an ML benchmark and an educational tool for quantum measurement.",
        "methodology": "Represent each 28×28 image as the amplitudes of a single photon’s spatial wave-function. Pad to dimension M=790 and learn a unitary transform U∈U(M) implemented by linear optics. U is parametrized as exp(iH) with H anti-Hermitian obtained from a real weight matrix W, guaranteeing unitarity. Stochastic gradient descent (cross-entropy loss) back-propagates through the complex matrix exponential (TensorFlow). After training, U is decomposed into beam-splitters and phase-shifters via the Reck scheme. Classical limit derived by maximum-likelihood per-pixel argument; quantum advantage evaluated via simulated photon statistics.",
        "experimental_setup": "Datasets: standard MNIST and Fashion-MNIST test sets (60k train /10k test each). Classical bound computed analytically from per-pixel class frequencies. Quantum model trained on the training split; evaluation measures the average probability that a single detected photon yields the correct label. Reported accuracies: classical 22.96% (MNIST) /21.38% (Fashion); quantum 41.27% /36.14%. Confusion matrices, mutual information, and down-sampled 10×10 MNIST demo (36.05% accuracy) are provided. All experiments are simulation-based in TensorFlow 2, code released with supplementary material.",
        "limitations": "Purely simulated; no physical implementation so optical loss, phase noise, detector dark counts, and component tolerances are ignored. Uses only single-photon, linear-optic interference—no entanglement or nonlinearities—so not scalable to general quantum computation. Device is task-specific and not easily reconfigurable. Assumes equal style-space dimension per class and ideal datasets; mislabeled data (e.g., Fashion-MNIST) and probability calibration issues remain. Results limited to small grayscale images; broader applicability untested.",
        "future_research_directions": "1) Build and characterize a real photonic prototype to validate simulated gains under realistic noise and loss. 2) Explore reconfigurable or programmable interferometers to support multiple tasks. 3) Extend to multi-photon or entangled inputs and investigate KLM-type architectures for more complex quantum ML. 4) Develop calibration or nonlinear post-processing to improve decision accuracy beyond raw probabilities. 5) Study robustness to dataset noise, optical imperfections, and scaling to higher-resolution images or other modalities. 6) Analyze theoretical limits when style-space allocation is uneven or when additional prior information is available.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Vocabulary-free Image Classification",
      "full_text": "Vocabulary-free Image Classification Alessandro Conti1 Enrico Fini1 Massimiliano Mancini1 Paolo Rota1 Yiming Wang2 Elisa Ricci1,2 1University of Trento 2Fondazione Bruno Kessler (FBK) Abstract Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as V ocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard- to-discriminate fine-grained categories. In this work, we first empirically verify that representing this semantic space by means of an external vision-language database is the most effective way to obtain semantically relevant content for classifying the image. We then propose Category Search from External Databases (CaSED), a method that exploits a pre-trained vision-language model and an external vision- language database to address VIC in a training-free manner. CaSED first extracts a set of candidate categories from captions retrieved from the database based on their semantic similarity to the image, and then assigns to the image the best matching candidate category according to the same vision-language model. Experiments on benchmark datasets validate that CaSED outperforms other complex vision- language frameworks, while being efficient with much fewer parameters, paving the way for future research in this direction1. 1 Introduction Large-scale Vision-Language Models (VLMs) [47, 62, 34] enabled astonishing progress in computer vision by aligning multimodal semantics in a shared embedding space. This paper focuses on their use for image classification, where models such as CLIP [ 47] demonstrated strength in zero-shot transfer. While we witnessed advances in VLM-based classification in many directions, e.g. prompt learning [53, 66], scaling up to larger models and datasets [ 25, 45, 6], or by jointly considering captioning task [62, 34], they all assume a finite set of target categories, i.e. the vocabulary, to be pre-defined and static (as shown in Fig. 1a). However, this assumption is fragile, as it is often violated in practical applications (e.g. robotics, autonomous driving) where semantic categories can either differ from the development/training to the deployment/testing or evolve dynamically over time. In this work, we remove this assumption and study the new task of V ocabulary-free Image Classifica- tion (VIC). The objective of VIC is to assign an image to a class that belongs to an unconstrained language-induced semantic space at test time, without a vocabulary, i.e. without a pre-defined set of categories (as shown in Fig. 1b). The unconstrained nature of the semantic space makes VIC a challenging problem. First, the search space is extremely large, with a cardinality on the order 1Code and demo is available at https://github.com/altndrr/vic 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.00917v3  [cs.CV]  12 Jan 2024Blue Jay Cassowary Flamingo Kiwi Pelican Cassowary candidate estimation Cassowary Emu Peacock Rhea Kiwi Cassowary Unconstrained  language-induced   semantic space (a) VLM-based classification (b) V ocabulary-free Image Classification Figure 1: Vision-Language Model (VLM)-based classification (a) assumes a pre-defined set of target categories, i.e. the vocabulary, while our novel task (b) lifts this assumption by directly operating on the unconstrained language-induced semantic space, without a known vocabulary. fv VLM and ft VLM denote the pre-trained vision and text models of a VLM, respectively. of millions of semantic concepts 2, way larger than any existing image classification benchmark (e.g. ImageNet-21k [10]). Second, it also includes very fine-grained concepts that might be hard to discriminate by the model. Third, as the categories encountered at test time are undefined beforehand, VIC calls for classification methods that do not rely on any vocabulary-aware supervision. Recent web-scale Vision-Language Databases (VLDs) [51, 54], offer a unique opportunity to address VIC, as they cover a wide set of semantic concepts, ranging from general to highly specific ones. We empirically show that such external databases allow identifying semantic content that is more relevant to the target category than the captioning-enhanced VLMs supporting visual queries (e.g. BLIP-2 [33]). Motivated by this observation, we propose a training-free method for VIC, Category Search from External Databases (CaSED), which jointly leverages the discriminative multimodal representations derived from CLIP [47] and the information provided by recent VLDs ( e.g. PMD [54]). CaSED operates in two steps: it first coarsely estimates a set of candidate categories for the test image, then it predicts the final category via multimodal matching. Specifically, we first retrieve the captions from the database that are semantically closer to the input image, from which we extract candidate categories via text parsing and filtering. We then estimate the similarity score between the input image and each candidate category via CLIP, using both visual and textual information, predicting as output the best matching candidate. CaSED exploits the pre-trained CLIP without further training, thus being flexible and computationally efficient. We experiment on several datasets, considering both coarse- (e.g. Caltech-101 [14], UCF101 [55]) and fine-grained (e.g. FGVC-Aircraft [40], Flowers-102 [43]) classification tasks. To quantitatively assess the performance of methods addressing VIC, we also propose a set of metrics to measure how well the predicted class matches the semantics of the ground-truth label. Across all tasks and metrics, CaSED consistently outperforms all baselines, including VLMs as complex as BLIP-2 [33]. We believe that thanks to its simplicity and effectiveness, our training-free CaSED can serve as a competitive baseline for future works aiming to address the challenging VIC task. To summarize, this work provides the following contributions: • We explore the task of V ocabulary-free Image Classification where the goal is to assign a class to an image over an unconstrained set of semantic concepts, overcoming the fundamental assumption of existing VLM-based methods for image classification. We formalize this task and suggest specific evaluation metrics that can be used as a reference for future works. • We propose CaSED, the first method to address VIC thanks to the adoption of large captioning databases. Notably, CaSED is training-free, not requiring any additional parameter nor finetuning of the network’s textual and visual encoders. • Our large-scale evaluation demonstrates that CaSED consistently outperforms a more complex VLM such as BLIP-2 [33] on VIC, while requiring much fewer parameters. 2We estimate the number of concepts using BabelNet [42], which is close to 4 million for English. 22 Related work Vision-Language Models. Leveraging large-scale datasets with image-text pairs [51, 50, 54], recent works train models by mapping the two modalities into a shared representation space [ 28, 18, 11, 47, 26, 35, 15]. A notable example is CLIP [ 47] which, using modality-specific encoders and a contrastive objective to align their output representations, showed remarkable performance on zero-shot classification. Subsequent works improved CLIP by e.g. connecting the two modalities via cross-modal attention [ 35], multi-object representation alignment [ 64], learning from weak- supervision [60] or unaligned data [54]. Another line of works improved vision-language pre-training for complex vision-language tasks, such as image captioning and visual question answering (VQA) [62, 22, 34, 1]. In this context, BLIP [34] exploits web data and generated captions to supervise pre-training of a multimodal architecture, outperforming existing VLMs on both captioning and VQA. The current state-of-the-art method BLIP-2 [33] trains a module connecting the two modalities on top of a frozen visual encoder and a frozen large-language model, enabling instructed zero-shot image-to-text generation. In this work, we challenge a fundamental assumption of zero-shot classification with VLMs: the set of target classes is known a priori. We propose a new task, VIC, which sidesteps this assumption, performing classification in a language-induced open-ended space of semantic categories. We show that even BLIP-2 struggles in this scenario while external multimodal databases provide valuable priors for inferring the semantic category of an image. As a final note, VIC differs from open- vocabulary recognition (e.g. [63, 17]) since the latter assumes that the list of target classes is known and available to the model during inference. Retrieval augmented models. In natural language processing, multiple works showed the benefit of retrieving information from external databases, improving the performance of large language models [19, 32, 3]. In computer vision, such a paradigm has been used mostly to deal with the imbalanced distribution of classes. Examples are [38, 39], addressing long-tail recognition by learning to retrieve training samples [38] or image-text pairs from an external database [39]. Similarly, [58] retrieves images from a given dataset to learn fine-grained visual representations. More recently, retrieval-augmentation has been extended to various types of sources for visual question answering [23], as well as to condition the generative process in diffusion models [2], or image captioning [48]. Our work is close in spirit to [ 39], as we exploit an external database. However, [ 39] assumes a pre-defined set of classes (and data) available for training a retrieval module, something we cannot have for the extremely large semantic space of VIC. In CaSED, retrieval is leveraged to first create a set of candidate classes, and to then perform the final class prediction. Moreover, we assume the database to contain only captions, and not necessarily paired image-text data, thus being less memory-demanding. Finally, the performance of retrieval models is largely affected by the employed database. In the context of VLMs, researchers collected multiple open datasets to study vision-language pre- training. Two notable examples are LAION-5B [51], collected by filtering Common Crawl [8] via CLIP, and the Public Multimodal Datasets (PMD) [54], collecting image-text pairs from different public datasets, such as Conceptual Captions [52, 5], YFCC100M [57], Wikipedia Image Text [56], and Redcaps [12]. In our experiments, we use a subset of PMD as database and investigate how classification performance varies based on the size and quality of the database. 3 Vocabulary-free Image Classification Preliminaries. Given the image spaceX and a set of class labelsC, a classification modelf : X →C is a function mapping an imagex ∈ Xto its corresponding class c ∈ C. While C may be represented by indices that refer to semantic classes, here we focus on cases where C consists of a set of concept names that correspond to real entities. Note that C is a finite set whose elements belong to the semantic space S, i.e. C ⊂ S. In standard image classification, C is given a priori, and f is learned on a dataset of image-label pairs. Recently, the introduction of contrastive-based VLMs [47, 25], which learn a cross-modal aligned feature space, revised this paradigm by defining a function fVLM that infers the similarity between an 3C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Average 0 20 40 60 80 100Accuracy (%) Figure 2: Results of our preliminary study, showing the top-1 accuracy when matching semantic descriptions to ground-truth class names in ten different datasets. We compare BLIP-2 (VQA) and BLIP-2 (Captioning) with Closest Caption and Captions Centroid , i.e.the average represen- tation of the retrieved captions. We additionally highlight the Upper bound for zero-shot CLIP. Representing the large semantic space as VLDs and retrieving captions from it produces semantically more similar outputs to ground-truth labels w.r.t. querying outputs from VQA-enabled VLMs, while requiring 10 times fewer parameters compared to the latter. image and a textual description t ∈ T, i.e. fVLM : X × T →R, with T being the language space. Given this function, classification can be performed with: f(x) = arg max c∈C fVLM(x, ϕ(c)) (1) where ϕ(c) is a string concatenation operation, combining a fixed text template, i.e. a prompt, with a class name. This definition allows for zero-shot transfer, i.e. performing arbitrary classification tasks by re-defining the set C at test time, without re-training the model. However, such zero-shot transfer setup still assumes that the setC is provided. In this work, V ocabulary-free Image Classification (VIC) is formulated to surpass this assumption. Task definition. VIC aims to assign a class c to an image x without prior knowledge on C, thus operating on the semantic class space S that contains all the possible concepts. Formally, we want to produce a function f mapping an image to a semantic label in S, i.e. f : X → S. Our task definition implies that at test time, the function f has only access to an input image x and a large source of semantic concepts that approximates S. VIC is a challenging classification task by definition due to the extremely large cardinality of the semantic classes in S. As an example, ImageNet- 21k [10], one of the largest classification benchmarks, is 200 times smaller than the semantic classes in BabelNet [42]. This large search space poses a prime challenge for distinguishing fine-grained concepts across multiple domains as well as ones that naturally follow a long-tailed distribution. Semantic space representation. As the main challenge of VIC, how to represent the large semantic space plays a fundamental role in the method design. We can either model the multimodal semantic space directly with a VLM equipped with an autoregressive language decoder [ 34] or via image- text retrieval from VLDs. Consequently, we can approach VIC either via VQA-enabled VLMs by querying for the candidate class given the input image, or by retrieving and processing data from an external VLD to obtain the candidate class. To investigate the two potential strategies, we perform a preliminary experimental analysis to under- stand how well the output of a method semantically captures the image category, or in other words, to assess the alignment of class boundaries in the visual and textual representations. Specifically, we compare the semantic accuracy of querying VQA VLMs and of retrieving from VLDs w.r.t. the ground-truth class labels. We consider the output of a method as correct if its closest textual embed- ding among the target classes of the dataset corresponds to the ground-truth class of the test sample3. We exploit the text encoder of CLIP (ViT-L) [47] to obtain textual embeddings. 3Note that this metric is not the standard accuracy in image classification as we use distances in the embedding space to ground predictions from the unconstrained semantic space to the set of classes in a specific dataset. 4Regarding experimented methods, we select BLIP-2 [33] to represent VQA-enabled VLMs for its state-of-the-art performance in VQA benchmarks, while we use a subset of PMD [54] as the VLD. In particular, we compare the following methods: i) BLIP-2 VQA, which directly queries BLIP-2 for the image category; ii) BLIP-2 Captioning, which queries BLIP-2 for the image caption; iii) Closest Caption, which is the closest caption to the image, as retrieved from the database; iv) Caption Centroid, which averages the textual embeddings of the 10 most similar captions to the input image. As we use CLIP embeddings, if visual and textual representations perfectly align, the performance would be the same as zero-shot CLIP with given target classes. We thus report zero-shot CLIP to serve as the upper bound for retrieval accuracy. We experiment on a variety of test datasets for both coarse- and fine-grained classification (see details in Sec. 5), and report the results in Fig. 2. The average textual embedding of the retrieved captions (i.e. Caption Centroid) achieves the best semantic accuracy for 9 datasets out of 10, consistently surpassing methods based on BLIP-2. On average, the accuracy achieved by Caption Centroid is 60.47%, which is +17.36% higher than the one achieved by BLIP-2 Captioning (43.11%). Moreover, Captions Centroid achieves results much closer to the CLIP upper bound (67.17%) than the other approaches. Notably, such VLD-based retrieval is also computationally more efficient, faster (~ 4 second for a batch size of 64 on a single A6000 GPU), and requires fewer parameters (approximately 10 times less) than BLIP-2 (see Tab. 7 in Appendix B). The results of this preliminary study clearly suggest that representing the large semantic space with VLDs can produce (via retrieval) semantically more relevant content to the input image, in comparison to querying VQA-enabled VLMs, while being computationally efficient. Based on this conclusion, we develop an approach, Category Search from External Databases (CaSED), that searches for the semantic class from the large semantic space represented in the captions of VLDs. 4 CaSED: Category Search from External Databases Our proposed method CaSED finds the best matching category within the unconstrained semantic space by multimodal data from large VLDs. Fig. 3 provides an overview of our proposed method. We first retrieve the semantically most similar captions from a database, from which we extract a set of candidate categories by applying text parsing and filtering techniques. We further score the candidates using the multimodal aligned representation of the large pre-trained VLM, i.e. CLIP [47], to obtain the best-matching category. We describe in detail each process in the following. 4.1 Generating candidate categories We first restrict the extremely large classification space to a few most probable candidate classes. Let fVLM be the pre-trained VLM and D be the external database of image captions. Given an input x, we retrieve the set Dx ⊂ D of K closest captions to the input image via Dx = top-k d∈D fVLM(x, d) = top-k d∈D ⟨fv VLM(x), ft VLM(d)⟩, (2) where fv VLM : X → Zis the visual encoder of the VLM,ft VLM : T → Zis the textual encoder, andZ is their shared embedding space. The operation ⟨·, ·⟩ indicates the computation of the cosine similarity. Note that our approach is agnostic to the particular form of D, and that it can accommodate a flexible database size by including captions from additional resources. From the set Dx, we then extract a finite set of candidate classes Cx by performing simple text parsing and filtering techniques, e.g. stop-words removal, POS tagging. Details on the filtering procedure can be found in Appendix A. 4.2 Multimodal candidate scoring Among the small set Cx, we score each candidate by accounting for both visual and textual semantic similarities using the VLM encoders, in order to select the best-matching class for the input image. Image-to-text score. As the image is the main driver for the semantic class we aim to recognize, we use the visual information to score the candidate categories. We denote sv c as the visual score of each 5a v g Check out this ﬁerce fowl -  the cassowary means  business! Check out this prehistoric-  looking bird! This colorful cassowary is  a true hidden gem of the  animal kingdom. Animal Gem Bird Cassowary Fowl Animal Bird Cassowary Fowl Gem Retrieval from  text database Check out this ﬁerce fowl -  the cassowary means  business! Check out this prehistoric-  looking bird! This colorful cassowary is  a true hidden gem of the  animal kingdom. … Cassowary Gem Cassowary … Cassowary Gem Bird Bird Figure 3: Overview of CaSED. Given an input image, CaSED retrieves the most relevant captions from an external database filtering them to extract candidate categories. We classify image -to- text and text -to- text , using the retrieved captions centroid as the textual counterpart of the input image. candidate category c and compute it as the similarity between the visual representation of the input image and the textual representation of the candidate name: sv c = ⟨fv VLM(x), ft VLM(c)⟩. (3) The higher value of sv c indicates a closer alignment between the target image and the candidate class. Text-to-text score. While the image-to-text score sv c is effective, there exists a well-known modality gap in the space Z, harming the performance of zero-shot models [ 36]. As suggested by Fig. 2 in Sec. 3, the semantic relevance of the retrieved captions, and their centroid in particular, is high w.r.t. the underlying ground-truth label. We are therefore motivated to exploit such attributes and introduce a unimodal text-to-text scoring to mitigate the modality gap of cross-modal scoring. Formally, we define the centroid ¯dx of the retrieved captions as: ¯dx = 1 K X d∈Dx ft VLM(d), (4) where K is the number of retrieved captions. We then define the text-based matching score st c as the similarity between the centroid and the candidate category: st c = ⟨ ¯dx, ft VLM(c)⟩. (5) A higher value of st c means a higher alignment between the caption centroid and the candidate embedding. Note that the semantic relevance of the caption centroid is an inherent property of the text encoder of VLMs (i.e. CLIP). Since CLIP is trained with an image-text alignment loss, its text encoder focuses on the visual elements of the caption, discarding parts that are either non-visual or non-relevant to the visual content. This improves the model’s robustness to noisy candidates. Final predicted candidate. To predict the final candidate, we merge the two scores, obtaining the final score sc for each candidate c as: sc = α σ(sv c) + (1 − α) σ(st c) (6) where σ(·) is the softmax operation on the two scores of each candidate class, and α is a hyperpa- rameter regulating the contribution of the two modalities. Finally we obtain the output category as f(x) = arg maxc∈Cx sc. Notably, CaSED respects the VIC task definition, performing classification without known class priors, while being training-free with the use of a pre-trained and frozen VLM. This makes the approach flexible and applicable to a variety of architectures and databases. 65 Experiments We evaluate CaSED in comparison to other VLM-based methods on the novel task V ocabulary-free Image Classification with extensive benchmark datasets covering both coarse-grained and fine-grained classification. We first describe the experimental protocol in terms of the datasets, the proposed evaluation metrics, and the baselines applicable to this task (Sec. 5.1). We then discuss the quantitative results regarding the comparison between our method and baselines (Sec. 5.2). Finally, we present a thorough ablation to justify the design choices of CaSED (Sec. 5.3). In addition, we provide a cost comparison between the baseline methods and CaSED in Appendix B. We also offer further ablation of our method in Appendix C regarding architecture and database, and showcase qualitative results of predicted categories from multiple datasets in Appendix D. 5.1 Experimental protocol Datasets. We follow existing works [53, 66] and use ten datasets that feature both coarse-grained and fine-grained classification in different domains: Caltech-101 (C101) [ 14], DTD [ 7], Eu- roSAT (ESAT) [21], FGVC-Aircraft (Airc.) [ 40], Flowers-102 (Flwr) [43], Food-101 (Food) [ 4], Oxford Pets (Pets), Stanford Cars (Cars) [ 29], SUN397 (SUN) [ 61], and UCF101 (UCF) [ 55]. Additionally, we used ImageNet [10] for hyperparameters tuning. Evaluation metrics. Due to the unconstrained nature of the semantic space, evaluating the effective- ness of methods for VIC is not trivial. In this paper, we propose to use two main criteria, namely semantic relevance, i.e. the similarity of the predicted class w.r.t. the ground-truth label, andimage grouping, i.e. the quality of the predicted classes for organizing images into clusters. For semantic, we consider i) Semantic Similarity, i.e. the similarity of predicted/ground-truth labels in a semantic space, and ii) Semantic IoU, i.e. the overlap of words between the prediction and the true label. More formally, given an input x with ground-truth label y and prediction ˆc = f(x), we compute the Semantic Similarityas ⟨g(ˆc), g(y)⟩, where g : T → Yis a function mapping text to an embedding space Y. Since we want to model free-form text, we use Sentence-BERT [ 49] as g. For Semantic IoU, given a predicted label c, and assuming c being a set of words, we compute the Semantic IoU as |c ∩ y|/|c ∪ y|, where y is the set of words in the ground-truth label. To assess grouping, we measure the classic Cluster Accuracyby first clustering images according to their predicted label, and then assigning each cluster to a ground-truth label with Hungarian matching. Sometimes, this mapping is resolved with a many-to-one match, where a predicted cluster is assigned to the most present ground-truth label. This evaluation draws inspiration from the protocols used for deep visual clustering [59, 24, 20]. Baselines. We consider three main groups of baselines for our comparisons. The most straightforward baselines consist of using CLIP with large vocabularies, such as WordNet [ 41] (117k names) or the English Words (234k names [16]). As an upper bound, we also consider CLIP with the perfect vocabulary, i.e. the ground-truth names of the target dataset (CLIP upper bound). Due to lack of space, we only report results for CLIP with ViT-L [13], while results with other architectures are reported in Appendix C. The second group of baselines consists of captioning methods, as captions can well describe the semantic content of images. We consider two options: captions retrieved from a database and captions generated by a pre-trained image captioning model. For the former we exploit a large collection of textual descriptions, retrieving the most fitting caption for the given image. For the latter, we exploit BLIP-2 [ 33] — a VLM with remarkable performance on a variety of tasks, including image captioning — to provide a description for the image. The last group of baselines consists of using a VQA model to directly predict the class name associated with the image. Again, we consider BLIP-2 [33], since being highly effective also in VQA. We evaluate BLIP-2 over both ViT-L and ViT-g throughout the experiments4. Implementation details. Our experiments were conducted using NVIDIA A6000 GPUs with mixed- bit precision. As database, we use a subset of PMD [ 54], containing five of its largest datasets: Conceptual Captions (CC3M) [52], Conceptual Captions 12M (CC12M) [5], Wikipedia Image Text (WIT) [56], Redcaps [12], and a subset of [57] used for PMD (YFCC100M*). Further details on the 4Following the BLIP-2 [33] demo, for captioning, we used the prompt “Question: what’s in the image? Answer:”. For VQA, we used “Question: what’s the name of the object in the image? Answer: a”. 7Method Cluster Accuracy (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 34.0 20.1 16.7 16.7 58.3 40.9 52.0 29.4 18.6 39.5 32.6 English Words 29.1 19.6 22.1 15.9 64.0 30.9 44.4 24.2 19.3 34.5 30.4 Caption Closest Caption 12.8 8.9 16.7 13.3 28.5 13.1 15.0 8.6 20.0 17.8 15.5 BLIP-2 (ViT-L) 26.5 11.7 23.3 5.4 23.6 12.4 11.6 19.5 14.8 25.7 17.4 BLIP-2 (ViT-g) 37.4 13.0 25.2 10.0 29.5 19.9 15.5 21.5 27.9 32.7 23.3 VQA BLIP-2 (ViT-L) 60.4 20.4 21.4 8.1 36.7 21.3 14.0 32.6 28.8 44.3 28.8 BLIP-2 (ViT-g) 62.2 23.8 22.0 15.9 57.8 33.4 23.4 36.4 57.2 55.4 38.7 CaSED 51.5 29.1 23.8 22.8 68.7 58.8 60.4 37.4 31.3 47.7 43.1 CLIP upper bound 87.6 52.9 47.4 31.8 78.0 89.9 88.0 65.3 76.5 72.5 69.0 Table 1: Cluster Accuracy on the ten datasets. Green is our method, gray shows the upper bound. Method Semantic Similarity (x100)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 48.6 32.7 24.4 18.9 55.9 49.6 53.7 44.9 28.8 44.2 40.2 English Words 39.3 31.6 19.1 18.6 43.4 38.0 44.2 36.0 19.9 34.7 32.5 Caption Closest Caption 42.1 23.9 23.4 29.2 40.0 46.9 40.2 39.8 49.2 40.3 37.5 BLIP-2 (ViT-L) 57.8 31.4 39.9 24.4 36.1 44.6 29.0 45.3 46.4 38.0 39.3 BLIP-2 (ViT-g) 63.0 33.1 36.2 24.3 45.2 51.6 31.6 48.3 61.0 44.6 43.9 VQA BLIP-2 (ViT-L) 70.5 34.9 29.7 29.1 48.8 42.0 40.0 50.6 52.4 48.6 44.7 BLIP-2 (ViT-g) 73.5 36.5 31.4 30.8 59.9 52.1 43.9 53.3 65.1 55.1 50.1 CaSED 65.7 40.0 32.0 30.3 55.5 64.5 62.5 52.5 47.4 54.1 50.4 CLIP upper bound 90.8 69.8 67.7 66.7 83.4 93.7 91.8 80.5 92.3 83.3 82.0 Table 2: Semantic Similarity on the ten datasets. Values are multiplied by x100 for readability. Green highlights our method and gray indicates the upper bound. selection are left for Appendix C. We speed up the retrieval process by embedding the database via the text encoder ft VLM and using fast indexing technique, i.e. FAISS [27]. We tuned the α hyperparameter of Eq. (6) and the number of retrieved captions K of our method on the ImageNet dataset, finding that α = 0.7 and K = 10 led to the best results. We use these values across all experiments. 5.2 Quantitative results The results of CaSED and the baselines are presented in Table 1, Table 2, and Table 3 for Cluster Accuracy ( %), Semantic Similarity, and Semantic IoU ( %), respectively. CaSED consistently outperforms all baselines in all metrics on average and in most of the datasets. Notably, CaSED surpasses BLIP-2 (VQA) over ViT-g by+4.4% in Cluster Accuracy and +1.7% on Semantic IoU, while using much fewer parameters (i.e. 102M vs 4.1B). The gap is even larger over the same visual backbone, with CaSED outperforming BLIP-2 on ViT-L (VQA) by+14.3% on Cluster Accuracy, +5.7 in Semantic Similarity, and +6.8% on Semantic IoU. These results highlight the effectiveness of CaSED, achieving the best performance both in terms of semantic relevance and image grouping. An interesting observation from the tables is that simply applying CLIP on top of a pre-defined, large vocabulary, is not effective in VIC. This is due to the challenge of classifying over a huge search space, where class boundaries are hard to model. This is confirmed by the results of CLIP with English Words having a larger search space but performing consistently worse than CLIP with WordNet across all metrics (e.g. −7.7 on Semantic Similarity, -3.2% on Semantic IoU). A final observation relates to the captioning models. Despite their ability to capture the image semantic even in challenging settings ( e.g. 39.3 Semantic Similarity of BLIP-2 ViT-L on ESAT), captions exhibit high variability across images of the same category. This causes the worst performance on Clustering and Semantic IoU across all approaches (e.g. almost −20% less than CaSED on average in terms of Cluster Accuracy), thus demonstrating that while captions can effectively describe the content of an image, they do not necessarily imply better categorization for VIC. 8Method Semantic IoU (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 15.0 3.0 1.3 0.5 31.3 7.8 14.7 9.0 4.8 3.8 9.1 English Words 8.0 2.0 0.0 1.1 16.4 2.0 17.2 8.1 2.7 1.8 5.9 Caption Closest Caption 4.5 0.8 1.3 1.9 5.9 3.1 3.0 2.3 11.4 1.0 3.5 BLIP-2 (ViT-L) 13.4 1.4 4.8 0.0 7.5 4.7 1.7 4.7 11.6 1.1 5.1 BLIP-2 (ViT-g) 16.8 1.8 4.1 0.1 13.9 7.9 2.9 5.7 24.7 1.9 8.0 VQA BLIP-2 (ViT-L) 36.1 1.8 7.0 0.1 21.5 3.7 5.7 11.5 18.9 2.5 10.9 BLIP-2 (ViT-g) 41.5 2.4 7.5 2.0 38.0 8.6 10.2 13.8 33.2 2.8 16.0 CaSED 35.4 5.1 2.3 4.8 33.1 19.4 35.1 17.2 16.2 8.4 17.7 CLIP upper bound 86.0 52.2 51.5 28.6 75.7 89.9 88.0 66.6 84.5 71.3 69.4 Table 3: Semantic IoU on the ten datasets. Green is our method, gray shows the upper bound. Candidates Scoring CA S-Sim. S-IoU Generation Vis. Lang. Generative [33]✓ 23.3 47.1 11.9 ✓ 41.7 49.3 17.0 Retrieval ✓ 42.7 50.3 17.0 ✓ ✓ 43.1 50.4 17.7 (a) Ablation on candidate generation and scoring. Database Size CA S-Sim. S-IoU CC3M 2.8M 34.2 47.9 13.1 WIT 4.8M 34.6 42.9 12.1 Redcaps 7.9M 42.0 49.5 17.2 CC12M 10.3M 44.0 51.3 18.3 YFCC100M* 29.9M 40.7 48.8 17.1 All 54.8M 43.1 50.4 17.7 (b) Ablation on the database. Table 4: Ablation studies. Metrics are averaged across the ten datasets. Green highlights our setting. Bold represents best, underline indicates second best. 5.3 Ablation studies In this section, we present the ablation study associated with different components of our approach. We first analyze the impact of our retrieval-based candidate selection and multimodal scoring. We then show the results of CaSED for different databases, and how the number of retrieved captions impacts the performance. Candidates generation. We consider two options to generate the set of candidate classes. The first uses BLIP-2 (ViT-g), asking for multiple candidate labels for the input image. The second is our caption retrieval and filtering strategy. Table 4a shows the results where, for fairness, we score both sets with the same VLM (i.e. CLIP ViT-L). Our approach consistently outperforms BLIP-2 across all metrics (e.g. 41.7 vs 23.3 for Cluster Accuracy). This confirms the preliminary results of our study in Fig. 2, with retrieved captions providing better semantic priors than directly using a powerful VLM. Multimodal scoring. The second ablation studies the effect of our proposed multimodal scoring vs its unimodal counterparts. As Table 4a shows, multimodal candidate scoring provides the best results across all metrics, with clear improvements over the visual modality alone ( i.e. +1.4% of Cluster Accuracy). Notably, scoring via language only partially fills the gap between visual and multimodal scoring (e.g. +1% on Cluster Accuracy and +1 on Semantic Similarity), confirming that caption centroids contain discriminative semantic information. Retrieval database. We analyze the impact of retrieving captions from different databases, using five public ones, i.e. CC3M, WIT, Redcaps, CC12M, and a subset of YFCC100M. The databases have different sizes (e.g. from 2.8M captions of CC3M to 29.9M captions of the YFCC100M subset), and different levels of noise. As shown in Table 4b, the results tend to improve as the size of the database increases (e.g. +8.9% on Cluster Accuracy over CC3M). However, the quality of the captions influences the performance, with CC12M and Redcaps alone achieving either comparable or slightly better results than the full database. These results suggest that while performance improves with the size of the database, the quality of the captions has a higher impact than the mere quantity. 91 2 5 10 20 Number of retrieved captions 10 20 30 40 50Metric value Figure 4: Ablation on the number of re- trieved captions. We report Cluster accuracy (%) , Semantic similarity , and Semantic IoU (%) . Number of captions. Finally, we check how performance varies w.r.t. the number of retrieved captions. As shown in Fig. 4, all metrics con- sistently improve as the number of captions in- creases from 1 to 10. After that, performance tends to saturate, with a slight decrease in terms of Semantic Similarity for K = 20. These re- sults suggest that while a minimum number of captions is needed to fully capture the semantics of the image, the possible interference of less- related (or even noisy) captions may impact the final performance. Future research may further improve the performance on VIC by focusing on how to deal with noisy retrieval results. 6 Discussion and conclusions In this work, we proposed a new task, VIC, which operates on an unconstrained semantic space, without assuming a pre-defined set of classes, a brittle assumption of VLM-based classification. We experimentally verified that multimodal databases provide good semantic priors to restrict the large search space, and developed CaSED, an efficient training-free approach that retrieves the closest captions to the input image to extract candidate categories and scores them in a multimodal fashion. On different benchmarks, CaSED consistently achieved better results than more complex VLMs. Limitations and future works. The performance of CaSED strongly depends on the choice of the retrieval database, with potential issues in retrieving concepts that are not well represented in the latter. Moreover, if the domain of application contains fine-grained concepts, a generic database might not be suitable. Without any prior information on the test labels, it is hard to predict the performance of a database a priori. On the other hand, CaSED can flexibly mitigate this issue by incrementally including new concepts in the database (even domain-specific ones) from textual corpora, without retraining. Future research may explore strategies to automatically select/extend a database based on test samples and/or pre-computing the best database to use in the absence of test label information. Additionally, as CaSED lacks control over the classes contained in the database, its predictions might reflect potential biases. Improvements in mitigating biases and data quality control would reduce this issue. Another limitation is that CaSED does not keep track of its output history. This may lead to inconsistent predictions, i.e. assigning slightly different labels to images of the same semantic concept (e.g. cassowary vs Casuarius). Equipping CaSED with a memory storing the predicted labels may address this issue. Finally, CaSED does not deal with different class granularities: e.g. an image of a cassowary can be as well predicted as a bird. Future works may disambiguate such cases by explicitly integrating the user needs within VIC models. Broader impact. CaSED addresses VIC in a scalable and training-free manner. We believe that our new problem formulation, metrics, and the effectiveness of CaSED will encourage future research in this topic, overcoming the limiting assumption of VLM-based classification and allowing the power of VLMs to benefit dynamic and unconstrained scenarios. Acknowledgements This work was supported by the MUR PNRR project FAIR - Future AI Research (PE00000013) and ICSC National Research Centre for High Performance Computing, Big Data and Quantum Computing (CN00000013), funded by the NextGenerationEU. E.R. is partially supported by the PRECRISIS, funded by the EU Internal Security Fund (ISFP-2022-TFI-AG-PROTECT-02-101100539), the EU project SPRING (No. 871245), and by the PRIN project LEGO-AI (Prot. 2020TA3K9N). The work was carried out in the Vision and Learning joint laboratory of FBK and UNITN. We also thank the Deep Learning Lab of the ProM Facility for the GPU time. 10References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022. [2] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented diffusion models. NeurIPS, 2022. [3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In ICML, 2022. [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In ECCV 2014, 2014. [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. [6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. [7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. [8] Common Crawl. Common crawl, 2023. Accessed May 17, 2023. https://commoncrawl.org/. [9] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive language- image pre-training: A clip benchmark of data, model, and supervision. arXiv, 2022. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. [11] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In CVPR, 2021. [12] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv, 2021. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004. [15] Enrico Fini, Pietro Astolfi, Adriana Romero-Soriano, Jakob Verbeek, and Michal Drozdzal. Improved baselines for vision-language pre-training, 2023. [16] FreeBSD. Web2 dictionary (revision 326913), 2023. Accessed May 17, 2023.https://svnweb.freebsd. org/base/head/share/dict/web2?view=markup&pathrev=326913. [17] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, 2022. [18] Lluis Gomez, Yash Patel, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised learning of visual features through embedding images into text topic spaces. In CVPR, 2017. [19] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In ICML, 2020. [20] Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Automat- ically discovering and learning new visual categories with ranking statistics. In ICLR, 2019. [21] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Selected Topics in Applied Earth Observations and Remote Sensing, 2019. [22] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In CVPR, 2022. [23] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In CVPR, 2023. [24] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, 2019. [25] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [27] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. Transactions on Big Data, 2019. 11[28] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In ECCV, 2016. [29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. [30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. [31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020. [32] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. NeurIPS, 2020. [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv, 2023. [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. [35] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. NeurIPS, 2021. [36] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS, 2022. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [38] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In CVPR, 2019. [39] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In CVPR, 2022. [40] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv, 2013. [41] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. [42] Roberto Navigli and Simone Paolo Ponzetto. Babelnet: Building a very large multilingual semantic network. In ACL, 2010. [43] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008. [44] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. NeurIPS, 2011. [45] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for zero-shot transfer learning. arXiv, 2021. [46] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [48] Rita Ramos, Desmond Elliott, and Bruno Martins. Retrieval-augmented image captioning. In EACL, 2023. [49] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP-IJCNLP, 2019. [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. [51] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In NeurIPS Workshop Datacentric AI, 2021. [52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics, 2018. [53] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. arXiv, 2022. [54] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR, 2022. 12[55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv, 2012. [56] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia- based image text dataset for multimodal multilingual machine learning. In Research and Development in Information Retrieval, 2021. [57] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 2016. [58] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Hervé Jégou. Grafit: Learning fine-grained image representations with coarse labels. In ICCV, 2021. [59] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels. In ECCV, 2020. [60] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In ICLR, 2022. [61] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large- scale scene recognition from abbey to zoo. In CVPR, 2010. [62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv, 2022. [63] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In CVPR, 2021. [64] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. In ICML, 2022. [65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. [66] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 2022. 13Appendix A Candidates filtering With the closest captions retrieved from the external database given the input image (please refer to Sec. 4 of the main manuscript for further details), we post-process them to filter out a set of candidate category names. We first create a set of all words that are contained in the captions. Then we apply sequentially three different groups of operations to the set to (i) remove noisy candidates, (ii) standardize their format, and (iii) filter them. With the first group of operations, we remove all the irrelevant textual contents, such as tokens (i.e. \"<PERSON>\"), URLs, or file extensions. Note that, for the file extensions, we remove the extension but retain the file name as it might contain candidate class names. We also remove all the words that are shorter than three characters and split compound words by underscores or dashes. Finally, we remove all those terms containing symbols or numbers and meta words that are irrelevant to the classification task, such as \"image\", \"photo\", or \"thumbnail\". As shown in Table 5, when compared to having no operation for candidate filtering (first row), this set of operations removes inappropriate content and increases the accuracy of clusters by +6.4% and improves the semantic IoU by +0.3. However, we can observe a drop in semantic similarity by −1.5. This might be due to the removal of unnatural words that could still describe well the content of the image, i.e. underline- or dash-separated words, or URLs since they are longer w.r.t. natural words. The second group of operations standardize the candidate names by aligning words that refer to the same semantic class to a standard format, reducing class redundancy. For example, \"cassowary\" and \"Cassowary\" will be considered as a single class instead of two. To this end, we perform two operations—lowercase conversion and singular form conversion. With such standardizing conversions, we observe a sizeable boost in terms of performances when compared to the results obtained by applying only removal-related operations. As shown in Table 5, we achieve higher results across all three metrics, leading to a relative improvement of +7.2%, +0.7, and +1.2 in terms of cluster accuracy, semantic similarity, and semantic IoU, respectively. The last group of operations considers two forms of filtering, where the first aims to filter out entire categories of words via Part-Of-Speech (POS) tag and the second aims to filter out rare and noisy contents based on the word occurrences. We select these two operations since common dataset class names do not contain terms that carry no semantics e.g. articles and pronouns, and since [9] showed that CLIP performs better when exposed to a smaller amount of unique tokens. The POS tagging5 categorizes words into groups, such as adjectives, articles, nouns, or verbs, enabling us to filter all the terms that are not semantically meaningful for a classification task. Regarding the occurrence filtering, we first count how often a word appears in the retrieved captions and then we remove words that appear only once to make the candidate list less noisy. We can see from Table 5 that the inclusion of this final set of operations scores the best among all three metrics when compared to the results obtained when only the previous two groups of operations are applied. Number of captions vs number of selected candidates. To complement the previous analysis, in Table 6, we report the number of unique candidates extracted by the candidate filtering procedure, averaged over the ten datasets, and with an increasing number of retrieved captions, i.e., 1, 2, 5, 10, and 20. In the table, we show both the number of candidates extracted and the number of selected words. As the number of retrieved captions increases, the unique number of candidate words also increases, i.e., from 3849 with 1 caption to 28781 with 20. However, the number of selected words stabilizes around 800 as soon as we retrieve more than 1 caption. Having more captions reduces the noises in the selected words, something that might be present when relying on a single caption. B Computational cost We analyze the computational efficiency of CaSED versus BLIP-2 performing VQA and captioning, and report their respective number of parameters and inference time in Table 7. Notably, the methods 5We use the NLP library flair (https://github.com/flairNLP/flair). 14Operations CA S-Sim. S-IoU Remove Standardize Filter 27.7 48.8 15.0 ✓ 34.1 47.3 15.3 ✓ ✓ 41.3 48.0 16.5 ✓ ✓ ✓ 43.1 50.4 17.7 Table 5: Ablation on the candidate filtering operations. Metrics are averaged across the ten datasets. Num. captionsCA S-Sim. S-IoU Candidates Selected 1 30.9 43.2 12.4 3849 1047 2 35.0 46.4 14.6 6867 854 5 42.3 50.6 17.5 14548 775 10 43.1 50.4 17.7 22475 794 20 42.9 49.3 17.1 28781 802 Table 6: Extended ablation on the number retrieved captions. Green represents our selected configuration. We expand the results of Fig. 4 in the main manuscript to show the number of unique words extracted from captions (i.e., “candidates”) and the number of words selected by CaSED for classification (i.e., “selected”). Results are averaged across the ten datasets. using external databases are consistently faster than BLIP-2. For instance, CaSED achieves a speed- up of 1.5x with respect to both the largest captioning model and the largest VQA model, while also achieving better performance. Overall, the fastest method is Closest Caption, which exploits the external database to retrieve a single caption and does not consider any candidate extraction pipeline. Conversely, our method retrieves the ten most similar captions and post-processes them to extract class name, resulting in a increase in inference time of approximately 3 times. Compared with the CLIP upper bound, our method considers multiple additional steps, each adding extra inference time. First, our method retrieves candidates from the external database to then extract the class names. Second, we have to forward the class names through the text encoder for each sample, while CLIP can forward them once and cache the features for later reuse. When using an external database, note that an increase in database size implies a minimal variation in retrieval time. This is demonstrated by the computational cost required by retrieving from the large LAION-400M [51] database. As the results show, inference time is comparable between CaSED and CaSED (LAION-400M) despite the latter being approximately 8 times larger. C Additional ablation study In this section, we show the performance of our model with different backbones, and other commonly used databases for retrieval. Backbone architecture. To answer this natural question about whether the outcome of our model depends on the backbone architecture, we further extend our main results with a CLIP model with a ResNet50 architecture. We report this additional ablation in Table 8, Table 9, and Table 10 for the cluster accuracy, the semantic IoU, and the semantic similarity, respectively. We can see that the performance with the CLIP ResNet50 is lower across all the metrics compared to CLIP ViT- L/14. This is expected since ResNet50 is a a smaller architecture, thus with a reduced capacity for semantic representation learning as compared to ViT-L/14. Nevertheless, our method with ResNet50 is still competitive against BLIP-2 models while using 40x fewer parameters (note that our ViT-L implementation uses 10x fewer parameters). Retrieval database. We analyze the impact of retrieving captions from databases of different scales, expanding our main results with the four unused databases of PMD, including COCO [ 37], SBU Captions [44], Localized Narratives [ 46], and Visual Genome [ 30]. Moreover, we evaluate our method on other three databases, namely Ade20K [65], OpenImages [31], and LAION-400M [51]. These databases further extend the range of database sizes, with Ade20k containing only 0.07M captions and LAION-400M having 413M. We report the results in Table 11, ordering the rows by the database size. Differently from the tables reported in the main results, the results in Table 11 15Method Num. Params. Inference time (ms)↓ Caption Closest Caption 0.43B 1390 ± 10 BLIP-2 (ViT-L) 3.46B 5710 ± 153 BLIP-2 (ViT-g) 4.37B 6870 ± 177 VQA BLIP-2 (ViT-L) 3.46B 5670 ± 135 BLIP-2 (ViT-g) 4.37B 6650 ± 117 CaSED 0.43B 4370 ± 13 CaSED (LAION-400M) 0.43B 4350 ± 16 CLIP upper bound 0.43B 645 ± 77 Table 7: Computational cost of different methods. Green is our method, gray shows the upper bound. Inference time is reported on batches of size 64, as the average over multiple runs. Method Cluster Accuracy (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 30.3 18.3 22.5 13.2 47.8 31.4 45.2 26.0 14.2 31.2 28.0 English Words 24.8 17.5 18.5 13.4 49.5 23.1 36.6 22.2 15.5 27.1 24.8 Caption Closest Caption 9.7 7.1 13.3 8.4 21.2 6.2 8.7 6.5 12.8 14.9 10.9 CaSED 44.6 23.9 12.5 15.3 58.8 48.7 50.1 32.8 24.6 33.9 34.5 CLIP upper bound 82.1 41.5 33.5 19.6 63.1 74.6 78.9 55.6 54.9 58.4 56.2 CLIP ViT-L/14 CLIP WordNet 34.0 20.1 16.7 16.7 58.3 40.9 52.0 29.4 18.6 39.5 32.6 English Words 29.1 19.6 22.1 15.9 64.0 30.9 44.4 24.2 19.3 34.5 30.4 Caption Closest Caption 12.8 8.9 16.7 13.3 28.5 13.1 15.0 8.6 20.0 17.8 15.5 CaSED 51.5 29.1 23.8 22.8 68.7 58.8 60.4 37.4 31.3 47.7 43.1 CLIP upper bound 87.6 52.9 47.4 31.8 78.0 89.9 88.0 65.3 76.5 72.5 69.0 Caption BLIP-2 (ViT-L) 26.5 11.7 23.3 5.4 23.6 12.4 11.6 19.5 14.8 25.7 17.4 BLIP-2 (ViT-g) 37.4 13.0 25.2 10.0 29.5 19.9 15.5 21.5 27.9 32.7 23.3 VQA BLIP-2 (ViT-L) 60.4 20.4 21.4 8.1 36.7 21.3 14.0 32.6 28.8 44.3 28.8 BLIP-2 (ViT-g) 62.2 23.8 22.0 15.9 57.8 33.4 23.4 36.4 57.2 55.4 38.7 Table 8: Cluster Accuracy on the ten datasets. Green is our method, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. are obtained on ImageNet. We first discuss the databases belonging to the PMD superset, and then analyze the results obtained with Ade20K, OpenImages, and LAION-400M. In the table, we also show the POS tag distribution (e.g. nouns, adjectives, verbs) of each dataset. Moreover, we report a metric, the semantic similarity to the closest caption in the dataset of each image (C.C. S-Sim) to check how close a database is to the target one (i.e. ImageNet). By comparing the nine databases of PMD, we can perceive a non-uniform variation in performance across databases, with a remarkable gap between the top-5 (highlighted in blue ) and the rest. The performance of the fifth-performing (i.e. WIT) and the sixth database (i.e. SBU Captions) differs by 12.8% on cluster accuracy, while the difference between the best-performing (i.e. CC12M) and the fifth is 10.9%. This observation motivates us to use the top-5 databases from PMD instead of the full superset to be efficient. We expand the best database with the other top-5 to ensure better coverage of class names in the textual descriptions. With the additional databases, i.e. Ade20K, OpenImages, and LAION-400M, we further notice that there is a correlation between the size of a database and its performance. This was also noted in our results reported in the main manuscript. Ade20K, the smallest dataset, comprises only about 0.07M captions and obtains the lowest performance among all metrics. As the size of databases increase, the performance generally improves. However, it is important to note that dataset size alone is not the only requirement for good results. LAION-400M is of a much larger size than our split of PMD, yet the performance with LAION-400M is worse, which we attributes to the impact of much noisier retrieval from database of such scale. Instead, datasets such as CC12M and Redcaps perform better than LAION-400M despite being approximately 40x smaller. This finding suggests that the quality of the databases could be of a higher importance than its size. 16Method Semantic IoU (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 9.8 1.9 0.9 0.0 21.2 5.5 14.1 7.3 3.5 2.6 6.7 English Words 5.1 0.8 0.0 0.1 12.4 1.8 13.2 7.8 2.5 1.3 4.5 Caption Closest Caption 3.4 0.5 0.1 1.5 6.6 2.2 2.2 2.1 9.1 0.7 2.8 CaSED 31.1 2.9 0.08 2.8 29.7 15.1 27.6 15.0 13.4 5.2 14.3 CLIP upper bound 81.5 41.4 33.9 16.5 60.2 74.6 78.9 56.9 66.5 57.1 56.8 CLIP ViT-L/14 CLIP WordNet 15.0 3.0 1.3 0.5 31.3 7.8 14.7 9.0 4.8 3.8 9.1 English Words 8.0 2.0 0.0 1.1 16.4 2.0 17.2 8.1 2.7 1.8 5.9 Caption Closest Caption 4.5 0.8 1.3 1.9 5.9 3.1 3.0 2.3 11.4 1.0 3.5 CaSED 35.4 5.1 2.3 4.8 33.1 19.4 35.1 17.2 16.2 8.4 17.7 CLIP upper bound 86.0 52.2 51.5 28.6 75.7 89.9 88.0 66.6 84.5 71.3 69.4 Caption BLIP-2 (ViT-L) 13.4 1.4 4.8 0.0 7.5 4.7 1.7 4.7 11.6 1.1 5.1 BLIP-2 (ViT-g) 16.8 1.8 4.1 0.1 13.9 7.9 2.9 5.7 24.7 1.9 8.0 VQA BLIP-2 (ViT-L) 36.1 1.8 7.0 0.1 21.5 3.7 5.7 11.5 18.9 2.5 10.9 BLIP-2 (ViT-g) 41.5 2.4 7.5 2.0 38.0 8.6 10.2 13.8 33.2 2.8 16.0 Table 9: Semantic IoU on the ten datasets. Green is our method, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. Method Semantic Similarity (x100)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 43.2 29.0 18.5 21.6 46.7 44.6 50.3 42.8 26.4 40.0 36.3 English Words 36.0 29.5 14.9 20.0 38.1 34.2 40.7 35.4 18.3 32.4 29.9 Caption Closest Caption 37.2 22.8 14.2 26.8 38.9 41.2 32.6 37.4 44.3 32.4 32.8 CaSED 62.3 36.4 22.6 28.7 52.8 59.0 57.0 50.2 42.9 46.2 45.8 CLIP upper bound 88.1 62.4 52.8 53.9 72.0 83.7 85.8 73.9 81.0 73.9 72.7 CLIP ViT-L/14 CLIP WordNet 48.6 32.7 24.4 18.9 55.9 49.6 53.7 44.9 28.8 44.2 40.2 English Words 39.3 31.6 19.1 18.6 43.4 38.0 44.2 36.0 19.9 34.7 32.5 Caption Closest Caption 42.1 23.9 23.4 29.2 40.0 46.9 40.2 39.8 49.2 40.3 37.5 CaSED 65.7 40.0 32.0 30.3 55.5 64.5 62.5 52.5 47.4 54.1 50.4 CLIP upper bound 90.8 69.8 67.7 66.7 83.4 93.7 91.8 80.5 92.3 83.3 82.0 Caption BLIP-2 (ViT-L) 57.8 31.4 39.9 24.4 36.1 44.6 29.0 45.3 46.4 38.0 39.3 BLIP-2 (ViT-g) 63.0 33.1 36.2 24.3 45.2 51.6 31.6 48.3 61.0 44.6 43.9 VQA BLIP-2 (ViT-L) 70.5 34.9 29.7 29.1 48.8 42.0 40.0 50.6 52.4 48.6 44.7 BLIP-2 (ViT-g) 73.5 36.5 31.4 30.8 59.9 52.1 43.9 53.3 65.1 55.1 50.1 Table 10: Semantic similarity on the ten datasets. Green is ours, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. Another observation pertains to the differences between captioning and visual question answer- ing (VQA) datasets (e.g. COCO, Localized Narratives, and Visual Genome) w.r.t. image-text datasets collected from the web (e.g. CC12M, Redcaps, and LAION-400M). In general, it appears that textual descriptions from captioning and VQA datasets perform worse than datasets scraped from the web. This difference in performance may be due to the type of description provided, as the former often describes specific regions of the image, while the latter provides a general description of the image as a whole. Since our prime objective is to understand the class name of the subject in the image (whether it be a location, a pet, or a car model), captions describing secondary objects can lead to sub-optimal results. Interestingly, we found that the average performance on the 10 datasets of a chosen textual database generally correlates with the accuracy on ImageNet and with the closest caption semantic similarity (C.C. S-Sim in Table 11). Examples are the subset of PMD and CC12M, achieving the best average results on the 10 datasets according to all metrics (Tab. 4b of the main paper) while being the best on the ImageNet validation set w.r.t. both the closest caption semantic similarity and the ViC metrics. The ImageNet validation set can thus be used as a proxy to pick the textual database in case of a lack of priors on the test set. 17Database Size CA S-Sim. S-IoU Nouns(%) Adjs(%) Verbs(%) Concepts C.C. S-Sim. Ade20K 0.07M 11.0 31.8 1.7 82.7 10.6 6.7 3.1K 19.0 COCO 0.9M 13.8 35.6 2.6 86.1 9.5 4.4 14.6K 22.6 SBU Captions 1.0M 20.1 40.8 5.7 98.0 1.0 0.9 29.3K 26.9 OpenImages 1.4M 16.8 37.8 4.2 85.4 10.0 4.5 21.4K 24.8 Loc. Narr. 1.9M 15.7 37.1 4.1 86.9 9.8 3.2 30.9K 24.9 CC3M 2.8M 37.9 53.9 16.2 57.9 23.6 18.5 28.4K 35.2 WIT 4.8M 32.9 47.9 14.5 99.8 0.05 0.07 163.7K 30.0 Visual Genome 5.4M 14.6 35.2 3.8 75.1 15.6 9.2 21.1K 26.7 Redcaps 7.9M 41.1 54.7 19.6 98.6 0.7 0.6 50.5K 37.5 CC12M 10.3M 43.8 57.4 21.2 96.6 1.8 1.5 36.7K 38.7 YFCC100M* 29.9M 39.1 53.5 18.9 99.7 0.2 0.1 179.6K 37.8 Ours 54.8M 41.5 55.7 20.4 99.8 0.05 0.05 334.1K 38.7 LAION-400M 413.8M37.7 52.7 18.7 99.4 0.5 0.1 1.68M 37.3 Table 11: Ablation on the databases on ImageNet. Green is our database, Blue shows the top-5 databases of PMD, which we used to create Ours. We also evaluate the semantic similarity of the closest caption to each image in the dataset (i.e., C.C. S-Sim.). Bold represents best, while underline second best. YFCC100M* indicates we use the subset of the dataset used in PMD. For what concerns the POS tag distribution, it is hard to extract any pattern that justifies the perfor- mance of different databases. For instance, CC3M is the only database without an extreme imbalance for nouns (i.e., 57.9% compared to >96% for the other in the top-5) while still achieving good performance. Moreover, while the number of concepts depends on the size of the database, there is no clear correlation between this value and the achieved scores. As an example, Localized Narratives has a comparable amount of concepts w.r.t. CC3M but a gap of >20% in cluster accuracy, and of 10 points in semantic similarity and IoU. Finally, it is worth noting that most of the datasets have an average of 12.6 words per caption, while Visual Genome has an average of only 5.1 words per caption. This characteristic may explain why Visual Genome behaves differently in terms of the database scaling rule of increasing performance with the database size. D Qualitative results Last, we report some qualitative results of our method applied on three different datasets, namely Caltech-101 (Fig. 5), Food101 (Fig. 6), and SUN397 (Fig. 7), where the first is coarse, and the last two are fine-grained, focusing on food plates and places respectively. For each, we present a batch of five images, where the first three represent success cases and the last two show interesting failure cases. Each sample shows the image we input to our method with the top-5 candidate classes. From the results, we can see that for many success cases, our method not only generates the correct class name and selects it as the best matching label, but it also provides valid alternatives for classification. For example, the third image in Fig 5 or the second image in Fig 6, where CaSED provides the names \"dessert\" for the cheesecake and the label \"bird\" for the ibis. This phenomenon also happens in failure cases, where e.g. the last sample in Fig 6 provides both the name \"pizza\" and the name \"margherita\" for the dish, despite selecting the wrong name from the set. Another interesting observation is that our method provides names for different objects in the same scene. For instance, the third and fourth samples in Fig 6 contain labels for both \"guacamole\" and \"tortillas\" for the first, and for \"mozzarella\", \"insalata\", and \"balsamic\" for the second. A further detail on the latter case is the ability of CLIP to reason in multiple languages since \"insalata\" translates to \"salad\" from Italian to English. Regarding failure cases, it is interesting to note that the candidate names and the predicted label often describe well the input image despite being wrong w.r.t the dataset label. For instance, the two failure cases in Fig. 7 select \"stadium\" and \"dumpsite\" when the ground-truth class names are \"football\" and \"garbage site\". In addition, for the first case, the exact name \"football\" is still available among the best candidate names, but our method considers \"stadium\" as a better fit. Another example is the last failure case in Fig 5, where the model assigns the name \"nokia\" to a Nokia 3310 cellphone, while the ground-truth class name is \"cellphone\". Also in this case, the ground-truth label is present in the candidate list but our method considers \"nokia\" a more fitting class. 18Figure 5: Qualitative results on Caltech-101. The first three samples represent success cases, the last two shows failure cases. Figure 6: Qualitative results on Food101. The first three samples represent success cases, the last two shows failure cases. Figure 7: Qualitative results on SUN397. The first three samples represent success cases, the last two shows failure cases. Finally, we notice the discovery of correlations between terms in the reasoning of our model. In the provided examples, it happens multiple times that the candidate class names do not describe objects in the scene but rather a correlated concept to the image. For instance, the third example in Fig 5 shows a Dalmatian, and among the candidate names there is \"cruella\", which is the name of the villain of the movie \"The Hundred and One Dalmatians\". Another instance of this appears in the first example of Fig 6, where the model correctly associates the \"bibimbap\" dish to its place of origin, Korea, with the candidate name \"korean\". 19",
      "references": [
        "Flamingo: a visual language model for few-shot learning.",
        "Retrieval-augmented diffusion models.",
        "Improving language models by retrieving from trillions of tokens.",
        "Food-101–mining discriminative components with random forests.",
        "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.",
        "Reproducible scaling laws for contrastive language-image learning.",
        "Describing textures in the wild.",
        "Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision.",
        "Imagenet: A large-scale hierarchical image database.",
        "Virtex: Learning visual representations from textual annotations.",
        "Redcaps: Web-curated image-text data created by the people, for the people.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.",
        "Improved baselines for vision-language pre-training.",
        "Scaling open-vocabulary image segmentation with image-level labels.",
        "Self-supervised learning of visual features through embedding images into text topic spaces.",
        "Retrieval augmented language model pre-training.",
        "Automatically discovering and learning new visual categories with ranking statistics.",
        "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.",
        "Scaling up vision-language pre-training for image captioning.",
        "Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.",
        "Invariant information clustering for unsupervised image classification and segmentation.",
        "Scaling up visual and vision-language representation learning with noisy text supervision.",
        "Billion-scale similarity search with GPUs.",
        "Learning visual features from large weakly supervised data.",
        "3d object representations for fine-grained categorization.",
        "Visual genome: Connecting language and vision using crowdsourced dense image annotations.",
        "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
        "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
        "Align before fuse: Vision and language representation learning with momentum distillation.",
        "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.",
        "Microsoft coco: Common objects in context.",
        "Large-scale long-tailed recognition in an open world.",
        "Retrieval augmented classification for long-tail visual recognition.",
        "Fine-grained visual classification of aircraft.",
        "Wordnet: a lexical database for english.",
        "Babelnet: Building a very large multilingual semantic network.",
        "Automated flower classification over a large number of classes.",
        "Im2text: Describing images using 1 million captioned photographs.",
        "Combined scaling for zero-shot transfer learning.",
        "Connecting vision and language with localized narratives.",
        "Learning transferable visual models from natural language supervision.",
        "Retrieval-augmented image captioning.",
        "Sentence-bert: Sentence embeddings using siamese bert-networks.",
        "Laion-5b: An open large-scale dataset for training next generation image-text models.",
        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.",
        "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.",
        "Test-time prompt tuning for zero-shot generalization in vision-language models.",
        "Flava: A foundational language and vision alignment model.",
        "Ucf101: A dataset of 101 human actions classes from videos in the wild.",
        "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning.",
        "Yfcc100m: The new data in multimedia research.",
        "Grafit: Learning fine-grained image representations with coarse labels.",
        "Scan: Learning to classify images without labels.",
        "Simvlm: Simple visual language model pretraining with weak supervision.",
        "Sun database: Large-scale scene recognition from abbey to zoo.",
        "Coca: Contrastive captioners are image-text foundation models.",
        "Open-vocabulary object detection using captions.",
        "Multi-grained vision language pre-training: Aligning texts with visual concepts.",
        "Scene parsing through ade20k dataset.",
        "Learning to prompt for vision-language models."
      ],
      "meta_data": {
        "arxiv_id": "2306.00917v3",
        "authors": [
          "Alessandro Conti",
          "Enrico Fini",
          "Massimiliano Mancini",
          "Paolo Rota",
          "Yiming Wang",
          "Elisa Ricci"
        ],
        "published_date": "2023-06-01T17:19:43Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the Novel Vocabulary-free Image Classification (VIC) task that removes the assumption of a pre-defined label set at test time, formulates evaluation metrics for it, and presents CaSED – a simple, training-free baseline that combines CLIP embeddings with retrieval from large vision-language caption databases to choose a class from an open semantic space. Demonstrates CaSED’s effectiveness over stronger generative VLMs such as BLIP-2 while using far fewer parameters.",
        "methodology": "CaSED (Category Search from External Databases) is built on a frozen CLIP model. For a query image, it 1) retrieves the top-K most similar captions from an external caption database using CLIP image–text similarity; 2) parses these captions with filtering, POS-tagging and frequency thresholds to get a small candidate label set; 3) scores each candidate by a weighted combination of (a) image–text similarity between the image embedding and the candidate word embedding, and (b) text–text similarity between the centroid of retrieved caption embeddings and the candidate embedding; 4) outputs the highest-scoring candidate. The pipeline is entirely training-free and database-agnostic, enabling scalable open-vocabulary recognition.",
        "experimental_setup": "Evaluated on ten standard image-classification datasets spanning coarse to fine granularity: Caltech-101, DTD, EuroSAT, FGVC-Aircraft, Flowers-102, Food-101, Oxford Pets, Stanford Cars, SUN397, UCF101. Hyper-parameters tuned on ImageNet. External caption database is a 54.8 M-caption subset of PMD (CC3M, CC12M, WIT, Redcaps, YFCC100M*); retrieval accelerated with FAISS. Baselines include CLIP with large static vocabularies, BLIP-2 for captioning and VQA, and a nearest-caption method. Metrics proposed: Cluster Accuracy (unsupervised grouping quality), Semantic Similarity (Sentence-BERT cosine between predicted and ground-truth labels), Semantic IoU (word-overlap). CaSED compared against baselines on all metrics, plus ablations on scoring modalities, database size, and number of retrieved captions, and a cost analysis of parameters and inference time.",
        "limitations": "Performance depends heavily on coverage and quality of the external caption database; rare or domain-specific concepts poorly represented will be missed. Captions may contain noise or biases that propagate to predictions. Candidate parsing may yield inconsistent label granularity or duplicates (e.g., ‘bird’ vs ‘cassowary’). Lacks temporal or memory coherence, so similar images can receive slightly different labels. Does not explicitly control for hierarchical granularity or user-specified semantic level. Current evaluation focuses on image datasets; performance on video or real-time systems not studied.",
        "future_research_directions": "1) Curate or automatically adapt retrieval databases to specific domains to improve coverage of fine-grained or novel classes. 2) Introduce learning components (e.g., prompt tuning, retrieval weighting) while keeping open-vocabulary capability. 3) Incorporate hierarchical reasoning or user constraints to control label granularity and disambiguate synonyms. 4) Add memory or clustering mechanisms for consistent naming across time or datasets. 5) Explore techniques to detect and mitigate biases inherited from web captions, and improve robustness to noisy retrievals.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Provable Benefit of Cutout and CutMix for Feature Learning",
      "full_text": "Provable Benefit of Cutout and CutMix for Feature Learning Junsoo Oh KAIST AI junsoo.oh@kaist.ac.kr Chulhee Yun KAIST AI chulhee.yun@kaist.ac.kr Abstract Patch-level data augmentation techniques such as Cutout and CutMix have demon- strated significant efficacy in enhancing the performance of vision tasks. However, a comprehensive theoretical understanding of these methods remains elusive. In this paper, we study two-layer neural networks trained using three distinct meth- ods: vanilla training without augmentation, Cutout training, and CutMix training. Our analysis focuses on a feature-noise data model, which consists of several label-dependent features of varying rarity and label-independent noises of differing strengths. Our theorems demonstrate that Cutout training can learn low-frequency features that vanilla training cannot, while CutMix training can learn even rarer features that Cutout cannot capture. From this, we establish that CutMix yields the highest test accuracy among the three. Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors “evenly” regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation. 1 Introduction Data augmentation is a crucial technique in deep learning, particularly in the image domain. It involves creating additional training examples by applying various transformations to the original data, thereby enhancing the generalization performance and robustness of deep learning models. Traditional data augmentation techniques typically focus on geometric transformations such as random rotations, horizontal and vertical flips, and cropping (Krizhevsky et al., 2012), or color-based adjustments such as color jittering (Simonyan and Zisserman, 2014). In recent years, several new data augmentation techniques have appeared. Among them, patch-level data augmentation techniques like Cutout (DeVries and Taylor, 2017) and CutMix (Yun et al., 2019) have received considerable attention for their effectiveness in improving generalization. Cutout is a straightforward method where random rectangular regions of an image are removed during training. In comparison, CutMix adopts a more complex strategy by cutting and pasting sections from different images and using mixed labels, encouraging the model to learn from blended contexts. The success of Cutout and CutMix has triggered the development of numerous variants including Random Erasing (Zhong et al., 2020), GridMask (Chen et al., 2020a), CutBlur (Yoo et al., 2020), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021). However, despite the empirical success of these patch-level data augmentation techniques in various image-related tasks, a lack of comprehensive theoretical understanding persists: why and how do they work? In this paper, we aim to address this gap by offering a theoretical analysis of two important patch-level data augmentation techniques: Cutout and CutMix. Our theoretical framework draws inspiration from a study by Shen et al. (2022), which explores a data model comprising multiple label-dependent feature vectors and label-independent noises of varying frequencies and intensities. The key idea of this work is that learning features with low frequency can be challenging due to strong noises (i.e., low signal-to-noise ratio). We focus on how Cutout and CutMix can aid in learning such rare features. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.23672v1  [cs.LG]  31 Oct 20241.1 Our Contributions In this paper, we consider a patch-wise data model consisting of features and noises, and use two-layer convolutional neural networks as learner networks. We focus on three different training methods: vanilla training without any augmentation, Cutout training, and CutMix training. We refer to these training methods in our problem setting as ERM, Cutout, and CutMix. We investigate how these methods affect the network’s ability to learn features. We summarize our contributions below: • We analyze ERM, Cutout, and CutMix, revealing that Cutout outperforms ERM since it enables the learning of rarer features compared to ERM (Theorem 3.1 and Theorem 3.2). Furthermore, CutMix demonstrates almost perfect performance (Theorem 3.3) by learning all features. • Our main intuition behind the negative result for ERM is that ERM learns to classify training samples by memorizing noise vectors instead of learning meaningful features if the features do not appear frequently enough. Hence, ERM suffers low test accuracy because it cannot learn rare features. However, Cutout alleviates this challenge by removing some of the strong noise patches, allowing it to learn rare features to some extent. • We prove the near-perfect performance of CutMix based on a novel technique that views the non-convex loss as a composition of a convex function and reparameterization. This enables us to characterize the global minimum of the loss and show that CutMix forces the model to activate almost uniformly across every patch of inputs, allowing it to learn all features. 1.2 Related Works Feature Learning Theory. Our work aligns with a recent line of studies investigating how training methods and neural network architectures influence feature learning. These studies focus on a specific data distribution composed of two components: label-dependent features and label-independent noise. The key contribution of this body of work is the exploration of which training methods or neural networks are most effective at learning meaningful features and achieving good generalization performance. Allen-Zhu and Li (2020) demonstrate that an ensemble model can achieve near-perfect performance by learning diverse features, while a single model tends to learn only certain parts of the feature space, leading to lower test accuracy. In other works, Cao et al. (2022); Kou et al. (2023a) explore the phenomenon of benign overfitting when training a two-layer convolutional neural network. The authors identify the specific conditions under which benign overfitting occurs, providing valuable insights into how these networks behave during training. Several other studies seek to understand various aspects of deep learning through the lens of feature learning (Zou et al., 2021; Jelassi and Li, 2022; Chen et al., 2022, 2023; Li and Li, 2023; Huang et al., 2023a,b). Theoretical Analysis of Data Augmentation. Several works aim to analyze traditional data augmentation from different perspectives, including kernel theory (Dao et al., 2019), margin-based approach (Rajput et al., 2019), regularization effects (Wu et al., 2020), group invariance (Chen et al., 2020b), and impact on optimization (Hanin and Sun, 2021). Moreover, many papers have explored various aspects of a recent technique called Mixup (Zhang et al., 2017). For example, studies have explored its regularization effects (Carratino et al., 2020; Zhang et al., 2020), its role in improving calibration (Zhang et al., 2022), its ability to find optimal decision boundaries (Oh and Yun, 2023) and its potential negative effects (Chidambaram et al., 2021; Chidambaram and Ge, 2024). Some works investigate the broader framework of Mixup, including CutMix, which aligns with the scope of our work. Park et al. (2022) study the regularization effect of mixed-sample data augmentation within a unified framework that contains both Mixup and CutMix. In Oh and Yun (2023), the authors analyze masking-based Mixup, which is a class of Mixup variants that also includes CutMix. In their context, they show that masking-based Mixup can deviate from the Bayes optimal classifier but require less training sample complexity. However, neither work provides a rigorous explanation for why CutMix has been successful. The studies most closely related to our work include Shen et al. (2022); Chidambaram et al. (2023); Zou et al. (2023). Shen et al. (2022) regard traditional data augmentation as a form of feature manipulation and investigate its advantages from a feature learning perspective. Both Chidambaram et al. (2023) and Zou et al. (2023) analyze Mixup within a feature learning framework. However, patch-level data augmentation such as Cutout and CutMix, which are the focus of our work, have not yet been explored within this context. 22 Problem Setting In this section, we introduce the data distribution and neural network architecture, and formally describe the three training methods considered in this paper. 2.1 Data Distribution We consider a binary classification problem on structured data, consisting of patches of label- dependent vectors (referred to as features) and label-independent vectors (referred to as noise). Definition 2.1 (Feature Noise Patch Data). We define a data distribution D on Rd×P × {−1, 1} such that (X, y) ∼ Dwhere X = \u0000 x(1), . . . ,x(P)\u0001 ∈ Rd×P and y ∈ {±1} is constructed as follows. 1. Choose the label y ∈ {±1} uniformly at random. 2. Let {vs,k}s∈{±1},k∈[K] ⊂ Rd be a set of orthonormal feature vectors. Choose the feature vector v ∈ Rd for data point X as v = vy,k with probability ρk from {vy,k}k∈[K] ⊂ Rd, where ρ1 + ··· + ρK = 1 and ρ1 ≥ ··· ≥ρK. In our setting, there are three types of features with significantly different frequencies: common features, rare features, and extremely rare features, ordered from most to least frequent. The indices of these features partition [K] into (KC, KR, KE). 3. We construct P patches of X as follows. • Feature Patch: Choose p∗ uniformly from [P] and we set x(p∗) = v. • Dominant Noise Patch: Choose ˜p uniformly from [P]\\{p∗}. We construct x(˜p) = αu+ξ(˜p) where αu is feature noise drawn uniformly from {αv1,1, αv−1,1} with 0 < α <1 and ξ(˜p) is Gaussian dominant noise drawn from N(0, σ2 dΛ). • Background Noise Patch: The remaining patches p ∈ [P] \\ {p∗, ˜p} consist of Gaussian background noise, i.e., we set x(p) = ξ(p) where ξ(p) ∼ N(0, σ2 bΛ). Here, the noise covariance matrix is defined as Λ := I − P s,k vs,kv⊤ s,k which ensures that Gaussian noises are orthogonal to all features. We assume that the dominant noise is stronger than the background noise, i.e., σb < σd. Our data distribution captures characteristics of image data, where the input consists of several patches. Some patches contain information relevant to the image labels, such as cat faces for the label “cat,” while other patches contain information irrelevant to the labels, such as the background. Intuitively, there are two ways to fit the given data: learning features or memorizing noise. If a model fits the data by learning features, it can correctly classify test data having the same features. However, if a model fits the data by memorizing noise, it cannot generalize to unseen data because noise patches are not relevant to labels. Thus, learning more features is crucial for achieving better generalization. In real-world scenarios, different features may appear with varying frequencies. For instance, the occurrences of cat’s faces and cat’s tails in a dataset might differ significantly, although both are relevant to the “cat” label. Our data distribution reflects these characteristics by considering features with varying frequencies. To emphasize the distinctions between the three training methods we analyze, we categorize features into three groups: common, rare, and extremely rare. We refer to data points containing these features as common data, rare data, and extremely rare data, respectively. We emphasize that these terminologies are chosen merely to distinguish the three different levels of rarity, and even “extremely rare” features appear in a nontrivial fraction of the training data with high probability (see our assumptions in Section 2.4). Comparison to Previous Work. Our data distribution is similar to those considered in Shen et al. (2022) and Zou et al. (2023), which investigate the benefits of standard data augmentation methods and Mixup by comparing them to vanilla training without any augmentation. These results consider two types of features—common and rare—with different levels of rarity, along with two types of noise: feature noise and Gaussian noise. However, we consider three types of features: common, rare, and extremely rare, and three types of noise: feature noise, dominant noise, and background noise. This distinction allows us to compare three distinct methods and demonstrate the differences between them, whereas Shen et al. (2022) and Zou et al. (2023) compared only two methods. 32.2 Neural Network Architecture For the prediction model, we focus on the following two-layer convolutional neural network where the weights in the second layer are fixed at 1 and −1, with only the first layer being trainable. Several works including Shen et al. (2022) and Zou et al. (2023) also focus on similar two-layer convolutional neural networks. Definition 2.2 (2-Layer CNN) . We define 2-layer CNN fW : Rd×P → R parameterized by W = {w1, w−1} ∈Rd×2. For each input X = \u0000 x(1), . . . ,x(P)\u0001 ∈ Rd×P , we define fW (X) := X p∈[P] ϕ \u0010D w1, x(p) E\u0011 − X p∈[P] ϕ \u0010D w−1, x(p) E\u0011 , where ϕ(·) is a smoothed version of leaky ReLU activation, defined as follows. ϕ(z) :=    z − (1−β)r 2 z ≥ r 1−β 2r z2 + βz 0 ≤ z ≤ r βz z ≤ 0 , where 0 < β≤ 1 and r >0. Previous works on the theory of feature learning often consider neural networks with (smoothed) ReLU or polynomial activation functions. However, we adopt a smoothed leaky ReLU activation, which always has a positive slope, to exclude the possibility of neurons “dying” during the complex optimization trajectory. Using smoothed leaky ReLU to analyze the learning dynamics of neural networks is not entirely new; there is a body of work that studies phenomena such as benign overfitting (Frei et al., 2022a) and implicit bias (Frei et al., 2022b; Kou et al., 2023b) by analyzing neural networks with (smoothed) leaky ReLU activation. A key difference between ReLU and leaky ReLU lies in the possibility of ReLU neurons “dying” in the negative region, where some negatively initialized neurons remain unchanged throughout training. As a result, using ReLU activation requires multiple neurons to ensure the survival of neurons at initialization, which becomes increasingly probable as the number of neurons increases. In contrast, the derivative of leaky ReLU is always positive, ensuring that a single neuron is often sufficient. Therefore, for mathematical simplicity, we consider the case where the network has a single neuron for each positive and negative output. We believe that our analysis can be extended to the multi-neuron case as we validate numerically in Appendix A.2. 2.3 Training Methods Using a training set sampled from the distribution D, we would like to train our network fW to learn to correctly classify unseen data points from D. We consider three learning methods: vanilla training without any augmentation, Cutout, and CutMix. We first introduce necessary notation for our data and parameters, and then formalize training methods within our framework. Training Data. We consider a training set Z = {(Xi, yi)}i∈[n] comprising n data points, each independently drawn from D. For each i ∈ [n], we denote Xi = (x(1) i , . . . ,x(P) i ). Initialization. We initialize the model parameters in our neural network using random initializa- tion. Specifically, we initialize the model parameter W(0) = {w(0) 1 , w(0) −1}, where w(0) 1 , w(0) −1 i.i.d. ∼ N(0, σ2 0Id). Let us denote updated model parameters at iteration t as W(t) = {w(t) 1 , w(t) −1}. 2.3.1 Vanilla Training The vanilla approach to training a model fW is solving the empirical risk minimization problem using gradient descent. We refer to this method as ERM. Then, ERM updates parameters W(t) of a model using the following rule. W(t+1) = W(t) − η∇W LERM \u0010 W(t) \u0011 , 4where η is a learning rate and LERM(·) is the ERM training loss defined as LERM(W) := 1 n X i∈[n] ℓ(yifW (Xi)), (1) where ℓ(·) is the logistic loss ℓ(z) = log(1 + e−z). 2.3.2 Cutout Training. Cutout (DeVries and Taylor, 2017) is a data augmentation technique that randomly cuts out rectangular regions of image inputs. In our patch-wise data, we regard Cutout training as using inputs with masked patches from the original data. For each subset C of [P] and i ∈ [n], we define augmented data Xi,C ∈ Rd×P as a data point generated by cutting the patches with indices in C out of Xi. We can represent Xi,C as Xi,C = \u0010 x(1) i,C, . . . ,x(P) i,C \u0011 , where x(p) i,C = ( x(p) i if p /∈ C, 0 otherwise. Note that the output of the model fW (·) on this augmented data point Xi,C is fW (Xi,C) = X p/∈C ϕ \u0010D w1, x(p) i E\u0011 − X p/∈C ϕ \u0010D w−1, x(p) i E\u0011 . Then, the objective function for Cutout training can be defined as LCutout(W) := 1 n X i∈[n] EC∼DC[ℓ(yifW (Xi,C))], where DC is a uniform distribution on the collection of subsets of [P] with cardinality C, where C is a hyperparameter satisfying 1 ≤ C < P 2 .1 We refer to the process of training our model using gradient descent on Cutout loss LCutout(W) as Cutout, and its update rule is W(t+1) = W(t) − η∇W LCutout \u0010 W(t) \u0011 , (2) where η is a learning rate. 2.3.3 CutMix Training. CutMix (Yun et al., 2019) involves not only cutting parts of images, but also pasting them into different images as well as assigning them mixed labels. For each subset S of [P] and i, j∈ [n], we define the augmented data point Xi,j,S ∈ Rd×P as the data obtained by cutting patches with indices in S from data Xi and pasting them into Xj at the same indices S. We can write Xi,j,S as Xi,j,S = \u0010 x(1) i,j,S, . . . ,x(P) i,j,S \u0011 , where x(p) i,j,S = ( x(p) i if p ∈ S, x(p) j otherwise. The one-hot encoding of the labels yi and yj are also mixed with proportions |S| P and 1 − |S| P , respectively. This mixed label results in the loss of the form |S| P ℓ(yifW (Xi,j,S)) + \u0012 1 − |S| P \u0013 ℓ(yjfW (Xi,j,S)). From this, the CutMix training loss LCutMix(W) can be defined as LCutMix(W) := 1 n2 X i,j∈[n] ES∼DS \" |S| P ℓ(yifW (Xi,j,S)) + \u0012 1 − |S| P \u0013 ℓ(yjfW (Xi,j,S)) # , where DS is a probability distribution on the set of subsets of[P] which samples S ∼ DS as follows.2 1DeVries and Taylor (2017) also employ a moderate size of cutting, such as cutting 16 × 16 pixels on CIFAR-10 data, which originally has 32 × 32 pixels. 2Other types of distributions, such as those considered in Yun et al. (2019), make the same conclusion. We adopt this distribution to make presentation simpler. 51. Choose the cardinality s of S uniformly at random from {0, 1, . . . , P}, and 2. Choose S uniformly at random from the collection of subsets of [P] with cardinality s. We refer to the process of training our network using gradient descent on CutMix loss LCutMix(W) as CutMix, and its update rule is W(t+1) = W(t) − η∇W LCutMix \u0010 W(t) \u0011 , (3) where η is a learning rate. 2.4 Assumptions on the Choice of Problem Parameters To control the quantities that appear in the analysis of training dynamics, we make assumptions on several quantities in our problem setting. For simplicity, we use choices of problem parameters as a function of the dimension of patches d and consider sufficiently large d. We use the standard asymptotic notation O(·), Ω(·), Θ(·), o(·), ω(·) to express the dependency on d. We also use eO(·), eΩ(·), eΘ(·) to hide logarithmic factors of d. Additionally, poly(d) (or polylog(d)) represents quantities that increase faster than dc1 (or (log d)c1 ) and slower than dc2 (or (log d)c2 ) for some constant 0 < c1 < c2. Similarly, o(1/poly(d)) (or o(1/polylog(d))) denotes some quantities that decrease faster than 1/dc (or 1/(log d)c) for any constant c. Finally, we use f(d) = o(g(d)/polylog(d)) when f(d)/g(d) = o(1/polylog(d)) for some function f and g of d. Assumptions. We assume that P = Θ(1) and P ≥ 8 for simplicity. Additionally, we consider a high-dimensional regime where the number of data points is much smaller than the dimension d, which is expressed as n = o \u0010 αβσ−1 d σbd 1 2 /polylog(d) \u0011 . We also assume that ρkn = ω \u0010 n 1 2 log d \u0011 for all k ∈ [K], which ensures the sufficiency of data points with each feature. In addition, as we will describe in Section 4, the relative scales between the frequencies of features and the strengths of noises play crucial roles in our analysis, as they serve as a proxy for the “learning speed” in the initial phase. For common features k ∈ KC, we assume ρk = Θ(1) and the learning speed of common features is much faster than that of dominant noise, which translates into the assumption σ2 dd = o(βn). For rare features k ∈ KR, we assume ρk = Θ(ρR) for some ρR, and we consider the case where the learning speed of rare features is much slower than that of dominant noise but faster than background noise, which is expressed as ρRn = o \u0000 α2σ2 dd/polylog(d) \u0001 and σ2 bd = o(βρRn). Finally, for extremely rare features k ∈ KE, we say ρk = Θ( ρE) for some ρE and their learning is even slower than that of background noises, which can be expressed as ρEn = o \u0000 α2σ2 bd/polylog(d) \u0001 . Lastly, we assume the strength of feature noise satisfies α = o \u0000 n−1βσ2 dd/polylog(d) \u0001 , and r, σ0, η >0 are sufficiently small so that σ0, r= o (α/polylog(d)), η = o \u0000 rσ−2 d d−1/polylog(d) \u0001 . We list our assumptions in Assumption B.1 and there are many choices of parameters satisfying the set of assumptions, including: P = 8, C= 2, n= Θ \u0000 d0.4\u0001 , α= Θ \u0000 d−0.02\u0001 , β= 1 polylog(d), σ0 = Θ(d−0.2), r= Θ(d−0.2), σd = Θ \u0000 d−0.305\u0001 , σb = Θ \u0000 d−0.375\u0001 , ρR = Θ \u0000 d−0.1\u0001 , ρE = Θ \u0000 d−0.195\u0001 , η= Θ(d−1). 3 Main Results In this section, we provide a characterization of the high probability guarantees for the behavior of models trained using three distinct methods we have introduced. We denote by T∗ the maximum admissible training iterates and we assume T∗ = poly(d) η with a sufficiently large polynomial in d. In all of our theorem statements, the randomness is over the sampling of training data and the initialization of models and all results hold under the condition that d is sufficiently large. The following theorem characterizes training accuracy and test accuracy achieved by ERM. 6Theorem 3.1. Let W(t) be iterates of ERM. Then with probability at least 1 − o \u0010 1 poly(d) \u0011 , there exists TERM such that any T ∈ [TERM, T∗] satisfies the following: • (Perfectly fits training set): For alli ∈ [n], yifW(T) (Xi) > 0. • (Random on (extremely) rare data):P(X,y)∼D [yfW(T) (X)>0]=1 −1 2 P k∈KR∪KE ρk±o \u0010 1 poly(d) \u0011 . The proof is provided in Appendix C.2. Theorem 3.1 demonstrates thatERM achieves perfect training accuracy; however, it performs almost like random guessing on unseen data points with rare and extremely rare features. This is because ERM can only learn common features and overfit rare or extremely rare data in the training set by memorizing noises to achieve perfect training accuracy. In comparison, we show that Cutout can perfectly fit both augmented training data and original training data, and it can also learn rare features that ERM cannot. However, Cutout still makes random guesses on test data with extremely rare features. We state these in the following theorem with the proof provided in Appendix D.2: Theorem 3.2. Let W(t) be iterates of Cutout training. Then with probability at least1−o \u0010 1 poly(d) \u0011 , there exists TCutout such that any T ∈ [TCutout, T∗] satisfies the following: • (Perfectly fits augmented data): For alli ∈ [n] and C ⊂[P] with |C| = C, yifW(T) (Xi,C) > 0. • (Perfectly fits original training data): For alli ∈ [n], yifW(T) (Xi) > 0. • (Random on extremely rare data): P(X,y)∼D [yfW(T) (X) > 0] = 1 − 1 2 P k∈KE ρk ± o \u0010 1 poly(d) \u0011 . In the case of CutMix, it is challenging to discuss training accuracy directly because the augmented data have soft labels generated by mixing pairs of labels. Instead, we prove that CutMix achieves a sufficiently small gradient of the loss, and the training accuracy on the original training data is perfect. We also demonstrate that CutMix achieves almost perfect test accuracy, as it learns all types of features regardless of rarity. Theorem 3.3. Let W(t) be iterates of CutMix training. Then with probability at least1−o \u0010 1 poly(d) \u0011 , there exists some TCutMix ∈ [0, T∗] that satisfies the following: • (Finds a near stationary point): \r\r∇W LCutMix \u0000 W(TCutMix)\u0001\r\r = 1 poly(d) . • (Perfectly fits original training data): For alli ∈ [n], yifW(TCutMix) (Xi) > 0. • (Almost perfectly classifies test data): P(X,y)∼D h yfW(TCutMix)(X) > 0 i = 1 − o \u0010 1 poly(d) \u0011 . To prove Theorem 3.3, we characterize the global minimum of objective loss ofCutMix. Surprisingly, at the global minimum, the model has the same outputs for all patches of the input data. In other words, the contributions of all feature vectors and noise vectors to the final outcome of the network are identical, regardless of their frequency and strength (see Section 4.2 for more details). Moreover, this uniform “contribution” is large enough, which allows the model to learn all types of features by reaching the global minimum. We provide the detailed proof in Appendix E.2. Our three main theorems elucidate the benefits of Cutout and CutMix. Cutout enables a model to learn rarer features than ERM, while CutMix can even outperform Cutout. These advantages in learning rarer features lead to improvements in generalization performance. 4 Overview of Analysis In this section, we discuss key proof ideas and the main challenges in our analysis. For ease of presentation, we consider the case α = 0. Although our assumptions do not allow the choice α = 0, the choice of nonzero α is to show guarantees on the test accuracy and does not significantly affect the feature learning aspect. 7To provide the proof overview, let us introduce some additional notation. For each i ∈ [n], recall that the corresponding input point can be written as Xi = (x(1) i , . . . ,x(P) i ). We use p∗ i and ˜pi to denote the indices of its feature patch and dominant noise patch, respectively. For each feature vector vs,k, where s ∈ {±1} and k ∈ [K], let Vs,k ⊂ [n] represent the set of indices of data points having the feature vector vs,k, and Vs = SK k=1 Vs,k denotes the set of indices of data with label s. For each data point i ∈ [n] and dominant or background noise patch p ∈ [P] \\ {p∗ i }, we refer to the Gaussian noise inside x(p) i as ξ(p) i . 4.1 Vanilla Training and Cutout Training We now explain whyERM fails to learn (extremely) rare features, whileCutout can learn rare features but not extremely rare features. Let us consider ERM. From (1), for s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }, the component of ws in the feature vector vs′,k’s direction is updated as D w(t+1) s , vs′,k E = D w(t) s , vs′,k E − ss′η n X j∈Vs′,k ℓ′(yjfW(t) (Xj))ϕ′ \u0010D w(t) s , vs′,k E\u0011 , (4) and similarly, the “update” of inner product of ws with a noise patch ξ(p) i can be written as D w(t+1) s , ξ(p) i E ≈ D w(t) s , ξ(p) i E − syiη n ℓ′(yifW(t) (Xi))ϕ′ \u0010D w(t) s , ξ(p) i E\u0011\r\r\rξ(p) i \r\r\r 2 , (5) where the approximation is due to the near-orthogonality of Gaussian random vectors in the high- dimensional regime. This approximation shows that ⟨w(t+1) s , vs′,k⟩’s and ⟨w(t) s , ξ(p) i ⟩’s are almost monotonically increasing or decreasing. We address the approximation errors using a variant of the technique introduced by Cao et al. (2022), as detailed in Appendix B.3. From (4) and (5), we can observe that in the early phase of training satisfying −ℓ′(yifW(t) (Xi)) = Θ(1), the main factor for the speed of learning features and noises are the number of feature occurrence |Vs′,k| and the strength of noise ∥ξ(p) i ∥2. From our assumptions introduced in Section 2.4, if we compare the learning speed of different components, we have common features ≫ dominant noises ≫ rare features ≫ background noises ≫ extremely rare features, in terms of “learning speed.” Based on this observation, we conduct a three-phase analysis for ERM. • Phase 1: Learning common features quickly. • Phase 2: Fitting (extremely) rare data by memorizing dominant noises instead of learning features. • Phase 3: A model cannot learn (extremely) rare features since gradients of all data are small. The main intuition behind why ERM cannot learn (extremely) rare features is that the gradients of all data containing these features become small after quickly memorizing dominant noise patches. In contrast, since Cutout randomly cuts some patches out, there exist augmented data points that do not contain dominant noises and have only features and background noises. This allows Cutout to learn rare features, thanks to these augmented data. However, extremely rare features cannot be learned since the learning speed of background noise is much faster and there are too many background noise patches to cut them all out. Remark 4.1. Shen et al. (2022) conduct analysis on vanilla training and training using standard data augmentation, sharing the same intuition in similar but different data models and neural networks. Also, we emphasize that we proved the model cannot learn (extremely) rare features even if we run poly(d) η iterations of GD, whereas Shen et al. (2022) only consider the first iteration that achieves perfect training accuracy. Practical Insights. In practice, images contain features and noise across several patches. A larger cutting size can be more effective in removing noise but may also remove important features that the model needs to learn. Thus, there is a trade-off in choosing the optimal cutting size, a trend also observed in DeVries and Taylor (2017). One limitation of Cutout is that it may not effectively remove dominant noise. Thus, dominant noise can persist in the augmented data, leading to potential noise memorization. We believe that developing strategies that can more precisely detect and remove these noise components from the image input could enhance the effectiveness of these methods. 84.2 CutMix Training In learning dynamics of ERM and Cutout, inner products between weight and data patches evolve (approximately) monotonically, which makes the analysis much more feasible. However, analyz- ing the learning dynamics of CutMix involves non-monotone change of inner products, which is inevitable since CutMix uses mixed labels; this is also demonstrated in our experimental results (Section 5,especially the leftmost plot in Figure 1). Non-monotonicity and non-convexity of the problem necessitates novel proof strategies. Let us define Z := {zs,k}s∈{±1},k∈[K] ∪ {z(p) i }i∈[n],p∈[P]\\{p∗ i } as a function of W as follows, z(p) i := ϕ \u0010D w1, ξ(p) i E\u0011 − ϕ \u0010D w−1, ξ(p) i E\u0011 , z s,k := ϕ(⟨w1, vs,k⟩) − ϕ(⟨w−1, vs,k⟩). Then, Z represents the contribution of each noise patch and feature vector to the neural network output, and the nonconvex function LCutMix(W) can be viewed as the composition of Z(W) and a convex function h(Z). By using the convexity of h(Z), we can characterize the global minimum of LCutMix(W). Surprisingly, we show that any global minimizer W∗ = {w∗ 1, w∗ −1} satisfies ϕ \u0010D w∗ s, x(p) i E\u0011 − ϕ \u0010D w∗ −s, x(p) i E\u0011 = Cs, for all s ∈ {±1}, i∈ Vs, and p ∈ [P], with some constants C1, C−1 = Θ(1). In other words, at the global minimum, the output of model on each patch of the training data is uniform across the set of data with the same labels. We also prove that CutMix can achieve a point close to the global minimum within poly(d) η iterations. As a result, the model trained by CutMix can learn all features including extremely rare features. The complete proof of Theorem 3.3 appears in Appendix E.2. Remark 4.2. Zou et al. (2023) investigate Mixup in a similar feature-noise model and show that Mixup can learn rarer features than vanilla training, with its benefits emerging from the early dynamics of training. However, our characterization of the global minimum of LCutMix(W) and experimental results in our setting (Section 5, Figure 1) suggest that the benefits of CutMix, especially for learning extremely rare features, arise from the later stages of training. This suggests that Mixup and CutMix have different underlying mechanisms for promoting feature learning. Practical Insights. The main underlying mechanism of CutMix is that it learns information almost uniformly from all patches in the training data. However, this approach also involves memorizing noise, which can potentially degrade performance in real-world scenarios. We believe that a more sophisticated strategy such as considering the positional information of patches as used in Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021) could improve the ability to learn more from patches containing features and reduce the impact of noise. 5 Experiments We conduct experiments both in our setting and real-world data CIFAR-10 to support our theoretical findings and intuition. We defer CIFAR-10 experiment results to Appendix A.1. For the numerical experiments on our setting, we set the number of patches P = 3 , dimension d = 2000, number of data points n = 300, dominant noise strength σd = 0.25, background noise strength σb = 0.15, and feature noise strength α = 0.005. The feature vectors are given as the standard basis e1, e2, e3, e4, e5, e6 ∈ Rd, where e1, e2, e3 are features for the positive label y = 1 and e4, e5, e6 are features for the negative label y = −1. We categorize e1 and e4 as common features with a frequency of 0.8, e2 and e5 as rare features with a frequency of 0.15, and lastly, e3 and e6 as extremely rare features with a frequency of 0.05. For the learner network, we set the slope of negative regime β = 0.1 and the length of the smoothed interval r = 1. We train models using three methods: ERM, Cutout, and CutMix with a learning rate η = 1. For Cutout, we cut a single patch of data (C = 1). We apply full-batch gradient descent for all methods; for Cutout and CutMix, we utilize all possible augmented data points.3 We note that this choice of problem parameters does not exactly match the technical assumptions in Section 2.4. However, we empirically observe the same conclusions, which suggests that our analysis could be extended beyond our assumptions. 3For CutMix, this may induce different choices of DS from those assumed in our analysis, but we mention that other general choices of DS do not alter the conclusions in our analysis. 9For each feature vector v of the positive label, we plot the output of the learned filters for the feature vector ϕ(⟨w(t) 1 , v⟩) − ϕ(⟨w(t) −1, v⟩) throughout training in Figure 1. Our numerical findings confirm that ERM can only learn common features, Cutout can learn common and rare features but cannot learn extremely rare features, and CutMix can learn all types of features. Especially, CutMix learn common features, rare features, and extremely rare features almost evenly. Also, we observed non- monotone behavior of the output in the case of CutMix, which motivated our novel proof technique. The same trends are observed with different architectures, such as a smoothed (leaky) ReLU network with multiple neurons, as detailed in Appendix A.2. 0 2000 4000 6000 8000 10000 Iterations 0 2 4 6 8 10Common Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0.0 0.2 0.4 0.6 0.8Rare Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0.0 0.2 0.4 0.6 0.8Extremely Rare Feature Output ERM Cutout CutMix Figure 1: Numerical results on our problem setting. We validate our findings on the trends of ERM, Cutout, and CutMix in learning common feature (Left), rare feature (Center), and extremely rare feature (Right). The output of the common feature trained by CutMix shows non-monotone behavior. 6 Conclusion We studied how Cutout and CutMix influence the ability to learn features in a patch-wise feature- noise data model learning with two-layer convolutional neural networks by comparing them with vanilla training. We showed that Cutout enables the learning of rare features that cannot be learned through vanilla training by mitigating the problem of memorizing label-independent noises instead of learning label-dependent features. Surprisingly, we further proved that CutMix can learn extremely rare features that Cutout cannot learn. We also present our theoretical insights on the underlying mechanism of these methods and provide experimental support. Limitation and Future Work. Our work has some limitations related to the neural network architecture, specifically, the use of a 2-layer two-neuron smoothed leaky ReLU network. Extending our results to neural networks with deeper, wider, and more general activation functions is a direction for future work. Another future direction is to develop patch-level data augmentation based on our theoretical findings. Also, it would be interesting to perform theoretical analysis on state-of-the-art patch-level data augmentation such as Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021). These methods utilize patch location information, thus it may require the development of a theoretical framework capturing more complex characteristics of image data. Acknowledgement This work was supported by three Institute of Information & communications Technology Planning & Evaluation (IITP) grants (No. RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST); No. RS-2022-II220184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics; No. RS-2024-00457882, AI Research Hub Project) funded by the Korean government (MSIT), and a National Research Foundation of Korea (NRF) grant (No. RS-2019-NR040050) funded by the Korean government (MSIT). CY acknowledges support from a grant funded by Samsung Electronics Co., Ltd. 10References Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020. Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends® in Machine Learning, 8(3-4):231–357, 2015. Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convo- lutional neural networks. Advances in neural information processing systems, 35:25237–25250, 2022. Luigi Carratino, Moustapha Cissé, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regulariza- tion. arXiv preprint arXiv:2006.06049, 2020. Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020a. Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 21321–21333, 2020b. Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. Advances in neural information processing systems, 35: 23049–23062, 2022. Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than sgd? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Muthu Chidambaram and Rong Ge. For better or for worse? learning minimum variance features with label augmentation. arXiv preprint arXiv:2402.06855, 2024. Muthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge. Towards understanding the data dependency of mixup-style training. arXiv preprint arXiv:2110.07647, 2021. Muthu Chidambaram, Xiang Wang, Chenwei Wu, and Rong Ge. Provably learning diverse features in multi-view data with midpoint mixup. In International Conference on Machine Learning, pages 5563–5599. PMLR, 2023. Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Ré. A kernel theory of modern data augmentation. In International conference on machine learning, pages 1528–1537. PMLR, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory, pages 2668–2703. PMLR, 2022a. Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks trained on high-dimensional data. arXiv preprint arXiv:2210.07082, 2022b. Boris Hanin and Yi Sun. How data augmentation affects optimization for linear regression. Advances in Neural Information Processing Systems, 34:8095–8105, 2021. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably ben- efit from structural information: A feature learning perspective. arXiv preprint arXiv:2306.13926, 2023a. 11Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In The Twelfth International Conference on Learning Representations, 2023b. Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In International Conference on Machine Learning, pages 9965–10040. PMLR, 2022. Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. In International Conference on Machine Learning, pages 5034–5044. PMLR, 2021. Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning, pages 5275–5285. PMLR, 2020. Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. arXiv preprint arXiv:2102.03065, 2021. Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In International Conference on Machine Learning , pages 17615–17659. PMLR, 2023a. Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Advances in neural information processing systems, 25, 2012. Binghui Li and Yuanzhi Li. Why clean generalization and robust overfitting both happen in adversarial training. arXiv preprint arXiv:2306.01271, 2023. Junsoo Oh and Chulhee Yun. Provable benefit of mixup for finding optimal decision boundaries. In International Conference on Machine Learning, pages 26403–26450. PMLR, 2023. Chanwoo Park, Sangdoo Yun, and Sanghyuk Chun. A unified analysis of mixed sample data augmentation: A loss function perspective. Advances in Neural Information Processing Systems, 35:35504–35518, 2022. Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does data augmentation lead to positive margin? In International Conference on Machine Learning, pages 5321–5330. PMLR, 2019. Ruoqi Shen, Sébastien Bubeck, and Suriya Gunasekar. Data augmentation as feature manipulation. In International conference on machine learning, pages 19773–19808. PMLR, 2022. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Ré. On the generalization effects of linear transformations in data augmentation. In International Conference on Machine Learning, pages 10410–10420. PMLR, 2020. Jaejun Yoo, Namhyuk Ahn, and Kyung-Ah Sohn. Rethinking data augmentation for image super- resolution: A comprehensive analysis and a new strategy. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8375–8384, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. 12Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020. Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In International Conference on Machine Learning, pages 26135–26160. PMLR, 2022. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug- mentation. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages 13001–13008, 2020. Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371, 2021. Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. In International Conference on Machine Learning, pages 43423–43479. PMLR, 2023. 13Contents 1 Introduction 1 1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Problem Setting 3 2.1 Data Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Neural Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Training Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.4 Assumptions on the Choice of Problem Parameters . . . . . . . . . . . . . . . . . 6 3 Main Results 6 4 Overview of Analysis 7 4.1 Vanilla Training and Cutout Training . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.2 CutMix Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5 Experiments 9 6 Conclusion 10 A Additional Experimental Results 15 A.1 Experiments on CIFAR-10 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 Additional Experimental Results on Our Data Distribution . . . . . . . . . . . . . 19 B Proof Preliminaries 20 B.1 Properties of the Choice of Problem Parameters . . . . . . . . . . . . . . . . . . . 20 B.2 Quantities at the Beginning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 Feature Noise Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C Proof for ERM 29 C.1 Proof of Lemma B.3 for ERM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 D Proof for Cutout 45 D.1 Proof of Lemma B.3 for Cutout . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 D.2 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 E Proof for CutMix 62 E.1 Proof of Lemma B.3 for CutMix . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 E.2 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 F Technical Lemmas 75 14A Additional Experimental Results For all experiments described in this section and in Section 5, we use NVIDIA RTX A6000 GPUs. A.1 Experiments on CIFAR-10 Dataset A.1.1 Experimental Support for Our Intuition We compare three methods, ERM training, Cutout training, and CutMix training on CIFAR-10 classification. For ERM training, we apply only random cropping and random horizontal flipping on train dataset. In comparison, for Cutout training and CutMix training, we additionally apply Cutout and CutMix, respectively, on training data. For Cutout training, we randomly cut 16 × 16 pixels of input images, and for CutMix training, we sample the mixing ratio from a beta distribution Beta(0.5, 0.5). We train ResNet-18 (He et al., 2016) for 200 epochs with a batch size of 128 using SGD with a learning rate 0.1, momentum 0.9, and weight decay 5 × 10−4. Trained models using ERM, Cutout, and CutMix achieve test accuracy 95.16%, 96.05%, and 96.29%, respectively. We randomly generate augmented data using CutMix from pairs of cat images and dog images in CIFAR-10 with varying mixing ratios λ = 1, 0.8, 0.6 (Dog:Cat = λ : 1 − λ). We randomly make 5, 000 (cat, dot)-pairs in CIFAR-10 training set and apply CutMix randomly 10 times. By repeating this procedure 10 times, we generate total 5, 000 × 10 × 10 = 500, 000 augmented samples for each mixing ratio λ. We plot a histogram of dog prediction output subtracted by cat prediction output (before applying the softmax function), evaluated on 500, 000 augmented data in Figure 2. 15  10  5  0 5 10 15 20 Dog Output - Cat Output 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Frequency ERM Cutout CutMix 15  10  5  0 5 10 15 20 Dog Output - Cat Output 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Frequency ERM Cutout CutMix 15  10  5  0 5 10 15 20 Dog Output - Cat Output 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35Frequency ERM Cutout CutMix Figure 2: Histogram of dog prediction output subtracted by cat prediction output evaluated on data points augmented by CutMix data using cat data and dog data with varying mixing ratio λ (Dog : Cat = λ : 1 − λ) (Left) λ = 1 , (Center) λ = 0.8, (Right) λ = 0.6 The leftmost plot represents the evaluation results for original dog images, as it uses a mixing ratio of λ = 1. We can observe that the output of the model trained using Cutout is skewed toward higher values compared to the output of the model trained using other methods. We believe this aligns with the theoretical intuition that Cutout learns more information from the original image using augmented data. The remaining two plots show the output for randomly augmented data using CutMix. We observe that the models trained with CutMix exhibit a shorter tail, supporting our intuition from the CutMix analysis that the models learn uniformly across all patches. A.1.2 Experimental Support for Our Findings We train ResNet-18 using ERM training, Cutout training, and CutMix training following the same experimental details described in Appendix A.1.1, except using only 10% of the training set. This data-hungry setting is intended to highlight the benefits of Cutout and CutMix. We then evaluated the trained models on the remaining 90% of the CIFAR-10 training dataset. The reason for evaluating the remaining training dataset is to analyze the misclassified data using C-score (Jiang et al., 2021), which is publicly available only for the training dataset. 15C-score measures the structural regularity of data, with lower values indicating examples that are more difficult to classify correctly. In our framework, data with harder-to-learn features (corresponding to rarer features) would likely have lower C-scores. Since directly extracting and quantitatively evaluating features learned by the models is challenging, we use the C-score as a proxy to evaluate the misclassified data across models trained by ERM, Cutout, and CutMix. Table 1 illustrates that Cutout tends to misclassify data with lower C-scores compared to ERM, indicating that Cutout learns more hard-to-learn features than vanilla training. Furthermore, the data misclassified by CutMix has even lower C-scores than those misclassified by Cutout, suggesting that CutMix is effective at learning features that are the most challenging to classify. This observation aligns with our theoretical findings, demonstrating that CutMix captures even more difficult features compared to both ERM and Cutout. Table 1: Mean and quantiles of the C-score on misclassified data across models trained with ERM, Cutout, and CutMix. The results indicate that Cutout tends to misclassify data with lower C-scores compared to ERM, while CutMix exhibits even lower C-scores. Method Mean Q1 Q2 Q3 ERM 0.687 0.615 0.782 0.841 Cutout 0.679 0.599 0.775 0.837 CutMix 0.670 0.575 0.767 0.835 Since directly visualizing features learned by a model is challenging, we present data that were misclassified by the model trained with ERM but correctly classified by the model trained with Cutout instead. In Figure 3, we show 7 samples per class with the lowest C-scores, which are considered to have rare features. Similarly, we also visualize data misclassified by the model trained with Cutout but correctly classified by the model trained with CutMix to represent extremely rare data in Figure 4. This approach allows us to interpret some (extremely) rare features in CIFAR-10, such as frogs with unusual colors. 16C-score: 0.024 C-score: 0.028 C-score: 0.029 C-score: 0.032 C-score: 0.032 C-score: 0.033 C-score: 0.037 C-score: 0.037 C-score: 0.038 C-score: 0.043  C-score: 0.045 C-score: 0.047 C-score: 0.049 C-score: 0.053 C-score: 0.056 C-score: 0.056 C-score: 0.057 C-score: 0.059 C-score: 0.060 C-score: 0.060 C-score: 0.065 C-score: 0.068 C-score: 0.072 C-score: 0.072 C-score: 0.077 C-score: 0.080 C-score: 0.081 C-score: 0.081  C-score: 0.083 C-score: 0.085  C-score: 0.085 C-score: 0.087  C-score: 0.088 C-score: 0.090 C-score: 0.091  C-score: 0.091 C-score: 0.094 C-score: 0.099 C-score: 0.102 C-score: 0.103 C-score: 0.105 C-score: 0.105 C-score: 0.108 C-score: 0.109 C-score: 0.111 C-score: 0.122 C-score: 0.124 C-score: 0.135  C-score: 0.143 C-score: 0.146 C-score: 0.150 C-score: 0.168  C-score: 0.180  C-score: 0.187 C-score: 0.187 C-score: 0.188  C-score: 0.204  C-score: 0.212 C-score: 0.213  C-score: 0.215 C-score: 0.222 C-score: 0.235  C-score: 0.255 C-score: 0.264  C-score: 0.308  C-score: 0.327  C-score: 0.344  C-score: 0.360  C-score: 0.432  C-score: 0.440 Figure 3: Examples of rare data in CIFAR-10 17C-score: 0.017 C-score: 0.017 C-score: 0.019 C-score: 0.021 C-score: 0.022 C-score: 0.029 C-score: 0.033 C-score: 0.033 C-score: 0.035 C-score: 0.037 C-score: 0.037 C-score: 0.039 C-score: 0.041 C-score: 0.042 C-score: 0.045 C-score: 0.048 C-score: 0.049 C-score: 0.051  C-score: 0.052 C-score: 0.052 C-score: 0.054 C-score: 0.054  C-score: 0.055 C-score: 0.056 C-score: 0.059 C-score: 0.059 C-score: 0.062  C-score: 0.064  C-score: 0.065 C-score: 0.069 C-score: 0.069 C-score: 0.071 C-score: 0.074 C-score: 0.074 C-score: 0.082 C-score: 0.091  C-score: 0.091 C-score: 0.096 C-score: 0.101 C-score: 0.103 C-score: 0.104 C-score: 0.105 C-score: 0.110 C-score: 0.111 C-score: 0.113 C-score: 0.114 C-score: 0.118 C-score: 0.124 C-score: 0.126 C-score: 0.128 C-score: 0.132 C-score: 0.134 C-score: 0.136 C-score: 0.139 C-score: 0.148 C-score: 0.148 C-score: 0.154  C-score: 0.168 C-score: 0.171 C-score: 0.174 C-score: 0.178 C-score: 0.196 C-score: 0.206  C-score: 0.212 C-score: 0.216 C-score: 0.247 C-score: 0.247 C-score: 0.250  C-score: 0.276 C-score: 0.283 Figure 4: Examples of extreme data in CIFAR-10 18A.2 Additional Experimental Results on Our Data Distribution In addition to the results described in Section 5, we further conducted numerical experiments on our data distribution by applying two variations to our architecture: increasing the number of neurons, and increasing the number of neurons with a smoothed ReLU activation (instead of smoothed leaky ReLU). We observed the same trends as predicted by our theoretical findings and shown in Figure 1. Let us describe the setting of our experiments in detail. In both cases, We set the number of patches P = 3, dimension d = 2000, and the number of data n = 300. The feature vectors are given by the standard basis e1, e2, e3, e4, e5, e6 ∈ Rd, where e1, e2, e3 are features for the positive label y = 1 and e4, e5, e6 are features for the negative label y = −1. We categorize e1 and e4 as common features, e2 and e5 as rare features, and lastly, e3 and e6 as extremely rare features. We apply full-batch gradient descent with learning rate η = 1 and for Cutout and CutMix, we utilize all possible augmented data. For the multi-neuron with smoothed Leaky ReLU case (Figure 5), we use 10 neurons for each positive/negative output with the slope of negative regime β = 0.1 and the length of polynomial regime r = 1. We set the strength of dominant noise σd = 0.25, the strength of background noise σb = 0.12, and the strength of feature noise α = 0.05. In addition, frequencies of common features, rare features, and extremely rare features are set to 0.72, 0.15, and 0.03, respectively. For the multi-neuron with smoothed ReLU case i.e., β = 0 (Figure 6), we set the length of the polynomial regime as r = 1, and we use 10 neurons for each positive/negative output. We set the remaining problem parameters as follows: the strength of dominant noise σd = 0.25, the strength of background noise σb = 0.12, and the strength of feature noise α = 0.05. In addition, frequencies of common features, rare features, and extremely rare features are set to0.75, 0.2, and 0.05, respectively. 0 2000 4000 6000 8000 10000 Iterations 0 2 4 6 8Common Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0 1 2 3 4Rare Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0.0 0.2 0.4 0.6 0.8Extremely Rare Feature Output ERM Cutout CutMix Figure 5: Multi-neuron with a smoothed leaky ReLU actiation 0 2000 4000 6000 8000 10000 Iterations 0 2 4 6 8 10Common Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0.0 0.5 1.0 1.5 2.0Rare Feature Output ERM Cutout CutMix 0 2000 4000 6000 8000 10000 Iterations 0.0 0.2 0.4 0.6 0.8Extremely Rare Feature Output ERM Cutout CutMix Figure 6: Multi-neuron with a smoothed ReLU 19B Proof Preliminaries B.1 Properties of the Choice of Problem Parameters In our analysis, we consider the choice of problem parameters as a function of the dimension of patches d and consider sufficiently large d. Let us summarize the assumptions on the parameters for the problem setting and assume they hold. Assumption B.1. The following conditions hold. A1. (The number of patches) P = Θ(1) and P ≥ 8. A2. (Overparameterized regime): n = o \u0010 αβσ−1 d σbd 1 2 /polylog(d) \u0011 . A3. (Sufficient feature data): For all k ∈ [K], ρkn = ω \u0010 n 1 2 log d \u0011 . A4. (Common feature vs dominant noise): For all k ∈ KC, ρk = Θ(1) and σ2 dd = o(βn). A5. (Rare feature vs noise): For all k ∈ KR, ρk = Θ(ρR) with ρRn = o \u0000 α2σ2 dd/polylog(d) \u0001 and σ2 bd = o(βρRn). A6. (Extremely rare feature vs background noise) For all k ∈ KE, ρk = Θ( ρE) with ρEn = o \u0000 α2σ2 bd/polylog(d) \u0001 . A7. (Strength of feature noise) α = o \u0000 n−1βσ2 dd/polylog(d) \u0001 . A8. σ0σ2 dd, r= o (α/polylog(d)) , η= o \u0000 rσ−2 d d−1/polylog(d) \u0001 We now present some properties derived from Assumption B.1, which are frequently used throughout our proof. From (A3), for all k ∈ [K], we have the following inequality: n ≥ ρ1n ≥ ρ2 kn = ω \u0000 log2 d \u0001 (6) From (A1) and (A2), and given that β <1, σb < σd, we have d >(βσ−1 d σbd 1 2 )2 > n2P > nP. (7) From (A2), (A3), and (A6), and given that α, β <1, we have σ2 dd > σ2 bd = ω(ρEn) = ω(1). (8) From (A1), (A2) and the fact that 0 < α <1, we have nP β−1σdσ−1 b d−1 2 = o \u0012 α polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 (9) From (A7) and (A4), we have αβ−1 < αβ−2 = o \u0012n−1β−1σ2 dd polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 (10) From (8) and (A8), η = o(1) and then we have η ≤ log(ηT ∗) 2 . (11) From (A2), (A3), (A4), and (A5) we have α−2 = o \u0012 σ2 dd ρRn \u0013 = o \u0000 ρ−1 R \u0001 = o \u0010 n 1 2 \u0011 = o \u0010 d 1 4 \u0011 . (12) 20B.2 Quantities at the Beginning We characterize some quantities at the beginning of training. Lemma B.2. Let Einit the event such that all the following holds: • 25 52 n ≤ |V1|, |V−1| ≤27 52 n • For each s ∈ {±1} and k ∈ [K], ρkn 4 ≤ |Vs,k| ≤3ρkn 4 • ∪i∈V1,1 {p∗ i } = [P] • For any s, s′ ∈ {±1} and k ∈ [K], \f\f\f D w(0) s , vs′,k E\f\f\f ≤ σ0 log d. • For any s ∈ {±1} and i ∈ [n], \f\f\f D w(0) s , ξ(˜pi) i E\f\f\f ≤ σ0σdd 1 2 log d. • For any s ∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i , ˜pi}, \f\f\f D w(0) s , ξ(p) i E\f\f\f ≤ σ0σbd 1 2 log d. • For any i, j∈ [n] with i ̸= j, 1 2 σ2 dd ≤ \r\r\rξ(˜pi) i \r\r\r 2 ≤ 3 2 σ2 dd and \f\f\f D ξ(˜pi) i , ξ(˜pj) j E\f\f\f ≤ σ2 dd 1 2 log d. • For any i, j∈ [n] and p ∈ [P] \\ {p∗ j , ˜pj}, \f\f\f D ξ(˜pi) i , ξ(p) j E\f\f\f ≤ σdσbd 1 2 log d. • For any i, j∈ [n] and p ∈ [P] \\ {p∗ i , ˜pi}, q∈ [P] \\ {p∗ j , ˜pj} with (i, p) ̸= (j, q), 1 2 σ2 bd ≤ \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 bd and \f\f\f D ξ(p) i , ξ(q) j E\f\f\f ≤ σ2 bd 1 2 log d. • {vs,k}s∈{±1},k∈[K] ∪ {x(p) i }i∈[n],p∈[P]\\{p∗ i } is linearly independent. Then, the event Einit occurs with probability at least 1 − o \u0010 1 poly(d) \u0011 . Also, if ξ ∼ N(0, σ2Λ) is independent of w(0) 1 , w(0) −1 and {(Xi, yi)}i∈[n], we have \f\f\f D w(0) 1 , ξ E\f\f\f, \f\f\f D w(0) −1, ξ E\f\f\f ≤ σ0σd 1 2 log d, and \f\f\f D ξ, ξ(p) i E\f\f\f ≤ σσdd 1 2 log d, for all i ∈ [n] and p ∈ [P] \\ {p∗ i }, with probability at least 1 − o \u0010 1 poly(d) \u0011 . Proof of Lemma B.2. Let us prove the first three points hold with probability at least1 −o \u0010 1 poly(d) \u0011 . By Höeffding’s inequality, P h\f\f\f|V1| −n 2 \f\f\f > n 52 i = P   \f\f\f\f\f\f X i∈[n] (1 yi=1 − E[1 yi=1]) \f\f\f\f\f\f > n 52   ≤ 2 exp \u0012 − 2 522 n \u0013 = o \u0012 1 poly(d) \u0013 , where the last equality is due to (6). In addition, for each s ∈ {±1}, k∈ [K], by Höeffding’s inequality P h\f\f\f|Vs,k| −ρk 2 n \f\f\f > ρk 4 n i = P   \f\f\f\f\f\f X i∈[n] \u0000 1 i∈Vs,k − E[1 i∈Vs,k ] \u0001 \f\f\f\f\f\f > ρk 4 n   ≤ 2 exp \u0012 −ρ2 k 8 n \u0013 = o \u0012 1 poly(d) \u0013 , where the last equality is due to (6). Also, for each i ∈ [n] and p ∈ [P], P[{i ∈ V1,1} ∩ {p∗ i = p}] = ρ1 P . 21Hence, P \u0002 ∪i∈V1,1 {p∗ i } ̸= [P] \u0003 ≤ X p∈[P] P h ∩i∈[n] \u0010 ({i ∈ V1,1} ∩ {p∗ i = p})∁ \u0011i = P \u0010 1 − ρ1 P \u0011n ≤ P exp \u0010 −ρ1 P n \u0011 = o \u0012 1 poly(d) \u0013 . Next, we will prove the remaining. Let us refer to the standard deviation of the Gaussian noise vector in p-th patch of i-th data as σi,p. In other words, for each i ∈ [n] and p ∈ [P] \\ {p∗ i }, σi,p = \u001aσd if p = ˜pi, σb otherwise. For each s, s′ ∈ {±1} and k ∈ [K], D w(0) s , vs′,k E ∼ N(0, σ0). Hence, by Höeffding’s inequality, we have P h\f\f\f D w(0) s , vs′,k E\f\f\f > σ0 log d i ≤ 2 exp   −(σ0 log d)2 2σ2 0 ! = o \u0012 1 poly(d) \u0013 . Let {ul}l∈[d−2K] be an orthonormal basis of the orthogonal complement of Span({vs,k}s∈{±1},k∈[K]). Note that for each s ∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i }, we can write ξ(p) i and ξ as ws(0) = σ0 X l∈[d−2K] zs,lul, ξ (p) i = σi,p X l∈[d−2K] z(p) i,l ul, ξ = σ X l∈[d−2K] zlul where zs,l, z(p) i,l , zl i.i.d. ∼ N(0, 1). The sub-gaussian norm of standard normal distribution N(0, 1) isq 8 3 . Then \u0010 z(p) i,l \u00112 − 1’s are mean zero sub-exponential with sub-exponential norm 8 3 (Lemma 2.7.6 in Vershynin (2018)). In addition, zs,lz(p) i,l ’s, z(p) i,l z(q) j,l ’s and z(p) i,l zl’s are mean zero sub-exponential with sub-exponential norm less than or equal to 8 3 (Lemma 2.7.7 in Vershynin (2018)). We use Bernstein’s inequality (Theorem 2.8.1 in Vershynin (2018)), withc being the absolute constant stated therein. We then have the following: 1 − P \u00141 2σ2 i,pd ≤ \r\r\rξ(p) i \r\r\r 2 ≤ 3 2σ2 i,pd \u0015 ≤ P \u0014\f\f\f\f \r\r\rξ(p) i \r\r\r 2 − σ2 i,p(d − 2K) \f\f\f\f ≥ σ2 i,pd 1 2 log d \u0015 = P   \f\f\f\f\f\f X l∈[d−2K] \u0012\u0010 z(p) i,l \u00112 − 1 \u0013\f\f\f\f\f\f ≥ d 1 2 log d   ≤ 2 exp \u0012 − 9cd log2 d 64(d − 2K) \u0013 ≤ 2 exp \u0012 −9c log2 d 64 \u0013 = o \u0012 1 poly(d) \u0013 , in addition, P h\f\f\f D ξ(p) i , ξ(q) j E\f\f\f ≥ σi,pσj,qd 1 2 log d i = P   \f\f\f\f\f\f X l∈[d−2K] z(p) i,l z(q) j,l \f\f\f\f\f\f ≥ d 1 2 log d   ≤ 2 exp \u0012 − 9cd log2 d 64(d − 2K) \u0013 ≤ 2 exp \u0012 −9c log2 d 64 \u0013 = o \u0012 1 poly(d) \u0013 . 22Similarly, we have P h\f\f\f D w(0) s , ξ(p) i E\f\f\f ≥ σ0σi,pd 1 2 log d i ≤ 2 exp \u0012 −9c log2 d 64 \u0013 = o \u0012 1 poly(d) \u0013 . Lastly, the last result holds almost surely due to (7). Applying the union bound to all events, each of which is at most poly(d) due to (7), leads us to our first conclusion. In addition, for each s ∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i }, P h\f\f\f D w(0) s , ξ E\f\f\f ≥ σ0σd 1 2 log d i ≤ 2 exp \u0012 −9c log2 d 64 \u0013 = o \u0012 1 poly(d) \u0013 , and P h\f\f\f D ξ(p) i , ξ E\f\f\f ≥ σi,pσd 1 2 log d i ≤ 2 exp \u0012 −9c log2 d 64 \u0013 = o \u0012 1 poly(d) \u0013 . Applying the union bound to all events, each of which is at most poly(d) due to (7), leads us to our second conclusion. B.3 Feature Noise Decomposition In our analysis, we use a technique that analyzes the coefficients of linear combinations of feature and noise vectors. A similar technique in a different data and network setting is introduced by Cao et al. (2022). Lemma B.3. If we run one of ERM, Cutout, and CutMix training to update parameters W(t) of a model fW(t) , then there exist coefficients (corresponding to each method)γ(t) s (s′, k)’s and ρ(t) s (i, p)’s so that we can write W(t) = {w(t) 1 , w(t) −1} as w(t) s = w(0) s + X k∈[K] γ(t) s (s, k)vs,k − X k∈[K] γ(t) s (−s, k)v−s,k + X i∈Vs,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syiρ(t) s (i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρ(t) s (i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2   where Fs denotes the set of indices of data with feature noise vs,1. Furthermore, if we run one of ERM and Cutout, the coefficients γ(t) s (s′, k)’s and ρ(t) s (i, p)’s are monotone increasing. We provide proof of Lemma B.3 for ERM in Appendix C.1, for Cutout in Appendix D.1 and for CutMix in Appendix E.1. Since Gaussian vectors in a high-dimensional regime are nearly orthogonal, we can use the coefficients to approximate the inner products or outputs of neurons. The following lemma quantifies the approximation error. Lemma B.4. Suppose the event Einit occurs and 0 ≤ γ(t) s (s′, k), ρ(t) s (i, p) ≤ eO(β−1) for all s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i } at iteration t. Then, for each s ∈ {±1}, k∈ [K], i∈ [n], and p ∈ [P] \\ {p∗ i }, the following holds: • \f\f\f D w(t) s , vs,k E − γ(t) s (s, k) \f\f\f, \f\f\fϕ \u0010D w(t) s , vs,k E\u0011 − γ(t) s (s, k) \f\f\f = o \u0010 1 polylog(d) \u0011 • \f\f\f D w(t) s , v−s,k E + γ(t) s (−s, k) \f\f\f, \f\f\fϕ \u0010D w(t) s , v−s,k E\u0011 + βγ(t) s (−s, k) \f\f\f = o \u0010 1 polylog(d) \u0011 • \f\f\f D w(t) yi , ξ(p) i E − ρ(t) yi (i, p) \f\f\f, \f\f\fϕ \u0010D w(t) yi , ξ(p) i E\u0011 − ρ(t) yi (i, p) \f\f\f = o \u0010 1 polylog(d) \u0011 • \f\f\f D w(t) −yi, ξ(p) i E + ρ(t) −yi(i, p) \f\f\f, \f\f\fϕ \u0010D w(t) −yi, ξ(p) i E\u0011 + βρ(t) −yi(i, p) \f\f\f = o \u0010 1 polylog(d) \u0011 23• \f\f\fϕ \u0010D w(t) yi , x(˜pi) i E\u0011 − ρ(t) yi (i, ˜pi) \f\f\f, \f\f\fϕ \u0010D w(t) −yi, x(˜pi) i E\u0011 + βρ(t) −yi(i, ˜pi) \f\f\f = o \u0010 1 polylog(d) \u0011 Proof of Lemma B.4. For each s ∈ {±1}, k∈ [K] \\ {1}, by (A8) and (8), we have \f\f\f D w(t) s , vs,k E − γ(t) s (s, k) \f\f\f = \f\f\f D w(0) s , vs,k E\f\f\f = eO(σ0) = o \u0012 1 polylog(d) \u0013 . Similarly, by (A8) and (8), \f\f\f D w(t) s , v−s,k E + γ(t) s (−s, k) \f\f\f = \f\f\f D w(0) s , v−s,k E\f\f\f = eO(σ0) = o \u0012 1 polylog(d) \u0013 , Next, we will consider the case of v1,1 and v−1,1. For each s ∈ {±1}, we have \f\f\f D w(t) s , vs,1 E − γ(t) s (s, 1) \f\f\f ≤ \f\f\f D w(0) s , vs,1 E\f\f\f + α X i∈[n] ρ(t) s (i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 ≤ eO(σ0) + eO \u0000 αnβ−1σ−2 d d−1\u0001 = o \u0012 1 polylog(d) \u0013 , where the last equality is due to (8) and (A7). Similarly, we have \f\f\f D w(t) s , v−s,1 E + γ(t) s (−s, 1) \f\f\f ≤ \f\f\f D w(0) s , v−s,1 E\f\f\f + α X i∈[n] ρ(t) s (i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 ≤ eO(σ0) + eO \u0000 αnβ−1σ−2 d d−1\u0001 = o \u0012 1 polylog(d) \u0013 . Hence, from (A8) and the fact that |ϕ(z) − z| ≤(1−β)r 2 for any z ≥ 0, we have \f\f\fϕ \u0010D w(t) s , vs,k E\u0011 − γ(t) s (s, k) \f\f\f ≤ \f\f\fϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010 γ(t) s (s, k) \u0011\f\f\f + \f\f\fϕ \u0010 γ(t) s (s, k) \u0011 − γ(t) s (s, k) \f\f\f ≤ \f\f\f D w(t) s , vs,k E − γ(t) s (s, k) \f\f\f + (1 − β)r 2 = o \u0012 1 polylog(d) \u0013 . and \f\f\fϕ \u0010D w(t) s , v−s,k E\u0011 + βγ(t) s (−s, k) \f\f\f = \f\f\fϕ \u0010D w(t) s , v−s,k E\u0011 − ϕ \u0010 −γ(t) s (−s, k) \u0011\f\f\f ≤ \f\f\f D w(t) s , v−s,k E + γ(t) s (−s, k) \f\f\f = o \u0012 1 polylog(d) \u0013 . For each i ∈ [n], and p ∈ [P] \\ {p∗ i }, we have \f\f\f D w(t) yi , ξ(p) i E − ρ(t) yi (i, p) \f\f\f ≤ \f\f\f D w(0) yi , ξ(p) i E\f\f\f + X j∈[n],q∈[P]\\{p∗ i } (j,q)̸=(i,p) ρ(t) yi (j, q) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(q) j \r\r\r 2 24≤ eO \u0010 σ0σdd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 1 polylog(d) \u0013 , where the last equality is due to (A8) and (9). By triangular inequality, (A8), and the fact that ϕ′ ≤ 1 and |ϕ(z) − z| ≤(1−β)r 2 for any z ≥ 0, we have \f\f\fϕ \u0010D w(t) yi , ξ(p) i E\u0011 − ρ(t) yi (i, p) \f\f\f ≤ \f\f\fϕ \u0010D w(t) yi , ξ(p) i E\u0011 − ϕ \u0010 ρ(t) yi (i, p) \u0011\f\f\f + \f\f\fϕ \u0010 ρ(t) yi (i, p) \u0011 − ρ(t) yi (i, p) \f\f\f ≤ \f\f\f D w(t) yi , ξ(p) i E − ρ(t) yi (i, p) \f\f\f + (1 − β)r 2 = o \u0012 1 polylog(d) \u0013 . Also, if i ∈ Fs for some s ∈ {±1}, \f\f\fϕ \u0010D w(t) yi , x(˜pi) i E\u0011 − ρ(t) yi (i, ˜pi) \f\f\f ≤ \f\f\fϕ \u0010D w(t) yi , ξ(˜pi) i E\u0011 − ρ(t) yi (i, ˜pi) \f\f\f + \f\f\fϕ \u0010D w(t) yi , x(˜pi) i E\u0011 − ϕ \u0010D w(t) yi , ξ(˜pi) i E\u0011\f\f\f ≤ \f\f\fϕ \u0010D w(t) yi , ξ(˜pi) i E\u0011 − ρ(t) yi (i, ˜pi) \f\f\f + α \f\f\f D w(t) yi , vs,1 E\f\f\f ≤ \f\f\fϕ \u0010D w(t) yi , ξ(˜pi) i E\u0011 − ρ(t) yi (i, ˜pi) \f\f\f + αγ(t) yi (s, 1) + α · o \u0012 1 polylog(d) \u0013 ≤ eO \u0000 αβ−1\u0001 + o \u0012 1 polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 , where we apply the triangular inequality, the fact that ϕ′ ≤ 1, the triangular inequality again, ρ(t) yi (s, 1) = eO(β−1) and (10) sequentially. Similarly, \f\f\f D w(t) −yi, ξ(p) i E + ρ(t) −yi(i, p) \f\f\f ≤ \f\f\f D w(0) −yi, ξ(p) i E\f\f\f + X j∈[n],q∈[P]\\{p∗ i } (j,q)̸=(i,p) ρ(t) −yi(j, q) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(q) j \r\r\r 2 ≤ eO(σ0σdd 1 2 ) + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 1 polylog(d) \u0013 , and \f\f\fϕ \u0010D w(t) −yi, ξ(p) i E\u0011 + βρ(t) −yi(i, p) \f\f\f = \f\f\fϕ \u0010D w(t) −yi, ξ(p) i E\u0011 − ϕ \u0010 −ρ(t) −yi(i, p) \u0011\f\f\f ≤ \f\f\f D w(t) −yi, ξ(p) i E + ρ(t) −yi(i, p) \f\f\f = o \u0012 1 polylog(d) \u0013 , Also, if i ∈ Fs for some s ∈ {±1}, \f\f\fϕ \u0010D w(t) −yi, x(˜pi) i E\u0011 + βρ(t) −yi(i, ˜pi) \f\f\f = \f\f\fϕ \u0010D w(t) −yi, ξ(˜pi) i E\u0011 + βρ(t) −yi(i, ˜pi) \f\f\f + \f\f\fϕ \u0010D w(t) −yi, x(˜pi) i E\u0011 − ϕ \u0010D w(t) −yi, ξ(˜pi) i E\u0011\f\f\f 25≤ \f\f\fϕ \u0010D w(t) −yi, ξ(˜pi) i E\u0011 + βρ(t) −yi(i, ˜pi) \f\f\f + α \f\f\f D w(t) −yi, vs,1 E\f\f\f ≤ \f\f\fϕ \u0010D w(t) −yi, ξ(˜pi) i E\u0011 + βρ(t) −yi(i, ˜pi) \f\f\f + αγ(t) −yi(s, 1) + α · o \u0012 1 polylog(d) \u0013 ≤ eO \u0000 αβ−1\u0001 + o \u0012 1 polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 . We define the set W as the collection of W = {w1, w−1}, where w1 − w(0) 1 , w−1 − w(0) −1 are elements of the subspace spanned by {vs,k}s∈{±1},k∈[K] ∪ n x(p) i o i∈[n],p∈[P]\\{p∗ i } . The follow- ing lemma guarantees the unique expression of any W ∈ Win the form of the feature noise decomposition. Lemma B.5. Suppose the event Einit occurs. For each element W = {w1, w−1} ∈ W, there exist unique coefficients γs(s′, k)’s and ρs(i, p)’s such that ws = w(0) s + X k∈[K] γs(s, k)vs,k − X k∈[K] γs(−s, k)v−s,k + X i∈Vs p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syiρs(i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρs(i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2   for each s ∈ {±1}. Using this fact, for each s∗ ∈ {±1} and k∗ ∈ [K], we can introduce a function Q(s∗,k∗) : W →Rd×2 such that for each W = {w1, w−1} ∈ W, Q(s∗,k∗)(W) = n Q(s∗,k∗) 1 (w1), Q(s∗,k∗) −1 (w−1) o is given by: Q(s∗,k∗) s (ws) = ss∗γs(s∗, k∗)vs∗,k∗ + ss∗ X i∈Vs∗,k∗,p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs∩Vs∗,k∗ ss∗ρs(i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s∩Vs∗,k∗ ss∗ρs(i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  . The function Q(s∗,k∗) plays a crucial role in Section C.2.4 and Section D.2.4. The key intuition behind our definition of Q(s∗,k∗) is that Q(s∗,k∗)(W(t)) represents the term updated by the data having the feature vector vs∗,k∗, where W(t) are the iterates of either ERM or Cutout. As expected from this intuition, if we sum all Q(s∗,k∗) 1 (w1) and Q(s∗,k∗) −1 (w−1) over all s∗ ∈ {±1} and k∗ ∈ [K], the result will be equal to w1 − w(0) 1 and w−1 − w(0) −1, respectively. Proof. From linear independency of {vs,k}s∈{±1},k∈[K] ∪ n x(p) i o i∈[n],p∈[P]\\{p∗ i } , we can express any element W = {w1, w−1} ∈ Was ws = w(0) s + X k∈[K] ˜γs(s, k)vs,k − X k∈[K] ˜γs(−s, k)v−s,k 26+ X i∈Vs, p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r − X i∈V−s, p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r (13) with unique {˜γs(s, k), ˜γs(−s, k)}s∈{±1},k∈[K] and {ρs(i, p)}s∈{±1},i∈[n],p∈[P]\\{i∗}. If we define γs(s, k) and γs(−s, k) as γs(s, k) = ˜γs(s, k), γs(−s, k) = ˜γs(−s, k) for k ̸= 1, and γs(s, 1) = ˜γs(s, 1) − α X i∈Fs syiρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 , γs(−s, 1) = ˜γs(−s, 1) + α X i∈F−s syiρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 , then we have ws = w(0) s + X k∈[K] γs(s, k)vs,k − X k∈[K] γs(−s, k)v−s,k + X i∈Vs p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s p∈[P]\\{p∗ i } ρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syiρs(i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρs(i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  . Next, we want to show the uniqueness part. Suppose {ˆγs(s, k), ˆγs(−s, k)}s∈{±1},k∈[K] and {ˆρs(i, p)}s∈{±1},i∈[n],p∈[P]\\{i∗} satisfies ws = w(0) s + X k∈[K] ˆγs(s, k)vs,k − X k∈[K] ˆγs(−s, k)v−s,k + X i∈Vs p∈[P]\\{p∗ i } ˆρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s p∈[P]\\{p∗ i } ˆρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syi ˆρs(i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syi ˆρs(i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  . We have ws = w(0) s + X k∈[K]\\{1} ˆγs(s, k)vs,k − X k∈[K]\\{1} ˆγs(−s, k)v−s,k +   ˆγs(s, 1) + α X i∈Fs syi ˆρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 ! vs,1 −  ˆγs(−s, 1) − α X i∈F−s syi ˆρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2  v−s,1 + X i∈Vs p∈[P]\\{p∗ i } ˆρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s p∈[P]\\{p∗ i } ˆρs(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 . From the uniqueness of (13), we have ˆγs(s, k) = ˜γs(s, k) = γs(s, k), ˆγs(−s, k) = ˜γs(−s, k) = γs(−s, k), for each s ∈ {±1}, k∈ [K] \\ {1}, and ˆρs(i, p) = ρs(i, p) for each i ∈ [n], p∈ [P] \\ {p∗ i }. Furthermore, ˆγs(s, 1) + α X i∈Fs syi ˆρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 = ˜γs(s, 1) = γs(s, 1) + α X i∈Fs syiρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 , 27and ˆγs(−s, 1)−α X i∈F−s syi ˆρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 = ˜γs(−s, 1) = γs(−s, 1)−α X i∈F−s syiρs(i, ˜pi) \r\r\rξ(˜pi) i \r\r\r −2 . Hence, we obtain the uniqueness of the expression and Q(s∗,k∗) is well defined for each s∗ ∈ {±1} and k∗ ∈ [K]. 28C Proof for ERM In this section, we use g(t) i := 1 1+exp(yifW(t) (Xi)) for each data i and iteration t, for simplicity. C.1 Proof of Lemma B.3 for ERM For s ∈ {±1} and iterate t, w(t+1) s − w(t) s = −η∇wsLERM \u0010 W(t) \u0011 = η n X i∈[n] syig(t) i X p∈[P] ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i = η n  X i∈Vs g(t) i X p∈[P] ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i − X i∈V−s g(t) i X p∈[P] ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i  , and we have X i∈Vs g(t) i X p∈[P] ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i = X k∈[K] X i∈Vs,k g(t) i ϕ′ \u0010D w(t) s , vs,k E\u0011 vs,k + X i∈Vs g(t) i X p∈[P]\\{p∗ i ,˜pi} ϕ′ \u0010D w(t) s , ξ(p) i E\u0011 ξ(p) i + X i∈Vs∩Fs g(t) i ϕ′ \u0010D w(t) s , αvs,1 + ξ(˜pi) i E\u0011\u0010 αvs,1 + ξ(˜pi) i \u0011 + X i∈Vs∩F−s g(t) i ϕ′ \u0010D w(t) s , αv−s,1 + ξ(˜pi) i E\u0011\u0010 αv−s,1 + ξ(˜pi) i \u0011 , and X i∈V−s g(t) i X p∈[P] ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i = X k∈[K] X i∈V−s,k g(t) i ϕ′ \u0010D w(t) s , v−s,k E\u0011 v−s,k + X i∈V−s g(t) i X p∈[P]\\{p∗ i ,˜pi} ϕ′ \u0010D w(t) s , ξ(p) i E\u0011 ξ(p) i + X i∈V−s∩Fs g(t) i ϕ′ \u0010D w(t) s , αvs,1 + ξ(˜pi) i E\u0011\u0010 αvs,1 + ξ(˜pi) i \u0011 + X i∈V−s∩F−s g(t) i ϕ′ \u0010D w(t) s , αv−s,1 + ξ(˜pi) i E\u0011\u0010 αv−s,1 + ξ(˜pi) i \u0011 . Hence, if we define γ(t) s (s′, k)’s and ρ(t) s (i, p)’s recursively by using the rule γ(t+1) s (s′, k) = γ(t) s (s′, k) + η n X i∈Vs′,k g(t) i ϕ′ \u0010D w(t) s , vs′,k E\u0011 , (14) ρ(t+1) s (i, p) = ρ(t) s (i, p) + η ng(t) i ϕ′ \u0010D w(t) s , x(p) i E\u0011\r\r\rξ(p) i \r\r\r 2 , (15) starting from γ(0) s (s′, k) = ρ(0) s (i, p) = 0 for each s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P]\\{p∗ i }, then we have w(t) s = w(0) s + X k∈[K] γ(t) s (s, k)vs,k − X k∈[K] γ(t) s (−s, k)v−s,k + X i∈Vs,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 29+ α   X i∈Fs syiρ(t) s (i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρ(t) s (i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  , for each s ∈ {±1}. Furthermore, γ(t) s (s′, k)’s and ρ(t) s (i, p)’s are monotone increasing. □ C.2 Proof of Theorem 3.1 To show Theorem 3.1, we present a structured proof comprising the following five steps: 1. Establish upper bounds on γ(t) s (s′, k)’s and ρ(t) s (i, p)’s to apply Lemma B.4 (Section C.2.1). 2. Demonstrate that the model learns common features quickly (Section C.2.2). 3. Show that the model overfits dominant noise in (extremely) rare data instead of learning its feature (Section C.2.3). 4. Confirm the persistence of this tendency until T∗ iterates (Section C.2.4). 5. Characterize train accuracy and test accuracy (Section C.2.5). C.2.1 Bounds on the Coefficients in Feature Noise Decomposition The following lemma provides upper bounds on Lemma B.3 during T∗ iterations. Lemma C.1. Suppose the event Einit occurs. For any t ∈ [0, T∗], we have 0 ≤ γ(t) s (s, k) + βγ(t) −s(s, k) ≤ 4 log(ηT ∗), 0 ≤ ρ(t) yi (i, p) + βρ(t) −yi(i, p) ≤ 4 log (ηT ∗) , for all s ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P]\\{p∗ i }. Consequently,γ(t) s (s′, k), ρ(t) s (i, p) = eO(β−1) for all s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }. Proof of Lemma C.1. The first argument implies the second argument since log(ηT ∗) = polylog(d) and γ(t) s (s′, k) ≤ β−1 \u0010 γ(t) s′ (s′, k) + βγ(t) s′ (s′, k) \u0011 , ρ (t) s (i, p) ≤ β−1 \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 , for all s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }. We will prove this by using induction on t. The initial case t = 0 is trivial. Suppose the given statement holds at t = T and consider the case t = T + 1. Let ˜Ts,k ≤ T denote the smallest iteration where γ( ˜Ts,k+1) s (s, k) + βγ( ˜Ts,k+1) −s (s, k) > 2 log(ηT ∗). We assume the existence of ˜Ts,k, as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (14) and (11): γ(T+1) s (s, k) + βγ(T+1) −s (s, k) = γ(T) s (s, k) + βγ(T) −s (s, k) + η n X i∈Vs,k g(T) i \u0012 ϕ′ \u0010D w(T) s , vs,k E\u0011 + βϕ′ \u0010D w(T) −s , vs,k E\u0011\u0013 ≤ 2 log(ηT ∗) + 2η ≤ 4 log(ηT ∗) Now suppose there exists such ˜Ts,k ≤ T. By (14), we have γ(T+1) s (s, k) + βγ(T+1) −s (s, k) = γ( ˜Ts,k) s (s, k) + βγ( ˜Ts,k) −s (s, k) + TX t= ˜Ts,k \u0010 γ(t+1) s (s, k) + βγ(t+1) −s (s, k) − γ(t) s (s, k) − βγ(t) −s(s, k) \u0011 ≤ 2 log(ηT ∗) + log(ηT ∗) + η n TX t= ˜Ts,k+1 X i∈Vs,k g(t) i \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 . 30The inequality is due to γ( ˜Ts,k) s (s, k) + βγ( ˜Ts,k) −s (s, k) ≤ 2 log(ηT ∗) from our choice of ˜Ts,k and η n X i∈Vs,k g( ˜Ts,k) i \u0012 ϕ′ \u0010D w( ˜Ts,k) s , vs,k E\u0011 + βϕ′ \u0010D w( ˜Ts,k) −s , vs,k E\u0011\u0013 ≤ 2η ≤ log(ηT ∗), from (11). For each t = ˜Ts,k + 1, . . . T, and i ∈ Vs,k, we have yifW(t) (Xi) = ϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010D w(t) −s, vs,k E\u0011 + X p∈[P]\\{p∗ i } \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − 2P · o \u0012 1 polylog(d) \u0013 ≥ 3 2 log(ηT ∗) The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of t, γ(t) s (s, k) + βγ(t) −s(s, k) ≥ 2 log(ηT ∗). Hence, we obtain η n TX t= ˜Ts,k X i∈Vs,k g(t) i \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 ≤ 2η n TX t= ˜Ts,k X i∈Vs,k exp (−yifW(t) (Xi)) ≤ 2|Vs,k| n (ηT ∗) exp \u0012 −3 2 log(ηT ∗) \u0013 ≤ 2√ηT ∗ ≤ log(ηT ∗), where the last inequality holds for any reasonably large T∗. Merging all inequalities together, we have γ(T+1) s (s, k) + βγ(T+1) −s (s, k) ≤ 4 log(ηT ∗). Next, we will follow similar arguments to show that ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) ≤ 4 log(ηT ∗) for each i ∈ [n] and p ∈ [P] \\ {p∗ i }. Let ˜T(p) i ≤ T be the smallest iteration such that ρ ( ˜T(p) i +1) yi (i, p) + βρ ( ˜T(p) i +1) −yi (i, p) > 2 log(ηT ∗). We assume the existence of ˜T(p) i , as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (15) and (11): ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) = ρ(T) yi (i, p) + βρ(T) −yi(i, p) + η ng(T) i \u0012 ϕ′ \u0010D w(t) s , x(p) i E\u0011 + βϕ′ \u0010D w(t) s , x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 ≤ 2 log(ηT ∗) + 2η ≤ 4 log(ηT ∗), where the first inequality is due to \r\r\rξ(p) i \r\r\r ≤ 3 2 σ2 dd and (A4), and the last inequality is due to (11). Now suppose there exists such ˜T(p) i ≤ T. By (15), we have ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) 31= ρ ( ˜T(p) i ) yi (i, p) + βρ ( ˜T(p) i ) −yi (i, p) + TX t= ˜T(p) i \u0010 ρ(t+1) yi (i, p) + βρ(t+1) −yi (i, p) − ρ(t) yi (i, p) − βρ(t) −yi(i, p) \u0011 ≤ 2 log(ηT ∗) + log(ηT ∗) + η n TX t= ˜T(p) i +1 g(t) i \u0012 ϕ′ \u0010D w(t) s , x(p) i E\u0011 + βϕ′ \u0010D w(t) −s, x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 The inequality is due to ρ ( ˜T(p) i ) yi (i, p) + βρ ( ˜T(p) i ) −yi (i, p) ≤ 2 log(ηT ∗) from our choice of ˜T(p) i and η ng ( ˜T(p) i ) i \u0014 ϕ′ \u0012\u001c w ( ˜T(p) i ) s , x(p) i \u001d\u0013 + βϕ′ \u0012\u001c w ( ˜T(p) i ) −s , x(p) i \u001d\u0013\u0015\r\r\rξ(p) i \r\r\r 2 ≤ 2η ≤ log(ηT ∗), from \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 dd, (A4), and (11). For each t = ˜T(p) i + 1, . . . , T, if i ∈ Vs,k, then we have yifW(t) (Xi) = ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011 + X q∈[P]\\{p} \u0010 ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011\u0011 ≥ ρ(t) yi (i, p) + βρ(t) −yi(i, p) + γ(t) yi (s, k) + βγ(t) −yi(s, k) + X q∈[P]\\{p,p∗ i } \u0010 ρ(t) yi (i, q) + βρ(t) −yi(i, q) \u0011 − 2P · o \u0012 1 polylog(d) \u0013 ≥ 3 2 log(ηT ∗). The first inequality is due to Lemma B.4 and the second inequality holds because from our choice of t, ρ(t) yi (i, p) + βρ(t) −yi(i, p) ≥ 2 log(ηT ∗). Therefore, we have η n TX t= ˜T(p) i +1 g(t) i \u0012 ϕ′ \u0010D w(t) yi , x(p) i E\u0011 + βϕ′ \u0010D w(t) −yi, x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 ≤ η TX t= ˜T(p) i +1 exp (−yifW(t) (Xi)) ≤ (ηT ∗) exp \u0012 −3 2 log(ηT ∗) \u0013 ≤ 1√ηT ∗ ≤ log(ηT ∗), where the first inequality is due to \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 d, (A4) and the last inequality holds for any reasonably large T∗. Merging all inequalities together, we conclude ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) ≤ 4 log(ηT ∗). C.2.2 Learning Common Features In the initial stages of training, the model quickly learns common features while exhibiting minimal overfitting to Gaussian noise. First, we establish lower bounds on the number of iterations ensuring that noise coefficients ρ(t) s (i, p) remain small, up to the order of 1 P . Lemma C.2. Suppose the event Einit occurs. There exists ˜T > n 6ηPσ 2 dd such that ρ(t) s (i, p) ≤ 1 4P for all 0 ≤ t <˜T , s∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i }. 32Proof of Lemma C.2. Let ˜T be the smallest iteration such that ρ( ˜T) s (i, p) ≥ 1 4P for some s ∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i }. We assume the existence of ˜T, as its absence would directly lead to our conclusion. Then, for any 0 ≤ t <˜T, we have ρ(t+1) s (i, p) = ρ(t) s (i, p) + η ng(t) i ϕ′ \u0010D w(t) s , x(p) i E\u0011\r\r\rξ(p) i \r\r\r 2 ≤ ρ(t) s (i, p) + 3ησ2 dd 2n , where the inequality is due to g(t) i < 1, ϕ′ ≤ 1, and \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 dd. Hence, we have 1 4P ≤ ρ( ˜T) s (i, p) = ˜T−1X t=0 \u0010 ρ(t+1) s (i, p) − ρ(t) s (i, p) \u0011 < 3ησ2 dd 2n ˜T , and we conclude ˜T > n 6ηPσ 2 dd which is the desired result. Next, we will show that the model learns common features in at least constant order within ˜T iterates. Lemma C.3. Suppose the event Einit occurs and ρk = ω \u0010 σ2 dd βn \u0011 for some k ∈ [K]. Then, for each s ∈ {±1}, there exists Ts,k ≤ 9n ηβ|Vs,k| such that γ(t) s (s, k) + βγ(t) −s(s, k) ≥ 1 for any t > Ts,k. Proof of Lemma C.3. Suppose γ(t) s (s, k) + βγ(t) −s(s, k) < 1 for all 0 ≤ t ≤ n 6ηPσ 2 dd . For each i ∈ Vs,k, we have yifW(t) (Xi) = ϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010D w(t) −s, vs,k E\u0011 + X p∈[P]\\{p∗ i } \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 ≤ γ(t) s (s, k) + βγ(t) −s(s, k) + X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 + 2P · o \u0012 1 polylog(d) \u0013 ≤ 1 + 2P · 1 4P + 2P · o \u0012 1 polylog(d) \u0013 ≤ 2. The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma C.2, and the last inequality is due to (A1). Thus, g(t) i = 1 1+exp(yifW(t) (Xi)) > 1 9 and we have γ(t+1) s (s, k) + βγ(t+1) −s (s, k) = γ(t) s (s, k) + βγ(t) −s(s, k) + η n X i∈Vs,k g(t) i \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + ηβ|Vs,k| 9n . Notice that |Vs,k| = ρkn. From the condition in the lemma statement, we have 9n ηβ|Vs,k| = o \u0010 n 6ηPσ 2 dd \u0011 . If we choose t0 ∈ h 9n ηβ|Vs,k|, n 6ηPσ 2 dd i , then 1 > γ(t0) s (s, k) + βγ(t0) −s (s, k) ≥ ηβ|Vs,k| 9n t0 ≥ 1, and this is contradictory; therefore, it cannot hold that γ(t) s (s, k) + βγ(t) −s(s, k) < 1 for all 0 ≤ t ≤ n 6ηPσ 2 dd . Hence, there exists 0 ≤ Ts,k < n 6ηPσ 2 dd such that γ(Ts,k+1) s (s, k) + βγ(Ts,k+1) −s (s, k) ≥ 1 and choose the smallest one. Then we obtain 1 > γ(Ts,k) s (s, k) + βγ(Ts,k) −s (s, k) ≥ ηβ|Vs,k| 9n Ts,k. Therefore, Ts,k < 9n ηβ|Vs,k| and this is what we desired. 33What We Have So Far. For any common feature vs,k with s ∈ {±1} and k ∈ KC, it satisfies ρk = w \u0010 σ2 dd βn \u0011 due to (A4). By Lemma C.3, at any iterate t ∈ \u0002¯T1, T∗\u0003 with ¯T1 := maxs∈{±1},k∈KC Ts,k, the following properties hold if the event Einit occurs: • (Learn common features): For any s ∈ {±1} and k ∈ KC, γ(t) s (s, k) + βγ(t) −s(s, k) = Ω(1). • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 . C.2.3 Overfitting (extremely) Rare Data In the previous step, we have shown that common data can be well-classified by learning common features. In this step, we will show that the model correctly classifies (extremely) rare data by overfitting dominant noise instead of learning its features. We first introduce lower bounds on the number of iterates such that feature coefficients γ(t) s (s′, k) remain small, up to the order of α2β−1. This lemma holds for any kind of features, but we will focus on (extremely) rare features. This does not contradict the results from Section C.2.2 for common features since the upper bound on the number of iterations in Lemma C.3 is larger than the lower bound on the number of iterations in this lemma. Lemma C.4. Suppose the event Einit occurs. For each s ∈ {±1} and k ∈ [K], there exists ˜Ts,k > nα2 ηβ|Vs,k| such that γ(t) s′ (s, k) ≤ α2β−1 for any 0 ≤ t <˜Ts,k and s′ ∈ {±1}. Proof of Lemma C.4. Let ˜Ts,k be the smallest iterate such that γ( ˜Ts,k) s′ (s, k) > α2β−1 for some s′ ∈ {±1}. We assume the existence of ˜Ts,k, as its absence would directly lead to our conclusion. For any 0 ≤ t <˜Ts,k, γ(t+1) s′ (s, k) = γ(t) s′ (s, k) + η n X i∈Vs,k g(t) i ϕ′ \u0010D w(t) s′ , vs,k E\u0011 ≤ γ(t) s′ (s, k) + η|Vs,k| n , and we have α2β−1 < γ( ˜Ts,k) s′ (s, k) = ˜Ts,k−1X t=0 \u0010 γ(t+1) s′ (s, k) − γ(t) s′ (s, k) \u0011 ≤ η|Vs,k| n ˜Ts,k. We conclude ˜Ts,k > nα2 ηβ|Vs,k| which is the desired result. Next, we will show that the model overfits (extremely) rare data by memorizing dominant noise patches in at least constant order within ˜Ts,k iterates. Lemma C.5. Suppose the event Einit occurs and ρk = o \u0010 α2σ2 dd n \u0011 . Then, for each i ∈ Vs,k, there exists Ti ∈ h ¯T1, 18n ηβσ2 dd i such that X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 ≥ 1, for any t > Ti. Proof of Lemma C.5. Suppose P p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 < 1 for 0 ≤ t ≤ nα2 ηβ|Vs,k|. From Lemma B.4 and Lemma C.4, we have yifW(t) (Xi) 34= ϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010D w(t) −s, vs,k E\u0011 + X p∈[P]\\{p∗ i } \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 ≤ γ(t) s (s, k) + βγ(t) −s(s, k) + X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 + 2P · o \u0012 1 polylog(d) \u0013 ≤ (1 + β)α2β−1 + 1 + 2P · o \u0012 1 polylog(d) \u0013 ≤ 2, where the last inequality is due to (10). Thus, we have g(t) i = 1 1+exp(yifW(t) (Xi)) ≥ 1 9 . Also, ρ(t+1) s (i, ˜pi) + βρ(t+1) −s (i, ˜pi) = ρ(t) s (i, ˜pi) + βρ(t) −s(i, ˜pi) + η ng(t) i \u0012 ϕ′ \u0010D w(t) s , x(˜pi) i E\u0011 + βϕ′ \u0010D w(t) −s, x(˜pi) i E\u0011\u0013\r\r\rξ(˜pi) i \r\r\r 2 ≥ ρ(t) s (i, ˜pi) + βρ(t) −s(i, ˜pi) + ηβσ2 dd 18n , where the last inequality is due to \r\r\rξ(˜pi) i \r\r\r 2 ≥ 1 2 σ2 dd and ϕ′ ≥ β. Notice that |Vs,k| = ρkn. From the given condition in the lemma statement, we have 18n ηβσ2 dd = o \u0010 nα2 ηβ|Vs,k| \u0011 . If we choose t0 ∈ h 18n ηβσ2 dd , nα2 ηβ|Vs,k| i , then we have 1 > X p∈[P]\\{p∗ i } \u0010 ρ(t0) s (i, p) + βρ(t0) −s (i, p) \u0011 ≥ ρ(t0) s (i, ˜pi) + βρ(t0) −s (i, ˜pi) ≥ ηβσ2 dd 18n t0 ≥ 1. This is a contradiction; therefore it cannot hold that P p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 < 1 for all 0 ≤ t ≤ nα2 ηβ|Vs,k|. Hence, we can choose the smallest 0 ≤ Ti < nα2 ηβ|Vs,k| such that P p∈[P]\\{p∗ i } \u0010 ρ(Ti+1) s (i, p) + βρ(Ti+1) −s (i, p) \u0011 ≥ 1. For any 0 ≤ t < Ti, 1 ≥ X p∈[P]\\{p∗ i } \u0010 ρ(Ti) s (i, p) + βρ(Ti) −s (i, p) \u0011 ≥ ρ(Ti) s (i, ˜pi) + βρ(Ti) −s (i, ˜pi) ≥ ηβσ2 dd 18n Ti, and we conclude that Ti ≤ 18n ηβσ2 dd . Lastly, we move on to proveTi > ¯T1. Combining Lemma C.2 and Lemma C.3 leads to X p∈[P]\\{p∗ i } \u0010 ρ( ¯T1) s (i, p) + βρ( ¯T1) −s (i, p) \u0011 ≤ 1 2. Thus, we have Ti > ¯T1 and this is what we desired. What We Have So Far. For any k ∈ KR ∪ KE, it satisfies ρk = o \u0010 α2σ2 dd n \u0011 due to (A5). By Lemma C.5 at iterate t ∈ [TERM, T∗] with TERM := max s∈{±1} k∈KR∪KE max i∈Vs,k Ti ∈ \u0002¯T1, T∗\u0003 the following properties hold if the event Einit occurs: • (Learn common features): For s ∈ {±1} and k ∈ KC, γ(t) s (s, k) + βγ(t) −s(s, k) = Ω(1), 35• (Overfit (extremely) rare data): For any s ∈ {±1}, k ∈ KR ∪ KE, and i ∈ Vs,k, X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 = Ω(1), • (Do not learn (extremely) rare features at TERM): For any s, s′ ∈ {±1} and k ∈ KR ∪ KE, γ(TERM) s′ (s, k) ≤ α2β−1. • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 . C.2.4 ERM cannot Learn (extremely) Rare Features Within Polynomial Times In this step, we will show that ERM cannot learn (extremely) rare features within the maximum admissible iterations T∗ = poly(d) η . From now on, we fix any s∗ ∈ {±1} and k∗ ∈ KR ∪ KE. Recall that we defined the set W and the function Q(s∗,k∗) : W →Rd×2 in Lemma B.5. Let us omit superscripts for simplicity. For each iteration t, Q(W(t)) represents the cumulative updates contributed by data points with feature vector vs∗,k∗ until t-th iteration. We will sequentially introduce several technical lemmas and by combining these lemmas, quantify update by data with feature vector vs∗,k∗ after TERM and derive our conclusion. Let us define W∗ = {w∗ 1, w∗ −1}, where w∗ s = w(TERM) s + M X i∈Vs∗,k∗ ξ(˜pi) i\r\r\rξ(˜pi) i \r\r\r 2 , for each s ∈ {±1} with M = 4β−1 log \u0010 2ηβ2T∗ α2 \u0011 . Note that (12), β <1, and T∗ = poly(d) η together imply M = eO \u0000 β−1\u0001 . Note that W(t), W∗ ∈ Wfor any t ≥ 0. Lemma C.6. Suppose the event Einit occurs. Then, \r\r\rQ \u0010 W(TERM) \u0011 − Q(W∗) \r\r\r 2 ≤ 12M2|Vs∗,k∗|σ−2 d d−1, where ∥·∥ denotes the Frobenius norm. Proof of Lemma C.6. For each s ∈ {±1}, ss∗ \u0010 Qs (w∗ s) − Qs \u0010 w(TERM) s \u0011\u0011 = Qs  ss∗M X i∈Vs∗,k∗ ξ(˜pi) i\r\r\rξ(˜pi) i \r\r\r   = M X i∈Vs∗,k∗ ξ(˜pi) i\r\r\rξ(˜pi) i \r\r\r 2 + αM   X i∈Fs∩Vs∗,k∗ vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s∩Vs∗,k∗ v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  , and we have \r\r\rQ \u0010 W(TERM) \u0011 − Q(W∗) \r\r\r 2 = \r\r\rQ1(w∗ 1) − Q1 \u0010 w(TERM) 1 \u0011\r\r\r 2 + \r\r\rQ−1(w∗ −1) − Q−1 \u0010 w(TERM) −1 \u0011\r\r\r 2 ≤ 2M2   X i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 + X i,j∈Vs∗,k∗,i̸=j \f\f\f D ξ(˜pi) i , ξ(˜pj) j E\f\f\f \r\r\rξ(˜pi) i \r\r\r 2 \r\r\rξ(˜pj) j \r\r\r 2   36+ 2M2  α2   X i∈Fs∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 + α2   X i∈F−s∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 . From the event Einit defined in Lemma B.2 and (A2), we have X i,j∈Vs∗,k∗,i̸=j \f\f\f D ξ(˜pi) i , ξ(˜pj) j E\f\f\f \r\r\rξ(˜pi) i \r\r\r 2 \r\r\rξ(˜pj) j \r\r\r 2 ≤ X i∈Vs∗,k∗ X j∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 eO \u0010 d−1 2 \u0011 ≤ X i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 eO \u0010 nd−1 2 \u0011 ≤ X i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 In addition, we have α2   X i∈Fs∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 + α2   X i∈F−s∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 ≤   X i∈Fs∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 +   X i∈F−s∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2   2 ≤ X i∈Fs∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 + X i∈F−s∩Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 = X i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 , where the first inequality is due to α <1 and the second inequality is due to P i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 ≤ 2|Vs∗,k∗|σ−2 d d−1 < 1 from (A5). Hence, from Einit, we obtain \r\r\rQ \u0010 W(TERM) \u0011 − Q(W∗) \r\r\r 2 ≤ 6M2 X i∈Vs∗,k∗ \r\r\rξ(˜pi) i \r\r\r −2 ≤ 12M2|Vs∗,k∗|σ−2 d d−1. Lemma C.7. Suppose the Einit occurs. For any t ≥ TERM and i ∈ Vs∗,k∗, it holds that ⟨yi∇W fW(t) (Xi), Q(W∗)⟩ ≥Mβ 2 . Proof of Lemma C.7. We have ⟨yi∇W fW(t) (Xi), Q(W∗)⟩ = X p∈[P] \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗(w∗ s∗), x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗(w∗ −s∗), x(p) i E\u0013 . For any s ∈ {±1} and p ∈ [P] \\ {p∗ i , ˜pi}, ss∗ D Qs(w∗ s), ξ(p) i E = ρ(TERM) s (i, p) + X j∈Vs∗,k∗,q∈[P]\\{p∗ j } (j,q)̸=(i,p) ρ(TERM) s (j, q) D ξ(p) i , ξ(q) j E \r\r\rξ(q) j \r\r\r 2 + X j∈Vs∗,k∗ M D ξ(p) i , ξ(˜pj) j E \r\r\rξ(˜pj) j \r\r\r 2 37≥ −eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 − eO \u0010 nMσbσ−1 d d−1 2 \u0011 = −o \u0012 1 polylog(d) \u0013 , (16) where the last equality is due to (9) and M = eO \u0000 β−1\u0001 . Also, for any s ∈ {±1}, ss∗ ⟨Qs(w∗ s), vs∗,k∗⟩ = γ(TERM) s (s∗, k∗) ≥ 0. In addition, ss∗ D Qs(w∗ s), x(˜pi) i E = ss∗ D Qs(w∗ s), ξ(˜pi) i E + ss∗ D Qs(w∗ s), x(˜pi) i − ξ(˜pi) i E ≥ ss∗ D Qs(w∗ s), ξ(˜pi) i E − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 = M + ρ(TERM) s (i, ˜pi) + X j∈Vs∗,k∗,q∈[P]\\{p∗ i } (j,q)̸=(i,˜pi) ρ(TERM) s (j, q) D ξ(˜pi) i , ξ(q) j E \r\r\rξ(q) j \r\r\r 2 + X j∈Vs∗,k∗\\{i} M D ξ(˜pi) i , ξ(˜pj) j E \r\r\rξ(˜pj) j \r\r\r 2 − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 ≥ M − eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 = M − o \u0012 1 polylog(d) \u0013 ≥ M 2 , (17) where the first inequality is due to the definition of Q and the second-to-last line is due to (9) and (A7). Hence, applying (16) and (17) for s = s∗, −s∗ and combining with ϕ′ ≥ β, we have ⟨yi∇W fW(t) (Xi), Q(W∗)⟩ ≥Mβ − o \u0012 1 polylog(d) \u0013 ≥ Mβ 2 . By combining Lemma C.6 and Lemma C.7, we can obtain the following result. Lemma C.8. Suppose the event Einit occurs. η n T∗ X t=TERM X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) ≤ \r\r\rQ \u0010 W(TERM) \u0011 − Q(W∗) \r\r\r 2 + 2ηT ∗e−Mβ 4 , where ∥·∥ denotes the Frobenius norm. Proof of Lemma C.8. Note that for any TERM ≤ t < T∗, Q \u0010 W(t+1) \u0011 = Q \u0010 W(t) \u0011 − η n∇W X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) , and thus \r\r\rQ \u0010 W(t) \u0011 − Q (W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q (W∗) \r\r\r 2 = 2η n * ∇W X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) , Q \u0010 W(t) \u0011 − Q (W∗) + − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) \r\r\r\r\r\r 2 38= 2η n * ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)), Q \u0010 W(t) \u0011+ − 2η n X i∈Vs∗,k∗ ℓ′(yifW(t) (Xi)) ⟨∇WyifW(t) (Xi), Q(W∗)⟩ −η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) \r\r\r\r\r\r 2 ≥ 2η n * ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)), Q \u0010 W(t) \u0011+ − Mβη n X i∈Vs∗,k∗ ℓ′(yifW(t) (Xi)) − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) \r\r\r\r\r\r 2 , where the last inequality is due to Lemma C.7. By the chain rule, we have * ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)), Q \u0010 W(t) \u0011+ = X i∈Vs∗,k∗ \" ℓ′(yifW(t) (Xi)) × X p∈[P] \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗ \u0010 w(t) s∗ \u0011 , x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗ \u0010 w(t) −s∗ \u0011 , x(p) i E\u0013# . For each s ∈ {±1}, i ∈ Vs∗,k∗, and p ∈ [P], \f\f\f D w(t) s , x(p) i E − D Qs \u0010 w(t) s \u0011 , x(p) i E\f\f\f = \f\f\f D w(t) s − Qs \u0010 w(t) s \u0011 , x(p) i E\f\f\f ≤ X j∈[n]\\Vs∗,k∗,q∈[P]\\{p∗ i } \f\f\f\f\f\f\f * ρ(t) s (j, q) ξ(q) j \r\r\rξ(q) j \r\r\r 2 , x(p) i +\f\f\f\f\f\f\f + α X j∈F1\\Vs∗,k∗ ρ(t) s (j, ˜pj) \r\r\rξ(˜pj) j \r\r\r −2 \f\f\f D v1,1, x(p) i E\f\f\f + α X j∈F−1\\Vs∗,k∗ ρ(t) s (j, ˜pj) \r\r\rξ(˜pj) j \r\r\r −2 \f\f\f D v−1,1, x(p) i E\f\f\f ≤ eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 + eO \u0000 α2β−1nσ−2 d d−1\u0001 = o \u0012 1 polylog(d) \u0013 , where the last inequality is due to Lemma C.1 and the event Einit. By Lemma F.1, X p∈[P] \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗ \u0010 w(t) s∗ \u0011 , x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗ \u0010 w(t) s∗ \u0011 , x(p) i E\u0013 ≤ X p∈[P] \u0012 ϕ \u0010D w(t) s∗ , x(p) i E\u0011 − ϕ \u0010D w(t) −s∗, x(p) i E\u0011\u0013 + rP + o \u0012 1 polylog(d) \u0013 = yifW(t) (Xi) + o \u0012 1 polylog(d) \u0013 where the last equality is due to r = o \u0010 1 polylog(d) \u0011 . Therefore, we have \r\r\rQ \u0010 W(t) \u0011 − Q (W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q(W∗) \r\r\r 2 39≥ 2η n X i∈Vs∗,k∗ ℓ′ (yifW(t) (Xi)) \u0012 yifW(t) (Xi) + o \u0012 1 polylog(d) \u0013 − Mβ 2 \u0013 − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)) \r\r\r\r\r\r 2 ≥ 2η n X i∈Vs∗,k∗ ℓ′(yifW(t) (Xi)) \u0012 yifW(t) (Xi) − Mβ 4 \u0013 − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)) \r\r\r\r\r\r 2 . From the convexity of ℓ(·), X i∈Vs∗,k∗ ℓ′(yifW(t) (Xi)) \u0012 yifW(t) (Xi) − Mβ 4 \u0013 ≥ X i∈Vs∗,k∗ \u0012 ℓ(yifW(t) (Xi)) − ℓ \u0012Mβ 4 \u0013\u0013 ≥ X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)) − ne−Mβ 4 . In addition, by Lemma F.2, η2 n2 \r\r\r\r\r\r ∇ X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) \r\r\r\r\r\r 2 ≤ 8η2P2σ2 dd|Vs∗,k∗| n2 X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)) ≤ η n X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)), where the last inequality is due to (A8), and we have \r\r\rQ \u0010 W(t) \u0011 − Q(W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q(W∗) \r\r\r 2 ≥ η n X i∈Vs∗,k∗ ℓ(yifW(t) (Xi)) − 2ηe−Mβ 4 . From telescoping summation, we have η n T∗ X t=TERM X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) ≤ \r\r\rQ \u0010 W(TERM) \u0011 − Q (W∗) \r\r\r 2 + 2ηT ∗e−Mβ 4 . Finally, we can prove that the model cannot learn (extremely) rare features within T∗ iterations. Lemma C.9. Suppose the event Einit occurs. For any T ∈ [TERM, T∗], we have γ(T) s (s∗, k∗) = eO \u0000 α2β−2\u0001 for each s ∈ {±1}. Proof of Lemma C.9. For any T ∈ [TERM, T∗], we have γ(T) s (s, k) = γ(TERM) s (s∗, k∗) + η n T−1X t=TERM X i∈Vs∗,k∗ g(t) i ϕ′ \u0010D w(t) s , vs∗,k∗ E\u0011 ≤ γ(TERM) s (s∗, k∗) + η n T−1X t=TERM X i∈Vs∗,k∗ g(t) i 40≤ γ(TERM) s (s∗, k∗) + η n T−1X t=TERM X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) , where the first inequality is due toϕ′ ≤ 1 and the second inequality is due to−ℓ′ ≤ ℓ. From the result of Section C.2.3 we know γ(TERM) s (s∗, k∗) ≤ α2β−1. Additionally, by Lemma C.8 and Lemma C.6, we have η n (T−1)X t=TERM X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) ≤ η n (T∗)X t=TERM X i∈Vs∗,k∗ ℓ (yifW(t) (Xi)) ≤ \r\r\rQ \u0010 W(TERM) \u0011 − Q(W∗) \r\r\r 2 + 2ηT ∗e−Mβ 4 ≤ 12M2|Vs∗,k∗|σ−2 d d−1 + 2ηT ∗e−Mβ 4 = eO \u0000 α2β−2\u0001 . The last line is due to (A5) and our choice M = 4β−1 log \u0010 2ηβ2T∗ α2 \u0011 . Thus, we have our conclusion. What We Have So Far. Suppose the event Einit occurs. For any t ∈ [TERM, T∗], we have • (Learn common features): For each s ∈ {±1} and k ∈ KC, γ(t) s (s, k) + βγ(t) −s(s, k) = Ω(1). • (Overfit (extremely) rare data): For each s ∈ {±1}, k∈ KR ∪ KE and i ∈ Vs,k, X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 = Ω(1). • (Cannot learn (extremely) rare features): For each s ∈ {±1} and k ∈ KR ∪ KE, γ(t) s (s, k), γ(t) −s(s, k) = O \u0000 α2β−2\u0001 . • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 , C.2.5 Train and Test Accuracy In this step, we will prove that the model trained by ERM has perfect training accuracy but has near-random guesses on (extremely) rare data. For any i ∈ Vs,k with s ∈ {±1} and k ∈ KC, by Lemma B.4, we have yifW(t) (Xi) = X p∈[P] \u0010 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0011 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − 2P · o \u0012 1 polylog(d) \u0013 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) − o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 > 0, for any t ∈ [TERM, T∗]. In addition, for any i ∈ Vs,k with s ∈ {±1} and k ∈ KR ∪ KE, we have yifW(t) (Xi) 41= X p∈[P] \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 = γ(t) s (s, k) + βγ(t) −s(s, k) + X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − 2P · o \u0012 1 polylog(d) \u0013 ≥ X p∈[P]\\{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 > 0, for any t ∈ [TERM, T∗]. We can conclude that ERM with t ∈ [TERM, T∗] iterates achieve perfect training accuracy. Next, let us move on to the test accuracy part. Let (X, y) ∼ Dbe a test data with X =\u0000 x(1), . . . ,x(P)\u0001 ∈ Rd×P having feature patch index p∗, dominant noise patch index ˜p, and feature vector vy,k. We have x(p) ∼ N(0, σ2 bΛ) for each p ∈ [P] \\ {p∗, ˜p} and x(˜p) − αvs,1 ∼ N(0, σ2 dΛ) for some s ∈ {±1}. Therefore, for all t ∈ [TERM, T∗] and p ∈ [P] \\ {p∗, ˜p}, \f\f\fϕ \u0010D w(t) 1 , x(p) E\u0011 − ϕ \u0010D w(t) −1, x(p) E\u0011\f\f\f ≤ \f\f\f D w(t) 1 − w(t) −1, x(p) E\f\f\f ≤ \f\f\f D w(0) 1 − w(0) −1, x(p) E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } \f\f\fρ(t) 1 (i, q) − ρ(t) −1(i, q) \f\f\f \f\f\f D ξ(q) i , x(p) E\f\f\f \r\r\rξ(q) i \r\r\r 2 ≤ eO \u0010 σ0σbd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 α polylog(d) \u0013 , (18) with probability at least 1 − o \u0010 1 poly(d) \u0011 due to Lemma B.2, (A8), (8), and (9). In addition, for any s′ ∈ {±1}, we have \f\f\f D w(t) s′ , x(˜p) − αvs,1 E\f\f\f ≤ \f\f\f D w(0) s′ , x(˜p) − αvs,1 E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } ρ(t) s′ (i, q) \f\f\f D ξ(q) i , x(˜p) − αvs,1 E\f\f\f \r\r\rξ(q) i \r\r\r 2 = eO \u0010 σ0σdd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 α polylog(d) \u0013 , (19) with probability at least 1 − o \u0010 1 poly(d) \u0011 due to Lemma B.2, (A8), (8), and (9). Case 1: k ∈ KC By Lemma B.2, (A8), and (10), \f\f\fϕ \u0010D w(t) 1 , x(˜p) E\u0011 − ϕ \u0010D w(t) −1, w(˜p) E\u0011\f\f\f ≤ \f\f\f D w(t) 1 − w(t) −1, x(˜p) E\f\f\f ≤ α \f\f\f D w(t) 1 − w(t) −1, vs,1 E\f\f\f + \f\f\f D w(t) 1 − w(t) −1, x(p) − αvs,1 E\f\f\f 42≤ α \u0010 γ(t) 1 (s, 1) + γ(t) −1(s, 1) \u0011 + α \f\f\f D w(0) 1 , vs,1 E\f\f\f + α \f\f\f D w(0) −1, vs,1 E\f\f\f + o \u0012 1 polylog(d) \u0013 ≤ eO \u0000 αβ−1\u0001 + eO (ασ0) + o \u0012 1 polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 , (20) with probability at least 1 − o \u0010 1 poly(d) \u0011 . Suppose (18) and (20) holds. By Lemma B.4, we have yfW(t) (X) = \u0012 ϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011\u0013 + X p∈[P]\\{p∗} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013 = γ(t) y (y, k) + βγ(t) −y(y, k) − o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 > 0. Therefore, we have P(X,y)∼D h yfW(t) (X) > 0 | x(p∗) = vy,k, k∈ KC i ≥ 1 − o \u0012 1 poly(d) \u0013 . (21) Case 2: k ∈ KR ∪ KE By triangular inequality and ϕ′ ≤ 1, we have ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 = ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 + \u0012 ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) s , αvs,1 E\u0011\u0013 − \u0012 ϕ \u0010D w(t) −s, x(˜p) E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011\u0013 ≥ ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 − \f\f\f D w(t) s , x(˜p) − αvs,1 E\f\f\f − \f\f\f D w(t) −s, x(˜p) − αvs,1 E\f\f\f. In addition, ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 = \u0012 ϕ \u0010 αγ(t) s (s, 1) \u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0013 + \u0012 ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010 αγ(t) s (s, 1) \u0011\u0013 − \u0012 ϕ \u0010D w(t) −s, αvs,1 E\u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0013 ≥ \u0012 ϕ \u0010 αγ(t) s (s, 1) \u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0013 − α \f\f\f D w(t) s , vs,1 E − γ(t) s (s, 1) \f\f\f − α \f\f\f D w(t) −s, vs,1 E + γ(t) −s(s, 1) \f\f\f = α \u0010 γ(t) s (s, 1) + βγ(t) −s(s, 1) \u0011 − α · o \u0012 1 polylog(d) \u0013 43= Ω(α), where the second equality is due to Lemma B.4 and (A8). If (19) holds, we have ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 = Ω(α) − o \u0012 α polylog(d) \u0013 = Ω(α). (22) Note that yfW(t) (X) = ϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011 + ϕ \u0010D w(t) y , x(˜p) E\u0011 − ϕ \u0010D w(t) −y, x(˜p) E\u0011 + X p∈[P]\\{p∗,˜p} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013 , and \f\f\fϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011\f\f\f + \f\f\f\f\f\f X p∈[P]\\{p∗,˜p} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013\f\f\f\f\f\f ≤ \f\f\f D w(t) y − w(t) −y, vy,k E\f\f\f + o \u0012 α polylog(d) \u0013 ≤ γ(t) 1 (y, k) + γ(t) −1(y, k) + \f\f\f D w(0) y − w(0) −y, vy,k E\f\f\f + o \u0012 α polylog(d) \u0013 ≤ O(α2β−2) + eO(σ0) + o \u0012 α polylog(d) \u0013 = o \u0012 α polylog(d) \u0013 < ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 , where the first inequality is due to (18), second-to-last line is due to (A8), (8) and (10), and the last inequality is due to (22). Therefore, we have yfW(t) (X) > 0 if y = s. Otherwise, yfW(t) (X) < 0. Therefore, we have P(X,y)∼D h yfW(t) (X) > 0 | x(p∗) = vy,k, k∈ KR ∪ KE i = 1 2 ± o \u0012 1 poly(d) \u0013 . (23) Hence, combining (21) and (23) implies P(X,y)∼D [yfW(t) (X) > 0] = X k∈KC ρk + 1 2   1 − X k∈KC ρk ! ± o \u0012 1 poly(d) \u0013 = 1 − 1 2 X k∈KR∪KE ρk ± o \u0012 1 poly(d) \u0013 . □ 44D Proof for Cutout In this section, we use g(t) i,C := 1 1+exp(yifW(t) (Xi,C)) for each data i, C ⊂[P] with |C| = C and iteration t, for simplicity. D.1 Proof of Lemma B.3 for Cutout For s ∈ {±1} and iterate t, w(t+1) s − w(t) s = −η∇wsLCutout \u0010 W(t) \u0011 = η n X i∈[n] syiEC∼DC  g(t) i,C X p/∈C ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i   = η n  X i∈Vs EC∼DC  g(t) i,C X p/∈C ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i   − X i∈V−s EC∼DC  g(t) i,C X p/∈C ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i    , and we have X i∈Vs EC∼DC  g(t) i,C X p/∈C ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i   = X k∈[K] X i∈Vs,k EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , vs,k E\u0011 · 1 p∗ i /∈C i vs,k + X i∈Vs X p∈[P]\\{p∗ i ,˜pi} EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , ξ(p) i E\u0011 · 1 p/∈C i ξ(p) i + X i∈Vs∩Fs EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , αvs,1 + ξ(˜pi) i E\u0011 · 1 ˜pi /∈C i\u0010 αvs,1 + ξ(˜pi) i \u0011 + X i∈Vs∩F−s EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , αv−s,1 + ξ(˜pi) i E\u0011 · 1 ˜pi /∈C i\u0010 αv−s,1 + ξ(˜pi) i \u0011 , and X i∈V−s EC∼DC  g(t) i,C X p/∈C ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i   = X k∈[K] X i∈V−s,k EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , v−s,k E\u0011 · 1 p∗ i /∈C i v−s,k + X i∈V−s X p∈[P]\\{p∗ i ,˜pi} EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , ξ(p) i E\u0011 · 1 p/∈C i ξ(p) i + X i∈V−s∩Fs EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , αvs,1 + ξ(˜pi) i E\u0011 · 1 ˜pi /∈C i\u0010 αvs,1 + ξ(˜pi) i \u0011 + X i∈V−s∩F−s EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , αv−s,1 + ξ(˜pi) i E\u0011 · 1 ˜pi /∈C i\u0010 αv−s,1 + ξ(˜pi) i \u0011 . Hence, if we define γ(t) s (s′, k)’s and ρ(t) s (i, p)’s recursively by using the rule γ(t+1) s (s′, k) = γ(t) s (s′, k) + η n X i∈Vs′,k EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , vs′,k E\u0011 · 1 p∗ i /∈C i , (24) 45ρ(t+1) s (i, p) = ρ(t) s (i, p) + η nEC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , x(p) i E\u0011 · 1 p/∈C i\r\r\rξ(p) i \r\r\r 2 , (25) starting from γ(0) s (s′, k) = ρ(0) s (i, p) = 0 for each s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P]\\{p∗ i }, then we have w(t) s = w(0) s + X k∈[K] γ(t) s (s, k)vs,k − X k∈[K] γ(t) s (−s, k)v−s,k + X i∈Vs,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s,p∈[P]\\{p∗ i } ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syiρ(t) s (i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρ(t) s (i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  , for each s ∈ {±1}. Furthermore, γ(t) s (s′, k)’s and ρ(t) s (i, p)’s are monotone increasing. □ D.2 Proof of Theorem 3.2 To show Theorem 3.2, we present a structured proof comprising the following five steps: 1. Establish upper bounds on γ(t) s (s′, k)’s and ρ(t) s (i, p)’s to apply Lemma B.4 (Section D.2.1). 2. Demonstrate that the model quickly learns common and rare features (Section D.2.2). 3. Show that the model overfits augmented data if it does not contain common or rare features (Section D.2.3). 4. Confirm the persistence of this tendency until T∗ iterates (Section D.2.4). 5. Characterize train accuracy and test accuracy (Section D.2.5). D.2.1 Bounds on the Coefficients in Feature Noise Decomposition The following lemma provides upper bounds on Lemma B.3 during T∗ iterations. Lemma D.1. Suppose the event Einit occurs. For any 0 ≤ t ≤ T∗, we have 0 ≤ γ(t) s (s, k) + βγ(t) −s(s, k) ≤ 4 log(ηT ∗), 0 ≤ ρ(t) yi (i, p) + βρ(t) −yi(i, p) ≤ 4 log (ηT ∗) , for all s ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P]\\{p∗ i }. Consequently, γ(t) s (s′, k), ρ(t) s (i, p) = eO(β−1) for all s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }. Proof of Lemma D.1. The first argument implies the second argument since log(ηT ∗) = polylog(d) and γ(t) s (s′, k) ≤ β−1 \u0010 γ(t) s′ (s′, k) + βγ(t) s′ (s′, k) \u0011 , ρ (t) s (i, p) ≤ β−1 \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 , for all s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }. We will prove the first argument by using induction on t. The initial case t = 0 is trivial. Suppose the statement holds at t = T and consider the case t = T + 1. Let ˜Ts,k ≤ T denote the smallest iteration where γ( ˜Ts,k+1) s (s, k) + βγ( ˜Ts,k+1) −s (s, k) > 2 log(ηT ∗). We assume the existence of ˜Ts,k, as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (24) and (11): γ(T+1) s (s, k) + βγ(T+1) −s (s, k) = γ(T) s (s, k) + βγ(T) −s (s, k) + η n X i∈Vs,k EC∼DC h g(T) i,C · 1 p∗ i /∈C i\u0012 ϕ′ \u0010D w(T) s , vs,k E\u0011 + βϕ′ \u0010D w(T) −s , vs,k E\u0011\u0013 46≤ 2 log(ηT ∗) + 2η ≤ 4 log(ηT ∗) By (24), we have γ(T+1) s (s, k) + βγ(T+1) −s (s, k) = γ( ˜Ts,k) s (s, k) + βγ( ˜Ts,k) −s (s, k) + TX t= ˜Ts,k \u0010 γ(t+1) s (s, k) + βγ(t+1) −s (s, k) − γ(t) s (s, k) − βγ(t) −s(s, k) \u0011 ≤ 2 log(ηT ∗) + log(ηT ∗) + η n TX t= ˜Ts,k+1 X i∈Vs,k EC∼DC \u0014 g(t) i,C \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 · 1 p∗ i /∈C \u0015 . The inequality is due to γ( ˜Ts,k) s (s, k) + βγ( ˜Ts,k) −s (s, k) ≤ 2 log(ηT ∗) and η n X i∈Vs,k EC∼DC \u0014 g( ˜Ts,k) i,C \u0012 ϕ′ \u0010D w( ˜Ts,k) s , vs,k E\u0011 + βϕ′ \u0010D w( ˜Ts,k) −s , vs,k E\u0011\u0013 · 1 p∗ i /∈C \u0015 ≤ 2η ≤ log(ηT ∗), from our choice of ˜Ts,k and η. For each t = ˜Ts,k + 1, . . . T, i ∈ Vs,k, and C ⊂[P] such that |C| = C and p∗ i /∈ C, we have yifW(t) (Xi,C) = ϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010D w(t) −s, vs,k E\u0011 + X p/∈C∪{p∗ i } \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + X p/∈C∪{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − 2P · o \u0012 1 polylog(d) \u0013 ≥ 3 2 log(ηT ∗) The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of t, γ(t) s (s, k) + βγ(t) −s(s, k) ≥ 2 log(ηT ∗). Hence, we obtain η n TX t= ˜Ts,k X i∈Vs,k EC∼DC \u0014 g(t) i,C \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 · 1 p∗ i /∈C \u0015 ≤ 2η n TX t= ˜Ts,k X i∈Vs,k EC∼DC \u0002 exp (−yifW(t) (Xi,C)) · 1 p∗ i /∈C \u0003 ≤ 2|Vs,k| n (ηT ∗) exp \u0012 −3 2 log(ηT ∗) \u0013 ≤ 2√ηT ∗ ≤ log(ηT ∗), where the last inequality holds for any reasonably large T∗. Merging all inequalities together, we have γ(T+1) s (s, k) + βγ(T+1) −s (s, k) ≤ 4 log(ηT ∗). Next, we will follow similar arguments to show that ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) ≤ 4 log(ηT ∗) for each i ∈ [n] and p ∈ [P] \\ {p∗ i }. 47Let ˜T(p) i ≤ T be the smallest iteration such that ρ ( ˜T(p) i +1) yi (i, p) + βρ ( ˜T(p) i +1) −yi (i, p) > 2 log(ηT ∗). We assume the existence of ˜T(p) i , , as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (25) and (11): ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) = ρ(T) yi (i, p) + βρ(T) −yi(i, p) + η nEC∼DC h g(T) i,C · 1 p/∈C i\u0012 ϕ′ \u0010D w(t) s , x(p) i E\u0011 + βϕ′ \u0010D w(t) s , x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 ≤ 2 log(ηT ∗) + 2η ≤ 4 log(ηT ∗), where the first inequality is due to \r\r\rξ(p) i \r\r\r ≤ 3 2 σ2 dd and (A4), and the last inequality is due to (11). Now we suppose there exists such ˜Ti ≤ T. By (25), we have ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) = ρ ( ˜T(p) i ) yi (i, p) + βρ ( ˜T(p) i ) −yi (i, p) + TX t= ˜T(p) i \u0010 ρ(t+1) yi (i, p) + βρ(t+1) −yi (i, p) − ρ(t) yi (i, p) − βρ(t) −yi(i, p) \u0011 ≤ 2 log(ηT ∗) + log(ηT ∗) + η n TX t= ˜T(p) i +1 EC∼DC h g(t) i,C · 1 p/∈C i\u0012 ϕ′ \u0010D w(t) s , x(p) i E\u0011 + βϕ′ \u0010D w(t) −s, x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 The inequality is due to ρ(t) yi (i, p) + βρ(t) −yi(i, p) ≤ 2 log(ηT ∗) our choice of ˜T(p) i and η nEC∼DC \u0014 g ( ˜T(p) i ) i,C · 1 p/∈C \u0015\u0012 ϕ′ \u0012\u001c w ( ˜T(p) i ) s , x(p) i \u001d\u0013 + βϕ′ \u0012\u001c w ( ˜T(p) i ) −s , x(p) i \u001d\u0013\u0013\r\r\rξ(p) i \r\r\r 2 ≤ 2η ≤ log(ηT ∗), from \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 dd, (A4), and (11). For each t = ˜T(p) i + 1, . . . , T, and C ⊂[P] such that |C| = C and p /∈ C, we have yifW(t) (Xi,C) = ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011 + X q /∈C∪{p} \u0010 ϕ \u0010D w(t) yi , x(q) i E\u0011 − ϕ \u0010D w(t) −yi, x(q) i E\u0011\u0011 ≥ ρ(t) yi (i, p) + βρ(t) −yi(i, p) − 2P · o \u0012 1 polylog(d) \u0013 ≥ 3 2 log(ηT ∗). The first inequality is due to Lemma B.4 and the second inequality holds since from our choice of t, ρ(t) yi (i, p) + βρ(t) −yi(i, p) ≥ 2 log(ηT ∗). Therefore, we have η n TX t= ˜T(p) i +1 EC∼DC h g(t) i,C · 1 p/∈C i\u0012 ϕ′ \u0010D w(t) yi , x(p) i E\u0011 + βϕ′ \u0010D w(t) −yi, x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 ≤ η TX t= ˜T(p) i +1 E C∼DC \u0002 exp (−yifW(t) (Xi,C)) 1 p/∈C \u0003 ≤ (ηT ∗) exp \u0012 −3 2 log(ηT ∗) \u0013 48≤ 1√ηT ∗ ≤ log(ηT ∗), where the first inequality is due to \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 dd and (A4). Hence, we conclude ρ(T+1) yi (i, p) + βρ(T+1) −yi (i, p) ≤ 4 log(ηT ∗). D.2.2 Learning Common Features and Rare Features In the initial stages of training, the model quickly learns common features while exhibiting minimal overfitting to Gaussian noise. First, we establish lower bounds on the number of iterations, ensuring that background noise coeffi- cients ρ(t) s (i, p) for p ̸= p∗ i , ˜pi remain small, up to the order of 1 P . Lemma D.2. Suppose the event Einit occurs. There exists ˜T > n 6ηPσ 2 bd such that ρ(t) s (i, p) ≤ 1 4P for all 0 ≤ t <˜T , s∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i , ˜pi}. Proof of Lemma D.2. Let ˜T be the smallest iteration such that ρ( ˜T) s (i, p) ≥ 1 4P for some s ∈ {±1}, i∈ [n] and p ∈ [P] \\ {p∗ i }. We assume the existence of ˜T, as its absence would directly lead to our conclusion. Then, for any 0 ≤ t <˜T, we have ρ(t+1) s (i, p) = ρ(t) s (i, p) + η nEC∼DC h g(t) i,Cϕ′ \u0010D w(t) s , x(p) i E\u0011 · 1 p/∈C i\r\r\rξ(p) i \r\r\r 2 < ρ(t) s (i, p) + 3ησ2 bd 2n , where the inequality is due to g(t) i,C < 1, ϕ′ ≤ 1, and \r\r\rξ(p) i \r\r\r 2 ≤ 3 2 σ2 bd. Hence, we have 1 4P ≤ ρ( ˜T) s (i, p) = ˜T−1X t=0 \u0010 ρ(t+1) s (i, p) − ρ(t) s (i, p) \u0011 < 3ησ2 bd 2n ˜T , and we conclude ˜T > n 6ηPσ 2 bd which is the desired result. Next, we will show that the model learns common features in at least constant order within ˜T iterates. Lemma D.3. Suppose the event Einit occurs and ρk = ω \u0010 σ2 bd βn \u0011 for some k ∈ [K]. Then, for each s ∈ {±1}. there exists Ts,k ≤ 9nP ηβ|Vs,k| such that γ(t) s (s, k) + βγ(t) −s(s, k) ≥ 1 for any t > Ts,k. Proof of Lemma D.3. Suppose γ(t) s (s, k) + βγ(t) −s(s, k) < 1 for all 0 ≤ t ≤ n 6ηPσ 2 bd . For each i ∈ Vs,k and C ⊂[P] with |C| = C such that p∗ i /∈ Cand ˜pi ∈ C, we have yifW(t) (Xi,C) = ϕ \u0010D w(t) s , vs,k E\u0011 − ϕ \u0010D w(t) −s, vs,k E\u0011 + X p/∈C∪{p∗ i } \u0012 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0013 ≤ γ(t) s (s, k) + βγ(t) −s(s, k) + X p/∈C∪{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 + 2P · o \u0012 1 polylog(d) \u0013 ≤ 1 + 2P · 1 4P + 2P · o \u0012 1 polylog(d) \u0013 ≤ 2. The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma D.2, and the last inequality is due to (A1). Thus, g(t) i,C = 1 1+exp(yifW(t) (Xi,C)) > 1 9 and we have γ(t+1) s (s, k) + βγ(t+1) −s (s, k) 49= γ(t) s (s, k) + βγ(t) −s(s, k) + η n X i∈Vs,k EC∼DC \u0014 g(t) i,C \u0012 ϕ′ \u0010D w(t) s , vs,k E\u0011 + βϕ′ \u0010D w(t) −s, vs,k E\u0011\u0013 · 1 p∗ i /∈C \u0015 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + ηβ 9n X i∈Vs,k EC∼DC[1 p∗ i /∈C∧˜pi∈C] = γ(t) s (s, k) + βγ(t) −s(s, k) + ηβ|Vs,k|C(P − C) 9nP(P − 1) ≥ γ(t) s (s, k) + βγ(t) −s(s, k) + ηβ|Vs,k| 9nP . From the given condition in the lemma statement, we have 9nP ηβ|Vs,k| = o \u0010 n 6ηPσ 2 bd \u0011 . If we choose t0 ∈ h 9nP ηβ|Vs,k|, n 6ηPσ 2 bd i , then 1 > γ(t0) s (s, k) + βγ(t0) −s (s, k) ≥ ηβ|Vs,k| 9nP t0 ≥ 1, and this is contradictory; therefore, it cannot hold that γ(t) s (s, k) + βγ(t) −s(s, k) < 1 for all 0 ≤ t ≤ n 6ηPσ 2 bd . Hence, there exists 0 ≤ Ts,k < n 6ηPσ 2 bd such that γ(Ts,k+1) s (s, k) + βγ(Ts,k+1) −s (s, k) ≥ 1 and choose the smallest one. Then we obtain 1 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) ≥ ηβ|Vs,k| 9nP Ts,k. Therefore, Ts,k ≤ 9nP ηβ|Vs,k| and this is what we desired. What We Have So Far. For any common feature or rare feature vs,k with s ∈ {±1} and k ∈ KC ∪ KR, it satisfies ρk = ω \u0010 σ2 bd βn \u0011 due to (A5). By Lemma D.3, at any iterate t ∈ \u0002¯T1, T∗\u0003 with ¯T1 := maxs∈{±1},k∈C Ts,k, the following properties hold if the event Einit occurs: • (Learn common/rare features): For s ∈ {±1} and k ∈ KC ∪KR, γ(t) s (s, k) +βγ(t) −s(s, k) = Ω(1), • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 . D.2.3 Overfitting Augmented Data In the previous step, we have shown that data containing common or rare features can be well- classified by learning common and rare features. In this step, we will show that the model correctly classifies the remaining training data by overfitting background noise instead of learning its features. We first introduce lower bounds on the number of iterates such that feature coefficients γ(t) s (s′, k) remain small, up to the order of α2β−1. This lemma holds to any kind of features, but we will focus on extremely rare features. This does not contradict the results from Section D.2.2 for common features and rare features since the upper bound on the number of iterations in Lemma D.3 is larger than the lower bound on the number of iterations in this lemma. Lemma D.4. Suppose the event Einit occurs. For each s ∈ {±1} and k ∈ [K], there exists ˜Ts,k ≥ nα2 ηβ|Vs,k| such that γ(t) s′ (s, k) ≤ α2β−1 for any 0 ≤ t <˜Ts,k and s′ ∈ {±1}. Proo of Lemma D.4. Let ˜Ts,k be the smallest iterate such that γ(t) s′ (s, k) > α2β−1 for some s′ ∈ {±1}. We assume the existence of ˜Ts,k, as its absence would directly lead to our conclusion. For any 0 ≤ t <˜Ts,k, γ(t+1) s′ (s, k) = γ(t) s′ (s, k)+ η n X i∈Vs,k EC∼DC h g(t) i,Cϕ′ \u0010D w(t) s′ , vs,k E\u0011 · 1 p∗ i /∈C i ≤ γ(t) s′ (s, k)+ η|Vs,k| n , 50and we have α2β−1 ≤ γ( ˜Ts,k) s′ (s, k) = ˜Ts,k−1X t=0 \u0010 γ(t+1) s′ (s, k) − γ(t) s′ (s, k) \u0011 ≤ η|Vs,k| n ˜Ts,k. We conclude ˜Ts,k ≥ nα2 ηβ|Vs,k| which is the desired result. Next, we will show that the model overfits data augmented not containing common or rare features in at least constant order within ˜Ts,k iterates. Lemma D.5. Suppose the eventEinit occurs and ρk = o \u0010 α2σ2 bd n \u0011 . For eachi ∈ [n] and C ⊂[P] with |C| = C, if (1) i ∈ Vyi,k and p∗ i /∈ Cor (2) i ∈ [n] and p∗ i ∈ C, then there exists Ti,C ∈ \u0014 ¯T1, 18n( P C) ηβσ2 bd \u0015 such that X p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 ≥ 1, for any t > Ti,C. Proof of Lemma D.5. We can address both cases in the statement simultaneously. SupposeP p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 < 1 for all 0 ≤ t ≤ nα2 ηβ|Vyi,k|. From Lemma B.4 and Lemma D.4, we have yifW(t) (Xi,C) = X p/∈C \u0012 ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011\u0013 ≤ γ(t) yi (yi, k) + βγ(t) −yi(yi, k) + X p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 + 2P · o \u0012 1 polylog(d) \u0013 ≤ (1 + β)α2β−1 + 1 + 2P · o \u0012 1 polylog(d) \u0013 ≤ 2, and g(t) i,C = 1 1+exp(yifW(t) (Xi,C)) ≥ 1 9 . Also, for each p /∈ C ∪ {p∗ i }, we have ρ(t+1) s (i, p) + βρ(t+1) −s (i, p) ≥ ρ(t) s (i, p) + βρ(t) −s(i, p) + η nPC′∼DC[C′ = C]g(t) i,C \u0012 ϕ′ \u0010D w(t) s , x(p) i E\u0011 + βϕ′ \u0010D w(t) −s, x(p) i E\u0011\u0013\r\r\rξ(p) i \r\r\r 2 ≥ ρ(t) s (i, p) + βρ(t) −s(i, p) + ηβσ2 bd 18n \u0000P C \u0001, where the last inequality is due to \r\r\rξ(p) i \r\r\r 2 ≥ 1 2 σ2 bd and ϕ′ ≥ β. We also have X p/∈C∪{p∗ i } \u0010 ρ(t+1) s (i, p) + βρ(t+1) −s (i, p) \u0011 ≥ X p/∈C∪{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 + ηβσ2 bd 18n \u0000P C \u0001 From the given condition in the lemma statement, we have 18n( P C) ηβσ2 bd = o \u0010 nα2 ηβ|Vs,k| \u0011 . If we choose t0 ∈ \u0014 18n( P C) ηβσ2 bd , nα2 ηβ|Vs,k| \u0015 , then we have 1 > X p/∈C∪{p∗ i } \u0010 ρ(t0) s (i, p) + βρ(t0) −s (i, p) \u0011 ≥ ηβσ2 bd 18n \u0000P C \u0001t0 ≥ 1, 51and this is a contradiction; therefore, it cannot hold that P p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 < 1 for all 0 ≤ t ≤ nα2 ηβ|Vyi,k|. Thus, there exists 0 ≤ Ti,C < nα2 ηβ|Vs,k| satisfying P p/∈C∪{p∗ i } \u0010 ρ(Ti,C+1) s (i, p) + βρ(Ti,C+1) −s (i, p) \u0011 ≥ 1 and let us choose the smallest one. For any 0 ≤ t < Ti,C, we have 1 ≥ X p/∈C∪{p∗ i } \u0010 ρ(Ti,C) s (i, p) + βρ(Ti,C) −s (i, p) \u0011 ≥ ησ2 bd 18n \u0000P C \u0001Ti, and we conclude that Ti,C ≤ 18n( P C) ηβσ2 bd . Lastly, we move on to proveTi,C > ¯T1. Combining Lemma D.2 and Lemma D.3 leads to X p/∈C∪{p∗ i }\\{p∗ i } \u0010 ρ( ¯T1) s (i, p) + βρ( ¯T1) −s (i, p) \u0011 ≤ 1 2. Thus, we have Ti,C > ¯T1 and this is what we desired. What We Have So Far. For any k ∈ KE, it satisfies ρk = o \u0010 α2n σ2 bd \u0011 due to (A6). By Lemma D.5 at iterate t ∈ [TCutout, T∗] with TCutout := max \u001a max k∈KE,i∈Vyi,k,p∗ i /∈C Ti,C, max i∈[n],p∗ i ∈C Ti,C \u001b ∈ \u0002¯T1, T∗\u0003 the following properties hold if the event Einit occurs: • (Learn common/rare features): For any s ∈ {±1} and k ∈ KC ∪ KR, γ(t) s (s, k) + βγ(t) −s(s, k) = Ω(1), • (Overfit augmented data with extremely rare features or no feature): For each i ∈ [n], k∈ KE, C ⊂[P] with |C| = C such that (1) i ∈ Vyi,k and p∗ i /∈ Cor (2) i ∈ [n] and p∗ i ∈ C X p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 = Ω(1). • (Do not learn extremely rare features at TCutout): For any s, s′ ∈ {±1} and k ∈ KE, γ(TCutout) s′ (s, k) ≤ α2β−1. • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 . D.2.4 Cutout cannot Learn Extremely Rare Features Within Polynomial Times In this step, We will show that Cutout cannot learn extremely rare features within the maximum admissible iterate T∗ = poly(d) η . we fix any s∗ ∈ {±1} and k∗ ∈ KE. Recall the function Q(s∗,k∗) : W →Rd×2, defined in Lemma B.5 and omit superscripts for simplicity. For each iteration t, Q(W(t)) represents quantities updates by data with feature vector vs∗,k∗ until t-th iteration. We will sequentially introduce several technical lemmas and by combining these lemmas, quantify update by data with feature vector vs∗,k∗ after TCutout and derive our conclusion. Let us define W∗ = {w∗ 1, w∗ −1}, where w∗ s = w(TCutout) s + M X i∈Vs∗,k∗ X p∈[P]\\{p∗ i ,˜pi} ξ(p) i\r\r\rξ(p) i \r\r\r 2 , where M = 4β−1 log \u0010 2ηβ2T∗ α2 \u0011 . Note that (12), β <1, and T∗ = poly(d) η together imply M = eO \u0000 β−1\u0001 . Note that W(t), W∗ ∈ Wfor any t ≥ 0. 52Lemma D.6. Suppose the event Einit occurs. Then, \r\r\rQ \u0010 W(TCutout) \u0011 − Q(W∗) \r\r\r 2 ≤ 8M2P|Vs∗,k∗|σ−2 b d−1. where ∥·∥ denotes the Frobenius norm. Proof of Lemma D.6. For each s ∈ {±1}, ss∗ \u0010 Qs (w∗ s) − Qs \u0010 w(TCutout) s \u0011\u0011 = Qs  ss∗M X i∈Vs∗,k∗ X p∈[P]\\{p∗ i ,˜pi} ξ(p) i\r\r\rξ(p) i \r\r\r   = M X i∈Vs∗,k∗ X p∈[P]\\{p∗ i ,˜pi} ξ(p) i\r\r\rξ(p) i \r\r\r 2 , and we have \r\r\rQ \u0010 W(TCutout) \u0011 − Q(W∗) \r\r\r 2 = \r\r\rQ1(w∗ 1) − Q1 \u0010 w(TCutout) 1 \u0011\r\r\r 2 + \r\r\rQ−1(w∗ −1) − Q−1 \u0010 w(TCutout) −1 \u0011\r\r\r 2 ≤ 2M2   X i∈Vs∗,k∗,p∈[P]\\{p∗ i ,˜pi} \r\r\rξ(p) i \r\r\r −2 + X i,j∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi},q∈[P]\\{p∗ j ,˜pj} (i,p)̸=(j,q) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(p) i \r\r\r 2 \r\r\rξ(q) j \r\r\r 2   . From Einit and (A2), we have X i,j∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi},q∈[P]\\{p∗ j ,˜pj} (i,p)̸=(j,q) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(p) i \r\r\r 2 \r\r\rξ(q) j \r\r\r 2 ≤ X i∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi} X j∈Vs∗,k∗ p∈[P]\\{p∗ j ,˜pj} \r\r\rξ(˜p) i \r\r\r −2 eO \u0010 d−1 2 \u0011 ≤ X i∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi} \r\r\rξ(˜p) i \r\r\r −2 eO \u0010 nP d−1 2 \u0011 ≤ X i∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi} \r\r\rξ(p) i \r\r\r −2 From the event Einit defined in Lemma B.2, we have X i∈Vs∗,k∗ p∈[P]\\{p∗ i ,˜pi} \r\r\rξ(p) i \r\r\r −2 ≤ 2P|Vs∗,k∗|σ−2 d d−1, and we obtain \r\r\rQ \u0010 W(TCutout) \u0011 − Q(W∗) \r\r\r 2 ≤ 4M2 X i∈Vs∗,k∗,p∈[P]\\{p∗ i } \r\r\rξ(p) i \r\r\r −2 ≤ 8M2P|Vs∗,k∗|σ−2 b d−1. Lemma D.7. Suppose the Einit occurs. For any t ≥ TCutout, i ∈ Vs∗,k∗ and any C ⊂[P] with |C| = C, it holds that ⟨yi∇W fW(t) (Xi,C), Q(W∗)⟩ ≥Mβ 2 . 53Proof of Lemma D.7. We have ⟨yi∇W fW(t) (Xi,C), Q(W∗)⟩ = X p/∈C \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗(w∗ s∗), x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗(w∗ −s∗), x(p) i E\u0013 . For any s ∈ {±1} and p ∈ [P] \\ {p∗ i , ˜pi}, ss∗ D Qs(w∗ s), ξ(p) i E ≥ M + ρ(TCutout) s (i, p) − X j∈[n],q∈[P]\\{p∗ j } (j,q)̸=(i,p) ρ(TCutout) s (j, q) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(q) j \r\r\r 2 − M X j∈Vs∗,k∗,q∈[P]\\{p∗ j ,˜pj} (j,q)̸=(i,p) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(q) j \r\r\r 2 ≥ M − eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = M − o \u0012 1 polylog(d) \u0013 ≥ M 2 , (26) where the last equality is due to (9). Also, for any s ∈ {±1}, ss∗ ⟨Qs(w∗ s), vs∗,k∗⟩ = γ(TCutout) s (s∗, k∗) ≥ 0. In addition, ss∗ D Qs(w∗ s), x(˜pi) i E = ss∗ D Qs(w∗ s), ξ(˜pi) i E + ss∗ D Qs(w∗ s), x(˜pi) i − ξ(˜pi) i E = ss∗ D Qs(w∗ s), ξ(˜pi) i E − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 = ρ(TCutout) s (i, ˜pi) + X j∈[n],q∈[P]\\{p∗ i } (j,q)̸=(i,˜pi) ρ(TCutout) s (j, q) D ξ(˜pi) i , ξ(q) j E \r\r\rξ(q) j \r\r\r 2 − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 ≥ −eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 − eO \u0000 α2β−1ρk∗nσ−2 d d−1\u0001 = −o \u0012 1 polylog(d) \u0013 , (27) where the last equality is due to (9) and (A7). For any C ⊂[P] with |C| = C, there exists p ∈ [P] \\ {p∗ i , ˜pi} such that p ̸= C since C < P 2 . By applying (26) and (27) for s = s∗, −s∗ and combining with ϕ′ ≥ β, we have ⟨yi∇W fW(t) (Xi,C), Q(W∗)⟩ ≥ \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗(w∗ s∗), x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗(w∗ −s∗), x(p) i E\u0013 + X q /∈C∪{p} \u0012 ϕ′ \u0010D w(t) s∗ , x(q) i E\u0011D Qs∗(w∗ s∗), x(q) i E − ϕ′ \u0010D w(t) −s∗, x(q) i E\u0011D Q−s∗(w∗ −s∗), x(q) i E\u0013 ≥ Mβ − o \u0012 1 polylog(d) \u0013 ≥ Mβ 2 . 54By combining Lemma D.6 and Lemma D.7, we can obtain the following result. Lemma D.8. Suppose the event Einit occurs. η n T∗ X t=TCutout X i∈Vs∗,k∗ EC∼DC[ℓ (yifW(t) (Xi,C))] ≤ \r\r\rQ \u0010 W(TCutout) \u0011 − Q(W∗) \r\r\r 2 + 2ηT ∗e−Mβ 4 , where ∥·∥ denotes the Frobenius norm. Proof of Lemma D.8. Note that for any TCutout ≤ t < T∗, Q \u0010 W(t+1) \u0011 = Q \u0010 W(t) \u0011 − η n∇W X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] . Therefore, we have \r\r\rQ \u0010 W(t) \u0011 − Q (W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q (W∗) \r\r\r 2 = 2η n * ∇W X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] , Q \u0010 W(t) \u0011 − Q (W∗) + − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] \r\r\r\r\r\r 2 = 2η n * ∇W X i∈Vs∗,k∗ EC∼DC [ℓ(yifW(t) (Xi,C))] , Q \u0010 W(t) \u0011+ − 2η n X i∈Vs∗,k∗ ⟨EC∼DC [ℓ′(yifW(t) (Xi,C))∇W yifW(t) (Xi,C)] , Q(W∗)⟩ − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] \r\r\r\r\r\r 2 ≥ 2η n * ∇W X i∈Vs∗,k∗ EC∼DC [ℓ(yifW(t) (Xi,C))] , Q \u0010 W(t) \u0011+ − Mβη n X i∈Vs∗,k∗ EC∼DC [ℓ′(yifW(t) (Xi,C))] − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] \r\r\r\r\r\r 2 , where the last inequality is due to Lemma D.7. By the chain rule, for each C ⊂[P] with |C| = C, we have * ∇W X i∈Vs∗,k∗ ℓ(yifW(t) (Xi,C)), Q \u0010 W(t) \u0011+ = X i∈Vs∗,k∗ \" ℓ′(yifW(t) (Xi,C)) × X p/∈C \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗ \u0010 w(t) s∗ \u0011 , x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗ \u0010 w(t) −s∗ \u0011 , x(p) i E\u0013# . For each s ∈ {±1}, i ∈ Vs∗,k∗, and p ∈ [P], \f\f\f D w(t) s , x(p) i E − D Qs \u0010 w(t) s \u0011 , x(p) i E\f\f\f = \f\f\f D w(t) s − Qs \u0010 w(t) s \u0011 , x(p) i E\f\f\f 55≤ X j∈[n]\\Vs∗,k∗,q∈[P]\\{p∗ i } \f\f\f\f\f\f\f * ρ(t) s (j, q) ξ(q) j \r\r\rξ(q) j \r\r\r 2 , x(p) i +\f\f\f\f\f\f\f + α X j∈F1\\Vs∗,k∗ ρ(t) s (j, ˜pj) \r\r\rξ(˜pj) j \r\r\r −2 \f\f\f D v1,1, x(p) i E\f\f\f + α X j∈F−1\\Vs∗,k∗ ρ(t) s (j, ˜pj) \r\r\rξ(˜pj) j \r\r\r −2 \f\f\f D v−1,1, x(p) i E\f\f\f ≤ eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 + eO \u0000 α2β−1nσ−2 d d−1\u0001 = o \u0012 1 polylog(d) \u0013 , where the last inequality is due to Lemma D.1 and the event Einit. By Lemma F.1, X p/∈C \u0012 ϕ′ \u0010D w(t) s∗ , x(p) i E\u0011D Qs∗ \u0010 w(t) s∗ \u0011 , x(p) i E − ϕ′ \u0010D w(t) −s∗, x(p) i E\u0011D Q−s∗ \u0010 w(t) s∗ \u0011 , x(p) i E\u0013 ≤ X p/∈C \u0012 ϕ \u0010D w(t) s∗ , x(p) i E\u0011 − ϕ \u0010D w(t) −s∗, x(p) i E\u0011\u0013 + rP + o \u0012 1 polylog(d) \u0013 = yifW(t) (Xi,C) + o \u0012 1 polylog(d) \u0013 , where the last equality is due to r = o \u0010 1 polylog(d) \u0011 . Therefore, we have \r\r\rQ \u0010 W(t) \u0011 − Q (W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q(W∗) \r\r\r 2 ≥ 2η n X i∈Vs∗,k∗ EC∼DC \u0014 ℓ′ (yifW(t) (Xi,C)) \u0012 yifW(t) (Xi,C) + o \u0012 1 polylog(d) \u0013 − Mβ 2 \u0013\u0015 − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ EC∼DC [ℓ(yifW(t) (Xi,C))] \r\r\r\r\r\r 2 ≥ 2η n X i∈Vs∗,k∗ EC∼DC \u0014 ℓ′(yifW(t) (Xi,C)) \u0012 yifW(t) (Xi,C) − Mβ 4 \u0013\u0015 − η2 n2 \r\r\r\r\r\r ∇W X i∈Vs∗,k∗ EC∼DC [ℓ(yifW(t) (Xi,C))] \r\r\r\r\r\r 2 . From the convexity of ℓ(·), X i∈Vs∗,k∗ EC∼DC \u0014 ℓ′(yifW(t) (Xi,C)) \u0012 yifW(t) (Xi,C) − Mβ 4 \u0013\u0015 ≥ X i∈Vs∗,k∗ EC∼DC \u0014\u0012 ℓ(yifW(t) (Xi,C)) − ℓ \u0012Mβ 4 \u0013\u0013\u0015 ≥ X i∈Vs∗,k∗ EC∼DC [ℓ(yifW(t) (Xi,C))] − ne−Mβ 4 . In addition, by Lemma F.3, η2 n2 \r\r\r\r\r\r ∇ X i∈Vs∗,k∗ EC∼DC[ℓ (yifW(t) (Xi,C))] \r\r\r\r\r\r 2 56≤ 8η2P2σ2 dd|Vs∗,k∗| n2 X i∈Vs∗,k∗ EC∼DC[ℓ(yifW(t) (Xi,C))] ≤ η n X i∈Vs∗,k∗ EC∼DC[ℓ(yifW(t) (Xi,C))], where the last inequality is due to (A8), and we have \r\r\rQ \u0010 W(t) \u0011 − Q(W∗) \r\r\r 2 − \r\r\rQ \u0010 W(t+1) \u0011 − Q(W∗) \r\r\r 2 ≥ η n X i∈Vs∗,k∗ EC∼DC[ℓ(yifW(t) (Xi,C))] − 2ηe−Mβ 4 . From telescoping summation, we have η n T∗ X t=TCutout X i∈Vs∗,k∗ EC∼DC[ℓ (yifW(t) (Xi,C))] ≤ \r\r\rQ \u0010 W(TCutout) \u0011 − Q (W∗) \r\r\r 2 + 2ηT ∗e−Mβ 4 . Finally, we can prove that the model cannot learn extremely rare features within T∗ iterations. Lemma D.9. Suppose the event Einit occurs. For any T ∈ [TCutout, T∗], we have γ(T) s (s∗, k∗) = eO(α2β−2) for each s ∈ {±1}. Proof of Lemma D.9. For any T ∈ [TCutout, T∗], we have γ(T) s (s∗, k∗) = γ(TCutout) s (s∗, k∗) + η n T−1X t=TCutout X i∈Vs∗,k∗ EC∼DC h g(t) i,C · 1 p/∈C i ϕ′ \u0010D w(t) s , vs∗,k∗ E\u0011 ≤ γ(TCutout) s (s∗, k∗) + η n T−1X t=TCutout X i∈Vs∗,k∗ EC∼DC h g(t) i,C i ≤ γ(TCutout) s (s∗, k∗) + η n T−1X t=TCutout X i∈Vs∗,k∗ EC∼DC [ℓ (yifW(t) (Xi,C))] , where the first inequality is due to ϕ′ ≤ 1 and the second inequality is due to −ℓ′ ≤ ℓ. From the result of Section D.2.3, γ(TCutout) s (s∗, k∗) ≤ α2β−1 and by Lemma D.8 and Lemma D.6, we have η n (T−1)X t=TCutout X i∈Vs∗,k∗ EC∼DC[ℓ (yifW(t) (Xi,C))] ≤ η n (T∗)X t=TCutout X i∈Vs∗,k∗ EC∼DC[ℓ (yifW(t) (Xi,C))] ≤ \r\r\rQ \u0010 W(TCutout) \u0011 − Q(W∗) \r\r\r 2 + 2ηT ∗e−Mβ 2 ≤ 8M2P|Vs∗,k∗|σ−2 b d−1 + 2ηT ∗e−Mβ 4 = eO \u0000 α2β−2\u0001 . The last line is due to (A6) and M = 4β−1 log \u0010 2ηβ2T∗ α2 \u0011 . This finishes the proof. What We Have So Far. Suppose the event Einit occurs. For any t ∈ [TCutout, T∗], we have • (Learn common/rare features): γ(t) s (s, k) + βγ(t) −s(s, k) = Ω(1) for each s ∈ {±1} and k ∈ KC ∪ KR • (Overfit augmented data with extremely rare features or no feature): For eachi ∈ [n], k∈ KE, C ⊂ [P] with |C| = C such that (1) i ∈ Vyi,k and p∗ i /∈ Cor (2) i ∈ [n] and p∗ i ∈ C X p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 = Ω(1). 57• (Cannot learn extreme features): γ(t) s (s, k), γ(t) −s(s, k) = O \u0000 α2β−2\u0001 for each s ∈ {±1} and k ∈ KE. • For any s ∈ {±1}, i∈ [n], and p ∈ [P] \\ {p∗ i }, ρ(t) s (i, p) = eO \u0000 β−1\u0001 , D.2.5 Train and Test Accuracy In this step, we will prove that the model trained by Cutout has perfect training accuracy on both augmented data and original data but has near-random guesses on test data with extremely rare data. For any i ∈ Vs,k with s ∈ {±1}, k ∈ KC ∪ KR and C ⊂[P] with |C| = C and p∗ i /∈ C, yifW(t) (Xi,C) = X p/∈C \u0010 ϕ \u0010D w(t) s , x(p) i E\u0011 − ϕ \u0010D w(t) −s, x(p) i E\u0011\u0011 = γ(t) s (s, k) + βγ(t) −s(s, k) + X p/∈C∪{p∗ i } \u0010 ρ(t) s (i, p) + βρ(t) −s(i, p) \u0011 − 2(P − C) · o \u0012 1 polylog(d) \u0013 ≥ γ(t) s (s, k) + βγ(t) −s(s, k) − 2(P − C) · o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 = Ω(1), for any t ∈ [TCutout, T∗]. In addition, for any i ∈ [n] and C ⊂[P] with |C| = C that does not correspond to the case above, by Lemma D.5 and Lemma B.4, we have yifW(t) (Xi,C) = X p/∈C \u0012 ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011\u0013 ≥ X p/∈C∪{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 − 2(P − C) · o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 = Ω(1), for any t ∈ [TCutout, T∗]. We can conclude that Cutout with t ∈ [TCutout, T∗] iterates achieve perfect training accuracy on augmented data. Next, we will show that Cutout achieves perfect training accuracy on the original data. For any i ∈ [n], let us choose C ⊂[P] with |C| = C such that p∗ i ∈ C. Then, from the result above, we have yifW(t) (Xi) = yifW(t) (Xi,C) + X p∈C \u0012 ϕ \u0010D w(t) yi , x(p) i E\u0011 − ϕ \u0010D w(t) −yi, x(p) i E\u0011\u0013 ≥ yifW(t) (Xi,C) + X p∈C\\{p∗ i } \u0010 ρ(t) yi (i, p) + βρ(t) −yi(i, p) \u0011 − C · o \u0012 1 polylog(d) \u0013 ≥ Ω(1), for any t ∈ [TCutout, T∗] and we conclude that Cutout with t ∈ [TCutout, T∗] iterates achieve perfect training accuracy on original data. Lastly, let us move on to the test accuracy part. Let (X, y) ∼ Dbe a test data with X =\u0000 x(1), . . . ,x(P)\u0001 ∈ Rd×P having feature patch p∗, dominant noise patch ˜p, and feature vector vy,k. We have x(p) ∼ N(0, σ2 bΛ) for each p ∈ [P] \\ {p∗, ˜p} and x(˜p) − αvs,1 ∼ N(0, σ2 dΛ) for some s ∈ {±1}. Therefore, for all t ∈ [TCutout, T∗] and p ∈ [P] \\ {p∗, ˜p}, \f\f\fϕ \u0010D w(t) 1 , x(p) E\u0011 − ϕ \u0010D w(t) −1, x(p) E\u0011\f\f\f 58≤ \f\f\f D w(t) 1 − w(t) −1, x(p) E\f\f\f ≤ \f\f\f D w(0) 1 − w(0) −1, x(p) E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } \f\f\fρ(t) 1 (i, q) − ρ(t) −1(i, q) \f\f\f \f\f\f D ξ(q) i , x(p) E\f\f\f \r\r\rξ(q) i \r\r\r 2 ≤ eO \u0010 σ0σbd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 α polylog(d) \u0013 , (28) with probability at least 1 − o \u0010 1 poly(d) \u0011 due to Lemma B.2, (A8), (8), and (9).. In addition, for any s′ ∈ {±1}, we have \f\f\f D w(t) s′ , x(˜p) − αvs,1 E\f\f\f ≤ \f\f\f D w(0) s′ , x(˜p) − αvs,1 E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } ρ(t) s′ (i, q) \f\f\f D ξ(q) i , x(˜p) − αvs,1 E\f\f\f \r\r\rξ(q) i \r\r\r 2 = eO \u0010 σ0σdd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 α polylog(d) \u0013 , (29) with probability at least 1 − o \u0010 1 poly(d) \u0011 due to Lemma B.2, (A8), (8), and (9). Case 1: k ∈ KC ∪ KR By Lemma B.2, (A7), and (10), \f\f\fϕ \u0010D w(t) 1 , x(˜p) E\u0011 − ϕ \u0010D w(t) −1, x(˜p) E\u0011\f\f\f ≤ \f\f\f D w(t) 1 − w(t) −1, x(˜p) E\f\f\f ≤ α \f\f\f D w(t) 1 − w(t) −1, vs,1 E\f\f\f + \f\f\f D w(t) 1 − w(t) −1, x(p) − αvs,1 E\f\f\f ≤ α \u0010 γ(t) 1 (s, 1) + γ(t) −1(s, 1) \u0011 + α \f\f\f D w(0) 1 , vs,1 E\f\f\f + α \f\f\f D w(0) −1, vs,1 E\f\f\f + o \u0012 1 polylog(d) \u0013 ≤ eO \u0000 αβ−1\u0001 + eO (ασ0) + o \u0012 1 polylog(d) \u0013 = o \u0012 1 polylog(d) \u0013 , (30) with probability at least 1 − o \u0010 1 poly(d) \u0011 . Suppose (28) and (30) holds. By Lemma B.4, we have yfW(t) (X) = \u0012 ϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011\u0013 + X p∈[P]\\{p∗} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013 = γ(t) y (y, k) + βγ(t) −y(y, k) − o \u0012 1 polylog(d) \u0013 = Ω(1) − o \u0012 1 polylog(d) \u0013 59> 0. Therefore, we have P(X,y)∼D h yfW(t) (X) > 0 | x(p∗) = vy,k, k∈ KC ∪ KR i ≥ 1 − o \u0012 1 poly(d) \u0013 . (31) Case 2: k ∈ KE By triangular inequality and ϕ′ ≤ 1, we have ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 = ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 + \u0012 ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) s , αvs,1 E\u0011\u0013 − \u0012 ϕ \u0010D w(t) −s, x(˜p) E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011\u0013 ≥ ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 − \f\f\f D w(t) s , x(˜p) − αvs,1 E\f\f\f − \f\f\f D w(t) −s, x(˜p) − αvs,1 E\f\f\f. In addition, ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010D w(t) −s, αvs,1 E\u0011 = \u0010 ϕ \u0010 αγ(t) s (s, 1) \u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0011 + \u0010 ϕ \u0010D w(t) s , αvs,1 E\u0011 − ϕ \u0010 αγ(t) s (s, 1) \u0011\u0011 − \u0010 ϕ \u0010D w(t) −s, αvs,1 E\u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0011 ≥ \u0010 ϕ \u0010 αγ(t) s (s, 1) \u0011 − ϕ \u0010 −αγ(t) −s(s, 1) \u0011\u0011 − α \f\f\f D w(t) s , vs,1 E − γ(t) s (s, 1) \f\f\f − α \f\f\f D w(t) −s, vs,1 E + γ(t) −s(s, 1) \f\f\f = α \u0010 γ(t) s (s, 1) + βγ(t) −s(s, 1) \u0011 − α · o \u0012 1 polylog(d) \u0013 = Ω(α), where the second equality is due to Lemma B.4 and (A8). If (29) holds, we have ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 = Ω(α) − o \u0012 α polylog(d) \u0013 = Ω(α). (32) Note that yfW(t) (X) = ϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011 + ϕ \u0010D w(t) y , x(˜p) E\u0011 − ϕ \u0010D w(t) −y, x(˜p) E\u0011 + X p∈[P]\\{p∗,˜p} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013 , and \f\f\fϕ \u0010D w(t) y , vy,k E\u0011 − ϕ \u0010D w(t) −y, vy,k E\u0011\f\f\f + \f\f\f\f\f\f X p∈[P]\\{p∗,˜p} \u0012 ϕ \u0010D w(t) y , x(p) E\u0011 − ϕ \u0010D w(t) −y, x(p) E\u0011\u0013\f\f\f\f\f\f ≤ \f\f\f D w(t) y − w(t) −y, vy,k E\f\f\f + o \u0012 α polylog(d) \u0013 60≤ γ(t) 1 (y, k) + γ(t) −1(y, k) + \f\f\f D w(0) y − w(0) −y, vy,k E\f\f\f + o \u0012 α polylog(d) \u0013 ≤ O(α2β−2) + eO(σ0) + o \u0012 α polylog(d) \u0013 = o \u0012 α polylog(d) \u0013 < ϕ \u0010D w(t) s , x(˜p) E\u0011 − ϕ \u0010D w(t) −s, x(˜p) E\u0011 , where the first inequality is due to (28), the second-to-last line is due to (A8), (8), and (10) , and the last inequality is due to (32). Therefore, we have yfW(t) (X) > 0 if y = s. Otherwise, yfW(t) (X) < 0. P(X,y)∼D h yfW(t) (X) > 0 | x(p∗) = vy,k, k∈ KE i = 1 2 ± o \u0012 1 poly(d) \u0013 . (33) Hence, combining (31) and (33) implies P(X,y)∼D [yfW(t) (X) > 0] = X k∈KC∪KR ρk + 1 2   1 − X k∈KC∪KR ρk ! ± o \u0012 1 poly(d) \u0013 = 1 − 1 2 X k∈KE ρk ± o \u0012 1 poly(d) \u0013 . □ 61E Proof for CutMix E.1 Proof of Lemma B.3 for CutMix For each i, j∈ [n] and S ⊂[P], let g(t) i,j,S := −|S| P yiℓ′\u0000 yifW(t) (Xi,j,S) \u0001 − \u0012 1 − |S| P \u0013 yjℓ′\u0000 yjfW(t) (Xi,j,S) \u0001 . For s ∈ {±1} and iterate t, w(t+1) s − w(t) s = −η∇wsLCutMix \u0010 W(t) \u0011 = η n2 X i,j∈[n] ES∼DS  sg(t) i,j,S  X p∈S ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) j     = sη n2 X s′∈{±1},k∈[K] X i∈Vs′,k,j∈[n] ES∼DS h g(t) i,j,S1 p∗ i ∈S + g(t) j,i,S1 p∗ i /∈S i ϕ′ \u0010D w(t) s , vs′,k E\u0011 vs′,k + sη n2 X i,j∈[n],p∈[P]\\{p∗ i } ES∼DS h g(t) i,j,S1 p∈S + g(t) j,i,S1 p/∈S i ϕ′ \u0010D w(t) s , x(p) i E\u0011 x(p) i . Hence, if we define γ(t) s (s′, k)’s and ρ(t) s (i, p)’s recursively by using the rule γ(t+1) s (s′, k) =γ(t) s (s′, k) +ss′η n2 X i∈Vs′,k,j∈[n] ES∼DS h g(t) i,j,S1 p∗ i ∈S + g(t) j,i,S1 p∗ i /∈S i ϕ′ \u0010D w(t) s , vs′,k E\u0011 , ρ(t+1) s (i, p) =ρ(t) s (i, p) +syiη n2 X j∈[n] ES∼DS h g(t) i,j,S1 p∈S + g(t) j,i,S1 p/∈S i ϕ′ \u0010D w(t) s , x(p) i E\u0011\r\r\rξ(p) i \r\r\r 2 , starting from γ(0) s (s′k) = ρ(0) s (i, p) = 0 for each s, s′ ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }, then we have w(t) s = w(0) s + X k∈[K] γ(t) s (s, k)vs,k − X k∈[K] γ(t) s (−s, k)v−s,k + X i∈Vs p∈[P]\\{˜pi} ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 − X i∈V−s p∈[P]\\{˜pi} ρ(t) s (i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 + α   X i∈Fs syiρ(t) s (i, ˜pi) vs,1 \r\r\rξ(˜pi) i \r\r\r 2 + X i∈F−s syiρ(t) s (i, ˜pi) v−s,1 \r\r\rξ(˜pi) i \r\r\r 2  , for each s ∈ {±1}. □ E.2 Proof of Theorem 3.3 We will prove that the conclusion of Theorem 3.3 holds when the eventEinit occurs. The proof of Theorem 3.3 is structured into the following six steps: 1. Introduce a reparametrization of the CutMix loss LCutMix(W) to a convex function h(Z) for ease of analysis (Section E.2.1). 2. Characterize a global minimum of h(Z) (Section E.2.2). 3. Evaluate strong convexity constant in the region near the global minimum ofh(Z) (Section E.2.3). 4. Show that near stationary point of h(Z) is close to a global minimum (Section E.2.4). 5. Prove that gradient descent on the CutMix loss LCutMix(W) achieves a near-stationary point of the reparametrized function h(Z) and perfect accuracy on original training data (Section E.2.5). 6. Evaluate the test accuracy of a model in near-stationary point (Section E.2.6). 62E.2.1 Reparametrization of CutMix Loss It is complicated to characterize the stationary points of CutMix loss LCutMix(W) due to its non- convexity. We will overcome this problem by introducing reparameterization of the objective function. Let us define z(p) i := ϕ \u0010D w1, x(p) i E\u0011 − ϕ \u0010D w−1, x(p) i E\u0011 , for i ∈ [n], p∈ [P] and zs,k := ϕ(⟨w1, vs,k⟩) − ϕ(⟨w−1, vs,k⟩), for each s ∈ {±1}, k∈ [K]. We can rewrite CutMix loss LCutMix(W) as a function h(Z) of the defined variables Z := {zs,k}s∈{±1},k∈[K] ∪ {z(p) i }i∈[n],p∈[P]\\{p∗ i } as follows. h(Z) := 1 n2 X i,j∈[n] ES∼DS   |S| P ℓ  yi  X p∈S z(p) i + X p/∈S z(p) j     + \u0012 1 − |S| P \u0013 ℓ  yj  X p∈S z(p) i + X p/∈S z(p) j       , where we write z(p∗ i ) i = zs,k if i ∈ Vs,k. For notational simplicity, let us consider Z as vectors in R2K+n(P−1) with the standard orthonormal basis {es,k}s∈{±1},k∈[K] ∪ n e(p) i o i∈[n],p∈[P]\\{p∗ i } which means Z = {zs,k}s∈{±1},k∈[K] ∪ n z(p) i o i∈[n],p∈[P]\\{p∗ i } = X s∈{±1},k∈[K] zs,kes,k + X i∈[n],p∈[P]\\{p∗ i } z(p) i e(p) i . If there is no confusion, we will use e(p∗ i ) i to represent es,k, for i ∈ Vs,k. By the chain rule, ∇W LCutMix(W) = J(W)∇Zh(Z), where each column of Jacobian matrix J(W) ∈ R2d×(n(P−1)+2K) is ∇W zs,k = \u0012 ϕ′(⟨w1, vs,k⟩)vs,k −ϕ′(⟨w−1, vs,k⟩)vs,k \u0013 ∈ R2d, ∇W z(p) i =   ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i −ϕ′ \u0010D w−1, x(p) i E\u0011 x(p) i   ∈ R2d. Let us characterize the smallest singular value σmin(J(W)) of the Jacobian matrix J(W). For any unit vector c = {cs,k}s∈{±1},k∈[K] ∪ n c(p) i o i∈[n],p∈[P]\\{p∗ i } ∈ R2K+n(P−1), we have ∥J(W)c∥2 = X s∈{±1},k∈[K] c2 s,k ∥∇W zs,k∥2 + X i∈[n],p∈[P]\\{p∗ i } \u0010 c(p) i \u00112 \r\r\r∇W z(p) i \r\r\r 2 + X s1,s2∈{±1},k1,k2∈[K] (s1,k1)̸=(s2,k2) cs1,k1 cs2,k2 ⟨∇W zs1,k1 , ∇W zs2,k2 ⟩ + 2 X s∈{±1},k∈[K] i∈[n],p∈[P]\\{p∗ i } cs,kc(p) i D ∇W zs,k, ∇W z(p) i E + X i∈[n],p∈[P]\\{p∗ i } j∈[n],q∈[P]\\{p∗ j } (i,p)̸=(j,q) c(p) i c(q) j D ∇W z(p) i , ∇W z(q) j E . 63For each s1, s2 ∈ {±1}, k1, k2 ∈ [K] such that (s1, k1) ̸= (s2, k2), and i ∈ [n], p∈ [P] \\ {p∗ i , ˜pi}, ⟨∇W zs1,k1 , ∇W zs2,k2 ⟩ = D ∇W zs1,k1 , ∇W z(p) i E = 0, and if k1 > 1 D ∇W zs1,k1 , ∇W z(p) i E = D ∇W zs1,k1 , ∇W z(˜pi) i E = 0, since ⟨vs1,k1 , vs2,k2 ⟩ = D vs1,k1 , ξ(p) i E = D vs1,k1 , ξ(˜pi) i E = 0. Also, for each s ∈ {±1} and i ∈ Fs, then 2 \f\f\fcs,1c(˜pi) i D ∇W zs,1, ∇W z(˜pi) i E\f\f\f = 2 \f\f\fcs,1c(˜pi) i \f\f\f \u0012 ϕ′(⟨w1, vs,1⟩)ϕ′ \u0010D w1, x(˜pi) i E\u0011 + ϕ′(⟨w−1, vs,1⟩)ϕ′ \u0010D w−1, x(˜pi) i E\u0011\u0013 α ≤ 4c2 s,1 \u0000 ϕ′(⟨w1, vs,1⟩)2 + ϕ′(⟨w−1, vs,1⟩)2\u0001 α2 \r\r\rx(˜pi) i \r\r\r 2 + 1 4 \u0010 c(˜pi) i \u00112 \u0012 ϕ′ \u0010D w1, x(˜pi) i E\u00112 + ϕ′ \u0010D w−1, x(˜pi) i E\u00112\u0013\r\r\rx(˜pi) i \r\r\r 2 < 1 2nc2 s,1 \u0000 ϕ′(⟨w1, vs,1⟩)2 + ϕ′(⟨w−1, vs,1⟩)2\u0001 + 1 4 \u0010 c(˜pi) i \u00112 \u0012 ϕ′ \u0010D w1, x(˜pi) i E\u00112 + ϕ′ \u0010D w−1, x(˜pi) i E\u00112\u0013\r\r\rx(˜pi) i \r\r\r 2 , where the last inequality holds since \r\r\rx(˜pi) i \r\r\r 2 = α2 + \r\r\rξ(˜pi) i \r\r\r 2 ≥ 1 2σ2 dd = ω(nα2), where we apply the fact from the event Einit defined in Lemma B.2 and (A7). Also, D ∇W z−s,1, ∇W z(˜pi) i E = 0. Furthermore, for each i, j∈ [n], p∈ [P] \\ {p∗ i }, q∈ [P] \\ {p∗ j } with (i, p) ̸= (j, q) satisfies \f\f\fc(p) i c(q) j D ∇Wz(p) i , ∇Wz(q) j E\f\f\f = \f\f\fc(p) i c(q) j \f\f\f \u0012 ϕ′ \u0010D w1, x(p) i E\u0011 ϕ′ \u0010D w1, x(q) j E\u0011 + ϕ′ \u0010D w−1, x(p) i E\u0011 ϕ′ \u0010D w−1, x(q) j E\u0011\u0013\f\f\f D x(p) i , x(q) j E\f\f\f ≤ 1 4Pn \u0010 c(p) i \u00112 \u0012 ϕ′ \u0010D w1, x(p) i E\u00112 + ϕ′ \u0010D w−1, x(p) i E\u00112 \u0013\r\r\rx(p) i \r\r\r 2 + 1 4Pn \u0010 c(q) j \u00112 \u0012 ϕ′ \u0010D w1, x(q) j E\u00112 + ϕ′ \u0010D w−1, x(q) j E\u00112 \u0013\r\r\rx(q) j \r\r\r 2 = 1 4Pn \u0012\u0010 c(p) i \u00112 \r\r\r∇Wz(p) i \r\r\r 2 + \u0010 c(q) j \u00112 \r\r\r∇Wz(q) j \r\r\r 2 \u0013 where the last inequality is due to AM-GM inequality and \r\r\rx(p) i \r\r\r · \r\r\rx(q) j \r\r\r ≥ 2nP \f\f\f D x(p) i , x(q) j E\f\f\f, which we show through a case analysis. For the case p = ˜pi and q = ˜pj, this inequality holds since \r\r\rx(p) i \r\r\r · \r\r\rx(q) j \r\r\r ≥ \r\r\rξ(p) i \r\r\r · \r\r\rξ(q) j \r\r\r ≥ 2nP \u0010\f\f\f D ξ(p) i , ξ(q) j E\f\f\f + α2 \u0011 ≥ 2nP \f\f\f D x(p) i , x(q) j E\f\f\f, where the second inequality is due to 1 2 \r\r\rξ(p) i \r\r\r · \r\r\rξ(q) j \r\r\r ≥ 2nP \f\f\f D ξ(p) i , ξ(q) j E\f\f\f, 1 2 \r\r\rξ(p) i \r\r\r · \r\r\rξ(q) j \r\r\r ≥ 2nP α2. which is implied by the fact from the event Einit defined in Lemma B.2, (A1), (A2), and (A7). In the remaining case, \r\r\rx(p) i \r\r\r · \r\r\rx(q) j \r\r\r ≥ \r\r\rξ(p) i \r\r\r · \r\r\rξ(q) j \r\r\r ≥ 2nP \f\f\f D ξ(p) i , ξ(q) j E\f\f\f = 2nP \f\f\f D x(p) i , x(q) j E\f\f\f, 64where the second inequality is due to the fact from event Einit defined in Lemma B.2, (A1), and (A2). For s ∈ {±1}, k∈ [K] and i ∈ [n], p∈ [P] \\ {p∗ i }, ∥∇W zs,k∥2 = ϕ′(⟨w1, vs,k⟩)2 + ϕ′(⟨w−1, vs,k⟩)2 ≥ 2β2, and \r\r\r∇W z(p) i \r\r\r 2 = \u0012 ϕ′ \u0010D w1, x(p) i E\u00112 + ϕ′ \u0010D w−1, x(p) i E\u00112\u0013\r\r\rx(p) i \r\r\r 2 ≥ β2σ2 i,pd ≥ β2, where the last inequality is due to (8). By merging all inequalities together, we have ∥J(W)c∥2 = X s∈{±1},k∈[K] c2 s,k∥∇W zs,k∥2 + X i∈[n],p∈[P]\\{p∗ i } \u0010 c(p) i \u00112 \r\r\r∇W z(p) i \r\r\r 2 + X s∈{±1},i∈Fs cs,1c(˜pi) i D ∇W zs,1, ∇W z(˜pi) i E + X i∈[n],p∈[P]\\{p∗ i } j∈[n],q∈[P]\\{p∗ j } (i,p)̸=(j,q) c(p) i c(q) j D ∇W z(p) i , ∇W z(q) j E ≥ X s∈{±1},k∈[K] c2 s,k∥∇W zs,k∥2 + X i∈[n],p∈[P]\\{p∗ i } \u0010 c(p) i \u00112 \r\r\r∇W z(p) i \r\r\r 2 − X s∈{±1},i∈Fs \u0012 1 2nc2 s,1∥∇W zs,1∥2 + 1 4 \u0010 c(˜pi) i \u00112 \r\r\r∇W z(˜pi) i \r\r\r 2\u0013 − 1 4P n X i∈[n],p∈[P]\\{p∗ i } j∈[n],q∈[P]\\{p∗ j } (i,p)̸=(j,q) \u0012\u0010 c(p) i \u00112 \r\r\r∇W z(p) i \r\r\r 2 + \u0010 c(q) j \u00112 \r\r\r∇W z(q) j \r\r\r 2\u0013 > 1 4 X s∈{±1},k∈[K] c2 s,k∥∇W zs,k∥2 + 1 4 X i∈[n],p∈[P]\\{p∗ i } \u0010 c(p) i \u00112 \r\r\r∇W z(p) i \r\r\r 2 ≥ β2 4 , and we conclude σmin(J(W)) ≥ β 2 for any W. E.2.2 Characterization of a Global Minimum of CutMix Loss In this section, we will check that h(Z) is strictly convex and it has a global minimum. For each i, j∈ [n] and S ⊂[P] let us define ai,j,S ∈ R2K+n(P−1) as ai,j,S = X p∈S e(p) i + X p/∈S e(p) j , and then h(Z) = 1 n2 X i,j∈[n] ES∼DS \u0014|S| P ℓ (yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 ℓ (yj⟨ai,j,S, Z⟩) \u0015 . Since ℓ(·) is convex, h(Z) is also convex. Note that ∇h(Z) = 1 n2 X i,j∈[n] ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 ai,j,S \u0015 , and ∇2h(Z) 65= 1 n2 X i,j∈[n] ES∼DS \u0014\u0012|S| P ℓ′′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 ℓ′′(yj⟨ai,j,S, Z⟩) \u0013 ai,j,Sa⊤ i,j,S \u0015 = 1 n2 X i,j∈[n] ES∼DS \u0002 ℓ′′(⟨ai,j,S, Z⟩)ai,j,Sa⊤ i,j,S \u0003 , where the last equality holds since ℓ′′(z) = ℓ′′(−z) for any z ∈ R. From the equation above, it suffices to show that {ai,j,S}i,j∈[n],S⊂[P] spans R2K+n(P−1) to show strict convexity of h(Z). We define a function I : [P] → [n] such that for each p ∈ [P], p∗ I(p) = p with x(p) I(p) = v1,1, where the existence is guaranteed by Lemma B.2 (but not necessarily unique). Then for any i ∈ [n] and p ∈ [p], we have ai,i,∅ + X q∈[P]\\{p} aI(q),i,{q} − (P − 1)aI(p),i,{p} = X p′∈[P] e(p′) i + X q∈[P]\\{p}  e1,1 + X p′∈[P]\\{q} e(p′) i   − (P − 1)  e1,1 + X p′∈[P]\\{p} e(p′) i   = X p′∈[P] e(p′) i +  (P − 1)e(p) i + (P − 2) X p′∈[P]\\{p} e(p′) i   − (P − 1) X p′∈[P]\\{p} e(p′) i = Pe(p) i . (34) Hence, {ai,j,S}i,j∈[n],S⊂[P] spans R2K+n(P−1) and h(Z) is strictly convex. Thus, it can have at most one global minimum. We want to show the existence of the global minimum and characterize it. n2∇h(Z) = X i,j∈[n] ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 ai,j,S \u0015 = 2 X i,j∈[n] p∈[P] ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 1 p∈S \u0015 e(p) i . We can simplify terms as X j∈[n] ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 1 p∈S \u0015 = X j∈Vyi ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 1 p∈S \u0015 + X j∈V−yi ES∼DS \u0014\u0012|S| P yiℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013 yjℓ′(yj⟨ai,j,S, Z⟩) \u0013 1 p∈S \u0015 = yi X j∈Vyi ES∼DS [ℓ′(yi⟨ai,j,S, Z⟩)1 p∈S] + yi X j∈V−yi ES∼DS \u0014\u0012 ℓ′(yi⟨ai,j,S, Z⟩) + \u0012 1 − |S| P \u0013\u0013 1 p∈S \u0015 = yi|V−yi|ES∼DS \u0014\u0012 1 − |S| P \u0013 1 p∈S \u0015 + yi X j∈[n] ES∼DS [ℓ′(yi⟨ai,j,S, Z⟩)1 p∈S], where the second equality holds since ℓ′(z) + ℓ′(−z) = −1. Also, for any p ∈ [P], ES∼DS \u0014\u0012 1 − |S| P \u0013 1 p∈S \u0015 = 1 P X q∈[P] ES∼DS \u0014\u0012 1 − |S| P \u0013 1 q∈S \u0015 66= 1 P ES∼DS   \u0012 1 − |S| P \u0013X q∈S 1 q∈S   = 1 P ES∼DS \u0014\u0012 1 − |S| P \u0013 |S| \u0015 = P − 1 6P . Hence, if X j∈[n] ES∼DS [ℓ′(yi⟨ai,j,S, Z⟩)1 p∈S] + P − 1 6P |V−yi| = 0, for all i ∈ [n] and p ∈ [P], then we have ∇h(Z) = 0. Let us consider a specific Z parameterized by z1, z−1, of the form z(p) i = yizyi for all i ∈ [n] and p ∈ [P]. We will find a stationary point with this specific form and then it should be the unique global minimum in the entire domain. Then for each i ∈ [n] and p ∈ [P], we have X j∈[n] ES∼DS [ℓ′(yi⟨ai,j,S, Z⟩)1 p∈S] = X j∈Vyi ES∼DS [ℓ′(yi ⟨ai,j,S, Z⟩)1 p∈S] + X j∈V−yi ES∼DS [ℓ′(yi⟨ai,j,S, Z⟩)1 p∈S] = |Vyi| ·ES∼DS [ℓ′(P zyi)1 p∈S] + |V−yi| ·ES∼DS [ℓ′(|S|zyi − (P − |S|)z−yi)1 p∈S] = 1 P X q∈[P] \u0010 |Vyi| ·ES∼DS [ℓ′(P zyi)1 q∈S] + |V−yi| ·ES∼DS [ℓ′(|S|zyi − (P − |S|)z−yi)1 q∈S] \u0011 = 1 P  |Vyi| ·ES∼DS  ℓ′(P zyi) X q∈S 1 q∈S   +|V−yi| ·ES∼DS  ℓ′(|S|zyi − (P − |S|)z−yi) X q∈S 1 q∈S     = 1 P \u0010 |Vyi| ·ES∼DS [|S|ℓ′(P zyi)] + |V−yi| ·ES∼DS [|S|ℓ′(|S|zyi − (P − |S|)z−yi)] \u0011 = |Vyi| 2 ℓ′(P zyi) + |V−yi| P ES∼DS [|S|ℓ′(|S|zyi − (P − |S|)z−yi)]. From Lemma F.4, there exists a unique minimizer ˆZ = {ˆzs,k}s∈{±1},k∈[K] ∪ n ˆz(p) i o i∈[n],p∈[P]\\{p∗ i } of h(Z) and it satisfies sˆzs,k = z∗ s = Θ(1) for all k ∈ [K] and yiˆz(p) i = z∗ yi = Θ(1) for all i ∈ [n] and p ∈ [P] \\ {p∗ i } due to (A1). E.2.3 Strong Convexity Near Global Minimum We will show that h(Z) is strongly convex in a set G containing a global minimum ˆZ where G is defined as follows. G := n Z ∈ R2K+n(P−1) : ∥Z − ˆZ∥∞ < ∥ ˆZ∥∞ o , here ∥·∥∞ is ℓ∞ norm. For any Z ∈ G and a unit vector c ∈ R2K+n(P−1) with c =P s∈{±1},k∈[K] cs,kes,k + P i∈[n],p∈[P]\\{p∗ i } c(p) i e(p) i , we have c⊤∇2h(Z)c = 1 n2 X i,j∈[n] ES∼DS \u0002 ℓ′′(⟨ai,j,S, Z⟩)⟨ai,j,S, c⟩2\u0003 ≥ ℓ′′(2P∥ ˆZ∥∞) n2 X i,j∈[n] ES∼DS [⟨ai,j,S, c⟩2]. 67Note that for each i ∈ [n], p∈ [P], from (34), we have c(p) i = D c, e(p) i E = 1 P ⟨c, ai,i,∅⟩ + 1 P X q∈[P]\\{p} ⟨c, aI(q),i,{q}⟩ −P − 1 P  c, aI(p),i,{p} \u000b , where we use the notational convention c(p∗ i ) i = cs,k for s ∈ {±1}, k∈ [K] and i ∈ Vs,k. By Cauchy-Schwartz inequality and the fact that PS∼DS [S = ∅], PS∼DS [S = {q}] ≥ 1 P(P+1) for all q ∈ [P], \u0010 c(p) i \u00112 =   1 P ⟨c, ai,i,∅⟩ + 1 P X q∈[P]\\{p}  c, aI(q),i,{q} \u000b − P − 1 P  c, aI(p),i,{p} \u000b   2 ≤   1 P2 + P − 1 P2 + \u0012 −P − 1 P \u00132! ⟨c, ai,i,∅⟩2 + X q∈[P]\\{p}  c, aI(q),i,{q} \u000b2 +  c, aI(p),i,{p} \u000b2   ≤   1 P2 + P − 1 P2 + \u0012 −P − 1 P \u00132! P(P + 1) X i,j∈[n] ES∼DS [⟨c, ai,j,S⟩2] ≤ 2P2 X i,j,∈[n] ES∼DS \u0002 ⟨c, ai,j,S⟩2\u0003 . Hence, we have c⊤∇2h(Z)c ≥ ℓ′′(2P∥ ˆZ∥∞) (4K + 2n(P − 1))P2n2 (4K + 2n(P − 1))P2 X i,j∈[n] ES∼DS h ⟨c, ai,j,S⟩2 i ≥ ℓ′′(2P∥ ˆZ∥∞) (4K + 2n(P − 1))P2n2   X s∈{±1},k∈[K] c2 s,k + X i∈[n],q∈[P]\\{p∗ i } \u0010 c(q) i \u00112   = ℓ′′(2P∥ ˆZ∥∞) (4K + 2n(P − 1))P2n2 , and we conclude h(Z) is µ-strongly convex in G where µ := ℓ′′(2P∥ ˆZ∥∞) (4K+2n(P−1))P2n2 . Due to (A1), (A2), and the fact that ∥ ˆZ∥∞ = Θ(1), we have µ ≥ 1 poly(d) . E.2.4 Near Stationary Points are Close to Global Minimum In this step, we want to show that near stationary points of h(Z) are close to a global minimum ˆZ. Lemma E.1. Suppose Z ∈ R2K+n(P−1) satisfies ∥∇h(Z)∥ < µϵwith some 0 < ϵ <∥ ˆZ∥∞ 2 . Then, we have \r\r\rZ − ˆZ \r\r\r < ϵ. Proof of Lemma E.1. If Z = ˆZ, we immediately have our conclusion. We may assume Z ̸= ˆZ. Let us define a function g : R → R as g(t) = h \u0010 ˆZ + t(Z − ˆZ) \u0011 . Then g is convex and g′(t) = D ∇h \u0010 ˆZ + t(Z − ˆZ) \u0011 , Z − ˆZ E , g′′(t) = \u0010 Z − ˆZ \u0011⊤ ∇2h \u0010 ˆZ + t(Z − ˆZ) \u0011\u0010 Z − ˆZ \u0011 . Furthermore, for 0 ≤ t ≤ t0 where t0 := ∥ ˆZ∥∞ 2∥Z− ˆZ∥∞ , ˆZ + t(Z − ˆZ) ∈ G, ∴ g′′(t) ≥ µ \r\r\rZ − ˆZ \r\r\r 2 . 68We can conclude g is µ \r\r\rZ − ˆZ \r\r\r 2 -strongly convex in [0, t0]. From strong convexity in [0, t0] and convexity in R, we have (g′(t0) − g′(0))t0 = g′(t0)t0 ≥ µ \r\r\rZ − ˆZ \r\r\r 2 t2 0, (g′(1) − g′(t0))(1 − t0) ≥ 0. If t0 < 1, we have ∥∇h(Z)∥ \r\r\rZ − ˆZ \r\r\r ≥ D ∇h(Z), Z − ˆZ E = g′(1) ≥ g′(t0) ≥ µ \r\r\rZ − ˆZ \r\r\r 2 t0, and ∥∇h(Z)∥ ≥µ \r\r\rZ − ˆZ \r\r\rt0 = µ \r\r\rZ − ˆZ \r\r\r \r\r\r ˆZ \r\r\r ∞ 2 \r\r\rZ − ˆZ \r\r\r ∞ ≥ µ \r\r\r ˆZ \r\r\r ∞ 2 , this is contradictory. Thus, we have t0 ≥ 1 and Z ∈ G. From the strong convexity of h(Z) in G, we have µ \r\r\rZ − ˆZ \r\r\r ≤ \r\r\r∇h(Z) − ∇h( ˆZ) \r\r\r = ∥∇h(Z)∥ < µϵ, and we have our conclusion \r\r\rZ − ˆZ \r\r\r < ϵ. E.2.5 Gradient Descent Achieves a Near Stationary Point We will show that LCutMix(W) is a smooth function. Lemma E.2. Suppose the event Einit occurs. CutMix Loss LCutMix(W) is L-smooth with L = 9r−1P σ2 dd. Proof of Lemma E.2. Note that ∇w1 LCutMix(W) = 1 n2 X i,j∈[n] ES∼DS \"\u0012|S| P yiℓ′(yifW (Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjfW (Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j    . Let fW = { ew1, ew−1} and W = {w1, w−1} be any parameters of the neural network fW . For any i, j∈ [n] and S ⊂[P], \u0012|S| P yiℓ′(yif fW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjf fW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D ew1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D ew1, x(p) j E\u0011 x(p) j   − \u0012|S| P yiℓ′(yifW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjfW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j   = \u0012|S| P yiℓ′(yif fW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjf fW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D ew1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D ew1, x(p) j E\u0011 x(p) j   − \u0012|S| P yiℓ′(yif fW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjf fW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j   69+ \u0012|S| P yiℓ′(yif fW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjf fW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j   − \u0012|S| P yiℓ′(yifW(Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjfW(Xi,j,S)) \u0013 ×  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j  . Since |ℓ′| ≤1, \f\f\f\f |S| P yiℓ′ \u0000 yiffW (Xi,j,S) \u0001 + \u0012 1 − |S| P \u0013 yjℓ′ \u0000 yjffW (Xi,j,S) \u0001\f\f\f\f ≤ 1, and since |ϕ′| ≤1, \r\r\r\r\r\r X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j \r\r\r\r\r\r ≤ P max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r. In addition, since ϕ is r−1-smooth, \r\r\r\r\r\r  X p∈S ϕ′ \u0010D ew1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D ew1, x(p) j E\u0011 x(p) j   −  X p∈S ϕ′ \u0010D w1, x(p) i E\u0011 x(p) i + X p/∈S ϕ′ \u0010D w1, x(p) j E\u0011 x(p) j   \r\r\r\r\r\r ≤ X p∈S \f\f\fϕ′ \u0010D ew1, x(p) i E\u0011 − ϕ′ \u0010D w1, x(p) i E\u0011\f\f\f \r\r\rx(p) i \r\r\r + X p/∈S \f\f\fϕ′ \u0010D ew1, x(p) j E\u0011 − ϕ′ \u0010D w1, x(p) j E\u0011\f\f\f \r\r\rx(p) j \r\r\r ≤ r−1 X p∈S \f\f\f D ew1 − w1, x(p) i E\f\f\f \r\r\rx(p) i \r\r\r + r−1 X p/∈S \f\f\f D ew1 − w1, x(p) j E\f\f\f \r\r\rx(p) j \r\r\r ≤ r−1P \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 ∥ ew1 − w1∥, and since ℓ′ and ϕ are 1-Lipschitz, we have \f\f\f\f\f \u0012|S| P yiℓ′(yiffW (Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjffW (Xi,j,S)) \u0013 − \u0012|S| P yiℓ′(yifW (Xi,j,S)) + \u0012 1 − |S| P \u0013 yjℓ′(yjfW (Xi,j,S)) \u0013\f\f\f\f\f ≤ \f\fffW (Xi,j,S) − fW (Xi,j,S) \f\f ≤ X p∈S \u0010\f\f\f D ew1 − w1, x(p) i E\f\f\f + \f\f\f D ew−1 − w−1, x(p) i E\f\f\f \u0011 + X p/∈S \u0010\f\f\f D ew1 − w1, x(p) j E\f\f\f + \f\f\f D ew−1 − w−1, x(p) j E\f\f\f \u0011 ≤ P max i∈[n],j∈[P] \r\r\rx(p) i \r\r\r(∥ ew1 − w1∥ + ∥ ew−1 − w−1∥) ≤ √ 2P max i∈[n],j∈[P] \r\r\rx(p) i \r\r\r \r\r\rfW − W \r\r\r. 70Therefore, \r\r\r∇w1 LCutMix(fW) − ∇w1 LCutMix(W) \r\r\r ≤ r−1P \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 ∥ ew1 − w1∥ + √ 2P2 \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 \r\r\rfW − W \r\r\r ≤ 2r−1P \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 \r\r\rfW − W \r\r\r, where the last equality is due to (A1) and (A8). In the same way, we can obtain \r\r\r∇w−1 LCutMix(fW) − ∇w−1 LCutMix(W) \r\r\r ≤ 2r−1P \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 \r\r\rfW − W \r\r\r, and \r\r\r∇LCutMix(fW) − ∇LCutMix(W) \r\r\r ≤ 4r−1P \u0012 max i∈[n],p∈[P] \r\r\rx(p) i \r\r\r \u00132 \r\r\rfW − W \r\r\r ≤ 9r−1P σ2 dd \r\r\rfW − W \r\r\r, where the last inequality holds since \r\r\rξ(p) i \r\r\r 2 < 3 2 σ2 dd and α2 ≤ 3 4 σ2 dd due to (A7). Hence, LCutMix(W) is L-smooth with L := 9r−1P σ2 dd. Since our objective function LCutMix(W) is L-smooth and η ≤ 1 L due to (A8), descent lemma (see Lemma 3.4 in Bubeck et al. (2015)) implies LCutMix \u0010 W(t+1) \u0011 − LCutMix \u0010 W(t) \u0011 ≤ −η 2 \r\r\r∇LCutMix \u0010 W(t) \u0011\r\r\r 2 , and by telescoping sum, we have 1 T T−1X t=0 \r\r\r∇LCutMix \u0010 W(t) \u0011\r\r\r 2 ≤ 2LCutMix \u0000 W(0)\u0001 ηT = Θ(1) ηT , (35) for any T >0. Choose ϵ = µβ∥ ˆZ∥∞ polylog(d) . Then from (35), there exists TCutMix ≤ poly(d) η such that \r\r\r∇LCutMix \u0010 W(TCutMix) \u0011\r\r\r ≤ ϵ. From characterization of σmin(J(W)) in Section E.2.1, ϵ ≥ \r\r\r∇LCutMix \u0010 W(TCutMix) \u0011\r\r\r ≥ σmin \u0010 J \u0010 W(TCutMix) \u0011\u0011\r\r\r∇h \u0010 Z(TCutMix) \u0011\r\r\r ≥ β 2 \r\r\r∇h \u0010 Z(TCutMix) \u0011\r\r\r, and thus \r\r\r∇h \u0010 Z(TCutMix) \u0011\r\r\r ≤ 2β−1ϵ = µ · 2∥ ˆZ∥∞ polylog(d). For sufficiently large d, the RHS becomes smaller than µ · ∥ ˆZ∥∞ 4 . Then, by Lemma E.1 we have seen in Section E.2.4, \r\r\rZ(TCutMix) − ˆZ \r\r\r ≤ ∥ ˆZ∥∞ 4 , and thus ϕ \u0010D w(TCutMix) yi , x(p) i E\u0011 − ϕ \u0010D w(TCutMix) −yi , x(p) i E\u0011 = Θ(1), for all i ∈ [n] and p ∈ [P], and therefore it reaches perfect training accuracy. 71E.2.6 Test Accuracy of Solution Found by Gradient Descent The final step is showing that W(TCutMix) reaches almost perfect test accuracy. From the results of Section E.2.5, we have ϕ \u0010D w(TCutMix) s , vs,k E\u0011 − ϕ \u0010D w(TCutMix) −s , vs,k E\u0011 = Θ(1), ϕ \u0010D w(TCutMix) yi , ξ(p) i E\u0011 − ϕ \u0010D w(TCutMix) −yi , ξ(p) i E\u0011 = Θ(1), for each s ∈ {±1}, k∈ [K], i∈ [n] and p ∈ [P] \\ {p∗ i }. For any u > v, by the mean value theorem, we have β(u − v) ≤ ϕ(u) − ϕ(v) = (u − v)ϕ(u) − ϕ(v) u − v ≤ (u − v). Hence, we have ϕ \u0010D w(TCutMix) s , vs,k E\u0011 − ϕ \u0010D w(TCutMix) −s , vs,k E\u0011 ≤ D w(TCutMix) s − w(TCutMix) −s , vs,k E , D w(TCutMix) s − w(TCutMix) −s , vs,k E ≤ β−1 \u0010 ϕ \u0010D w(TCutMix) s , vs,k E\u0011 − ϕ \u0010D w(TCutMix) −s , vs,k E\u0011\u0011 , and Ω(1) ≤ D w(TCutMix) s − w(TCutMix) −s , vs,k E ≤ O(β−1), for each s ∈ {±1} and k ∈ [K]. Similarly, for all i ∈ [n] and p ∈ [P] \\ {p∗ i }, ϕ \u0010D w(TCutMix) yi , ξ(p) i E\u0011 − ϕ \u0010D w(TCutMix) −yi , ξ(p) i E\u0011 ≤ D w(TCutMix) yi − w(TCutMix) −yi , ξ(p) i E , D w(TCutMix) yi − w(TCutMix) −yi , ξ(p) i E ≤ β−1 \u0010 ϕ \u0010D w(TCutMix) yi , ξ(p) i E\u0011 − ϕ \u0010D w(TCutMix) −yi , ξ(p) i E\u0011\u0011 , and Ω(1) ≤ D w(TCutMix) yi − w(TCutMix) −yi , ξ(p) i E ≤ O(β−1). By Lemma B.3, w(TCutMix) 1 − w(TCutMix) −1 = w(0) 1 − w(0) −1 + X s∈{±1},k∈[K] sγ(s, k)vs,k + X i∈[n],p∈[P]\\{p∗ i } yiρ(i, p) ξ(p) i\r\r\rξ(p) i \r\r\r 2 , where for each s ∈ {±1}, γ(s, 1) = γ(TCutMix) 1 (s, 1) + γ(TCutMix) −1 (s, 1) + α X i∈Fs yi \u0010 ρ(TCutMix) 1 (i, ˜pi) + ρ(TCutMix) −1 (i, ˜pi) \u0011\r\r\rξ(˜pi) i \r\r\r −2 , and γ(s, k) = γ(TCutMix) 1 (s, k) + γ(TCutMix) −1 (s, k), ρ(i, p) = ρ(TCutMix) 1 (i, p) + ρ(TCutMix) −1 (i, p), for each s ∈ {±1}, k∈ [K] \\ {1}, i∈ [n] and p ∈ [P] \\ {p∗ i }. If we choose j ∈ [n], q∈ [P] \\ {p∗ j } such that ρ(j, q) = maxi∈[n],p∈[P]\\{p∗ i } ρ(i, p), then we have D w(TCutMix) yj − w(TCutMix) −yj , ξ(q) j E = D w(0) yj − w(0) −yj , ξ(q) j E + ρ(j, q) + yj X i∈[n],p∈[P]\\{p∗ i } (i,p)̸=(j,q) yiρ(i, p) D ξ(p) i , ξ(q) j E \r\r\rξ(p) i \r\r\r 2 . 72From the event Einit defined in Lemma B.2, (A8), and (8), \f\f\f D w(0) yj − w(0) −yj , ξ(q) j E\f\f\f = o \u0012 1 polylog(d) \u0013 ≤ 1 2 D w(TCutMix) yj − w(TCutMix) −yj , ξ(q) j E , where the inequality holds since D w(TCutMix) yj − w(TCutMix) −yj , ξ(q) j E = Ω(1). In addition, by triangular inequality, we have \f\f\f\f\f\f\f\f X i∈[n],p∈[P]\\{p∗ i } (i,p)̸=(j,q) yiρ(i, p) D ξ(p) i , ξ(q) j E \r\r\rξ(p) i \r\r\r 2 \f\f\f\f\f\f\f\f ≤ X i∈[n],p∈[P]\\{p∗ i } (i,p)̸=(j,q) ρ(i, p) \f\f\f D ξ(p) i , ξ(q) j E\f\f\f \r\r\rξ(p) i \r\r\r 2 ≤ ρ(j, q) eO \u0010 nP σdσ−1 b d−1 2 \u0011 ≤ ρ(j, q) 2 , where the last inequality is due to (9). Hence, 1 3ρ(j, q) ≤ \f\f\f D w(TCutMix) yj − w(TCutMix) −yj , ξ(q) j E\f\f\f ≤ 3ρ(j, q) and we have ρ(j, q) = eO(β−1). Let (X, y) ∼ Dbe a test data with X = \u0000 x(1), . . . ,x(P)\u0001 ∈ Rd×P having feature patch p∗, dominant noise patch ˜p, and feature vector vy,k. We have x(p) ∼ N(0, σ2 bΛ) for each p ∈ [P] \\ {p∗, ˜p} and x(˜p) − αvs,1 ∼ N(0, σ2 dΛ) for some s ∈ {±1}. Therefore, for all p ∈ [P] \\ {p∗, ˜p} \f\f\fϕ \u0010D w(TCutMix) 1 , x(p) E\u0011 − ϕ \u0010D w(TCutMix) −1 , x(p) E\u0011\f\f\f ≤ \f\f\f D w(TCutMix) 1 − w(TCutMix) −1 , x(p) E\f\f\f = \f\f\f D w(0) 1 − w(0) −1, x(p) E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } ρ(i, q) \f\f\f D ξ(q) i , x(p) E\f\f\f \r\r\rξ(q) i \r\r\r 2 ≤ eO \u0010 σ0σbd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 1 polylog(d) \u0013 , (36) with probability at least 1 − o \u0010 1 poly(d) \u0011 due to Lemma B.2. In addition, \f\f\fϕ \u0010D w(TCutMix) 1 , x(˜p) E\u0011 − ϕ \u0010D w(TCutMix) −1 , x(˜p) E\u0011\f\f\f ≤ \f\f\f D w(TCutMix) 1 − w(TCutMix) −1 , x(˜p) E\f\f\f ≤ α \f\f\f D w(TCutMix) 1 − w(TCutMix) −1 , vs,1 E\f\f\f + \f\f\f D w(TCutMix) 1 − w(TCutMix) −1 , x(˜p) − αvs,1 E\f\f\f ≤ αβ−1 \f\f\fϕ \u0010D w(TCutMix) 1 , vs,1 E\u0011 − ϕ \u0010D w(TCutMix) −1 , vs,1 E\u0011\f\f\f + \f\f\f D w(0) 1 − w(0) −1, x(˜p) − αvs,1 E\f\f\f + X i∈[n],q∈[P]\\{p∗ i } ρ(i, q) \f\f\f D ξ(q) i , x(˜p) − αvs,1 E\f\f\f \r\r\rξ(q) i \r\r\r 2 ≤ eO \u0000 αβ−1\u0001 + eO \u0010 σ0σdd 1 2 \u0011 + eO \u0010 nP β−1σdσ−1 b d−1 2 \u0011 = o \u0012 1 polylog(d) \u0013 , (37) with probability at least 1 − o \u0010 1 poly(d) \u0011 , where the last equality is due to (8), (9), (10), and (A8). 73Suppose (36) and (37) holds. Then, yfW(TCutMix) (X) = \u0010 ϕ \u0010D w(TCutMix) y , vy,k E\u0011 − ϕ \u0010D w(TCutMix) −y , vy,k E\u0011\u0011 + X p∈[P]\\{p∗} \u0010 ϕ \u0010D w(TCutMix) y , x(p) E\u0011 − ϕ \u0010D w(TCutMix) −y , x(p) E\u0011\u0011 = Ω(1) − o \u0012 1 polylog(d) \u0013 > 0. Hence, we have our conclusion. □ 74F Technical Lemmas In this section, we introduce technical lemmas that are used for proving the main theorems. We present their proofs here for better readability. The following lemma is used in Section C.2.4 and Section D.2.4: Lemma F.1. For any z, δ∈ R, |ϕ(z) − (z + δ)ϕ′(z)| ≤r + |δ|. Proof of Lemma F .1. ϕ(z) − zϕ′(z) =    z − 1−β 2 r − z = −1−β 2 r = −1−β 2 r if z ≥ r 1−β 2r z2 + βz − \u0010 1−β r z + β \u0011 z = 1−β 2r z2 if 0 ≤ z ≤ r βz − βz = 0 if z <0 , and we obtain |ϕ(z) − (z + δ)ϕ′(z)| ≤ |ϕ(z) − zϕ′(z)| + |δ|ϕ′(z) ≤ 1 − β 2 r + |δ| ≤r + |δ|. The following lemma is used in Section C.2.4. Lemma F.2. Suppose Einit occurs. Then, for any model parameter W = {w1, w−1}, we have \r\r\r\r\r\r ∇W X i∈Vs,k ℓ (yifW (Xi)) \r\r\r\r\r\r 2 ≤ 8P2σ2 dd|Vs,k| X i∈Vs,k ℓ(yifW (Xi)), for each s ∈ {±1} and k ∈ [K]. Proof of Lemma F .2.For each s ∈ {±1} and i ∈ [n], we have ∥∇wsfW (Xi)∥ = \r\r\r\r\r\r X p∈[P] ϕ′ \u0010D ws, x(p) i E\u0011 x(p) i \r\r\r\r\r\r ≤ P max p∈[P] \r\r\rx(p) i \r\r\r ≤ 2P σdd 1 2 , where the inequality is due to the condition from the event Einit defined in Lemma B.2 and (A7).. Therefore, for each s ∈ {±1}, we have \r\r\r\r\r\r ∇ws X i∈Vs,k ℓ (yifW (Xi)) \r\r\r\r\r\r 2 = \r\r\r\r\r\r X i∈Vs,k ℓ′ (yifW (Xi)) ∇wsfW (Xi) \r\r\r\r\r\r 2 ≤   X i∈Vs,k ℓ′ (yifW (Xi)) ∥∇wsfW (Xi)∥   2 ≤ 4P2σ2 dd   X i∈Vs,k ℓ′ (yifW (Xi))   2 ≤ 4P2σ2 dd|Vs,k| X i∈Vs,k (ℓ′ (yifW (Xi)))2 ≤ 4P2σ2 dd|Vs,k| X i∈Vs,k ℓ (yifW (Xi)) . The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to 0 ≤ −ℓ′ ≤ 1, which can be used to show (ℓ′)2 ≤ −ℓ′ ≤ ℓ. As a result, we have our conclusion:\r\r\r\r\r\r ∇W X i∈Vs,k ℓ (yifW (Xi)) \r\r\r\r\r\r 2 = \r\r\r\r\r\r ∇w1 X i∈Vs,k ℓ (yifW (Xi)) \r\r\r\r\r\r 2 + \r\r\r\r\r\r ∇w−1 X i∈Vs,k ℓ (yifW (Xi)) \r\r\r\r\r\r 2 75≤ 8P2σ2 dd|Vs,k| X i∈Vs,k ℓ(yifW (Xi)). The following lemma is used in Section D.2.4. Lemma F.3. Suppose Einit occurs. Then, for any model parameter W = {w1, w−1}, we have \r\r\r\r\r\r ∇ X i∈Vs,k EC∼DC[ℓ (yifW(t) (Xi,C))] \r\r\r\r\r\r 2 ≤ 8P2σ2 dd|Vs,k| X i∈Vs,k EC∼DC[ℓ(yifW(t) (Xi,C))] for each s ∈ {±1} and k ∈ [K]. Proof of Lemma F .3.For each s ∈ {±1}, i ∈ [n] and C ⊂[P] with |C| = C, we have ∥∇wsfW (Xi,C)∥ = \r\r\r\r\r\r X p/∈C ϕ′ \u0010D ws, x(p) i E\u0011 x(p) i \r\r\r\r\r\r ≤ P max p∈[P] \r\r\rx(p) i \r\r\r ≤ 2P σdd 1 2 , where the inequality is due to the condition from the event Einit defined in Lemma B.2 and (A7). Therefore, for any s ∈ {±1}, we have \r\r\r\r\r\r ∇ws X i∈Vs,k EC∼DC[ℓ (yifW (Xi,C))] \r\r\r\r\r\r 2 = \r\r\r\r\r\r X i∈Vs,k EC∼DC[ℓ′ (yifW (Xi,C)) ∇wsfW (Xi,C)] \r\r\r\r\r\r 2 ≤   X i∈Vs,k EC∼DC [ℓ′ (yifW (Xi,C)) ∥∇wsfW (Xi,C)∥]   2 ≤ 4P2σ2 dd   X i∈Vs,k EC∼DC [ℓ′ (yifW (Xi,C))]   2 ≤ 4P2σ2 dd|Vs,k| X i∈Vs,k EC∼DC h (ℓ′ (yifW (Xi,C)))2i ≤ 4P2σ2 dd|Vs,k| X i∈Vs,k EC∼DC[ℓ (yifW (Xi,C))]. The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to 0 ≤ −ℓ′ ≤ 1, which can be used to show (ℓ′)2 ≤ −ℓ′ ≤ ℓ. As a result, we have our conclusion: \r\r\r\r\r\r ∇W X i∈Vs,k EC∼DC [ℓ (yifW (Xi,C))] \r\r\r\r\r\r 2 = \r\r\r\r\r\r ∇w1 X i∈Vs,k EC∼DC [ℓ (yifW (Xi,C))] \r\r\r\r\r\r 2 + \r\r\r\r\r\r ∇w−1 X i∈Vs,k EC∼DC [ℓ (yifW (Xi,C))] \r\r\r\r\r\r 2 ≤ 8P2σ2 dd|Vs,k| X i∈Vs,k EC∼DC [ℓ(yifW (Xi,C))] . 76The following lemma guarantees the existence and characterizes the minimum of the CutMix loss in Section E.2.2. Lemma F.4. Suppose the event Einit occurs. Let g1, g−1 : R × R → R be defined as gs(z1, z−1) := |Vs| |V−s|ℓ′(P zs) + 2 P ES∼DS [|S|ℓ′(|S|zs − (P − |S|)z−s)] + P − 1 3P , for each s ∈ {±1}. There exist unique z∗ 1, z∗ −1 > 0 such that g1(z∗ 1, z∗ −1) = g−1(z∗ 1, z∗ −1) = 0 . Furthermore, we have z∗ 1, z∗ −1 = Θ(1). Proof of Lemma F .4.For each z1 > 0, g−1(z1, 0) = \u0012|V−1| |V1| + 1 \u0013 · \u0012 −1 2 \u0013 + 2 P ES∼DS [|S|ℓ′(−(P − |S|)z1)] + P − 1 3P < \u0012|V−1| |V1| + 1 \u0013 · \u0012 −1 2 \u0013 + P − 1 3P < 0, since ℓ′(z) ≤ −1 2 for any z ≤ 0 and we use 25 52 n ≤ |V1|, |V−1| ≤27 52 n from the event Einit defined in Lemma B.2. In addition, g−1(z1, P z1 + log 9) = |V−1| |V1| ℓ′(P2z1 + P log 9) + 2 P ES∼DS [|S|ℓ′(|S|P z1 + |S| log 9− (P − |S|)z1)] + P − 1 3P ≥ \u0012|V−1| |V1| + 1 \u0013 ℓ′(log 9) + P − 1 3P > 0, where we use 25 52 n ≤ |V1|, |V−1| ≤27 52 n from the event Einit defined in Lemma B.2 and (A1) for the last inequality. Since z 7→ g−1(z1, z) is strictly increasing and by intermediate value theorem, there exists S : (0, ∞) → (0, ∞) such that z = S(z1) is a unique solution of g−1(z1, z) = 0 and S(z1) < P z1 + log 9. Note that S is strictly increasing since g−1(z1, z−1) is strictly decreasing with respect to z1 and strictly increasing with respect to z−1. Also, if S(z) is bounded above, i.e., there exists some U >0 such that S(z) ≤ U for any z >0, lim z→∞ g−1(z, S(z)) = lim z→∞ \u0012|V−1| |V1| ℓ′ (P S(z)) + 2 P ES∼DS \u0002 |S|ℓ′\u0000 |S|S(z) − (P − |S|)z \u0001\u0003 + P − 1 3P \u0013 ≤ lim z→∞ \u0012|V−1| |V1| ℓ′ (P U) + 2 P ES∼DS \u0002 |S|ℓ′\u0000 |S|U − (P − |S|)z \u0001\u0003 + P − 1 3P \u0013 ≤ −2 P ES∼DS \u0002 |S| ·1 |S|̸=P \u0003 + P − 1 3P = −P − 1 P + 1 + P − 1 3P < 0, and it is contradictory. Hence, we have limz→∞ S(z) = ∞. Let us choose z > 0 such that z = 1 P log   3P \u0010 1 + |V1| |V−1| \u0011 P − 1 − 1  , and thus ℓ′(P z) = − P − 1 3P \u0010 1 + |V1| |V−1| \u0011. We have g1(z, S(z)) = |V1| |V−1|ℓ′(P z) + 2 P ES∼DS \u0002 |S|ℓ′\u0000 |S|z − (P − |S|)S(z) \u0001\u0003 + P − 1 3P 77≤ \u0012 |V1| |V−1| + 1 \u0013 ℓ′(P z) + P − 1 3P = 0. Next, we will prove the existence of z∗ > 0 such that g1(z∗, S(z∗)) > 0. Let us choose ϵ >0 such that ϵ−1 = max ( 3P(P + 1)|V−1| (P − 2)(P + 2)|V1| + 3(P − 1) P − 2 , 3 2 \u0012 1 + P(P + 1)|V−1| (P − 1)(P − 2)|V1| \u0013 , 12P P − 7 \u0012 1 + |V−1| |V1| \u0013 , 12P(P + 1) (P − 2)(P + 2) \u0012 1 + |V1| |V−1| \u0013) , (38) and note that ϵ = Θ(1) . Since limz→∞ S(z) = ∞, we can choose z∗ such that ℓ′ \u00001 2 min {z∗, S(z∗)} \u0001 = −ϵ 2 . Then, for any t ≥ z∗ 2 , we have −ϵ < ℓ′(t) < 0 and − 1 < ℓ′(−t) < −1 + ϵ. (39) From the definition of S and (39) with t = P S(z∗) > 1 2 min{z∗, S(z∗)}, we have ES∼DS \u0002 |S|ℓ′\u0000 |S|S(z∗) − (P − |S|)z∗\u0001\u0003 = −P 2 \u0012|V−1| |V1| ℓ′\u0000 P S(z∗) \u0001 + P − 1 3P \u0013 < −P − 1 6 + P|V−1| 2|V1| ϵ. (40) If S(z∗) − (P − 1)z∗ ≥ 0, then P S(z∗) > (P − 1)S(z∗) − z∗ > . . . >2S(z∗) − (P − 2)z∗ = z∗ + S(z∗) + S(z∗) − (P − 1)z∗ ≥ z∗ + S(z∗) ≥ 1 2 min{z∗, S(z∗)}, and we have −P − 1 6 + P|V−1| 2|V1| ϵ >ES∼DS \u0002 |S|ℓ′\u0000 |S|S(z∗) − (P − |S|)z∗\u0001\u0003 = 1 P + 1   ℓ′\u0000 S(z∗) − (P − 1)z∗\u0001 + PX m=2 mℓ′\u0000 mS(z∗) + (P − m)z∗\u0001 ! ≥ 1 P + 1 \u0012 −1 2 − \u0012P(P + 1) 2 − 1 \u0013 ϵ \u0013 , where the last inequality is due to (39). This is contradictory to (38), especially the first term inside the maximum, and we have S(z∗) − (P − 1)z∗ < 0. In addition, if (P − 1)S(z∗) − z∗ ≤ 0, then P z∗ > (P − 1)z∗ − S(z∗) > . . . >2z∗ − (P − 2)S(z∗) = z∗ + S(z∗) + z∗ − (P − 1)S(z∗) ≥ z∗ + S(z∗) ≥ 1 2 min{z∗, S(z∗)}, and we have − P − 1 6 − P|V−1| 2|V1| ϵ < ES∼DS \u0002 |S|ℓ′\u0000 |S|S(z∗) − (P − |S|)z∗\u0001\u0003 = 1 P + 1   P ℓ′\u0000 P S(z∗) \u0001 + (P − 1)ℓ′\u0000 (P − 1)S(z∗) − z∗\u0001 + P−2X m=0 mℓ′\u0000 mS(z∗) − (P − m)z∗\u0001 ! < 1 P + 1 \u0012 −(P − 1)(P − 2) 2 (1 − ϵ) − P − 1 2 \u0013 , 78where the last inequality is due to (39). This is contradictory to (38), especially the second term inside the maximum, and we have (P − 1)S(z∗) − z∗ > 0. Note that we have −ϵ 2 = ℓ′ \u00121 2 min{z∗, S(z∗)} \u0013 ≥ ℓ′ \u0012 z∗ 2P \u0013 , and since ϵ = Θ(1) in (38), we have z∗ ≤ 2P log \u00002 ϵ − 1 \u0001 = O(1). Thus, we have S(z∗) −(P −1)z∗ < 0 < (P −1)S(z∗) −z∗ < P S(z∗). One can consider dividing the interval [S(z∗) − (P − 1)z∗, P S(z∗)] into a grid of length z∗ + S(z∗). Then, the interval is equally divided into P − 1 sub-intervals and 0 belongs to one of them. In other words, there exists k ∈ [P − 2] such that kS(z∗) − (P − k)z∗ ≤ 0 < (k + 1)S(z∗) − (P − k − 1)z∗, and note that if P = 3, then k = 1. The rest of the proof is divided into two cases: (k + 1)S(z∗) − (P − k − 1)z∗ ≥ 1 2 (z∗ + S(z∗)) or (k + 1)S(z∗) − (P − k − 1)z∗ < 1 2 (z∗ + S(z∗)). In both cases, we show that g1(z∗, S(z∗)) > 0. Case 1: (k + 1)S(z∗) − (P − k − 1)z∗ ≥ 1 2 (z∗ + S(z∗)) From (39), we have −1 < ℓ′(−P z∗) < ··· < ··· < ℓ′\u0000 (k − 1)S(z∗) − (P − k + 1)z∗\u0001 < −1 + ϵ, and −ϵ < ℓ′\u0000 (k + 1)S(z∗) − (P − k − 1)z∗\u0001 < ··· < ℓ′\u0000 P S(z∗) \u0001 < 0. Thus, we have ES∼DS \u0002 |S|ℓ′\u0000 |S|S(z∗) − (P − |S|)z∗) \u0001 | \u0003 > 1 P + 1 \u0012 kℓ′\u0000 kS(z∗) − (P − k)z∗\u0001 − k(k − 1) 2 \u0013 − P 2 ϵ. and we obtain k >P−1 2 since k(k + 1) 2 = k(k − 1) 2 + k > −P(P + 1) 2 ϵ − (P + 1)ES∼DS [|S|ℓ′(|S|S(z∗) − (P − |S|)z∗)] + kℓ′(kS(z∗) − (P − k)z∗) + k > −P(P + 1) 2 \u0012 1 + |V−1| |V1| \u0013 ϵ + (P − 1)(P + 1) 6 ≥ P−1 2 \u0000P−1 2 + 1 \u0001 2 , where the second inequality is due to (40) and the fact that ℓ′ ≥ −1, and the last inequality is due to (38), especially the third term inside the maximum. Note that since k ∈ N, k ≥ P 2 . Note that from (39), we have −1 < ℓ′\u0000 − P S(z∗) \u0001 < ··· < ℓ′\u0000 (P − k − 1)z∗ − (k + 1)S(z∗) \u0001 < −1 + ϵ, and −ϵ < ℓ′\u0000 (P − k + 1)z∗ − (k − 1)z∗\u0001 < ··· < ℓ′(P z∗) < 0. Hence, we obtain ES∼DS \u0002 |S|ℓ′\u0000 |S|z∗ − (P − |S|)S(z∗) \u0001\u0003 ≥ 1 P + 1 \u0012 −(P − k − 1)(P − k) 2 − 1 2(P − k) − ((P − k + 1) +··· + P) ϵ \u0013 79≥ −(P − k)2 2(P + 1) − 1 P + 1 · P(P + 1) 2 ϵ ≥ − P2 8(P + 1) − P 2 ϵ, where we use k ≥ P 2 for the last inequality. Therefore, we have g1(z∗, S(z∗)) = |V1| |V−1|ℓ′(P z∗) + 2 P ES∼DS [|S|ℓ′(|S|z∗ − (P − |S|)S(z∗))] + P − 1 3P ≥ − \u0012 |V1| |V−1| + 1 \u0013 ϵ − P 4(P + 1) + P − 1 3P > 0, where the last inequality is due to (38), especially the fourth term inside the maximum. Case 2: (k + 1)S(z∗) − (P − k − 1)z∗ < 1 2 (z∗ + S(z∗)) In this case, we have kS(z∗) − (P − k)z∗ ≤ −1 2 (z∗ + S(z∗)). From (39), we have −1 < ℓ′(−P z∗) < ··· < ℓ′\u0000 kS(z∗) − (P − k)z∗\u0001 < −1 + ϵ, and −ϵ < ℓ′\u0000 (k + 2)S(z∗) − (P − k − 2)z∗\u0001 < ··· < ℓ′\u0000 P S(z∗) \u0001 < 0. Thus, we have ES∼DS [|S|ℓ′(|S|S(z∗) − (P − |S|)z∗))|] > 1 P + 1 \u0012 (k + 1)ℓ′\u0000 (k + 1)S(z∗) − (P − k − 1)z∗\u0001 − k(k + 1) 2 \u0013 − P 2 ϵ, and we obtain k >P−1 2 since (k + 1)2 2 = k(k + 1) 2 + k + 1 2 > −P(P + 1) 2 ϵ − (P + 1)ES∼DS [|S|ℓ′(|S|S(z∗) − (P − |S|)z∗)] + (k + 1)ℓ′((k + 1)S(z∗) − (P − k − 1)z∗) + k + 1 2 > −P(P + 1) 2 \u0012 1 + |V−1| |V1| \u0013 ϵ + (P − 1)(P + 1) 6 > \u0000P−1 2 + 1 \u00012 2 , where the second inequality is due to (40) and the fact that ℓ′(z) ≥ −1 2 ∀z ≥ 0, and the last inequality is due to our (38), especially the third term inside the maximum. Note that since k ∈ N, we have k ≥ P 2 . Note that from (39), we have −1 < ℓ′\u0000 − P S(z∗) \u0001 < ··· < ℓ′\u0000 (P − k − 2)z∗ − (k + 2)S(z∗) \u0001 < −1 + ϵ, and −ϵ < ℓ′\u0000 (P − k)z∗ − kz∗\u0001 < ··· < ℓ′(P z∗) < 0. Hence, we obtain ES∼DS [|S|ℓ′(|S|z∗ − (P − |S|)S(z∗)] ≥ 1 P + 1 \u0012 −(P − k − 1)(P − k) 2 − ((P − k) + ··· + P) ϵ \u0013 80≥ −(P − k)(P − k − 1) 2(P + 1) − 1 P + 1 · P(P + 1) 2 ϵ ≥ −(P − k)2 2(P + 1) − 1 P + 1 · P(P + 1) 2 ϵ ≥ − P2 8(P + 1) − P 2 ϵ, where we use k ≥ P 2 for the last inequality. Therefore, we have g1(z∗, S(z∗)) = |V1| |V−1|ℓ′(P z∗) + 2 P ES∼DS [|S|ℓ′(|S|z∗ − (P − |S|)S(z∗))] + P − 1 3P ≥ − \u0012 |V1| |V−1| + 1 \u0013 ϵ − P 4(P + 1) + P − 1 3P > 0, where the last inequality is due to (38), especially the fourth term inside the maximum. In both cases, we have g1(z∗, S(z∗)) > 0. By intermediate value theorem, there exist unique z∗ 1, z∗ −1 > 0 such that g1(z∗ 1, z∗ −1) = g−1(z∗ 1, z∗ −1) = 0 . In addition, z ≤ z∗ 1 ≤ z∗ and we have z1 = Θ(1) since z = Ω(1) and z∗ = O(1). By using a similar argument, we can show that z∗ −1 = Θ(1), and we have our conclusion. 81",
      "references": [
        "Towards understanding ensemble, knowledge distillation and self-distillation in deep learning",
        "Convex optimization: Algorithms and complexity",
        "Benign overfitting in two-layer convolutional neural networks",
        "On mixup regularization",
        "Gridmask data augmentation",
        "A group-theoretic framework for data augmentation",
        "Towards understanding the mixture-of-experts layer in deep learning",
        "Why does sharpness-aware minimization generalize better than sgd?",
        "For better or for worse? learning minimum variance features with label augmentation",
        "Towards understanding the data dependency of mixup-style training",
        "Provably learning diverse features in multi-view data with midpoint mixup",
        "A kernel theory of modern data augmentation",
        "Improved regularization of convolutional neural networks with cutout",
        "Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data",
        "Implicit bias in leaky relu networks trained on high-dimensional data",
        "How data augmentation affects optimization for linear regression",
        "Deep residual learning for image recognition",
        "Graph neural networks provably benefit from structural information: A feature learning perspective",
        "Understanding convergence and generalization in federated learning through feature learning theory",
        "Towards understanding how momentum improves generalization in deep learning",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup",
        "Co-mixup: Saliency guided joint mixup with supermodular diversity",
        "Benign overfitting in two-layer relu convolutional neural networks",
        "Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data",
        "mixup: Beyond empirical risk minimization",
        "How does mixup help with robustness and generalization?",
        "When and how mixup improves calibration",
        "Random erasing data augmentation",
        "A unified analysis of mixed sample data augmentation: A loss function perspective",
        "Does data augmentation lead to positive margin?",
        "Data augmentation as feature manipulation",
        "Very deep convolutional networks for large-scale image recognition",
        "High-dimensional probability: An introduction with applications in data science",
        "On the generalization effects of linear transformations in data augmentation",
        "Rethinking data augmentation for image super-resolution: A comprehensive analysis and a new strategy",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "Provable benefit of mixup for finding optimal decision boundaries",
        "The benefits of mixup for feature learning",
        "Understanding the generalization of adam in learning neural networks with proper regularization",
        "Why clean generalization and robust overfitting both happen in adversarial training",
        "Characterizing structural regularities of labeled data in overparameterized models"
      ],
      "meta_data": {
        "arxiv_id": "2410.23672v1",
        "authors": [
          "Junsoo Oh",
          "Chulhee Yun"
        ],
        "published_date": "2024-10-31T06:41:10Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Provides the first provable explanation for why patch-level data augmentations Cutout and CutMix improve generalization. Within a theoretical patch-wise feature-noise model and a 2-layer CNN, authors (1) show vanilla ERM memorizes noise and misses rare features, (2) prove Cutout enables learning of rare features, outperforming ERM, and (3) prove CutMix learns even extremely rare features and attains near-optimal test accuracy. A new loss re-parameterization shows CutMix forces uniform patch activation, explaining its strength.",
        "methodology": "• Patch-wise data model: each sample has one feature patch, one dominant noise patch, and background noise; features split into common, rare, extremely rare.\n• Learner: 2-layer convolutional net with smoothed leaky-ReLU; second-layer weights fixed.\n• Training regimes analysed: (i) ERM (no augmentation), (ii) Cutout (randomly mask C patches), (iii) CutMix (randomly swap patches and mix labels).\n• Analytical tools: feature-noise decomposition to track inner-products; monotone dynamics for ERM/Cutout; novel convex re-parameterization (h(Z)) plus strong convexity around global optimum for CutMix; high-dimensional probability to bound overlaps; derive sample-complexity-dependent theorems giving training/test accuracies.",
        "experimental_setup": "Synthetic data following the theoretical model: d=2000, three patches, 300 samples, feature frequencies 0.8/0.15/0.05, full-batch GD, learning rate 1. Measured evolution of neuron outputs for common/rare/extremely rare features under ERM, Cutout, CutMix.\nReal-data sanity check: CIFAR-10 classification with ResNet-18; ERM vs Cutout vs CutMix trained 200 epochs; test accuracies 95.16%, 96.05%, 96.29%. Additional analysis with CutMix-generated dog/cat blends and C-score statistics to show which samples each method misclassifies.",
        "limitations": "1. Network restricted to a two-layer, two-neuron smoothed leaky-ReLU CNN; results may not transfer to deeper or different architectures.\n2. Data model uses orthogonal feature vectors and Gaussian noises with specific variance hierarchy; real images violate these assumptions.\n3. High-dimensional asymptotics and precise scaling relations between parameters are required; constants not specified.\n4. Analysis shows CutMix also memorizes noise uniformly, which might hurt robustness but is not investigated.\n5. Experiments are small-scale; CIFAR-10 gains are modest and synthetic setup does not test robustness or localization.",
        "future_research_directions": "• Extend theoretical framework to deeper / wider networks and other activations (ReLU, vision transformers).\n• Relax data-generation assumptions (non-orthogonal, correlated features, realistic textures) and study finite-dimensional regimes.\n• Incorporate spatial information into the theory to analyse advanced variants such as PuzzleMix or Co-Mixup.\n• Design new patch-level augmentations that retain CutMix’s feature coverage while reducing noise memorization.\n• Empirically test predicted behaviour on larger datasets (ImageNet) and study effects on robustness, calibration, and transfer learning.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Adversarial AutoAugment",
      "full_text": "arXiv:1912.11188v1  [cs.CV]  24 Dec 2019 Published as a conference paper at ICLR 2020 ADV E R S A R IA L AU TO AU G M E N T Xinyu Zhang Qiang W ang Huawei Huawei zhangxinyu10@huawei.com wangqiang168@huawei.com Jian Zhang Zhao Zhong Huawei Huawei zhangjian157@huawei.com zorro.zhongzhao@huawei.com ABSTRACT Data augmentation (DA) has been widely utilized to improve g eneralization in training deep neural networks. Recently, human-designed d ata augmentation has been gradually replaced by automatically learned augmenta tion policy. Through ﬁnding the best policy in well-designed search space of data augmentation, Au- toAugment (Cubuk et al., 2018) can signiﬁcantly improve val idation accuracy on image classiﬁcation tasks. However, this approach is not co mputationally practi- cal for large-scale problems. In this paper, we develop an ad versarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object an d augmentation pol- icy search loss. The augmentation policy network attempts t o increase the train- ing loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we re use the computation in target network training for policy evaluation, and dispe nse with the retraining of the target network. Compared to AutoAugment, this leads t o about 12× reduc- tion in computing cost and 11× shortening in time overhead on ImageNet. W e show experimental results of our approach on CIF AR-10/CIF A R-100, ImageNet, and demonstrate signiﬁcant performance improvements over state-of-the-art. On CIF AR-10, we achieve a top-1 test error of 1.36%, which is the currently best per- forming single model. On ImageNet, we achieve a leading perf ormance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data. 1 I NTRODUC TI ON Massive amount of data have promoted the great success of dee p learning in academia and industry. The performance of deep neural networks (DNNs) would be impr oved substantially when more su- pervised data is available or better data augmentation meth od is adapted. Data augmentation such as rotation, ﬂipping, cropping, etc., is a powerful technique to increase the amount and diversit y of data. Experiments show that the generalization of a neural networ k can be efﬁciently improved through manually designing data augmentation policies. However, t his needs lots of knowledge of human expert, and sometimes shows the weak transferability acros s different tasks and datasets in practi- cal applications. Inspired by neural architecture search ( NAS)(Zoph & Le, 2016; Zoph et al., 2017; Zhong et al., 2018a;b; Guo et al., 2018), a reinforcement lea rning (RL) (Williams, 1992) method called AutoAugment is proposed by Cubuk et al. (2018), which can automatically learn the aug- mentation policy from data and provide an exciting performa nce improvement on image classiﬁca- tion tasks. However, the computing cost is huge for training and evaluating thousands of sampled policies in the search process. Although proxy tasks, i.e., smaller models and reduced datasets, are taken to accelerate the searching process, tens of thousand s of GPU-hours of consumption are still required. In addition, these data augmentation policies op timized on proxy tasks are not guaranteed to be optimal on the target task, and the ﬁxed augmentation po licy is also sub-optimal for the whole training process. 1Published as a conference paper at ICLR 2020 Policy Network  Target Network  Network  Training Dataset  Pre-process Large Batch  Policy  Search  Minimize  Training Loss  Maximize  Training Loss  Training  Losses  Moving Average  & Normalize  {߬ଵ,߬ଶ,߬ଷ … ,߬ெ} + Sampled Policies  ߬ଵ ߬ଶ ߬ெ ߬ଷ Mini-Batch  … ࣦଵ ࣦଶ ࣦெ … Figure 1: The overview of our proposed method. W e formulate i t as a Min-Max game. The data of each batch is augmented by multiple pre-processing c omponents with sampled policies {τ1, τ 2, · · · , τ M }, respectively. Then, a target network is trained to minimiz e the loss of a large batch, which is formed by multiple augmented instances of th e input batch. W e extract the training losses of a target network corresponding to different augme ntation policies as the reward signal. Fi- nally, the augmentation policy network is trained with the g uideline of the processed reward signal, and aims to maximize the training loss of the target network t hrough generating adversarial policies. In this paper, we propose an efﬁcient data augmentation meth od to address the problems mentioned above, which can directly search the best augmentation poli cy on the full dataset during training a target network, as shown in Figure 1. W e ﬁrst organize the ne twork training and augmentation policy search in an adversarial and online manner. The augme ntation policy is dynamically changed along with the training state of the target network, rather t han ﬁxed throughout the whole training process like normal AutoAugment (Cubuk et al., 2018). Due to reusing the computation in policy evaluation and dispensing with the retraining of the target network, the computing cost and time overhead are extremely reduced. Then, the augmentation pol icy network is taken as an adversary to explore the weakness of the target network. W e augment the da ta of each min-batch with various adversarial policies in parallel, rather than the same data augmentation taken in batch augmentation (BA) (Hoffer et al., 2019). Then, several augmented instanc es of each mini-batch are formed into a large batch for target network learning. As an indicator of t he hardness of augmentation policies, the training losses of the target network are used to guide the po licy network to generate more aggres- sive and efﬁcient policies based on REINFORCE algorithm (Wi lliams, 1992). Through adversarial learning, we can train the target network more efﬁciently an d robustly. The contributions can be summarized as follows: • Our method can directly learn augmentation policies on targ et tasks, i.e., target networks and full datasets, with a quite low computing cost and time ov erhead. The direct policy search avoids the performance degradation caused by the pol icy transfer from proxy tasks to target tasks. • W e propose an adversarial framework to jointly optimize tar get network training and aug- mentation policy search. The harder samples augmented by ad versarial policies are con- stantly fed into the target network to promote robust featur e learning. Hence, the general- ization of the target network can be signiﬁcantly improved. • The experiment results show that our proposed method outper forms previous augmentation methods. For instance, we achieve a top-1 test error of 1.36% with PyramidNet+ShakeDrop (Y amada et al., 2018) on CIF AR-10, which is the state-of-the -art performance. On Ima- geNet, we improve the top-1 accuracy of ResNet-50 (He et al., 2016) from 76.3% to 79.4% without extra data, which is even 1.77% better than AutoAugment (Cubuk et al., 2018). 2Published as a conference paper at ICLR 2020 2 R ELATED WORK Common data augmentation, which can generate extra samples by some label-preserved transforma- tions, is usually used to increase the size of datasets and im prove the generalization of networks, such as on MINST , CIF AR-10 and ImageNet (Krizhevsky et al., 2012; W an et al., 2013; Szegedy et al., 2015). However, human-designed augmentation policies are speciﬁed for different datasets. For example, ﬂipping, the widely used transformation on CIF AR- 10/CIF AR-100 and ImageNet, is not suitable for MINST , which will destroy the property of origi nal samples. Hence, several works (Lemley et al., 2017; Cubuk et al., 2018 ; Lin et al., 2019; Ho et al., 2019) have attempted to automatically learn data augmentation polici es. Lemley et al. (2017) propose a method called Smart Augmentation, which merges two or more samples of a class to improve the generaliza- tion of a target network. The result also indicates that an au gmentation network can be learned when a target network is being training. Through well designing t he search space of data augmentation policies, AutoAugment (Cubuk et al., 2018) takes a recurren t neural network (RNN) as a sample controller to ﬁnd the best data augmentation policy for a sel ected dataset. T o reduce the computing cost, the augmentation policy search is performed on proxy t asks. Population based augmentation (PBA) (Ho et al., 2019) replaces the ﬁxed augmentation polic y with a dynamic schedule of aug- mentation policy along with the training process, which is m ostly related to our work. Inspired by population based training (PBT) (Jaderberg et al., 2017), t he augmentation policy search problem in PBA is modeled as a process of hyperparameter schedule lea rning. However, the augmentation schedule learning is still performed on proxy tasks. The lea rned policy schedule should be manually adjusted when the training process of a target network is non -matched with proxy tasks. Another related topic is Generative Adversarial Networks ( GANs) (Goodfellow et al., 2014), which has recently attracted lots of research attention due to its fascinating performance, and also been used to enlarge datasets through directly synthesizing new images (Tran et al., 2017; Perez & W ang, 2017; Antoniou et al., 2017; Gurumurthy et al., 2017; Frid-A dar et al., 2018). Although we formu- late our proposed method as a Min-Max game, there exists an ob vious difference with traditional GANs. W e want to ﬁnd the best augmentation policy to perform i mage transformation along with the training process, rather than synthesize new images. Pe ng et al. (2018) also take such an idea to optimize the training process of a target network in human po se estimation. 3 M ETHOD In this section, we present the implementation of Adversarial AutoAugment. First, the motivation for the adversarial relation between network learning and a ugmentation policy is discussed. Then, we introduce the search space with the dynamic augmentation policy. Finally, the joint framework for network training and augmentation policy search is pres ented in detail. 3.1 M OT IV AT IO N S Although some human-designed data augmentations have been used in the training of DNNs, such as randomly cropping and horizontally ﬂipping on CIF AR-10/ CIF AR-100 and ImageNet, limited randomness will make it very difﬁcult to generate effective samples at the tail end of the training. T o struggle with the problem, more randomness about image tr ansformation is introduced into the search space of AutoAugment (Cubuk et al., 2018) (described in Section 3.2). However, the learned policy is ﬁxed for the entire training process. All of possib le instances of each example will be send to the target network repeatedly, which still results i n an inevitable overﬁtting in a long-epoch training. This phenomenon indicates that the learned polic y is not adaptive to the training process of a target network, especially found on proxy tasks. Hence, th e dynamic and adversarial augmentation policy with the training process is considered as the crucia l feature in our search space. Another consideration is how to improve the efﬁciency of the policy search. In AutoAugment (Cubuk et al., 2018), to evaluate the performance of augment ation policies, a lot of child models should be trained from scratch nearly to convergence. The co mputation in training and evaluat- ing the performance of different sampled policies can not be reused, which leads to huge waste of computation resources. In this paper, we propose a computin g-efﬁcient policy search framework through reusing prior computation in policy evaluation. On ly one target network is used to evaluate the performance of different policies with the help of the tr aining losses of corresponding augmented 3Published as a conference paper at ICLR 2020 Epoch 30 Epoch 90 Epoch 120  …...  Epoch 60  …...  …...  …...  TranslateX, 6  Posterize, 5  Solarize, 6  Posterize, 5  Color, 8  Constrast, 8  Cutout, 3  ShearX, 9  Posterize, 5  Cutout, 7  TranslateX, 9  Cutout, 3  Rotate, 7  Color, 5  Equalize, 9  Invert, 8  TranslateX, 6 Posterize, 5  TranslateY, 8  Cutout, 5  Rotate, 9  Sharpness, 7  Equalize, 4  TranslateX, 6  TranslateX, 1 Color, 8  Rotate, 5  Invert, 7  ShearY, 8  TranslateX, 8  AutoContrast, 3  Posterize, 7  Original  ߬ଵ ߬ଶ ߬ଷ ߬ெ …...  Policy  …...  Figure 2: An example of dynamic augmentation policies learn ed with ResNet-50 on ImageNet. With the training process of the target network, harder augm entation policies are sampled to combat overﬁtting. Intuitively, more geometric transformations , such as TranslateX, ShearY and Rotate, are picked in our sampled policies, which is obviously diffe rent from AutoAugment (Cubuk et al., 2018) concentrating on color-based transformations. instances. The augmentation policy network is learned from the intermediate state of the target net- work, which makes generated augmentation policies more agg ressive and adaptive. On the contrary, to combat harder examples augmented by adversarial policie s, the target network has to learn more robust features, which makes the training more efﬁciently. 3.2 S E A RCH SPACE In this paper, the basic structure of the search space of Auto Augment (Cubuk et al., 2018) is re- served. An augmentation policy is deﬁned as that it is compos ed by 5 sub-policies, each sub-policy contains two image operations to be applied orderly, each op eration has two corresponding parame- ters, i.e., the probability and magnitude of the operation. Finally, the 5 best policies are concatenated to form a single policy with 25 sub-policies. For each image i n a mini-batch, only one sub-policy will be randomly selected to be applied. T o compare with Auto Augment (Cubuk et al., 2018) con- veniently, we just slightly modify the search space with rem oving the probability of each operation. This is because that we think the stochasticity of an operati on with a probability requires a certain epochs to take effect, which will detain the feedback of the i ntermediate state of the target network. There are totally 16 image operations in our search space, in cluding ShearX/Y , TranslateX/Y , Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Con trast, Color, Brightness, Sharpness, Cutout (Devries & T aylor, 2017) and Sample Pairing (Inoue, 2018). T he range of the magnitude is also discretized uniformly into 10 values. T o guarantee the convergence during adversarial learning,the magnitude of all the operations are set in a moderate range. 1 Besides, the randomness during the training process is introduced into our search space. Hence , the search space of the policy in each epoch has |S| = (16 ×10)10 ≈ 1. 1×1022 possibilities. Considering the dynamic policy, the number of possible policies with the whole training process can be e xpressed as |S|#epochs. An example of dynamically learning the augmentation policy along with the training process is shown in Figure 2. W e observe that the magnitude (an indication of difﬁculty ) gradually increases with the training process. 1 The more details about the parameter setting please refer to AutoAugment (Cubuk et al., 2018). 4Published as a conference paper at ICLR 2020 3.3 A DV E RS A RIA L LE A RN IN G In this section, the adversarial framework of jointly optim izing network training and augmentation policy search is presented in detail. W e use the augmentatio n policy network A(·, θ) as an adver- sary, which attempts to increase the training loss of the tar get network F(·, w) through adversarial learning. The target network is trained by a large batch form ed by multiple augmented instances of each batch to promote invariant learning (Salazar et al., 20 18), and the losses of different augmen- tation policies applied on the same data are used to train the augmentation policy network by RL algorithm. Considering the target network F(·, w) with a loss function L[F(x, w), y], where each example is transformed by some random data augmentation o(·), the learning process of the target network can be deﬁned as the following minimization problem w∗ = arg min w E x∼ Ω L[F(o(x), w), y], (1) where Ω is the training set, x and y are the input image and the corresponding label, respective ly. The problem is usually solved by vanilla SGD with a learning r ate η and batch size N, and the training procedure for each batch can be expressed as wt+1 = wt − η 1 N N∑ n=1 ∇wL[F(o(xn), w, y n]. (2) T o improve the convergence performance of DNNs, more random and efﬁcient data augmentation is performed under the help of the augmentation policy netwo rk. Hence, the minimization problem should be slightly modiﬁed as w∗ = arg min w E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y], (3) where τ(·) represents the augmentation policy generated by the networ k A(·, θ). Accordingly, the training rule can be rewritten as wt+1 = wt − η 1 M · N M∑ m=1 N∑ n=1 ∇wL[F(τm(xn), w), y n], (4) where we introduce M different instances of each input example augmented by adve rsarial policies {τ1, τ 2, · · · , τ M }. For convenience, we denote the training loss of a mini-batc h corresponding to the augmentation policy τm as Lm = 1 N N∑ n=1 L[F(τm(xn), w), y n]. (5) Hence, we have an equivalent form of Equation 4 wt+1 = wt − η 1 M M∑ m=1 ∇wLm. (6) Note that the training procedure can be regarded as a larger N · M batch training or an average over M instances of gradient computation without changing the lea rning rate, which will lead to a reduction of gradient variance and a faster convergence of t he target network Hoffer et al. (2019). However, overﬁtting will also come. T o overcome the problem , the augmentation policy network is designed to increase the training loss of the target netwo rk with harder augmentation policies. Therefore, we can mathematically express the object as the f ollowing maximization problem θ∗ = arg max θ J(θ), where J(θ) = E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y]. (7) Similar to AutoAugment (Cubuk et al., 2018), the augmentati on policy network is also implemented as a RNN shown in Figure 3. At each time step of the RNN controll er, the softmax layer will predict 5Published as a conference paper at ICLR 2020 Select the  type of Op0  Select the  magnitude  of Op0  Select the  type of Op1  Select the  magnitude  of Op1  Hidden  Layer  Softmax  Layer  Embedding Embedding Embedding Embedding  Embedding  Layer  Figure 3: The basic architecture of the controller for gener ating a sub-policy, which consists of two operations with corresponding parameters, the type and mag nitude of each operation. When a policy contains Q sub-policies, the basic architecture will be repeated Q times. Following the setting of AutoAugment (Cubuk et al., 2018), the number of sub-policie s Q is set to 5 in this paper. an action corresponding to a discrete parameter of a sub-pol icy, and then an embedding of the predicted action will be fed into the next time step. In our ex periments, the RNN controller will predict 20 discrete parameters to form a whole policy. However, there has a severe problem in jointly optimizing ta rget network training and augmentation policy search. This is because that non-differentiable aug mentation operations break gradient ﬂow from the target network F to the augmentation policy network A (W ang et al., 2017; Peng et al., 2018). As an alternative approach, REINFORCE algorithm (Wi lliams, 1992) is applied to optimize the augmentation policy network as ∇θ J(θ) = ∇θ E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y] ≈ ∑ m Lm∇θ pm = ∑ m Lmpm∇θ log pm = E τ ∼A (·, θ ) Lm∇θ log pm ≈ 1 M M∑ m=1 Lm∇θ log pm, (8) where pm represents the probability of the policy τm. T o reduce the variance of gradient ∇θ J(θ), we replace the training loss of a mini-batch Lm with ˆLm a moving average over a certain mini- batches2, and then normalize it among M instances as ˜Lm. Hence, the training procedure of the augmentation policy network can be expressed as ∇θ J(θ) ≈ 1 M M∑ m=1 ˜Lm∇θ log pm, θe+1 = θe + β 1 M M∑ m=1 ˜Lm∇θ log pm, (9) The adversarial learning of target network training and aug mentation policy search is summarized as Algorithm 1. 4 E XPERIME NT S AND ANALYSIS In this section, we ﬁrst reveal the details of experiment set tings. Then, we evaluate our proposed method on CIF AR-10/CIF AR-100, ImageNet, and compare it wit h previous methods. Results in Figure 4 show our method achieves the state-of-the-art perf ormance with higher computing and time efﬁciency 3. 2 The length of the moving average is ﬁxed to an epoch in our expe riments. 3 T o clearly present the advantage of our proposed method, we n ormalize the performance of our method in the Figure 4, and the performance of AutoAugment is plotted a ccordingly . 6Published as a conference paper at ICLR 2020 Algorithm 1 Joint Training of T arget Network and Augmentation Policy Ne twork Initialization: target network F(·, w), augmentation policy network A(·, θ) Input: input examples x, corresponding labels y 1: for 1 ≤ e ≤ epochs do 2: Initialize ˆLm = 0 , ∀m ∈ { 1, 2, · · · , M }; 3: Generate M policies with the probabilities {p1, p 2, · · · , p M }; 4: for 1 ≤ t ≤ T do 5: Augment each batch data with M generated policies, respectively; 6: Update we,t +1 according to Equation 4; 7: Update ˆLm through moving average, ∀m ∈ { 1, 2, · · · , M }; 8: Collect { ˆL1, ˆL2, · · · , ˆLM }; 9: Normalize ˆLm among M instances as ˜Lm, ∀m ∈ { 1, 2, · · · , M }; 10: Update θe+1 via Equation 9; 11: Output w∗ , θ∗ 4.1 E X P E RIM E N T SE T T IN G S The RNN controller is implemented as a one-layer LSTM (Hochr eiter & Schmidhuber, 1997). W e set the hidden size to 100, and the embedding size to 32. W e use Adam optimizer (Kingma & Ba, 2015) with a initial learning rate 0. 00035 to train the controller. T o avoid unexpected rapid conver- gence, an entropy penalty of a weight of 0. 00001 is applied. All the reported results are the mean of ﬁve runs with different initializations. 4.2 E X P E RIM E N T S O N CIFAR-10 A N D CIFAR-100 CIF AR-10 dataset (Krizhevsky & Hinton, 2009) has totally 60 000 images. The training and test sets have 50000 and 10000 images, respectively. Each image i n size of 32 × 32 belongs to one of 10 classes. W e evaluate our proposed method with the fo llowing models: Wide-ResNet- 28-10 (Zagoruyko & Komodakis, 2016), Shake-Shake (26 2x32d ) (Gastaldi, 2017), Shake-Shake (26 2x96d) (Gastaldi, 2017), Shake-Shake (26 2x112d) (Gast aldi, 2017), PyramidNet+ShakeDrop (Han et al., 2017; Y amada et al., 2018). All the models are tra ined on the full training set. T raining details: The Baseline is trained with the standard data augmentation , namely, randomly cropping a part of 32 × 32 from the padded image and horizontally ﬂipping it with a prob ability of 0. 5. The Cutout (Devries & T aylor, 2017) randomly select a 16 × 16 patch of each image, and then set the pixels of the selected patch to zeros. For our met hod, the searched policy is applied in addition to standard data augmentation and Cutout. For each image in the training process, standard data augmentation, the searched policy and Cutout are appli ed in sequence. For Wide-ResNet-28- 10, the step learning rate (LR) schedule is adopted. The cosi ne LR schedule is adopted for the other models. More details about model hyperparameters are suppl ied in A.1. Choice of M: T o choose the optimal M, we select Wide-ResNet-28-10 as a target network, and evaluate the performance of our proposed method verse diffe rent M, where M ∈ { 2, 4, 8, 16, 32}. From Figure 5, we can observe that the test accuracy of the mod el improves rapidly with the increase of M up to 8. The further increase of M does not bring a signiﬁcant improvement. Therefore, to balance the performance and the computing cost, M is set to 8 in all the following experiments. CIF AR-10 results: In T able 1, we report the test error of these models on CIF AR-1 0. For all of these models, our proposed method can achieve better perfor mance compared to previous methods. W e achieve 0. 78% and 0. 68% improvement on Wide-ResNet-28-10 compared to AutoAugment and PBA, respectively. W e achieve a top-1 test error of 1. 36% with PyramidNet+ShakeDrop, which is 0. 1% better than the current state-of-the-art reported in Ho et a l. (2019). As shown in Figure 6(a) and 6(b),we further visualize the probability distributio n of the parameters of the augmentation poli- cies learned with PyramidNet+ShakeDrop on CIF AR-10 over ti me. From Figure 6(a), we can ﬁnd that the percentages of some operations, such as TranslateY , Rotate, Posterize, and SampleParing, gradually increase along with the training process. Meanwh ile, more geometric transformations, such as TranslateX, TranslateY , and Rotate, are picked in th e sampled augmentation policies, which 7Published as a conference paper at ICLR 2020 $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013\u0013 \u0003 2XU\u00030HWKRG  $XWR$XJPHQW  $FFXUDF\\  RQ\u0003,PDJH1HW  $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013 \u0003 &RPSXWLQJ  (IILFLHQF\\  7LPH  (IILFLHQF\\  \u0014\u0013\u0013\b\u0003 \u0014\u0013\u0013\b\u0003\u0014\u0013\u0013\b\u0003 \u0014\u0013 \u0013\b \u0003 \u0014\u0013 \u0013\b \u0003 Figure 4: The Comparison of normalized performance between AutoAugment and our method. Please refer to the following tables for more details. /uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019/uni00000016/uni00000015 /uni00000030 /uni0000001c/uni0000001a/uni00000011/uni00000017 /uni0000001c/uni0000001a/uni00000011/uni00000018 /uni0000001c/uni0000001a/uni00000011/uni00000019 /uni0000001c/uni0000001a/uni00000011/uni0000001a /uni0000001c/uni0000001a/uni00000011/uni0000001b /uni0000001c/uni0000001a/uni00000011/uni0000001c /uni0000001c/uni0000001b/uni00000011/uni00000013 /uni0000001c/uni0000001b/uni00000011/uni00000014 /uni0000001c/uni0000001b/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c Figure 5: The T op-1 test accuracy of Wide- ResNet-28-10 on CIF AR-10 verse different M, where M ∈ { 2, 4, 8, 16, 32}. 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) ShearX ShearY TranslateX TranslateY Rotate AutoContrast Invert Equalize Solarize Posterize SampleParing Cutout Color Constrast Brightness Sharpness (a) Operations 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) 9 8 7 6 5 4 3 2 1 0 (b) Magnitudes Figure 6: Probability distribution of the parameters in the learned augmentation policies on CIF AR- 10 over time. The number in (b) represents the magnitude of on e operation. Larger number stands for more dramatic image transformations. The probability d istribution of each parameter is the mean of each ﬁve epochs. is different from color-focused AutoAugment (Cubuk et al., 2018) on CIF AR-10. Figure 6(b) shows that large magnitudes gain higher percentages during train ing. However, at the tail of training, low magnitudes remain considerable percentages. This indicat es that our method does not simply learn the transformations with the extremes of the allowed magnit udes to spoil the target network. CIF AR-100 results: W e also evaluate our proposed method on CIF AR-100, as shown i n T able 2. As we can observe from the table, we also achieve the state-of -the-art performance on this dataset. 4.3 E X P E RIM E N T S O N IM AG E NE T As a great challenge in image recognition, ImageNet dataset (Deng et al., 2009) has about 1.2 mil- lion training images and 50000 validation images with 1000 c lasses. In this section, we directly search the augmentation policy on the full training set and t rain ResNet-50 (He et al., 2016), ResNet- 50-D (He et al., 2018) and ResNet-200 (He et al., 2016) from sc ratch. T raining details: For the baseline augmentation, we randomly resize and crop e ach input image to a size of 224 × 224, and then horizontally ﬂip it with a probability of 0. 5. For AutoAugment (Cubuk et al., 2018) and our method, the baseline augmentati on and the augmentation policy are both used for each image. The cosine LR schedule is adopted in the training process. The model hyperparameters on ImageNet is also detailed in A.1. ImageNet results: The performance of our proposed method on ImageNet is presen ted in T able 3. It can be observed that we achieve a top-1 accuracy 79. 40% on ResNet-50 without extra data. T o 8Published as a conference paper at ICLR 2020 T able 1: T op-1 test error (%) on CIF AR-10. W e replicate the re sults of Baseline, Cutout and Au- toAugment methods from Cubuk et al. (2018), and the results o f PBA from Ho et al. (2019) in all of our experiments. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 3.87 3.08 2.68 2.58 1.90±0.15 Shake-Shake (26 2x32d) 3.55 3.02 2.47 2.54 2.36±0.10 Shake-Shake (26 2x96d) 2.86 2.56 1.99 2.03 1.85±0.12 Shake-Shake (26 2x112d) 2.82 2.57 1.89 2.03 1.78±0.05 PyramidNet+ShakeDrop 2.67 2.31 1.48 1.46 1.36±0.06 T able 2: T op-1 test error (%) on CIF AR-100. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 18.80 18.41 17.09 16.73 15.49±0.18 Shake-Shake (26 2x96d) 17.05 16.00 14.28 15.31 14.10±0.15 PyramidNet+ShakeDrop 13.99 12.19 10.67 10.94 10.42±0.20 the best of our knowledge, this is the highest top-1 accuracy for ResNet-50 learned on ImageNet. Besides, we only replace the ResNet-50 architecture with Re sNet-50-D, and achieve a consistent improvement with a top-1 accuracy of 80. 00%. T able 3: T op-1 / T op-5 test error (%) on ImageNet. Note that th e result of ResNet-50-D is achieved only through substituting the architecture. Model Baseline AutoAugment PBA Our Method ResNet-50 23.69 / 6.92 22.37 / 6.18 - 20.60±0.15 / 5.53±0.05 ResNet-50-D 22.84 / 6.48 - - 20.00±0.12 / 5.25±0.03 ResNet-200 21.52 / 5.85 20.00 / 4.90 - 18.68±0.18 / 4.70±0.05 4.4 A BL AT IO N ST U DY T o check the effect of each component in our proposed method, we report the test error of ResNet-50 on ImageNet the following augmentation methods in T able 4. • Baseline: Training regularly with the standard data augmentation an d step LR schedule. • Fixed: Augmenting all the instances of each batch with the standar d data augmentation ﬁxed throughout the entire training process. • Random: Augmenting all the instances of each batch with randomly an d dynamically generated policies. • Ours: Augmenting all the instances of each batch with adversaria l policies sampled by the policy network along with the training process. From the table, we can ﬁnd that Fixed can achieve 0. 99% error reduction compared to Baseline. This shows that a large-batch training with multiple augmen ted instances of each mini-batch can indeed improve the generalization of the model, which is con sistent with the conclusion presented in Hoffer et al. (2019). In addition, the test error of Random is 1. 02% better than Fixed. This indicates that augmenting batch with randomly generated policies can reduce overﬁtting in a certain extent. Furthermore, our method achieves the best test error of 20. 60% through augmenting samples with adversarial policies. From the result, we can conclude that these policies generated by the policy network are more adaptive to the training process, and make t he target network have to learn more robust features. 4.5 C O M P U T IN G CO S T A N D TIM E OV E RH E A D Computing Cost: The computation in target network training is reused for pol icy evaluation. This makes the computing cost in policy search become negligible . Although there exists an increase of 9Published as a conference paper at ICLR 2020 T able 4: T op-1 test error (%) of ResNet-50 with different aug mentation methods on ImageNet. Method Aug. Policy Enlarge Batch LR Schedule T est Error Baseline standard M = 1 step 23.69 Fixed standard M = 8 cosine 22.70 Random random M = 8 cosine 21.68 Ours adversarial M = 8 cosine 20.60 computing cost in target network training, the total comput ing cost in training one target network with augmentation policies is quite small compared to prior work. Time Overhead: Since we just train one target network with a large batch dist ributedly and simul- taneously, the time overhead of the large-batch training is equal to the regular training. Meanwhile, the joint optimization of target network training and augme ntation policy search dispenses with the process of ofﬂine policy search and the retraining of a targe t network, which leads to a extreme time overhead reduction. In T able 5, we take the training of ResNet-50 on ImageNet as an example to compare the computing cost and time overhead of our method and AutoAugment. From th e table, we can ﬁnd that our method is 12× less computing cost and 11× shorter time overhead than AutoAugment. T able 5: The comparison of computing cost (GPU hours) and tim e overhead (days) in training ResNet-50 on ImageNet between AutoAugment and our method. T he computing cost and time overhead are estimated on 64 NVIDIA T esla V100s. Method Computing Cost Time Overhead Searching Training T otal Searching Training T otal AutoAugment 15000 160 15160 10 1 11 Our Method ∼0 1280 1280 ∼0 1 1 4.6 T RA N S F E RA BIL IT Y ACRO S S DATA S E T S A N D ARCH IT E CT U RE S T o further show the higher efﬁciency of our method, the trans ferability of the learned augmentation policies is evaluated in this section. W e ﬁrst take a snapsho t of the adversarial training process of ResNet-50 on ImageNet, and then directly use the learned dyn amic augmentation policies to regu- larly train the following models: Wide-ResNet-28-10 on CIF AR-10/100, ResNet-50-D on ImageNet and ResNet200 on ImageNet. T able 6 presents the experimenta l results of the transferability. From the table, we can ﬁnd that a competitive performance can be st ill achieved through direct policy transfer. This indicates that the learned augmentation policies tra nsfer well across datasets and architectures. However, compared to the proposed method, t he policy transfer results in an obvious performance degradation, especially the transfer across d atasets. T able 6: T op-1 test error (%) of the transfer of the augmentat ion policies learned with ResNet-50 on ImageNet. Method Dataset AutoAugment Our Method Policy Transfer Wide-ResNet-28-10 CIF AR-10 2.68 1.90 2.45±0.13 Wide-ResNet-28-10 CIF AR-100 17.09 15.49 16.48±0.15 ResNet-50-D ImageNet - 20.00 20.20±0.05 ResNet-200 ImageNet 20.00 18.68 19.05±0.10 5 C ONCLUSIO N In this paper, we introduce the idea of adversarial learning into automatic data augmentation. The policy network tries to combat the overﬁtting of the target n etwork through generating adversarial policies with the training process. T o oppose this, robust f eatures are learned in the target network, which leads to a signiﬁcant performance improvement. Meanw hile, the augmentation policy search is performed along with the training of a target network, and the computation in network training is reused for policy evaluation, which can extremely reduce the search cost and make our method more computing-efﬁcient. 10Published as a conference paper at ICLR 2020 REFERENC ES Antreas Antoniou, Amos J. Storkey, and Harrison Edwards. Da ta augmentation generative adver- sarial networks. ICLR, 2017. Ekin D. Cubuk, Barret Zoph, Dandelion Man ´ e, V ijay V asudeva n, and Quoc V . Le. Autoaugment: Learning augmentation policies from data. CVPR, 2018. Jia Deng, W ei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li F ei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. T errance Devries and Graham W . T aylor. Improved regulariza tion of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017. Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldber ger, and Hayit Greenspan. Synthetic data augmentation using GAN for improved liver lesion class iﬁcation. IEEE International Sym- posium on Biomedical Imaging (ISBI), 2018. Xavier Gastaldi. Shake-shake regularization. CoRR, abs/1705.07485, 2017. Ian J. Goodfellow , Jean Pouget-Abadie, Mehdi Mirza, Bing Xu , David W arde-Farley, Sherjil Ozair, Aaron Courville, and Y oshua Bengio. Generative adversaria l networks. NIPS, 2014. Minghao Guo, Zhao Zhong, W ei Wu, Dahua Lin, and Junjie Y an. IR LAS: inverse reinforcement learning for architecture search. CoRR, abs/1812.05285, 2018. Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and V enkatesh Babu Radhakrishnan. Deli- gan : Generative adversarial networks for diverse and limit ed data. CVPR, 2017. Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal resi dual networks. CVPR, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep r esidual learning for image recog- nition. CVPR, 2016. T ong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. CoRR, abs/1812.01187, 2018. Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen . Population based augmentation: Efﬁcient learning of augmentation policy schedules. ICML, 2019. Sepp Hochreiter and Jrgen Schmidhuber. Long short-term mem ory. Neural Computation, 1997. Elad Hoffer, T al Ben-Nun, Itay Hubara, Niv Giladi, T orsten H oeﬂer, and Daniel Soudry. Augment your batch: better training with larger batches. CoRR, abs/1901.09335, 2019. Hiroshi Inoue. Data augmentation by pairing samples for ima ges classiﬁcation. CoRR, abs/1801.02929, 2018. Max Jaderberg, V alentin Dalibard, Simon Osindero, W ojciec h M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol V inyals, Tim Green, Iain Dunning, Karen Simon yan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural net works. CoRR, abs/1711.09846, 2017. Diederik P . Kingma and Jimmy Ba. Adam: A method for stochasti c optimization. ICLR, 2015. Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple l ayers of features from tiny images. T echnical report, University of T oronto, 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im agenet classiﬁcation with deep convo- lutional neural networks. NIPS, 2012. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation - learning an optimal data augmentation strategy. CoRR, abs/1703.08383, 2017. Chen Lin, Minghao Guo, Chuming Li, W ei Wu, Dahua Lin, W anli Ou yang, and Junjie Y an. Online hyper-parameter learning for auto-augmentation strategy . CoRR, abs/1905.07373, 2019. 11Published as a conference paper at ICLR 2020 Xi Peng, Zhiqiang T ang, Fei Y ang, Rog ´ erio Schmidt Feris, an d Dimitris N. Metaxas. Jointly op- timize data augmentation and network training: Adversaria l data augmentation in human pose estimation. CVPR, 2018. Luis Perez and Jason W ang. The effectiveness of data augment ation in image classiﬁcation using deep learning. CoRR, abs/1712.04621, 2017. Julian Salazar, Davis Liang, Zhiheng Huang, and Zachary C. L ipton. Invariant representation learn- ing for robust deep networks. NeurIPS W orkshop, 2018. Christian Szegedy, W ei Liu, Y angqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, V incent V anhoucke, and Andrew Rabinovich. G oing deeper with convolutions. CVPR, 2015. T oan Tran, Trung Pham, Gustavo Carneiro, Lyle J. Palmer, and Ian D. Reid. A bayesian data augmentation approach for learning deep models. NIPS, 2017. Li W an, Matthew Zeiler, Sixin Zhang, Y ann LeCun, and Rob Ferg us. Regularization of neural networks using dropconnect. ICML, 2013. Xiaolong W ang, Abhinav Shrivastava, and Abhinav Gupta. A-f ast-rcnn: Hard positive generation via adversary for object detection. CVPR, 2017. Ronald J. Williams. Simple statistical gradient-followin g algorithms for connectionist reinforcement learning. Machine Learning, 1992. Y oshihiro Y amada, Masakazu Iwamura, and Koichi Kise. Shake drop regularization. CoRR, abs/1802.02375, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networ ks. British Machine V ision Confer- ence, 2016. Zhao Zhong, Junjie Y an, and Cheng-Lin Liu. Practical networ k blocks design with Q-learning. CVPR, 2018a. Zhao Zhong, Zichen Y ang, Boyang Deng, Junjie Y an, W ei Wu, Jin g Shao, and Cheng-Lin Liu. BlockQNN: Efﬁcient block-wise neural network architectur e generation. CoRR, abs/1808.05584, 2018b. Barret Zoph and Quoc V . Le. Neural architecture search with r einforcement learning. ICLR, 2016. Barret Zoph, V ijay V asudevan, Jonathon Shlens, and Quoc V . L e. Learning transferable architectures for scalable image recognition. CVPR, 2017. A A PPENDIX A.1 H Y P E RPA RA M E T E RS W e detail the model hyperparameters on CIF AR-10/CIF AR-100 and ImageNet in T able 7. 12Published as a conference paper at ICLR 2020 T able 7: Model hyperparameters on CIF AR-10/CIF AR-100 and I mageNet. LR represents learning rate, and WD represents weight decay. W e do not speciﬁcally t une these hyperparameters, and all of these are consistent with previous works, expect for the n umber of epochs. Dataset Model Batch Size (N · M) LR WD Epoch CIF AR-10 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-10 Shake-Shake (26 2x32d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x96d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x112d) 128 · 8 0.2 1e-4 600 CIF AR-10 PyramidNet+ShakeDrop 128 · 8 0.1 1e-4 600 CIF AR-100 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-100 Shake-Shake (26 2x96d) 128 · 8 0.1 5e-4 1200 CIF AR-100 PyramidNet+ShakeDrop 128 · 8 0.5 1e-4 1200 ImageNet ResNet-50 2048 · 8 0.8 1e-4 120 ImageNet ResNet-50-D 2048 · 8 0.8 1e-4 120 ImageNet ResNet-200 2048 · 8 0.8 1e-4 120 13",
      "references": [
        "Data augmentation generative adversarial networks",
        "Autoaugment: Learning augmentation policies from data",
        "Imagenet: A large-scale hierarchical image database",
        "Improved regularization of convolutional neural networks with cutout",
        "Synthetic data augmentation using GAN for improved liver lesion classification",
        "Shake-shake regularization",
        "Generative adversarial networks",
        "IR LAS: inverse reinforcement learning for architecture search",
        "Deligan : Generative adversarial networks for diverse and limited data",
        "Deep pyramidal residual networks",
        "Deep residual learning for image recognition",
        "Bag of tricks for image classification with convolutional neural networks",
        "Population based augmentation: Efficient learning of augmentation policy schedules",
        "Long short-term memory",
        "Augment your batch: better training with larger batches",
        "Data augmentation by pairing samples for images classification",
        "Population based training of neural networks",
        "Adam: A method for stochastic optimization",
        "Learning multiple layers of features from tiny images",
        "Imagenet classification with deep convolutional neural networks",
        "Smart augmentation - learning an optimal data augmentation strategy",
        "Online hyper-parameter learning for auto-augmentation strategy",
        "Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation",
        "The effectiveness of data augmentation in image classification using deep learning",
        "Invariant representation learning for robust deep networks",
        "Going deeper with convolutions",
        "A bayesian data augmentation approach for learning deep models",
        "Regularization of neural networks using dropconnect",
        "A-fast-rcnn: Hard positive generation via adversary for object detection",
        "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "Shake drop regularization",
        "Wide residual networks",
        "Practical network blocks design with Q-learning",
        "BlockQNN: Efficient block-wise neural network architecture generation",
        "Neural architecture search with reinforcement learning",
        "Learning transferable architectures for scalable image recognition"
      ],
      "meta_data": {
        "arxiv_id": "1912.11188v1",
        "authors": [
          "Xinyu Zhang",
          "Qiang Wang",
          "Jian Zhang",
          "Zhao Zhong"
        ],
        "published_date": "2019-12-24T03:17:17Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Adversarial AutoAugment (AAA), an online, computation-efficient data-augmentation policy search that frames augmentation as an adversarial game against the target network. It cuts search cost (~12×) and time (~11×) versus AutoAugment while achieving state-of-the-art accuracies on CIFAR-10/100 and ImageNet (e.g., 79.4% top-1 on ResNet-50).",
        "methodology": "• Min-max formulation: augmentation policy network A(·,θ) seeks to maximize target network loss, while target network F(·,w) minimizes it.\n• Policy network: 1-layer LSTM controller outputs 20 discrete actions that define 5 sub-policies (2 ops each) drawn from 16 transformation types, 10 magnitude bins; no operation probabilities.\n• Training loop: For each mini-batch, M (chosen as 8) different policies generate M augmented versions; these are concatenated into a large batch (N×M) used to update F.\n• Rewards: per-policy training losses L_m collected online; moving-average and normalization applied; policy network updated with REINFORCE.\n• Dynamic/adaptive policies evolve throughout training; no separate proxy task or network retraining, reusing forward/backward passes of F for policy evaluation.",
        "experimental_setup": "Datasets & Models:\n– CIFAR-10/100: Wide-ResNet-28-10, Shake-Shake (26 2×32d, 96d, 112d), PyramidNet+ShakeDrop.\n– ImageNet (ILSVRC-12): ResNet-50, ResNet-50-D, ResNet-200.\nTraining Details:\n– Baseline augment = random crop/flip; Cutout optionally.\n– AAA uses same plus searched policies; M=8; cosine LR except Wide-ResNet (step); epochs 200–1200 CIFAR, 120 ImageNet; batch size 128×8 (CIFAR) / 2048×8 (ImageNet).\nEvaluation Metrics: top-1/ top-5 accuracy on test/validation sets.\nBaselines compared: Baseline, Cutout, AutoAugment, Population-Based Augmentation (PBA), Random and Fixed policy variants.\nAdditional analyses: ablation on policy type, compute/time cost comparison (64 V100s), cross-dataset/architecture transfer experiments.",
        "limitations": "1. Requires large effective batch (factor M), increasing GPU memory and some training compute (1.28k GPU-h for ImageNet vs 160 for baseline).\n2. Search space limited to predefined 16 2-D image ops with discrete magnitudes; excludes probabilities and novel transforms.\n3. Uses high-variance REINFORCE; convergence may be sensitive to hyper-parameters (entropy penalty, learning rates, moving-average window).\n4. Evaluated only on classification; impact on detection, segmentation, or non-vision domains unknown.\n5. Adversarial policies tailored to training loss, not explicitly assessed for robustness to distribution shifts or adversarial attacks.",
        "future_research_directions": "• Reduce memory/compute by adaptive M or gradient sharing; explore differentiable or evolutionary alternatives to REINFORCE.\n• Extend search space to include probabilities, continuous magnitudes, mixup/cutmix, semantic or learned transformations, or generative models.\n• Apply AAA to other tasks: object detection, segmentation, video action recognition, NLP, audio.\n• Investigate policy transfer and meta-learning for few-shot or continual learning scenarios.\n• Study effects on robustness to distribution shift and adversarial examples, potentially integrating with adversarial training.\n• Provide theoretical analysis of convergence and generalization of adversarial augmentation frameworks.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Deep AutoAugment",
      "full_text": "Published as a conference paper at ICLR 2022 DEEP AUTO AUGMENT Yu Zheng1, Zhi Zhang2, Shen Yan1, Mi Zhang1 1Michigan State University, 2Amazon Web Services zhengy30@msu.edu, zhiz@amazon.com, {yanshen6, mizhang}@msu.edu ABSTRACT While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of ﬁxing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching conver- gence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default aug- mentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA. 1 I NTRODUCTION Augmentation Policy Default Transformation  1 (A) Transformation  2 Transformation  2 Transformation  1 Transformation  1 Default Transformation  2 Augmentation Policy Transformation  2 Transformation  2 Transformation  1 Transformation  1 Transformation  K-1 Transformation  K-1 Transformation  K Transformation  K (B) ... Figure 1: (A) Existing automated data augmentation meth- ods with shallow augmentation policy followed by hand- picked transformations. (B) DeepAA with deep augmentation policy with no hand-picked transformations. Data augmentation (DA) is a powerful tech- nique for machine learning since it effec- tively regularizes the model by increas- ing the number and the diversity of data points (Goodfellow et al., 2016; Zhang et al., 2017). A large body of data aug- mentation transformations has been pro- posed (Inoue, 2018; Zhang et al., 2018; DeVries & Taylor, 2017; Yun et al., 2019; Hendrycks et al., 2020; Yan et al., 2020) to improve model performance. While ap- plying a set of well-designed augmentation transformations could help yield consider- able performance enhancement especially in image recognition tasks, manually select- ing high-quality augmentation transforma- tions and determining how they should be combined still require strong domain exper- tise and prior knowledge of the dataset of interest. With the recent trend of automated machine learning (AutoML), data augmen- tation search ﬂourishes in the image domain (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020; Liu et al., 2021), which yields signiﬁcant performance improvement over hand-crafted data augmentation methods. Although data augmentation policies in previous works (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020) contain multiple transformations applied sequentially, only one or two transformations of each sub-policy are found through searching whereas the rest transformations are hand-picked and applied by default in addition to the found policy (Figure 1(A)). From this perspective, we believe that previous automated methods are not entirely automatedas they are still built upon hand-crafted default augmentations. 1 arXiv:2203.06172v2  [cs.CV]  15 Mar 2022Published as a conference paper at ICLR 2022 In this work, we propose Deep AutoAugment (DeepAA), a multi-layer data augmentation search method which aims to remove the need of hand-crafted default transformations (Figure 1(B)). DeepAA fully automates the data augmentation process by searching a deep data augmentation policy on an expanded set of transformations that includes the widely adopted search space and the default transformations (e.g. ﬂips, Cutout, crop). We formulate the search of data augmentation policy as a regularized gradient matching problem by maximizing the cosine similarity of the gradients between augmented data and original data with regularization. To avoid exponential growth of dimensionality of the search space when more augmentation layers are used, we incrementally stack augmentation layers based on the data distribution transformed by all the previous augmentation layers. We evaluate the performance of DeepAA on three datasets – CIFAR-10, CIFAR-100, and ImageNet – and compare it with existing automated data augmentation search methods including AutoAugment (AA) (Cubuk et al., 2019), PBA (Ho et al., 2019), Fast AutoAugment (FastAA) (Lim et al., 2019), Faster AutoAugment (Faster AA) (Hataya et al., 2020), DADA (Li et al., 2020), RandAugment (RA) (Cubuk et al., 2020), UniformAugment (UA) (LingChen et al., 2020), TrivialAugment (TA) (M¨uller & Hutter, 2021), and Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Our results show that, without any default augmentations, DeepAA achieves the best performance compared to existing automatic augmentation search methods on CIFAR-10, CIFAR-100 on Wide-ResNet-28-10 and ImageNet on ResNet-50 and ResNet-200 with standard augmentation space and training procedure. We summarize our main contributions below: • We propose Deep AutoAugment (DeepAA), a fully automated data augmentation search method that ﬁnds a multi-layer data augmentation policy from scratch. • We formulate such multi-layer data augmentation search as a regularized gradient matching problem. We show that maximizing cosine similarity along the direction of low variance is effective for data augmentation search when augmentation layers go deep. • We address the issue of exponential growth of the dimensionality of the search space when more augmentation layers are added by incrementally adding augmentation layers based on the data distribution transformed by all the previous augmentation layers. • Our experiment results show that, without using any default augmentations, DeepAA achieves stronger performance compared with prior works. 2 R ELATED WORK Automated Data Augmentation.Automating data augmentation policy design has recently emerged as a promising paradigm for data augmentation. The pioneer work on automated data augmentation was proposed in AutoAugment (Cubuk et al., 2019), where the search is performed under reinforce- ment learning framework. AutoAugment requires to train the neural network repeatedly, which takes thousands of GPU hours to converge. Subsequent works (Lim et al., 2019; Li et al., 2020; Liu et al., 2021) aim at reducing the computation cost. Fast AutoAugment (Lim et al., 2019) treats data augmentation as inference time density matching which can be implemented efﬁciently with Bayesian optimization. Differentiable Automatic Data Augmentation (DADA) (Li et al., 2020) further reduces the computation cost through a reparameterized Gumbel-softmax distribution (Jang et al., 2017). RandAugment (Cubuk et al., 2020) introduces a simpliﬁed search space containing two interpretable hyperparameters, which can be optimized simply by grid search. Adversarial AutoAugment (AdvAA) (Zhang et al., 2019) searches for the augmentation policy in an adversarial and online manner. It also incorporates the concept of Batch Augmentaiton (Berman et al., 2019; Hoffer et al., 2020), where multiple adversarial policies run in parallel. Although many automated data augmentation methods have been proposed, the use of default augmentations still imposes strong domain knowledge. Gradient Matching. Our work is also related to gradient matching. In (Du et al., 2018), the authors showed that the cosine similarity between the gradients of different tasks provides a signal to detect when an auxiliary loss is helpful to the main loss. In (Wang et al., 2020), the authors proposed to use cosine similarity as the training signal to optimize the data usage via weighting data points. A similar approach was proposed in (M¨uller et al., 2021), which uses the gradient inner product as a per-example reward for optimizing data distribution and data augmentation under the reinforcement learning framework. Our approach also utilizes the cosine similarity to guide the data augmentation 2Published as a conference paper at ICLR 2022 search. However, our implementation of cosine similarity is different from the above from two aspects: we propose a Jacobian-vector product form to backpropagate through the cosine similarity, which is computational and memory efﬁcient and does not require computing higher order derivative; we also propose a sampling scheme that effectively allows the cosine similarity to increase with added augmentation stages. 3 D EEP AUTO AUGMENT 3.1 O VERVIEW Data augmentation can be viewed as a process of ﬁlling missing data points in the dataset with the same data distribution (Hataya et al., 2020). By augmenting a single data point multiple times, we expect the resulting data distribution to be close to the full dataset under a certain type of transformation. For example, by augmenting a single image with proper color jittering, we obtain a batch of augmented images which has similar distribution of lighting conditions as the full dataset. As the distribution of augmented data gets closer to the full dataset, the gradient of the augmented data should be steered towards a batch of original data sampled from the dataset. In DeepAA, we formulate the search of the data augmentation policy as a regularized gradient matching problem, which manages to steer the gradient to a batch of original data by augmenting a single image multiple times. Speciﬁcally, we construct the augmented training batch by augmenting a single training data point multiple times following the augmentation policy. We construct a validation batch by sampling a batch of original data from the validation set. We expect that by augmentation, the gradient of augmented training batch can be steered towards the gradient of the validation batch. To do so, we search for data augmentation that maximizes the cosine similarity between the gradients of the validation data and the augmented training data. The intuition is that an effective data augmentation should preserve data distribution (Chen et al., 2020) where the distribution of the augmented images should align with the distribution of the validation set such that the training gradient direction is close to the validation gradient direction. Another challenge for augmentation policy search is that the search space can be prohibitively large with deep augmentation layers (K ≥5). This was not a problem in previous works, where the augmentation policies is shallow ( K ≤2). For example, in AutoAugment Cubuk et al. (2019), each sub-policy contains K = 2transformations to be applied sequentially, and the search space of AutoAugment contains 16 image operations and 10 discrete magnitude levels. The resulting number of combinations of transformations in AutoAugment is roughly (16 ×10)2 = 25,600, which is handled well in previous works. However, when discarding the default augmentation pipeline and searching for data augmentations from scratch, it requires deeper augmentation layers in order to perform well. For a data augmentation with K = 5sequentially applied transformations, the number of sub-policies is (16 ×10)5 ≈1011, which is prohibitively large for the following two reasons. First, it becomes less likely to encounter a good policy by exploration as good policies become more sparse on high dimensional search space. Second, the dimension of parameters in the policy also grows with K, making it more computational challenging to optimize. To tackle this challenge, we propose to build up the full data augmentation by progressively stacking augmentation layers, where each augmentation layer is optimized on top of the data distribution transformed by all previous layers. This avoids sampling sub-policies from such a large search space, and the number of parameters of the policy is reduced from |T|K to T for each augmentation layer. 3.2 S EARCH SPACE Let O denote the set of augmentation operations ( e.g. identity, rotate, brightness), m denote an operation magnitude in the set M, and xdenote an image sampled from the space X. We deﬁne the set of transformations as the set of operations with a ﬁxed magnitude as T := {t|t= o(·; m), o∈ O and m∈M}. Under this deﬁnition, every tis a map t: X→X , and there are |T|= |M|·|O| possible transformations. In previous works (Cubuk et al., 2019; Lim et al., 2019; Li et al., 2020; Hataya et al., 2020), a data augmentation policy Pconsists of several sub-policies. As explained above, the size of candidate sub-policies grows exponentially with depth K. Therefore, we propose a practical method that builds up the full data augmentation by progressively stacking augmentation layers. The ﬁnal data augmentation policy hence consists of Klayers of sequentially applied policy P= {P1,··· ,PK}, where policy Pk is optimized conditioned on the data distribution augmented 3Published as a conference paper at ICLR 2022 by all previous (k−1) layers of policies. Thus we write the policy as a conditional distribution Pk := pθk(n|{P1,··· ,Pk−1}) where ndenotes the indices of transformations in T. For the purpose of clarity, we use a simpliﬁed notation as pθk to replace pθk(n|{P1,··· ,Pk−1}). 3.3 A UGMENTATION POLICY SEARCH VIA REGULARIZED GRADIENT MATCHING Assume that a single data point xis augmented multiple times following the policy pθ. The resulting average gradient of such augmentation is denoted as g(x,θ), which is a function of data xand policy parameters θ. Let vdenote the gradients of a batch of the original data. We optimize the policy by maximizing the cosine similarity between the gradients of the augmented data and a batch of the original data as follows: θ= arg max θ cosineSimilarity(v,g(x,θ)) (1) = arg max θ vT ·g(x,θ) ∥v∥·∥g(x,θ)∥ where ∥·∥denotes the L2-norm. The parameters of the policy can be updated via gradient ascent: θ←θ+ η∇θ cosineSimilarity(v,g(x,θ)), (2) where ηis the learning rate. 3.3.1 P OLICY SEARCH FOR ONE LAYER We start with the case where the data augmentation policy only contains a single augmentation layer, i.e., P= {pθ}. Let L(x; w) denote the classiﬁcation loss of data point xwhere w∈RD represents the ﬂattened weights of the neural network. Consider applying augmentation on a single data point xfollowing the distribution pθ. The resulting averaged gradient can be calculated analytically by averaging all the possible transformations in T with the corresponding probability p(θ): g(x; θ) = |T|∑ n=1 pθ(n)∇wL(tn(x); w) (3) = G(x) ·pθ where G(x) = [ ∇wL(t1(x); w),··· ,∇wL(t|T|(x); w) ] is a D×|T|Jacobian matrix, and pθ = [pθ(1),··· ,pθ(|T|)]T is a |T|dimensional categorical distribution. The gradient w.r.t. the cosine similarity in Eq. (2) can be derived as: ∇θ cosineSimilarity(v,g(x; θ)) =∇θpθ ·r (4) where r= G(x)T ( v ∥g(θ)∥−vTg(θ) ∥g(θ)∥2 · g(θ) ∥g(θ)∥ ) (5) which can be interpreted as a reward for each transformation. Therefore, pθ ·rin Eq.(4) represents the average reward under policy pθ. 3.3.2 P OLICY SEARCH FOR MULTIPLE LAYERS The above derivation is based on the assumption that g(θ) can be computed analytically by Eq.(3). However, when K ≥2, it becomes impractical to compute the average gradient of the augmented data given that the search space dimensionality grows exponentially with K. Consequently, we need to average the gradient of all |T|K possible sub-policies. To reduce the parameters of the policy to T for each augmentation layer, we propose to incrementally stack augmentations based on the data distribution transformed by all the previous augmentation layers. Speciﬁcally, let P= {P1,··· ,PK}denote the K-layer policy. The policy Pk modiﬁes the data distribution on top of the data distribution augmented by the previous (k−1) layers. Therefore, the policy at the kth layer is a distribution Pk = pθk(n) conditioned on the policies {P1,··· ,Pk−1} 4Published as a conference paper at ICLR 2022 where each one is a |T|-dimensional categorical distribution. Given that, the Jacobian matrix at the kth layer can be derived by averaging over the previous(k−1) layers of policies as follows: G(x)k = |T|∑ nk−1=1 ··· |T|∑ n1=1 pθk−1 (nk−1) ···pθ1 (n1)[∇wL((t1 ◦tnk−1 ···◦ tn1 )(x); w),··· , ∇wL((t|T|◦tnk−1 ◦···◦ tn1 )(x); w)] (6) where Gk can be estimated via the Monte Carlo method as: ˜Gk(x) = ∑ ˜nk−1∼pθk ··· ∑ ˜n1∼pθ1 [∇wL((t1 ◦t˜nk−1 ···◦ t˜n1 )(x); w),··· , ∇wL((t|T|◦t˜nk−1 ◦···◦ t˜n1 )(x); w)] (7) where ˜nk−1 ∼pθk−1 (n), ··· ,˜n1 ∼pθ1 (n). The average gradient at the kth layer can be estimated by the Monte Carlo method as: ˜g(x; θk) = ∑ ˜nk∼pθk ··· ∑ ˜n1∼pθ1 ∇wL((t˜nk ◦···◦ t˜n1 )(x); w) . (8) Therefore, the reward at the kth layer is derived as: ˜rk(x) = ( ˜Gk(x) )T ( v ∥˜gk(x; θk)∥−vT˜gk(x; θk) ∥˜gk(x; θk)∥2 · ˜gk(x; θk) ∥˜gk(x; θk)∥ ) . (9) To prevent the augmentation policy from overﬁtting, we regularize the optimization by avoiding optimizing towards the direction with high variance. Thus, we penalize the average reward with its standard deviation as rk = Ex{˜rk(x)}−c· √ Ex{(˜rk(x) −Ex{˜rk(x)})2}, (10) where we use 16 randomly sampled images to calculate the expectation. The hyperparameter c controls the degree of regularization, which is set to 1.0. With such regularization, we prevent the policy from converging to the transformations with high variance. Therefore the parameters of policy Pk (k≥2) can be updated as: θ←θk + η∇θk cosineSimilarity(v,g(θk)) (11) where ∇θ cosineSimilarity(v,gk(x; θ)) =∇θpθk ·rk. (12) 4 E XPERIMENTS AND ANALYSIS Benchmarks and Baselines.We evaluate the performance of DeepAA on three standard benchmarks: CIFAR-10, CIFAR-100, ImageNet, and compare it against a baseline based on standard augmentations (i.e., ﬂip left-righ, pad-and-crop for CIFAR-10/100, and Inception-style preprocesing (Szegedy et al., 2015) for ImageNet) as well as nine existing automatic augmentation methods including (1) AutoAugment (AA) (Cubuk et al., 2019), (2) PBA (Ho et al., 2019), (3) Fast AutoAugment (Fast AA) (Lim et al., 2019), (4) Faster AutoAugment (Hataya et al., 2020), (5) DADA (Li et al., 2020), (6) RandAugment (RA) (Cubuk et al., 2020), (7) UniformAugment (UA) (LingChen et al., 2020), (8) TrivialAugment (TA) (M¨uller & Hutter, 2021), and (9) Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Search Space.We set up the operation setO to include 16 commonly used operations (identity, shear- x, shear-y, translate-x, translate-y, rotate, solarize, equalize, color, posterize, contrast, brightness, sharpness, autoContrast, invert, Cutout) as well as two operations (i.e., ﬂips and crop) that are used as the default operations in the aforementioned methods. Among the operations in O, 11 operations are associated with magnitudes. We then discretize the range of magnitudes into 12 uniformly spaced levels and treat each operation with a discrete magnitude as an independent transformation. Therefore, the policy in each layer is a 139-dimensional categorical distribution corresponding to |T|= 139 {operation, magnitude}pairs. The list of operations and the range of magnitudes in the standard augmentation space are summarized in Appendix A. 5Published as a conference paper at ICLR 2022 4.1 P ERFORMANCE ON CIFAR-10 AND CIFAR-100 Policy Search.Following (Cubuk et al., 2019), we conduct the augmentation policy search based on Wide-ResNet-40-2 (Zagoruyko & Komodakis, 2016). We ﬁrst train the network on a subset of 4,000 randomly selected samples from CIFAR-10. We then progressively update the policy network parameters θk (k= 1,2,··· ,K) for 512 iterations for each of the Kaugmentation layers. We use the Adam optimizer (Kingma & Ba, 2015) and set the learning rate to 0.025 for policy updating. Policy Evaluation.Using the publicly available repository of Fast AutoAugment (Lim et al., 2019), we evaluate the found augmentation policy on both CIFAR-10 and CIFAR-100 using Wide-ResNet- 28-10 and Shake-Shake-2x96d models. The evaluation conﬁgurations are kept consistent with that of Fast AutoAugment. Results. Table 1 reports the Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d, respectively. The results of DeepAA are the average of four independent runs with different initializations. We also show the 95% conﬁdence interval of the mean accuracy. As shown, DeepAA achieves the best performance compared against previous works using the standard augmentation space. Note that TA(Wide) uses a wider (stronger) augmentation space on this dataset. Baseline AA PBA FastAA FasterAA DADA RA UA TA(RA) TA(Wide)1 DeepAA CIFAR-10WRN-28-10 96.1 97.4 97.4 97.3 97.4 97.3 97.3 97.33 97.46 97.46 97.56±0.14Shake-Shake(26 2x96d) 97.1 98.0 98.0 98.0 98.0 98.0 98.0 98.1 98.05 98.21 98.11±0.12 CIFAR-100WRN-28-10 81.2 82.9 83.3 82.7 82.7 82.5 83.3 82.82 83.54 84.33 84.02±0.18Shake-Shake(26 2x96d) 82.9 85.7 84.7 85.1 85.0 84.7 - - - 86.19 85.19±0.28 Table 1: Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.2 P ERFORMANCE ON IMAGE NET Policy Search.We conduct the augmentation policy search based on ResNet-18 (He et al., 2016). We ﬁrst train the network on a subset of 200,000 randomly selected samples from ImageNet for 30 epochs. We then use the same settings as in CIFAR-10 for updating the policy parameters. Policy Evaluation.We evaluate the performance of the found augmentation policy on ResNet-50 and ResNet-200 based on the public repository of Fast AutoAugment (Lim et al., 2019). The parameters for training are the same as the ones of (Lim et al., 2019). In particular, we use step learning rate scheduler with a reduction factor of 0.1, and we train and evaluate with images of size 224x224. Results. The performance on ImageNet is presented in Table 2. As shown, DeepAA achieves the best performance compared with previous methods without the use of default augmentation pipeline. In particular, DeepAA performs better on larger models (i.e. ResNet-200), as the performance of DeepAA on ResNet-200 is the best within the 95% conﬁdence interval. Note that while we train DeepAA using the image resolution (224×224), we report the best results of RA and TA, which are trained with a larger image resolution (244×224) on this dataset. Baseline AA Fast AA Faster AA DADA RA UA TA(RA)1 TA(Wide)2 DeepAA ResNet-50 76.3 77.6 77.6 76.5 77.5 77.6 77.63 77.85 78.07 78.30±0.14 ResNet-200 78.5 80.0 80.6 - - - 80.4 - - 81.32±0.17 Table 2: Top-1 test accuracy (%) on ImageNet for ResNet-50 and ResNet-200. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.3 P ERFORMANCE WITH BATCH AUGMENTATION Batch Augmentation (BA) is a technique that draws multiple augmented instances of the same sample in one mini-batch. It has been shown to be able to improve the generalization performance of the 1On CIFAR-10/100, TA (Wide) uses a wider (stronger) augmentation space, while the other methods including TA (RA) uses the standard augmentation space. 6Published as a conference paper at ICLR 2022 network (Berman et al., 2019; Hoffer et al., 2020). AdvAA (Zhang et al., 2019) directly searches for the augmentation policy under the BA setting whereas for TA and DeepAA, we apply BA with the same augmentation policy used in Table 1. Note that since the performance of BA is sensitive to the hyperparameters (Fort et al., 2021), we have conducted a grid search on the hyperparameters of both TA and DeepAA (details are included in Appendix D). As shown in Table 3, after tuning the hyperparameters, the performance of TA (Wide) using BA is already better than the reported performance in the original paper. The performance of DeepAA with BA outperforms that of both AdvAA and TA (Wide) with BA. AdvAA TA(Wide) (original paper) TA(Wide) (ours) DeepAA CIFAR-10 98.1±0.15 98.04±0.06 98.06±0.23 98.21±0.14 CIFAR-100 84.51±0.18 84.62±0.14 85.40±0.15 85.61±0.17 Table 3: Top-1 test accuracy (%) on CIFAR-10/100 dataset with WRN-28-10 with Batch Augmentation (BA), where eight augmented instances were drawn for each image. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.4 U NDERSTANDING DEEPAA Figure 2: Top-1 test accuracy (%) on ImageNet of DeepAA-simple, DeepAA, and other automatic augmentation methods on ResNet-50. Effectiveness of Gradient Matching. One uniqueness of DeepAA is the regularized gradient matching objective. To examine its effectiveness, we remove the impact coming from multiple aug- mentation layers, and only conduct search for a sin- gle layer of augmentation policy. When evaluating the searched policy, we apply the default augmenta- tion in addition to the searched policy. We refer to this variant as DeepAA-simple. Figure 2 compares the Top-1 test accuracy on ImageNet using ResNet- 50 between DeepAA-simple, DeepAA, and other automatic augmentation methods. While there is 0.22% performance drop compared to DeepAA, with a single augmentation layer, DeepAA-simple still outperforms other methods and is able to achieve similar performance compared to TA (Wide) but with a standard augmentation space and trains on a smaller image size (224×224 vs 244×224). Policy Search Cost.Table 4 compares the policy search time on CIFAR-10/100 and ImageNet in GPU hours. DeepAA has comparable search time as PBA, Fast AA, and RA, but is slower than Faster AA and DADA. Note that Faster AA and DADA relax the discrete search space to a continuous one similar to DARTS (Liu et al., 2018). While such relaxation leads to shorter searching time, it inevitably introduces a discrepancy between the true and relaxed augmentation spaces. Dataset AA PBA Fast AA Faster AA DADA RA DeepAA CIFAR-10/100 5000 5 3.5 0.23 0.1 25 9 ImageNet 15000 - 450 2.3 1.3 5000 96 Table 4: Policy search time on CIFAR-10/100 and ImageNet in GPU hours. Impact of the Number of Augmentation Layers.Another uniqueness of DeepAA is its multi-layer search space that can go beyondtwo layers which existing automatic augmentation methods were designed upon. We examine the impact of the number of augmentation layers on the performance of DeepAA. Table 5 and Table 6 show the performance on CIFAR-10/100 and ImageNet respectively with increasing number of augmentation layers. As shown, for CIFAR-10/100, the performance gradually improves when more augmentation layers are added until we reach ﬁve layers. The performance does not improve when the sixth layer is added. For ImageNet, we have similar 1TA (RA) achieves 77.55% top-1 accuracy with image resolution 224×224. 2TA (Wide) achieves 77.97% top-1 accuracy with image resolution 224×224. 7Published as a conference paper at ICLR 2022 Figure 3: The distribution of operations at each layer of the policy for CIFAR-10/100 and ImageNet. The probability of each operation is summed up over all 12 discrete intensity levels (see Appendix B and C) of the corresponding transformation. observation where the performance stops improving when more than ﬁve augmentation layers are included. 1 layer 2 layers 3 layers 4 layers 5 layers 6 layers CIFAR-10 96.3±0.21 96.6±0.18 96.9±0.12 97.4±0.14 97.56±0.14 97.6±0.12 CIFAR-100 80.9±0.31 81.7±0.24 82.2±0.21 83.7±0.24 84.02±0.18 84.0±0.19 Table 5: Top-1 test accuracy of DeepAA on CIFAR-10/100 for different numbers of augmentation layers. The results are averaged over 4 independent runs with different initializations with the 95% conﬁdence interval denoted by ±. 1 layer 3 layers 5 layers 7 layers ImageNet 75.27±0.19 78.18±0.22 78.30±0.14 78.30±0.14 Table 6: Top-1 test accuracy of DeepAA on ImageNet with ResNet-50 for different numbers of augmentation layers. The results are averaged over 4 independent runs w/ different initializations with the 95% conﬁdence interval denoted by ±. Figure 3 illustrates the distributions of operations in the policy for CIFAR-10/100 and ImageNet respectively. As shown in Figure 3(a), the augmentation of CIFAR-10/100 converges to identity transformation at the sixth augmentation layer, which is a natural indication of the end of the augmentation pipeline. We have similar observation in Figure 3(b) for ImageNet, where the identity transformation dominates in the sixth augmentation layer. These observations match our results listed in Table 5 and Table 6. We also include the distribution of the magnitude within each operation for CIFAR-10/100 and ImageNet in Appendix B and Appendix C. Validity of Optimizing Gradient Matching with Regularization.To evaluate the validity of opti- mizing gradient matching with regularization, we designed a search-free baseline named “DeepTA”. In DeepTA, we stack multiple layers of TA on the same augmentation space of DeepAA without using default augmentations. As stated in Eq.(10) and Eq.(12), we explicitly optimize the gradient similarities with the average reward minus its standard deviation. The ﬁrst term – the average reward Ex{˜rk(x)}– encourages the direction of high cosine similarity. The second term – the standard deviation of the reward √ Ex{(˜rk(x) −Ex{˜rk(x)})2}– acts as a regularization that penalizes the direction with high variance. These two terms jointly maximize the gradient similarity along the direction with low variance. To illustrate the optimization trajectory, we design two metrics that are closely related to the two terms in Eq.(10): the mean value, and the standard deviation of the improvement of gradient similarity. The improvement of gradient similarity is obtained by subtracting the cosine similarity of the original image batch from that of the augmented batch. In our experiment, the mean and standard deviation of the gradient similarity improvement are calculated over 256 independently sampled original images. 8Published as a conference paper at ICLR 2022 (a) Mean of the gradient similarity improvement (b) Standard deviation of the gradi- ent similarity improvement (c) Mean accuracy over different aug- mentation depth Figure 4: Illustration of the search trajectory of DeepAA in comparison with DeepTA on CIFAR-10. As shown in Figure 4(a), the cosine similarity of DeepTA reaches the peak at the ﬁfth layer, and stacking more layers decreases the cosine similarity. In contrast, for DeepAA, the cosine similarity increases consistently until it converges to identity transformation at the sixth layer. In Figure 4(b), the standard deviation of DeepTA signiﬁcantly increases when stacking more layers. In contrast, in DeepAA, as we optimize the gradient similarity along the direction of low variance, the standard deviation of DeepAA does not grow as fast as DeepTA. In Figure 4(c), both DeepAA and DeepTA reach peak performance at the sixth layer, but DeepAA achieves better accuracy compared against DeepTA. Therefore, we empirically show that DeepAA effectively scales up the augmentation depth by increasing cosine similarity along the direction with low variance, leading to better results. Comparison with Other Policies.In Figure 7 in Appendix E, we compare the policy of DeepAA with the policy found by other data augmentation search methods including AA, FastAA and DADA. We have three interesting observations: • AA, FastAA and DADA assign high probability (over 1.0) on ﬂip, Cutout and crop, as those transformations are hand-picked and applied by default. DeepAA ﬁnds a similar pattern that assigns high probability on ﬂip, Cutout and crop. • Unlike AA, which mainly focused on color transformations, DeepAA has high probability over both spatial and color transformations. • FastAA has evenly distributed magnitudes, while DADA has low magnitudes (common issues in DARTS-like method). Interestingly, DeepAA assigns high probability to the stronger magnitudes. 5 C ONCLUSION In this work, we present Deep AutoAugment (DeepAA), a multi-layer data augmentation search method that ﬁnds deep data augmentation policy without using any hand-picked default transforma- tions. We formulate data augmentation search as a regularized gradient matching problem, which maximizes the gradient similarity between augmented data and original data along the direction with low variance. Our experimental results show that DeepAA achieves strong performance without using default augmentations, indicating that regularized gradient matching is an effective search method for data augmentation policies. Reproducibility Statement: We have described our experiment settings in great details. The evaluation of the found data augmentation policy is based the public repository of Fast AutoAugment. We believe that our results can be readily reproduced. ACKNOWLEDGEMENT We thank Yi Zhu, Hang Zhang, Haichen Shen, Mu Li, and Alexander Smola for their help with this work. This work was partially supported by NSF Award PFI:BIC-1632051 and Amazon AWS Machine Learning Research Award. 9Published as a conference paper at ICLR 2022 REFERENCES Maxim Berman, Herv´e J´egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a uniﬁed image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019. Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21(245):1–71, 2020. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, volume 33, pp. 702–703, 2020. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224, 2018. Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. Drawing multiple augmentation samples per image during training efﬁciently decreases test error. arXiv preprint arXiv:2105.13343, 2021. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.Deep learning. MIT press Cambridge, 2016. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In European Conference on Computer Vision, pp. 1–16. Springer, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. International Conference on Learning Representations, 2020. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In European Conference on Computer Vision, pp. 580–595. Springer, 2020. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 10Published as a conference paper at ICLR 2022 Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Raﬁ Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. arXiv preprint arXiv:2003.14348, 2020. Aoming Liu, Zehao Huang, Zhiwu Huang, and Naiyan Wang. Direct differentiable augmentation search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12219–12228, 2021. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018. Samuel M¨uller, Andr´e Biedenkapp, and Frank Hutter. In-loop meta-learning with gradient-alignment reward. arXiv preprint arXiv:2102.03275, 2021. Samuel G. M¨uller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmenta- tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 774–782, October 2021. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015. Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig. Optimizing data usage via differentiable rewards. In International Conference on Machine Learning, pp. 9983–9995. PMLR, 2020. Ross Wightman, Hugo Touvron, and Herv ´e J ´egou. Resnet strikes back: An improved training procedure in timm. volume 34, 2021. Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. In arXiv preprint arXiv: 2001.00677, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. International Conference on Learning Representations, 2018. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. In International Conference on Learning Representations, 2019. 11Published as a conference paper at ICLR 2022 A A LIST OF STANDARD AUGMENTATION SPACE Operation Magnitude Identity - ShearX [-0.3, 0.3] ShearY [-0.3, 0.3] TranslateX [-0.45, 0.45] TranslateY [-0.45, 0.45] Rotate [-30, 30] AutoContrast - Invert - Equalize - Solarize [0, 256] Posterize [4, 8] Contrast [0.1, 1.9] Color [0.1, 1.9] Brightness [0.1, 1.9] Sharpness [0.1, 1.9] Flips - Cutout 16 (60) Crop - Table 7: List of operations in the search space and the corresponding range of magnitudes in the standard augmentation space. Note that some operations do not use magnitude parameters. We add ﬂip and crop to the search space which were found in the default augmentation pipeline in previous works. Flips operates by randomly ﬂipping the images with 50% probability. In line with previous works, crop denotes pad-and-crop and resize-and-crop transforms for CIFAR10/100 and ImageNet respectively. We set Cutout magnitude to 16 for CIFAR10/100 dataset to be the same as the Cutout in the default augmentation pipeline. We set Cutout magnitude to 60 pixels for ImageNet which is the upper limit of the magnitude used in AA (Cubuk et al., 2019). 12Published as a conference paper at ICLR 2022 B T HE DISTRIBUTION OF MAGNITUDES FOR CIFAR-10/100 Figure 5: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for CIFAR-10/100. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 13Published as a conference paper at ICLR 2022 C T HE DISTRIBUTION OF MAGNITUDES FOR IMAGE NET Figure 6: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for ImageNet. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 14Published as a conference paper at ICLR 2022 D H YPERPARAMETERS FOR BATCH AUGMENTATION The performance of BA is sensitive to the training settings (Fort et al., 2021; Wightman et al., 2021). Therefore, we conduct a grid search on the learning rate, weight decay and number of epochs for TA and DeepAA with Batch Augmentation. The best found parameters are summarized in Table 8 in Appendix. We did not tune the hyperparameters of AdvAA (Zhang et al., 2019) since AdvAA claims to be adaptive to the training process. Dataset Augmentation Model Batch Size Learning Rate Weight Decay Epoch CIFAR-10 TA (Wide) WRN-28-10 128×8 0.2 0.0005 100 DeepAA WRN-28-10 128×8 0.2 0.001 100 CIFAR-100 TA (Wide) WRN-28-10 128×8 0.4 0.0005 35 DeepAA WRN-28-10 128×8 0.4 0.0005 35 Table 8: Model hyperparameters of Batch Augmentation on CIFAR10/100 for TA (Wide) and DeepAA. Learning rate, weight decay and number of epochs are found via grid search. 15Published as a conference paper at ICLR 2022 E C OMPARISON OF DATA AUGMENTATION POLICY Sampling probability of each transformations cumulated over all augmentation layers  (a) DeepAA (b) AA (c) FastAA (d) DADA Figure 7: Comparison of the policy of DeepAA and some publicly available augmentaiotn policy found by other methods including AA, FastAA and DADA on CIFAR-10. Since the compared methods have varied numbers of augmentation layers, we cumulate the probability of each operation over all the augmentation layers. Thus, the cumulative probability can be larger than 1. For AA, Fast AA and DADA, we add additional 1.0 probability to ﬂip, Cutout and Crop, since they are applied by default. In addition, we normalize the magnitude to the range [-5, 5], and use color to distinguish different magnitudes. 16",
      "references": [
        "Multigrain: a unified image embedding for classes and instances",
        "A group-theoretic framework for data augmentation",
        "Autoaugment: Learning augmentation strategies from data",
        "Randaugment: Practical automated data augmentation with a reduced search space",
        "Improved regularization of convolutional neural networks with cutout",
        "Adapting auxiliary losses using gradient similarity",
        "Drawing multiple augmentation samples per image during training efficiently decreases test error",
        "Deep learning",
        "Faster autoaugment: Learning augmentation strategies using backpropagation",
        "Deep residual learning for image recognition",
        "Augmix: A simple data processing method to improve robustness and uncertainty",
        "Population based augmentation: Efficient learning of augmentation policy schedules",
        "Augment your batch: Improving generalization through instance repetition",
        "Data augmentation by pairing samples for images classification",
        "Categorical reparameterization with gumbel-softmax",
        "Adam: A method for stochastic optimization",
        "Differentiable automatic data augmentation",
        "Fast autoaugment",
        "Uniformaugment: A search-free probabilistic data augmentation approach",
        "Direct differentiable augmentation search",
        "Darts: Differentiable architecture search",
        "In-loop meta-learning with gradient-alignment reward",
        "Trivialaugment: Tuning-free yet state-of-the-art data augmentation",
        "Going deeper with convolutions",
        "Optimizing data usage via differentiable rewards",
        "Resnet strikes back: An improved training procedure in timm",
        "Improve unsupervised domain adaptation with mixup training",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "Wide residual networks",
        "Understanding deep learning requires rethinking generalization",
        "mixup: Beyond empirical risk minimization",
        "Adversarial autoaugment"
      ],
      "meta_data": {
        "arxiv_id": "2203.06172v2",
        "authors": [
          "Yu Zheng",
          "Zhi Zhang",
          "Shen Yan",
          "Mi Zhang"
        ],
        "published_date": "2022-03-11T18:57:27Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Deep AutoAugment (DeepAA), a fully automated data-augmentation search method that removes reliance on hand-crafted default transformations by progressively building a multi-layer augmentation pipeline from scratch. Frames policy search as a regularized gradient-matching problem, introduces incremental layer-wise optimization to control search-space explosion, and achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet without default augmentations.",
        "methodology": "1. Search space: 139 image transformations (operation, discrete magnitude) including flips, crop, Cutout.\n2. Policy structure: K sequential augmentation layers; each layer is a categorical distribution over transformations.\n3. Objective: maximize cosine similarity between gradients from augmented single-image batch and a validation batch; add variance penalty to encourage low-variance directions (regularized gradient matching).\n4. Optimization: layer-wise stacking—optimize layer k conditioned on distributions from previous layers; Monte-Carlo estimate gradients; update with Adam.\n5. Search cost: ~9 GPU-h on CIFAR, 96 GPU-h on ImageNet.",
        "experimental_setup": "Datasets: CIFAR-10, CIFAR-100, ImageNet.\nSearch phase: Wide-ResNet-40-2 on 4k samples of CIFAR-10; ResNet-18 on 200k ImageNet subset (224×224), 30 epochs.\nEvaluation: CIFAR—Wide-ResNet-28-10 and Shake-Shake-26-2x96d; ImageNet—ResNet-50 and ResNet-200.\nBaselines compared: Baseline flips+crop, AutoAugment, PBA, FastAA, FasterAA, DADA, RandAugment, UniformAugment, TrivialAugment, AdvAA.\nMetrics: Top-1 classification accuracy (95% CI over 4 runs).\nAdditional study: batch augmentation (8 repeats) and ablations on layer depth, search objective.\nCompute: standard augmentation space, public FastAA training pipeline.",
        "limitations": "1. Search still requires significant compute; slower than FasterAA and DADA.\n2. Currently limited to vision tasks and fixed discrete operation set; unexplored applicability to other domains or richer augmentations.\n3. Assumes access to model gradients and validation data during search, restricting use with black-box models.\n4. Performance gains plateau after ~5 layers; deeper pipelines converge to identity, indicating diminishing returns.\n5. Hyperparameters (regularization constant, learning rate, layer count) tuned empirically; sensitivity not fully analyzed.",
        "future_research_directions": "1. Extend regularized gradient-matching search to non-image modalities (text, audio, multimodal).\n2. Reduce computational cost via surrogate models, weight-sharing, or efficient gradient approximations.\n3. Explore continuous or learned operation primitives to enlarge transformation space.\n4. Develop adaptive stopping criteria for number of layers instead of fixed K.\n5. Combine DeepAA with adversarial or self-supervised objectives for robustness and unsupervised settings.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Federated Learning from Small Datasets",
      "full_text": "Published as a conference paper at ICLR 2023 FEDERATED LEARNING FROM SMALL DATASETS Michael Kamp Institute for AI in medicine (IKIM) University Hospital Essen, Essen Germany, and Ruhr-University Bochum, Bochum Germany, and Monash University, Melbourne, Australia michael.kamp@uk-essen.de Jonas Fischer Harvard T.H. Chan School of Public Health Department of Biostatistics Boston, MA, United States jfischer@hsph.harvard.edu Jilles Vreeken CISPA Helmholtz Center for Information Security Saarbr¨ucken, Germany vreeken@cispa.de ABSTRACT Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning. 1 I NTRODUCTION How can we learn high quality models when data is inherently distributed across sites and cannot be shared or pooled? In federated learning, the solution is to iteratively train models locally at each site and share these models with the server to be aggregated to a global model. As only models are shared, data usually remains undisclosed. This process, however, requires sufficient data to be available at each site in order for the locally trained models to achieve a minimum quality—even a single bad model can render aggregation arbitrarily bad (Shamir and Srebro, 2014). In many relevant applications this requirement is not met: In healthcare settings we often have as little as a few dozens of samples (Granlund et al., 2020; Su et al., 2021; Painter et al., 2020). Also in domains where deep learning is generally regarded as highly successful, such as natural language processing and object detection, applications often suffer from a lack of data (Liu et al., 2020; Kang et al., 2019). To tackle this problem, we propose a new building block called daisy-chaining for federated learning in which models are trained on one local dataset after another, much like a daisy chain. In a nutshell, at each client a model is trained locally, sent to the server, and then—instead of aggregating local models—sent to a random other client as is (see Fig. 1). This way, each local model is exposed to a daisy chain of clients and their local datasets. This allows us to learn from small, distributed datasets simply by consecutively training the model with the data available at each site. Daisy-chaining alone, however, violates privacy, since a client can infer from a model upon the data of the client it received it from (Shokri et al., 2017). Moreover, performing daisy-chaining naively would lead to overfitting which can cause learning to diverge (Haddadpour and Mahdavi, 2019). In this paper, we propose to combine daisy-chaining of local datasets with aggregation of models, both orchestrated by the server, and term this method federated daisy-chaining (FEDDC). 1 arXiv:2110.03469v3  [cs.LG]  12 Oct 2023Published as a conference paper at ICLR 2023 Figure 1: Federated learning settings. A standard federated learning setting with training of local models at clients (middle) with aggregation phases where models are communicated to the server, aggregated, and sent back to each client (left). We propose to add daisy chaining (right), where local models are sent to the server and then redistributed to a random permutation of clients as is. We show that our simple, yet effective approach maintains privacy of local datasets, while it prov- ably converges and guarantees improvement of model quality in convex problems with a suitable aggregation method. Formally, we show convergence for FEDDC on non-convex problems. We then show for convex problems that FEDDC succeeds on small datasets where standard federated learning fails. For that, we analyze FEDDC combined with aggregation via the Radon point from a PAC-learning perspective. We substantiate this theoretical analysis for convex problems by showing that FEDDC in practice matches the accuracy of a model trained on the full data of the SUSY binary classification dataset with only 2 samples per client, outperforming standard federated learning by a wide margin. For non-convex settings, we provide an extensive empirical evaluation, showing that FEDDC outperforms naive daisy-chaining, vanilla federated learning FEDAVG (McMahan et al., 2017), FEDPROX (Li et al., 2020a), FEDADAGRAD , FEDADAM, and FEDYOGI (Reddi et al., 2020) on low-sample CIFAR10 (Krizhevsky, 2009), including non-iid settings, and, more importantly, on two real-world medical imaging datasets. Not only does FEDDC provide a wide margin of improve- ment over existing federated methods, but it comes close to the performance of a gold-standard (centralized) neural network of the same architecture trained on the pooled data. To achieve that, it requires a small communication overhead compared to standard federated learning for the additional daisy-chaining rounds. As often found in healthcare, we consider a cross-SILO scenario where such small communication overhead is negligible. Moreover we show that with equal communication, standard federated averaging still underperforms in our considered settings. In summary, our contributions are (i) FEDDC, a novel approach to federated learning from small datasets via a combination of model permutations across clients and aggregation, (ii) a formal proof of convergence for FEDDC, (iii) a theoretical guarantee that FEDDC improves models in terms of ϵ, δ-guarantees which standard federated learning can not, (iv) a discussion of the privacy aspects and mitigations suitable for FEDDC, including an empirical evaluation of differentially private FEDDC, and (v) an extensive set of experiments showing that FEDDC substantially improves model quality on small datasets compared to standard federated learning approaches. 2 R ELATED WORK Learning from small datasets is a well studied problem in machine learning. In the literature, we find both general solutions, such as using simpler models and transfer learning (Torrey and Shavlik, 2010), and more specialized ones, such as data augmentation (Ibrahim et al., 2021) and few-shot learning (Vinyals et al., 2016; Prabhu et al., 2019). In our scenario overall data is abundant, but the problem is that data is distributed into small local datasets at each site, which we are not allowed to pool. Hao et al. (2021) propose local data augmentation for federated learning, but their method requires a sufficient quality of the local model for augmentation which is the opposite of the scenario we are considering. Huang et al. (2021) provide generalization bounds for federated averaging via the NTK-framework, but requires one-layer infinite-width NNs and infinitesimal learning rates. Federated learning and its variants have been shown to learn from incomplete local data sources, e.g., non-iid label distributions (Li et al., 2020a; Wang et al., 2019) and differing feature distributions (Li et al., 2020b; Reisizadeh et al., 2020a), but fail in case of large gradient diversity (Haddadpour and Mahdavi, 2019) and strongly dissimilar label distribution (Marfoq et al., 2021). For small 2Published as a conference paper at ICLR 2023 datasets, local empirical distributions may vary greatly from the global distribution: the difference of empirical to true distribution decreases exponentially with the sample size (e.g., according to the Dvoretzky–Kiefer–Wolfowitz inequality), but for small samples the difference can be substantial, in particular if the distribution differs from a Normal distribution (Kwak and Kim, 2017). Shamir and Srebro (2014) have shown the adverse effect of bad local models on averaging, proving that even due to a single bad model averaging can be arbitrarily bad. A different approach to dealing with biased local data is by learning personalized models at each client. Such personalized FL (Li et al., 2021) can reduce sample complexity, e.g., by using shared representations (Collins et al., 2021) for client-specific models, e.g., in the medical domain (Yang et al., 2021), or by training sample-efficient personalized Bayesian methods (Achituve et al., 2021). It is not applicable, however, to settings where you are not allowed to learn the biases or batch effects of local clients, e.g., in many medical applications where this would expose sensitive client information. Kiss and Horvath (2021) propose a decentralized and communication-efficient variant of federated learning that migrates models over a decentralized network, storing incoming models locally at each client until sufficiently many models are collected on each client for an averaging step, similar to Gossip federated learing (Jelasity et al., 2005). The variant without averaging is similar to simple daisy-chaining which we compare to in Section 7. FEDDC is compatible with any aggregation operator, including the Radon machine (Kamp et al., 2017), the geometric median (Pillutla et al., 2022), or neuron-clustering (Yurochkin et al., 2019), and can be straightforwardly combined with approaches to improve communication-efficiency, such as dynamic averaging (Kamp et al., 2018), and model quantization (Reisizadeh et al., 2020b). We combine FEDDC with averaging, the Radon machine, and FedProx (Li et al., 2020a) in Sec. 7. 3 P RELIMINARIES We assume iterative learning algorithms (cf. Chp. 2.1.4 Kamp, 2019) A : X × Y × H → Hthat update a model h ∈ Husing a dataset D ⊂ X × Yfrom an input space X and output space Y, i.e., ht+1 = A(D, ht). Given a set of m ∈ N clients with local datasets D1, . . . , Dm ⊂ X × Ydrawn iid from a data distribution D and a loss function ℓ : Y × Y →R, the goal is to find a single model h∗ ∈ Hthat minimizes the risk ε(h) = E(x,y)∼D[ℓ(h(x), y)]. In centralized learning, datasets are pooled as D = S i∈[m] Di and A is applied to D until convergence. Note that applying A on D can be the application to any random subset, e.g., as in mini-batch training, and convergence is measured in terms of low training loss, small gradient, or small deviation from previous iterate. In standard federated learning (McMahan et al., 2017), A is applied in parallel for b ∈ N rounds on each client locally to produce local models h1, . . . , hm. These models are then centralized and aggregated using an aggregation operator agg : Hm → H, i.e., h = agg(h1, . . . , hm). The aggregated model h is then redistributed to local clients which perform another b rounds of training using h as a starting point. This is iterated until convergence of h. When aggregating by averaging, this method is known as federated averaging (FEDAVG). Next, we describe FEDDC. 4 F EDERATED DAISY-CHAINING We propose federated daisy chaining as an extension to federated learning in a setup with m clients and one designated sever.1 We provide the pseudocode of our approach as Algorithm 1. The client: Each client trains its local model in each round on local data (line 4), and sends its model to the server every b rounds for aggregation, where b is the aggregation period, and every d rounds for daisy chaining, where d is the daisy-chaining period (line 6). This re-distribution of models results in each individual model conceptually following a daisy chain of clients, training on each local dataset. Such a daisy chain is interrupted by each aggregation round. The server: Upon receiving models, in a daisy-chaining round (line 9) the server draws a random permutation π of clients (line 10) and re-distributes the model of clienti to client π(i) (line 11), while in an aggregation round (line 12), the server instead aggregates all local models and re-distributes the aggregate to all clients (line 13-14). 1This star-topology can be extended to hierarchical networks in a straightforward manner. Federated learning can also be performed in a decentralized network via gossip algorithms (Jelasity et al., 2005). 3Published as a conference paper at ICLR 2023 Algorithm 1:Federated Daisy-Chaining FEDDC Input: daisy-chaining period d, aggregation period b, learning algorithm A, aggregation operator agg, m clients with local datasets D1, . . . , Dm, total number of rounds T Output: final model aggregate hT 1 initialize local models h1 0, . . . , hm 0 2 Locally at client i at time t do 3 sample S from Di 4 hi t ← A(S, hi t−1) 5 if t mod d= d − 1 or t mod b= b − 1 then 6 send hi t to server 7 receive new hi t from server // receives either aggregate ht or some hj t 8 At serverat time t do 9 if t mod d= d − 1 then // daisy chaining 10 draw permutation π of [1,m] at random 11 for all i ∈ [m] send model hi t to client π(i) 12 else ift mod b= b − 1 then // aggregation 13 ht ← agg(h1 t , . . . , hm t ) 14 send ht to all clients Communication complexity: Note that we consider cross-SILO settings, such as healthcare, were communication is not a bottleneck and, hence, restrict ourselves to a brief discussion in the interest of space. Communication between clients and server happens in O(T d + T b ) many rounds, where T is the overall number of rounds. Since FEDDC communicates every dth and bth round, the amount of communication rounds is similar to FEDAVG with averaging period bFedAvg = min{d, b}. That is, FEDDC increases communication over FEDAVG by a constant factor depending on the setting of b and d. The amount of communication per communication round is linear in the number of clients and model size, similar to federated averaging. We investigate the performance of FEDAVG provided with the same communication capacity as FEDDC in our experiments and in App. A.3.6. 5 T HEORETICAL GUARANTEES In this section, we formally show that FEDDC converges for averaging. We, further, provide theoretical bounds on the model quality in convex settings, showing that FEDDC has favorable generalization error in low sample settings compared to standard federated learning. More formally, we first show that under standard assumptions on the empirical risk, it follows from a result of Yu et al. (2019) that FEDDC converges when using averaging as aggregation and SGD for learning—a standard setting in, e.g., federated learning of neural networks. We provide all proofs in the appendix. Corollary 1. Let the empirical risks Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client i ∈ [m] be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with averaging and SGD has a convergence rate ofO(1/ √ mT), where T is the number of local updates. Since model quality in terms of generalization error does not necessarily depend on convergence of training (Haddadpour and Mahdavi, 2019; Kamp et al., 2018), we additionally analyze model quality in terms of probabilistic worst-case guarantees on the generalization error (Shalev-Shwartz and Ben-David, 2014). The average of local models can yield as bad a generalization error as the worst local model, hence, using averaging as aggregation scheme in standard federated learning can yield arbitrarily bad results (cf. Shamir and Srebro, 2014). As the probability of bad local models starkly increases with smaller sample sizes, this trivial bound often carries over to our considered practical settings. The Radon machine (Kamp et al., 2017) is a federated learning approach that overcomes these issues for a wide range of learning algorithms and allows us to analyze (non-trivial) quality bounds of aggregated models under the assumption of convexity. Next, we show that FEDDC can improve model quality for small local datasets where standard federated learning fails to do so. A Radon point (Radon, 1921) of a set of points S from a space X is—similar to the geometric median—a point in the convex hull of S with a high centrality (i.e., a Tukey depth (Tukey, 1975; 4Published as a conference paper at ICLR 2023 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 centralized (test) rounds accuracy train test (a) FEDDC with Radon point with d = 1, b = 50. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (b) Federated learning with Radon point with b = 1. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (c) Federated learning with Radon point with b = 50. Figure 2: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for (a) FEDDC (d = 1, b= 50) and standard federated learning using Radon points for aggregation with (b) b = 1, i.e., the same amount of communication as FEDDC, and (c) b = 50, i.e., the same aggregation period as FEDDC. The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line. Gilad-Bachrach et al., 2004) of at least 2). For a Radon point to exist, S ⊂ Xhas to have a minimum size r ∈ N called the Radon number of X. For X ⊆Rd the radon number is d + 2. Here, the set of points S are the local models, or more precisely their parameter vectors. We make the following standard assumption (V on Luxburg and Sch¨olkopf, 2011) on the local learning algorithm A. Assumption 2((ϵ, δ)-guarantees). The learning algorithm A applied on a dataset drawn iid from D of size n ≥ n0 ∈ N produces a model h ∈ Hs.t. with probability δ ∈ (0, 1] it holds for ϵ >0 that P(ε(h) > ϵ) < δ. The sample size n0 is monotonically decreasing in δ and ϵ (note that typically n0 is a polynomial in ϵ−1 and log(δ−1)). Here ε(h) is the risk defined in Sec. 3. Now let r ∈ N be the Radon number of H, A be a learning algorithm as in assumption 2, and risk ε be convex. Assume m ≥ rh many clients with h ∈ N. For ϵ >0, δ∈ (0, 1] assume local datasets D1, . . . , Dm of size larger thann0(ϵ, δ) drawn iid from D, and h1, . . . , hm be local models trained on them using A. Let rh be the iterated Radon point (Clarkson et al., 1996) with h iterations computed on the local models (for details, see App. A.2). Then it follows from Theorem 3 in Kamp et al. (2017) that for all i ∈ [m] it holds that P(ε(rh) > ϵ) ≤ (r P(ε(hi) > ϵ))2h (1) where the probability is over the random draws of local datasets. That is, the probability that the aggregate rh is bad is doubly-exponentially smaller than the probability that a local model is bad. Note that in PAC-learning, the error bound and the probability of the bound to hold are typically linked, so that improving one can be translated to improving the other (V on Luxburg and Sch¨olkopf, 2011). Eq. 1 implies that the iterated Radon point only improves the guarantee on the confidence compared to that for local models ifδ < r−1, i.e. P(ε(rh) > ϵ) ≤ (r P(ε(hi) > ϵ))2h < (rδ)2h < 1 only holds for rδ < 1. Consequently, local models need to achieve a minimum quality for the federated learning system to improve model quality. Corollary 3. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0 and any h ∈ N, if local datasets D1, . . . , Dm with m ≥ rh are smaller than n0(ϵ, r−1), then federated learning using the Radon point does not improve model quality in terms of (ϵ, δ)-guarantees. In other words, when using aggregation by Radon points alone, an improvement in terms of(ϵ, δ)- guarantees is strongly dependent on large enough local datasets. Furthermore, given δ > r−1, the guarantee can become arbitrarily bad by increasing the number of aggregation rounds. Federated Daisy-Chaining as given in Alg. 1 permutes local models at random, which is in theory equivalent to permuting local datasets. Since the permutation is drawn at random, the amount of permutation rounds T necessary for each model to observe a minimum number of distinct datasets k with probability 1 − ρ can be given with high probability via a variation of the coupon collector problem as T ≥ d m ρ 1 m (Hm − Hm−k), where Hm is the m-th harmonic number—see Lm. 5 in 5Published as a conference paper at ICLR 2023 App. A.5 for details. It follows that when we perform daisy-chaining withm clients and local datasets of size n for at least dmρ− 1 m (Hm − Hm−k) rounds, then each local model will with probability at least 1 −ρ be trained on at least kn distinct samples. For an ϵ, δ-guarantee, we thus need to set b large enough so that kn ≥ n0(ϵ, √ δ) with probability at least 1 − √ δ. This way, the failure probability is the product of not all clients observing k distinct datasets and the model having a risk larger than ϵ, which is √ δ √ δ = δ. Proposition 4.Let H be a model space with Radon numberr ∈ N, ε a convex risk , andA a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0, δ ∈ (0, r−1) and any h ∈ N, and local datasets D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 (2) improves model quality in terms of (ϵ, δ)-guarantees. This result implies that if enough daisy-chaining rounds are performed in-between aggregation rounds, federated learning via the iterated Radon point improves model quality in terms of (ϵ, δ)-guarantees: the resulting model has generalization error smaller than ϵ with probability at least 1 − δ. Note that the aggregation period cannot be arbitrarily increased without harming convergence. To illustrate the interplay between these variables, we provide a numerical analysis of Prop. 4 in App. A.5.1. This theoretical result is also evident in practice, as we show in Fig. 2. There, we compare FEDDC with standard federated learning and equip both with the iterated Radon point on the SUSY binary classification dataset (Baldi et al., 2014). We train a linear model on 441 clients with only 2 samples per client. After 500 rounds FEDDC daisy-chaining every round (d = 1) and aggregating every fifty rounds (b = 50) reached the test accuracy of a gold-standard model that has been trained on the centralized dataset (ACC= 0.77). Standard federated learning with the same communication complexity using b = 1 is outperformed by a large margin (ACC=0.68). We additionally provide results of standard federated learning withb = 50 (ACC=0.64), which shows that while the aggregated models perform reasonable, the standard approach heavily overfits on local datasets if not pulled to a global average in every round. More details on this experiment can be found in App. A.3.2. In Sec. 7 we show that the empirical results for averaging as aggregation operator are similar to those for the Radon machine. First, we discuss the privacy-aspects of FEDDC. 6 D ATA PRIVACY 0 5 · 104 10 · 104 15 · 104 20 · 104 0 0.2 0.4 0.6 0.8 rounds accuracy FEDDC DP-FEDDC (S= 2,σ= 0.01) DP-FEDDC (S= 2,σ= 0.02) DP-FEDDC (S= 4,σ= 0.05) Figure 3: Differential privacy results. Com- parison of FEDDC (top solid line) to FEDDC with clipped parameter updates and Gaussian noise (dashed lines) on CIFAR10 with 250 clients. A major advantage of federated over centralized learn- ing is that local data remains undisclosed to anyone but the local client, only model parameters are exchanged. This provides a natural benefit to data privacy, which is the main concern in applications such as healthcare. However, an attacker can make inferences about lo- cal data from model parameters (Ma et al., 2020) and model updates or gradients (Zhu and Han, 2020). In the daisy-chaining rounds of FEDDC clients receive a model that was directly trained on the local data of an- other client, instead of a model aggregate, potentially facilitating membership inference attacks (Shokri et al., 2017)—reconstruction attacks (Zhu and Han, 2020) remain difficult because model updates cannot be in- ferred since the server randomly permutes the order of clients in daisy-chaining rounds. Should a malicious client obtain model updates through additional attacks, a common defense is applying appropriate clipping and noise before sending models. This guarantees ϵ, δ-differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. This technique is also proven to defend against backdoor and poisoning attacks (Sun et al., 2019). Moreover, FEDDC is compatible with standard defenses against such attacks, such as noisy or robust aggregation (Liu et al., 2022)—FEDDC with the Radon machine is an example of robust aggregation. We illustrate the effectiveness ofFEDDC 6Published as a conference paper at ICLR 2023 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 centralized (test) rounds accuracy train test (a) FEDDC with d = 1, b = 200. 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 rounds (b) FEDAVG with b = 1. 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 rounds (c) FEDAVG with b = 200. Figure 4: Synthetic data results. Comparison of FEDDC (a), FEDAVG with same communication (b) and same averaging period (c) for training fully connected NNs on synthetic data. We report mean and confidence accuracy per client in color and accuracy of central learning as dashed black line. with differential privacy in the following experiment. We train a small ResNet on250 clients using FEDDC with d = 2 and b = 10, postponing the details on the experimental setup to App. A.1.1 and A.1.2. Differential privacy is achieved by clipping local model updates and adding Gaussian noise as proposed by Geyer et al. (2017). The results as shown in Figure 3 indicate that the standard trade-off between model quality and privacy holds for FEDDC as well. Moreover, for mild privacy settings the model quality does not decrease. That is, FEDDC is able to robustly predict even under differential privacy. We provide an extended discussion on the privacy aspects ofFEDDC in App. A.7. 7 E XPERIMENTS ON DEEP LEARNING Our approach FEDDC, both provably and empirically, improves model quality when using Radon points as aggregation which, however, require convex problems. For non-convex problems, in particular deep learning, averaging is the state-of-the-art aggregation operator. We, hence, evaluate FEDDC with averaging against the state of the art in federated learning on synthetic and real world data using neural networks. As baselines, we consider federated averaging (FEDAVG) (McMahan et al., 2017) with optimal communication,FEDAVG with equal communication asFEDDC, and simple daisy-chaining without aggregation. We further consider the 4 state-of-the-art methods FEDPROX (Li et al., 2020a), FEDADAGRAD , FEDYOGI , and FEDADAM (Reddi et al., 2020). As datasets we consider a synthetic classification dataset, image classification in CIFAR10 (Krizhevsky, 2009), and two real medical datasets: MRI scans for brain tumors,2 and chest X-rays for pneumonia3. We provide additional results on MNIST in App. A.3.8. Details on the experimental setup are in App. A.1.1,A.1.2, code is publicly available at https://github.com/kampmichael/FedDC. Synthetic Data: We first investigate the potential ofFEDDC on a synthetic binary classification dataset generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features. On this dataset, we train a simple fully connected neural network with 3 hidden layers on m = 50 clients with n = 10 samples per client. We compare FEDDC with daisy-chaining period d = 1 and aggregation period b = 200 to FEDAVG with the same amount of communication b = 1 and the same averaging period b = 200. The results presented in Fig. 4 show that FEDDC achieves a test accuracy of 0.89. This is comparable to centralized training on all data which achieves a test accuracy of 0.88. It substantially outperforms both FEDAVG setups, which result in an accuracy of 0.80 and 0.76. Investigating the training of local models between aggreation periods reveals that the main issue of FEDAVG is overfitting of local clients, where FEDAVG train accuracy reaches 1.0 quickly after each averaging step. With these promising results on vanilla neural networks, we next turn to real-world image classification problems typically solved with CNNs. CIFAR10: As a first challenge for image classification, we consider the well-known CIFAR10 image benchmark. We first investigate the effect of the aggregation periodb on FEDDC and FEDAVG, separately optimizing for an optimal period for both methods. We use a setting of 250 clients with 2kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection 3kaggle.com/praveengovi/coronahack-chest-xraydataset 7Published as a conference paper at ICLR 2023 a small version of ResNet, and 64 local samples each, which simulates our small sample setting, drawn at random without replacement (details in App. A.1.2). We report the results in Figure 5 and set the period for FEDDC to b = 10, and consider federated averaging with periods of both b = 1 (equivalent communication to FEDDC with d = 1, b= 10) and b = 10 (less communication than FEDDC by a factor of 10) for all subsequent experiments. 1 10 20 50 100 200 500 ∞ 0 0.2 0.4 0.6 0.8 averaging periodb accuracy FEDDC FEDAVG Figure 5: Averaging periods on CIFAR10. For 150 clients with small ResNets and 64 samples per client, we visualize the test accuracy (higher is better) of FEDDC and FEDAVG for different aggregation periods b. Next, we consider a subset of 9600 samples spread across 150 clients (i.e. 64 samples per client), which corresponds to our small sample setting. Now, each client is equipped with a larger, untrained ResNet18.4 Note that the com- bined amount of examples is only one fifth of the original training data, hence we cannot ex- pect typical CIFAR10 performance. To obtain a gold standard for comparison, we run cen- tralized learning CENTRAL , separately optimiz- ing its hyperparameters, yielding an accuracy of around 0.65. All results are reported in Ta- ble 1, where we report FEDAVG with b = 1 and b = 10, as these were the best performing set- tings and b = 1 corresponds to equal amounts of communication as FEDDC. We use a daisy chaining period of d = 1 for FEDDC throughout all experiments for consistency, and provide results for larger daisy chaining periods in App. A.3.5, which, depending on the data distribution, might be favorable. We observe that FEDDC achieves substantially higher accuracy over the baseline set by federated averaging. In App. A.3.7 we show that this holds also for client subsampling. Upon further inspection, we see that FEDAVG drastically overfits, achieving training accuracies of 0.97 (App. A.3.1), a similar trend as on the synthetic data before. Daisy-chaining alone, apart from privacy issues, also performs worse than FEDDC. Intriguingly, also the state of the art shows similar trends. FEDPROX, run with optimal b = 10 and µ = 0.1, only achieves an accuracy of 0.51 and FEDADAGRAD , FEDYOGI , and FEDADAM show even worse performance of around 0.22, 0.31, and 0.34, respectively. While applied successfully on large-scale data, these methods seem to have shortcomings when it comes to small sample regimes. To model different data distributions across clients that could occur in for example our healthcare setting, we ran further experiments on simulated non-iid data, gradually increasing the locally available data, as well as on non-privacy preserving decentralized learning. We investigate the effect of non-iid data on FEDDC by studying the “pathological non-IID partition of the data” (McMahan et al., 2017). Here, each client only sees examples from 2 out of the 10 classes of CIFAR10. We again use a subset of the dataset. The results in Tab. 2 show that FEDDC outperforms FEDAVG by a wide margin. It also outperforms FEDPROX, a method specialized on heterogeneous datasets in our considered small sample setting. For a similar training setup as before, we show results for gradually increasing local datasets in App. A.3.4. Most notably, FEDDC outperforms FEDAVG even with 150 samples locally. Only when the full CIFAR10 dataset is distributed across the clients, FEDAVG is on par with FEDDC (see App. Fig. 7). We also compare with distributed training through gradient sharing (App. A.3.3), which discards any privacy concerns, implemented by mini-batch SGD with parameter settings corresponding to our federated setup as well as a separately optimized version. The results show that such an approach is outperformed by both FEDAVG as well as FEDDC, which is in line with previous findings and emphasize the importance of model aggregation. As a final experiment on CIFAR10, we consider daisy-chaining with different combinations of aggregation methods, and hence its ability to serve as a building block that can be combined with other federated learning approaches. In particular, we consider the same setting as before and combine FEDPROX with daisy chaining. The results, reported in Tab. 2, show that this combination is not only successful, but also outperforms all others in terms of accuracy. Medical image data: Finally, we consider two real medical image datasets representing actual health related machine learning tasks, which are naturally of small sample size. For the brain MRI scans, we simulate25 clients (e.g., hospitals) with8 samples each. Each client is equipped with a CNN 4Due to hardware restrictions we are limited to training 150 ResNets, hence 9600 samples across 150 clients. 8Published as a conference paper at ICLR 2023 CIFAR10 MRI Pneumonia FEDDC (ours) 62.9 ±0.02 78.4 ±0.61 83.2 ±0.84 DC (baseline) 58.4 ±0.85 57.7 ±1.57 79.8 ±0.99 FEDAVG (b=1) 55.8 ±0.78 74.1 ±1.68 80.1 ±1.53 FEDAVG (b=10) 48.7 ±0.87 75.6 ±1.18 79.4 ±1.11 FEDPROX 51.1 ±0.80 76.5 ±0.50 80.0 ±0.36 FEDADAGRAD 21.8 ±0.01 45.7 ±1.25 62.5 ±0.01 FEDYOGI 31.4 ±4.37 71.3 ±1.62 77.6 ±0.64 FEDADAM 34.0 ±0.23 73.8 ±1.98 73.5 ±0.36 CENTRAL 65.1 ±1.44 82.1 ±1.00 84.1 ±3.31 Table 1: Results on image data, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average). CIFAR10 FEDDC 62.9 ±0.02 FEDDC +F EDPROX 63.2 ±0.38 Non-IID FEDDC 34.2 ±0.61 FEDAVG (b=1) 30.2 ±2.11 FEDAVG (b=10) 24.9 ±1.95 FEDPROX 32.8 ±0.00 FEDADAGRAD 11.7 ±0.00 FEDADAM 13.0 ±0.00 FEDYOGI 12.5 ±0.04 Table 2: Combination of FEDDC with FEDAVG and FEDPROX and non-iid results on CIFAR10. (see App. A.1.1). The results for brain tumor prediction evaluated on a test set of53 of these scans are reported in Table 1. Overall, FEDDC performs best among the federated learning approaches and is close to the centralized model. Whereas FEDPROX performed comparably poorly on CIFAR10, it now outperforms FEDAVG. Similar to before, we observe a considerable margin between all competing methods and FEDDC. To investigate the effect of skewed distributions of sample sizes across clients, such as smaller hospitals having less data than larger ones, we provide additional experiments in App. A.3.5. The key insight is that also in these settings, FEDDC outperforms FEDAVG considerably, and is close to its performance on the unskewed datasets. For the pneumonia dataset, we simulate 150 clients training ResNet18 (see App. A.1.1) with 8 samples per client, the hold out test set are 624 images. The results, reported in Table 1, show similar trends as for the other datasets, with FEDDC outperforming all baselines and the state of the art, and being within the performance of the centrally trained model. Moreover it highlights that FEDDC enables us to train a ResNet18 to high accuracy with as little as 8 samples per client. 8 D ISCUSSION AND CONCLUSION We propose to combine daisy-chaining and aggregation to effectively learn high quality models in a federated setting where only little data is available locally. We formally prove convergence of our approach FEDDC, and for convex settings provide PAC-like generalization guarantees when aggregating by iterated Radon points. Empirical results on the SUSY benchmark underline these theoretical guarantees, with FEDDC matching the performance of centralized learning. Extensive empirical evaluation shows that the proposed combination of daisy-chaining and aggregation enables federated learning from small datasets in practice.When using averaging, we improve upon the state of the art for federated deep learning by a large margin for the considered small sample settings. Last but not least, we show that daisy-chaining is not restricted to FEDDC, but can be straight-forwardly included in FEDAVG, Radon machines, and FEDPROX as a building block, too. FEDDC permits differential privacy mechanisms that introduce noise on model parameters, offering protection against membership inference, poisoning and backdoor attacks. Through the random permutations in daisy-chaining rounds, FEDDC is also robust against reconstruction attacks. Through the daisy-chaining rounds, we see a linear increase in communication. As we are primarily interested in healthcare applications, where communication is not a bottleneck, such an increase in communi- cation is negligible. Importantly, FEDDC outperforms FEDAVG in practice also when both use the same amount of communication. Improving the communication efficiency considering settings where bandwidth is limited, e.g., model training on mobile devices, would make for engaging future work. We conclude that daisy-chaining lends itself as a simple, yet effective building block to improve federated learning, complementing existing work to extend to settings where little data is available per client. F EDDC, thus, might offer a solution to the open problem of federated learning in healthcare, where very few, undisclosable samples are available at each site. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGMENTS The authors thank Sebastian U. Stich for his detailed comments on an earlier draft. Michael Kamp received support from the Cancer Research Center Cologne Essen (CCCE). Jonas Fischer is supported by a grant from the US National Cancer Institute (R35CA220523). REFERENCES Idan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized feder- ated learning with gaussian processes. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021. 3 Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. International Journal of Security and Networks, 10(3):137–150, 2015. 22 Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5(1):1–9, 2014. 6, 15 Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019. 22 Kenneth L Clarkson, David Eppstein, Gary L Miller, Carl Sturtivant, and Shang-Hua Teng. Approxi- mating center points with iterative radon points. International Journal of Computational Geometry & Applications, 6(03):357–377, 1996. 5, 15 Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representa- tions for personalized federated learning. In Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2089–2099. PMLR, 18–24 Jul 2021. 3 Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017. 7 Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby. Bayes and tukey meet at the center point. In International Conference on Computational Learning Theory, pages 549–563. Springer, 2004. 5 Kristin L Granlund, Sui-Seng Tee, Hebert A Vargas, Serge K Lyashchenko, Ed Reznik, Samson Fine, Vincent Laudone, James A Eastham, Karim A Touijer, Victor E Reuter, et al. Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1. Cell metabolism, 31(1):105–114, 2020. 1 Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019. 1, 2, 4 Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3310–3319, 2021. 2 Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pages 4423–4434. PMLR, 2021. 2 Marwa Ibrahim, Mohammad Wedyan, Ryan Alturki, Muazzam A Khan, and Adel Al-Jumaily. Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation. Journal of Healthcare Engineering, 2021, 2021. 2 M´ark Jelasity, Alberto Montresor, and Ozalp Babaoglu. Gossip-based aggregation in large dynamic networks. ACM Transactions on Computer Systems (TOCS), 23(3):219–252, 2005. 3 10Published as a conference paper at ICLR 2023 Michael Kamp. Black-Box Parallelization for Machine Learning. PhD thesis, Rheinische Friedrich- Wilhelms-Universit¨at Bonn, Universit¨ats-und Landesbibliothek Bonn, 2019. 3 Michael Kamp, Mario Boley, Olana Missura, and Thomas G ¨artner. Effective parallelisation for machine learning. In Advances in Neural Information Processing Systems , volume 30, pages 6480–6491. Curran Associates, Inc., 2017. 3, 4, 5, 14, 15 Michael Kamp, Linara Adilova, Joachim Sicking, Fabian H ¨uger, Peter Schlicht, Tim Wirtz, and Stefan Wrobel. Efficient decentralized deep learning by dynamic model averaging. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 393–409. Springer, 2018. 3, 4 Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8420–8429, 2019. 1 P´eter Kiss and Tomas Horvath. Migrating models: A decentralized view on federated learning. In Proceedings of the Workshop on Parallel, Distributed, and Federated Learning. Springer, 2021. 3 Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, 2009. 2, 7 Sang Gyu Kwak and Jong Hae Kim. Central limit theorem: the cornerstone of modern statistics. Korean journal of anesthesiology, 70(2):144, 2017. 3 Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 18 Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713–10722, 2021. 3 Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Conference on Machine Learning and Systems, 2020a, 2020a. 2, 3, 7 Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In International Conference on Learning Representations, 2020b. 2 Pei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng. A survey of text data augmentation. In 2020 International Conference on Computer Communication and Network Security (CCNS), pages 191–195. IEEE, 2020. 1 Pengrui Liu, Xiangrui Xu, and Wei Wang. Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives. Cybersecurity, 5(1):4, 2022. 6, 22 Chuan Ma, Jun Li, Ming Ding, Howard H Yang, Feng Shu, Tony QS Quek, and H Vincent Poor. On safeguarding privacy and security in the framework of federated learning. IEEE network, 34(4): 242–248, 2020. 6, 22 Othmane Marfoq, Giovanni Neglia, Aur´elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2021. 2 Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli- gence and Statistics, pages 1273–1282, 2017. 2, 3, 7, 8 Peter Neal. The generalised coupon collector problem. Journal of Applied Probability , 45(3): 621–629, 2008. 20 11Published as a conference paper at ICLR 2023 Corrie A Painter, Esha Jain, Brett N Tomson, Michael Dunphy, Rachel E Stoddard, Beena S Thomas, Alyssa L Damon, Shahrayz Shah, Dewey Kim, Jorge G´omez Tejeda Za˜nudo, et al. The angiosar- coma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research. Nature medicine, 26(2):181–187, 2020. 1 F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 7, 14 Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142–1154, 2022. 3 Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chaplain, David Sontag, and Xavier Amatriain. Few-shot learning for dermatological disease diagnosis. In Machine Learning for Healthcare Conference, pages 532–552. PMLR, 2019. 2 Johann Radon. Mengen konvexer K¨orper, die einen gemeinsamen Punkt enthalten. Mathematische Annalen, 83(1):113–115, 1921. 4, 15 Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2020. 2, 7, 14 Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. In Advances in Neural Information Processing Systems, volume 33, pages 21554–21565. Curran Associates, Inc., 2020a. 2 Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quan- tization. In International Conference on Artificial Intelligence and Statistics, pages 2021–2031. PMLR, 2020b. 3 Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. 4 Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850– 857. IEEE, 2014. 1, 3, 4, 16 Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2017. 1, 6, 22 Xiaoping Su, Xiaofan Lu, Sehrish Khan Bazai, Eva Comp´erat, Roger Mouawad, Hui Yao, Morgan Rouprˆet, Jean-Philippe Spano, David Khayat, Irwin Davidson, et al. Comprehensive integrative profiling of upper tract urothelial carcinomas. Genome biology, 22(1):1–25, 2021. 1 Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019. 6, 22 Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pages 242–264. IGI global, 2010. 2 John W Tukey. Mathematics and picturing data. In Proceedings of the International Congress of Mathematics, volume 2, pages 523–531, 1975. 4 Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, volume 29, pages 3630–3638. Curran Associates, Inc., 2016. 2 Ulrike V on Luxburg and Bernhard Sch¨olkopf. Statistical learning theory: Models, concepts, and results. In Handbook of the History of Logic, volume 10, pages 651–706. Elsevier, 2011. 5 12Published as a conference paper at ICLR 2023 Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Fed- erated learning with matched averaging. In International Conference on Learning Representations, 2019. 2 Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–3469, 2020. 6, 22 Qian Yang, Jianyi Zhang, Weituo Hao, Gregory P. Spell, and Lawrence Carin. Flop: Federated learning on medical datasets using partial networks. InProceedings of the 27th ACM SIGKDD Con- ference on Knowledge Discovery and Data Mining, page 3845–3853. Association for Computing Machinery, 2021. 3 Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693–5700, 2019. 4, 19 Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. InInternational Conference on Machine Learning, pages 7252–7261. PMLR, 2019. 3 Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. Batchcrypt: Effi- cient homomorphic encryption for cross-silo federated learning. In USENIX Annual Technical Conference, pages 493–506, 2020. 22 Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated learning, pages 17–31. Springer, 2020. 6 13Published as a conference paper at ICLR 2023 A A PPENDIX A.1 D ETAILS ON EXPERIMENTAL SETUP In this section we provide all details to reproduce the empirical results presented in this paper. Furthermore, the implementation provided at https://github.com/kampmichael/FedDC allows to directly reproduce the result. Experiments were conducted on an NVIDIA DGX with six A6000 GPUs. A.1.1 N ETWORK ARCHITECTURES Here, we detail network architectures considered in our empirical evaluation. MLP for Synthetic DataA standard multilayer perceptron (MLP) with ReLU activations and three linear layers of size 100,50,20. Averaging round experiment For this set of experiments we use smaller versions of ResNet architectures with 3 blocks, where the blocks use 16, 32, 64 filters, respectively. In essence, these are smaller versions of the original ResNet18 to keep training of 250 networks feasible. CIFAR10 & Pneumonia For CIFAR10, we consider a standard ResNet18 architecture, where weights are initialized by a Kaiming Normal and biases are zero-initialized. Each client constructs and initializes a ResNet network separately. For pneumonia, X-ray images are resized to (224, 224). MRI For the MRI data, we train a small convolutional network of architecture Conv(32)-Batchnorm- ReLU-MaxPool-Conv(64)-Batchnorm-ReLU-MaxPool-Linear, where Conv(x) are convolutional layers with x filters of kernel size 3. The pooling layer uses a stride of 2 and kernel size of 2. The Linear layer is of size 2 matching the number of output classes. All scan images are resized to (150, 150). A.1.2 T RAINING SETUP In this section, we give additional information for the training setup for each individual experiment of our empirical evaluation. SUSY experiments SUSY is a binary classification dataset with18 features. We train linear models with stochastic gradient descent (learning rate 0.0001, found by grid-search on an independent part of the dataset) on 441 clients, aggregating every 50 rounds via the iterated Radon point (Kamp et al., 2017) with h = 2 iterations. F EDDC performs daisy-chaining with period d = 1. The test accuracy is evaluated on a test set with 1 000 000samples drawn iid at random. Synthetic Data The synthetic binary classification dataset is generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features of which 20 are informative, 60 are redundant, and 5 are repeated. We generate 3 clusters per class with a class separation of 1.0, a shift of 1.0 and a scale of 3.0. Class labels are randomly flipped with probability 0.02. Averaging rounds parameter optimizationTo find a suitable number when averaging should be carried out, we explore b ∈ {1, 10, 20, 50, 100, 200, 500, ∞} on CIFAR10 using 250 clients each equipped with a small ResNet. We assign 64 samples to each client drawn at random (without replacement) from the CIFAR10 training data and use a batch size of 64. For each parameter, we train for 10k rounds with SGD using cross entropy loss and initial learning rate of 0.1, multiplying the rate by a factor of .5 every 2500 rounds. FedAdam, FedAdagrad, and FedYogiWe use the standard values forβ1 and β2, i.e., β1 = 0.9, β2 = 0.999, as suggested in Reddi et al. (2020). We optimized learning rate ηl and global learning rate η from the set {0.001, 0.01, 0.1, 1.0, 2.0} yielding optimal parameters ηl = 0.1 and η = 1.0. 14Published as a conference paper at ICLR 2023 CIFAR10 differential privacy and main experimentsWe keep the same experimental setup as for hyperparameter tuning, but now use 100 clients each equipped with a ResNet18. A.2 I TERATED RADON POINTS AND THE RADON MACHINE The Radon machine (Kamp et al., 2017) aggregates a set S = h1, . . . , hm of local models hi ∈ H via the iterated Radon point algorithm (Clarkson et al., 1996). For models with d ∈ N parameters, r = d + 2 many models are required to compute a single Radon point, where r is called the Radon number of H. Let m = rh for some h ∈ N, then the iterated Radon point aggregates models in h iterations. In each iteration, the set S is partitioned into subsets of size r and the Radon point of each subset is calculated. The final step of each iteration is to replace the set S of models by the set of Radon points. After h iterations, a single Radon point rh is obtained as the aggregate. Radon points can be obtained by solving a system of linear equations of size r × r (Kamp et al., 2017): In his main theorem, Radon (1921) gives the following construction of a Radon point for a set S = {s1, ..., sr} ⊆Rd. Find a non-zero solution λ ∈ R|S| for the following linear equations. rX i=1 λisi = (0, . . . ,0) , rX i=1 λi = 0 Such a solution exists, since |S| > d+ 1 implies that S is linearly dependent. Then, let I, Jbe index sets such that for all i ∈ I : λi ≥ 0 and for all j ∈ J : λj < 0. Then a Radon point is defined by r(λ) = X i∈I λi Λ si = X j∈J λj Λ sj , where Λ = P i∈I λi = −P j∈J λj. Any solution to this linear system of equations is a Radon point. The equation system can be solved in time r3. By setting the first element of λ to one, we obtain a unique solution of the system of linear equations. Using this solution λ, we define the Radon point of a set S as r(S) = r(λ) in order to resolve ambiguity. A.3 A DDITIONAL EMPIRICAL RESULTS In Sec. 7 we have shown that FEDDC performs well on benchmark and real-world datasets. In the following we provide additional empirical results, both to investigate the main results more closely, as well as to further investigate the properties of FEDDC. For the CIFAR10 experiment, we investigate training accuracy (App. A.3.1) and present results for distributed mini-batch SGD (App. A.3.3). For the SUSY experiment, we compare to FEDAVG (App. A.3.2). As additional experiments, we investigate the impact of local dataset size (App. A.3.4) and skewed dataset size distributions (App. A.3.5), and analyze the communication-efficiency of FEDDC (App. A.3.6). Finally, we present results on MNIST where FEDDC achieves state-of-the-art accuracy (App. A.3.8). A.3.1 T RAIN AND TEST ACCURACIES ON CIFAR10: In Table 3 we provide the accuracies on the entire training set for the final model, together with test accuracies, on CIFAR10. The high training accuracies of FEDAVG (≈ 0.97)—and to a lesser degree FEDPROX (0.96)—indicate overfitting on local data sets. The poor training performance of FEDADAGRAD , FEDYOGI , and FEDADAM hint at insufficient model updates. A possible explanation is that the adaptive learning rate parameter (which is proportional to the sum of past model updates) becomes large quickly, essentially stopping the training process. The likely reason is that due to large differences in local data distributions, model updates after each aggregation round are large. A.3.2 A DDITIONAL RESULTS ON SUSY In Sec. 5 we compared FEDDC to federated learning with the iterated Radon point. For completeness, we compare it to FEDAVG as well, i.e., federated learning using averaging on the same SUSY binary classification dataset (Baldi et al., 2014). The results shown in Fig. 6 are in line with the findings in Sec. 5: FEDDC with the iterated Radon point outperforms FEDAVG both with the same amount of communication (b = 1) and the same aggregation period (b = 50). The results for b = 50 show that 15Published as a conference paper at ICLR 2023 Test Train FEDDC (ours) 62.9 ±0.02 94.7 ±0.52 DC (baseline) 58.4 ±0.85 94.1 ±2.31 FEDAVG (b=1) 55.8 ±0.78 97.2 ±0.87 FEDAVG (b=10) 48.7 ±0.87 97.4 ±0.23 FEDPROX 51.1 ±0.80 95.9 ±0.42 FEDADAGRAD 21.8 ±0.01 31.7 ±0.25 FEDYOGI 31.4 ±4.37 72.4 ±0.90 FEDADAM 34.0 ±0.23 73.9 ±0.89 Table 3: Train and test accuracy on CIFAR10 of the final model over three runs (± denotes maximum deviation from the average). 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 centralized (test) rounds accuracy train test (a) FEDDC with Radon point with d = 1, b = 50. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (b) Federated learning with averag- ing with b = 1. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (c) Federated learning with averag- ing with b = 50. Figure 6: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for FEDDC with the iterated Radon point (a) and FEDAVG with b = 1 (b) as well as FEDAVG with b = 50 (c). The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line. FEDAVG exhibits the same behavior on local training sets that indicates overfitting. Overall, FEDAVG performs comparably to federated learning with the iterated Radon point. That is, FEDAVG (b = 1 and b = 50) has an accuracy of 0.67, resp. 0.64, compared to 0.68, resp. 0.64 for federated learning with the iterated Radon point. A.3.3 C OMPARISON WITH DISTRIBUTED MINI -BATCH SGD ON CIFAR10 We compare to distributed mini-batch SGD, i.e., central updates where gradients are computed distributedly on CIFAR10. We use the same setup as for the other experiments, i.e., m = 150 clients and a mini-batch size of B = 64, so that the effective mini-batch size for each update is mB = 9600, the optimal learning rate is λ = 0.01. Here, mini-batch SGD achieves a test accuracy of 19.47 ±0.68. Since a plausible explanation for the poor performance is the large mini-batch size, we compare it to the setting with B = 1 to achieve the minimum effective mini-batch size of B = 150. The results are substantially improved to an accuracy of 50.14 ± 0.63, underlining the negative effect of the large batch size in line with the theoretical analysis of Shamir and Srebro (2014). Running it 64 times the number of rounds that FedDC uses improves the accuracy just slightly to 54.3. Thus, even with optimal B = 1 and a 64-times slower convergence, mini-batch SGD is outperformed by both FedAvg and FedDC, since it cannot use the more favorable mini-batch size of B = 64 on m = 150 clients. A.3.4 L OCAL DATASET SIZE In our experiments, we used dataset sizes common in the medical domain, e.g., for radiological images. To further investigate the impact of local dataset sizes on the performance of FEDDC wrt. FEDAVG, we evaluate the performance for local dataset sizes ranging from 2 to 256 (given the size of CIFAR10, 256 is the maximum without creating overlap between local datasets). The results in 16Published as a conference paper at ICLR 2023 Fig. 7 show that FEDDC outperforms all baselines for smaller local datasets. Only for as much as 256 examples FEDAVG performs as good as FEDDC. These results further confirm that FEDDC is capable of handling heterogeneous data: for n <10 the clients only see a subset of labels due to the size of their local datasets (with n=2, each client can at most observe two classes). We find this a more natural non-iid setting. These results indicate that the shuffle mechanism indeed mitigates data heterogeneity well. A further study of the impact of non-iid data, a comparison with personalized FL, and potential improvements to the shuffling scheme are interesting directions for future work Figure 7: Test accuracy wrt. local dataset size on CIFAR10 with 150 clients ( n = 2 i for i ∈ {1, . . . ,8}) with linear (left) and logarithmic (right) x-axis. A.3.5 R EALISTIC DATASET SIZE DISTRIBUTION In medical applications, a common scenario is that some hospitals, e.g., university clinics, hold larger datasets, while small clinics, or local doctors’ offices only hold very small datasets. To simulate such a scenario, we draw local dataset sizes for a dataset of size n so that a fraction c of the clients hold only a minimum number of samples nmin (the local doctor’s offices), and the other clients have an increasing local dataset size starting from nmin until all data is distributed. That is, for clients i = [1, . . . , m− ⌊cm⌋] the dataset sizes are given by nmin + ai with a = 2(n − (⌊cm⌋)nmin) ⌊(1 − c)m⌋(⌊(1 − c)m⌋ −1) . We use the MRI brainscan dataset with c = 0.3 and nmin = 2. The results presented in Tab. 4 show that FEDDC performs well in that setting. FEDDC (d = 4, b= 10) outperforms all other methods with an accuracy of around 0.81, is similar to FEDDC on equally distributed data (0.79), and is even close to the centralized gold-standard (0.82). MRI FEDDC (d=1, b=10) 74.7 ±0.61 FEDDC (d=2, b=10) 74.4 ±1.15 FEDDC (d=4, b=10) 80.6 ±0.66 FEDDC (d=5, b=10) 77.8 ±1.42 DC (baseline) 53.9 ±0.25 FEDAVG (b=1) 70.1 ±2.59 FEDAVG (b=10) 75.3 ±2.37 central 79.9 ±6.23 Table 4: Results for realistic dataset size distribution on MRI, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average). 17Published as a conference paper at ICLR 2023 A.3.6 C OMMUNICATION EFFICIENCY OF FEDDC Although communication is not a concern in cross-silo applications, such as healthcare, the commu- nication efficiency of FEDDC is important in classical federated learning applications. We therefore compare FEDDC with varying amounts of communication to FEDAVG on CIFAR10 in Tab. 5. The results show that FEDDC (d=2) outperforms FEDAVG (b=1), thus outperforming just using half the amount of communication, and FEDDC (d=5) performs similar to FEDAVG (b=1), thus outperforming using five times less communication. FEDDC with d = 10 and b = 10 significantly outperforms FEDAVG (b=10), which corresponds to the same amount of communication in this low-sample setting. CIFAR10 FEDDC (d=1,b=10) 62.9 ±0.02 FEDDC (d=2,b=10) 60.8 ±0.65 FEDDC (d=5,b=10) 55.4 ±0.11 FEDDC (d=10,b=20) 53.8 ±0.47 FEDAVG (b=1) 55.8 ±0.78 FEDAVG (b=10) 48.7 ±0.87 central 65.1 ±1.44 Table 5: Communication efficiency of FEDDC compared to FEDAVG., where FEDDC (d=1,b=10) and FEDAVG (b=1), respectively FEDDC (d=10,b=20) and FEDAVG (b=10) have the same amount of communication. A.3.7 C LIENT SUBSAMPLING A widely used technique to improve communication-efficiency in federated learning is to subsample clients in each communication round. For example, instead of averaging the models of all clients in vanilla FedAvg, only a subset of clients sends their models and receives the average of this subset. By randomly sampling this subset, eventually all clients will participate in the process. The fraction C ∈ (0, 1] of clients sampled is a hyperparameter. In cross-SILO applications, such as healthcare, communication-efficiency is not relevant and client participation is assumed to beC = 1.0. Client subsampling can naturally be used in FEDDC by sampling clients both in daisy-chaining and aggregation rounds. We conducted an experiment on CIFAR10 where we compare FEDDC using subsampling to FEDAVG (b = 10). The results in Table 6 show that FEDDC indeed works well with client subsampling and outperforms FEDAVG with C = 0.2, similar to full client participation (C=1.0). However, due to the restricted flow of information, the training process is slowed. By prolonging training from 10000 to 30000 rounds, FEDDC with C = 0.2 reaches virtually the same performance as in full client participation, but with higher variance. The same holds true forFEDAVG. T = 10 000 T = 30 000 C = 1.0 C = 0.2 C = 0.2 FEDDC (d=1,b=10) 62.9 ±0.02 53.2 ±3.42 61.0 ±1.07 FEDAVG (b=10) 48.7 ±0.87 45.9 ±6.94 49.3 ±5.23 Table 6: Client subsampling of FEDDC compared to F EDAVG on CIFAR10. A.3.8 A DDITIONAL RESULTS ON MNIST In order to further demonstrate the efficiency of FEDDC on clients that achieve state-of-the-art performance we perform experiments on the MNIST (LeCun et al., 1998) dataset. We use a CNN with two convolutional layers with max-pooling, followed by two linear layers with 1024, resp. 100 neurons. Centralized training on all 60 000training samples of MNIST achieves a test accuracy of 0.994 which is similar to the state-of-the-art. The results for m = 50 clients in Tab. 7 show that 18Published as a conference paper at ICLR 2023 FEDDC outperforms FEDAVG both with the same amount of communication, i.e., FEDAVG (b = 1) and FEDAVG (b = 10). In line with the results on CIFAR10 (cf. Fig. 7), the advantage of FEDDC shrinks with increasing local dataset size. Using n = 1200, i.e., the full training set distributed over m = 50 clients, results in virtually the same performance of FEDDC and F EDAVG, both reaching a test accuracy of around 0.96. FEDDC (d=1,b=10) F EDAVG (b=1) F EDAVG (b=10) n = 8 87 .6 84 .4 84 .9 n = 1200 96 .7 96 .3 96 .5 Table 7: Performance of FEDDC and F EDAVG on MNIST for varying local dataset sizes n. A.4 P ROOF OF CONVERGENCE Corollary. Let the empirical risks Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with averaging and SGD as learning algorithm has a convergence rate of O(1/ √ mT), where T ∈ N is the number of local updates. Proof. We assume that each client i ∈ [m] uses SGD as learning algorithm. In each iteration t ∈ [T], a client i computes the gradient Gi t = ∇ℓ(hi t(x), y) with x, y∈ Di drawn randomly and updates the local model hi t+1 = hi t − γGi t, where γ >0 denotes the learning rate. FEDDC with daisy-chaining period d and averaging period b, run for T local iterations, computes T/b local gradients at each of the m clients before averaging. Each local gradient is computed on an iid sample from D, independent of whether local models are permuted. Therefore, FEDDC with averaging and SGD is equivalent to parallel restarted SGD (PR-SGD) (Yu et al., 2019) withb/d times larger local datasets. Yu et al. (2019) analyze the convergence of PR-SGD with respect to the average ht of local models in round t. Since Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, Theorem 1 in Yu et al. (2019) is applicable. It then follows from Corollary 1 (Yu et al., 2019) that forγ = √m/(L √ T) and b ≤ T 1 4 /m 3 4 it holds that 1 T E h TX t=1 E(x,y)∼D h Ei emp \u0000 ht(x), y \u0001ii ≤ 2L√ mT \u0010 E(x,y)∼D h Ei emp \u0000 h0(x), y \u0001i −E(x,y)∼D h Ei emp \u0010 h ∗ (x), y \u0011i\u0011 + 1√ mT \u0000 4G2 + σ2\u0001 ∈ O \u0012 1√ mT \u0013 . Here, the first expectation is over the draw of local datasets and h ∗ is given by h ∗ = arg min h∈H E(x,y)∼D h Eemp(h(x), y) i . Thus, FEDDC with averaging and SGD converges in O(1/ √ mT). A.5 P ROOF OF MODEL QUALITY IMPROVEMENT BY FEDDC In order to proof Prop. 4, we first need the following Lemma. Lemma 5. Given δ ∈ (0, 1], m ∈ N clients, and k ∈ [m], if Algorithm 1 with daisy chaining period d ∈ N is run for T ∈ N rounds with T ≥ d m ρ 1 m (Hm − Hm−k) where Hm is the m-th harmonic number, then each local model has seen at least k distinct datasets with probability 1 − ρ. 19Published as a conference paper at ICLR 2023 Note that Hm ≈ log m + γ + 1 2 + O \u0000 1 m \u0001 where γ ≈ 0.5772156649 denotes the Euler-Mascheroni- constant. Proof. For a single local model, it follows from the coupon collector problem (Neal, 2008) that the expected number of daisy-chaining rounds R required to see at least k out of m clients is at least m(Hm − Hm−k), where Hm = mX i=1 1 i is the m-th harmonic number. To see that, consider that for the first distinct client the chance to pick it is m m−1 , for the second m m−2 and for the k-th it is m m−k+1 , which sums up to m(Hm − Hm−k). Applying the Markov inequality yields P \u0012 R ≥ 1 ρm(Hm − Hm−k) \u0013 ≤ ρ . The probability for all local models to have seen at least k clients then is at most ρm. Thus, if we perform at least R ≥ m ρ 1 m m(Hm − Hm−k) daisy-chaining rounds, then the probability that each local model has not seen at least k distinct datasets is smaller than ρ. The result follows from the fact that the number of daisy-chaining rules is R = T/d. Note that Hk can be approximated as Hm ≈ log m + γ + 1 2 + O \u0012 1 m \u0013 where γ = limm→∞(Hm − ln m) ≈ 0.5772156649 denotes the Euler-Mascheroni-constant. From this it follows that Hm − Hm−k ≈ ln m m − k + O \u0012 1 m − 1 m − k \u0013 With this, we can now proof Prop. 4 that we restate here for convenience. Proposition. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0, δ ∈ (0, r−1) and any h ∈ N, and local datasets D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 improves model quality in terms of (ϵ, δ)-guarantees. Proof. For b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 it follows from Lemma 5 with k = l n−1n0 \u0010 ϵ, √ δ \u0011m that with probability 1 − √ δ all local models are trained on at least kn = n0 \u0010 ϵ, √ δ \u0011 samples. Thus an (ϵ, √ δ)-guarantee holds for each model with probability 1 − √ δ. It follows from Eq. 1 that the probability that the risk is higher than ϵ is P(ε(rh) > ϵ) < \u0010 r √ δ √ δ \u00112h = (rδ)2h . The result follows from δ < r−1 and Eq. (1). 20Published as a conference paper at ICLR 2023 A.5.1 N UMERICAL ANALYSIS OF PROPOSITION 4 The lower bound on the aggregation period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 grows linearly with the daisy-chaining period and a factor depending on the number of clients m, the error probability δ, and the required number of hops l n−1n0 \u0010 ϵ, √ δ \u0011m . Applying the bound to the experiment using the Radon point on SUSY in Sec. 5 with m = 441 clients, daisy-chaining period d = 1, and local dataset size of n = 2 for local learners achieving an (ϵ = 0.05, δ= 0.01)-guarantee requires b ≥ 49.9 to improve model quality according to Prop. 4. For b = 50 like in our experiments, Prop. 4 thus predicts that model quality is improved, under the assumption that n0(ϵ, δ) = 1 ϵ log 1 δ . Even though, the experiments on CIFAR10 in Section 7 are non-convex, and thus Prop. 4 does not apply, we can still evaluate the predicted required aggregation period: with m = 150 clients, daisy-chaining period d = 1, local dataset size of n = 64, and local learners achieving an (ϵ = 0.01, δ= 0.01)-guarantee requires b ≥ 8.81. We now analyze the scaling behavior with the error probability δ for various local dataset sizes in Fig. 8a. The lower the error probability, the larger the required aggregation period b, in particular for small local datasets. If local datasets are sufficiently large, the aggregation period can be chosen very small. In Fig. 8b we investigate the required aggregation period b for local learners achieving an (ϵ = 0.01, δ= 0.01)-guarantee in relation to the local dataset size n. Indeed, the smaller the local dataset size, the larger the required aggregation period. We also see that for smaller numbers of clients, more aggregation rounds are required, since the chance of a model visiting the same client multiple times is larger. 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 error probabilityδ aggregation periodb n= 8 n= 16 n= 32 n= 64 n= 128 n= 256 (a) Aggregation period b for error probability δ for varying local dataset sizes with m = 150clients and ϵ = 0.01. 23 24 25 26 27 0 100 200 number local samplesn aggregation periodb m = 50 m = 100 m = 150 m = 500 m = 2000 (b) Aggregation period b for local dataset size n for varying numbers of clients m, with ϵ = 0.01 and δ = 0.01. A.6 N OTE ON COMMUNICATION COMPLEXITY In our analysis of the communication complexity, we assume the total amount of communication to be linear in the number of communication rounds. For some communication systems the aggregated model can be broadcasted to individual clients which is not possible in daisy-chaining rounds, reducing the communication complexity in FedAvg. In most scenarios, fiber or GSM networks are used where each model has to be sent individually, so there is no substantial difference between broadcasting a model to all clients and sending an individual model to each client. Therefore, also in this case the amount of communication rounds determines the overall amount of communication. 21Published as a conference paper at ICLR 2023 A.7 E XTENDED DISCUSSION ON PRIVACY Federated learning only exchanges model parameters, and no local data. It is, however, possible to infer upon local data given the model parameters, model updates or gradients (Ma et al., 2020). In classical federated learning there are two types of attacks that would allow such inference: (i) an attacker intercepting the communication of a client with the server, obtaining models and model updates to infer upon the clients data, and (ii) a malicious server obtaining models to infer upon the data of each client. A malicious client cannot learn about a specific other client’s data, since it only obtains the average of all local models. In federated daisy-chaining there is a third possible attack: (iii) a malicious client obtaining model updates from another client to infer upon its data. In the following, we discuss potential defenses against these three types of attacks in more detail. Note that we limit the discussion on attacks that aim at inferring upon local data, thus breaching data privacy. Poisoning (Bhagoji et al., 2019) and backdoor (Sun et al., 2019) attacks are an additional threat in federated learning, but are of less importance for our main setting in healthcare: there is no obvious incentive for a hospital to poison a prediction. It is possible that FEDDC presents a novel risk surface for those attacks, but such attack strategies are non-obvious. Robust aggregation, such as the Radon point, are suitable defenses against such attacks (Liu et al., 2022). Moreover, the standard mechanisms that guarantee differential privacy also defend against backdoor and poising attacks (Sun et al., 2019). A general and wide-spread approach to tackle all three possible attack types is to add noise to the model parameters before sending. Using appropriate clipping and noise, this guarantees ϵ, δ- differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. We empirically demonstrated that FEDDC performs well under such noise in Sec. 6. Another approach to tackle an attack on communication (i) is to use encrypted communication. One can furthermore protect against a malicious server (ii) by using homomorphic encryption that allows the server to average models without decrypting them (Zhang et al., 2020). This, however, only works for particular aggregation operators and does not allow to perform daisy-chaining. Secure daisy-chaining in the presence of a malicious server (ii) can, however, be performed using asymmetric encryption. Assume each client creates a public-private key pair and shares the public key with the server. To avoid the malicious server to send clients its own public key and act as a man in the middle, public keys have to be announced (e.g., by broadcast). While this allows sending clients to identify the recipient of their model, no receiving client can identify the sender. Thus, inference on the origin of a model remains impossible. For a daisy-chaining round the server sends the public key of the receiving client to the sending client, the sending client checks the validity of the key and sends an encrypted model to the server which forwards it to the receiving client. Since only the receiving client can decrypt the model, the communication is secure. In standard federated learning, a malicious client cannot infer specifically upon the data of another client from model updates, since it only receives the aggregate of all local models. In federated daisy-chaining, it receives the model from a random, unknown client in each daisy-chaining round. Now, the malicious client can infer upon the membership of a particular data point in the local dataset of the client the model originated from, i.e., through a membership inference attack (Shokri et al., 2017). Similarly, the malicious client can infer upon the presence of data points with certain attributes in the dataset (Ateniese et al., 2015). The malicious client, however, does not know the client the model was trained on, i.e., it does not know the origin of the dataset. Using a random scheduling of daisy-chaining and aggregation rounds at the server, the malicious client cannot even distinguish between a model from another client or the average of all models. Nonetheless, daisy-chaining opens up new potential attack vectors (e.g., clustering received models to potentially determine their origins). These potential attack vectors can be tackled in the same way as in standard federated learning, i.e., by adding noise to model parameters as discussed above, since “[d]ifferentially private models are, by construction, secure against membership inference attacks” (Shokri et al., 2017). 22",
      "references": [
        "Personalized federated learning with gaussian processes",
        "Analyzing federated learning through an adversarial lens",
        "Approximating center points with iterative radon points",
        "Exploiting shared representations for personalized federated learning",
        "Differentially private federated learning: A client level perspective",
        "Bayes and tukey meet at the center point",
        "Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1",
        "On the convergence of local descent methods in federated learning",
        "Towards fair federated learning with zero-shot data augmentation",
        "Fl-ntk: A neural tangent kernel-based framework for federated learning analysis",
        "Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation",
        "Gossip-based aggregation in large dynamic networks",
        "Black-Box Parallelization for Machine Learning",
        "Effective parallelisation for machine learning",
        "Efficient decentralized deep learning by dynamic model averaging",
        "Few-shot object detection via feature reweighting",
        "Migrating models: A decentralized view on federated learning",
        "Central limit theorem: the cornerstone of modern statistics",
        "Model-contrastive federated learning",
        "Federated optimization in heterogeneous networks",
        "Fedbn: Federated learning on non-iid features via local batch normalization",
        "A survey of text data augmentation",
        "Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives",
        "On safeguarding privacy and security in the framework of federated learning",
        "Federated multi-task learning under a mixture of distributions",
        "Communication-efficient learning of deep networks from decentralized data",
        "The generalised coupon collector problem",
        "The angiosarcoma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research",
        "Scikit-learn: Machine learning in Python",
        "Robust aggregation for federated learning",
        "Few-shot learning for dermatological disease diagnosis",
        "Mengen konvexer Körper, die einen gemeinsamen Punkt enthalten",
        "Adaptive federated optimization",
        "Robust federated learning: The case of affine distribution shifts",
        "Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization",
        "Understanding machine learning: From theory to algorithms",
        "Distributed stochastic optimization and learning",
        "Membership inference attacks against machine learning models",
        "Comprehensive integrative profiling of upper tract urothelial carcinomas",
        "Can you really backdoor federated learning?",
        "Transfer learning",
        "Mathematics and picturing data",
        "Matching networks for one shot learning",
        "Statistical learning theory: Models, concepts, and results",
        "Federated learning with matched averaging",
        "Federated learning with differential privacy: Algorithms and performance analysis",
        "Flop: Federated learning on medical datasets using partial networks",
        "Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning",
        "Bayesian nonparametric federated learning of neural networks",
        "Batchcrypt: Efficient homomorphic encryption for cross-silo federated learning",
        "Deep leakage from gradients",
        "Searching for exotic particles in high-energy physics with deep learning",
        "Learning multiple layers of features from tiny images"
      ],
      "meta_data": {
        "arxiv_id": "2110.03469v3",
        "authors": [
          "Michael Kamp",
          "Jonas Fischer",
          "Jilles Vreeken"
        ],
        "published_date": "2021-10-07T13:49:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Federated Daisy-Chaining (FEDDC), a hybrid training protocol that alternates random permutation of models across clients with periodic server-side aggregation. This design mitigates the failure of traditional federated averaging when local datasets are extremely small. The paper supplies: (1) the FEDDC algorithm, (2) convergence proof for non-convex objectives with averaging, (3) PAC-style generalization guarantees for convex problems when combined with Radon-point aggregation, (4) privacy analysis and differentially-private variant, and (5) extensive experiments showing large accuracy gains over FEDAVG, FEDPROX, FedAdam/Yogi/Adagrad, and pure daisy-chaining on SUSY, CIFAR10, and two medical imaging tasks.",
        "methodology": "Key idea: Each client trains its model for a few local steps, sends it to the server; instead of immediately averaging, the server randomly permutes and forwards the untouched model to another client (daisy-chaining). After a fixed number of such rounds (parameter b), the server aggregates all current client models using a selectable operator (mean, Radon point, FedProx, etc.) and sends the aggregate back. Analysis shows that daisy-chaining effectively concatenates tiny local datasets into a virtual larger dataset per model while the aggregation step prevents divergence/overfitting. Proofs derive O(1/√(mT)) convergence for SGD with averaging and double-exponential improvement in PAC bounds using iterated Radon points. Privacy is addressed via differential privacy noise and optional encryption.",
        "experimental_setup": "Convex test: SUSY dataset (5M samples) split over 441 clients with only 2 samples each; linear classifier, Radon aggregation, compare FEDDC (d=1,b=50) with FEDAVG and centralized. Non-convex tests: • Synthetic 100-feature classification, 50 clients, 10 samples each, 3-layer MLP. • CIFAR10: 150–250 clients, 64 samples each, ResNet-18 or mini-ResNet; IID and pathological non-IID splits; variations of d, b, client subsampling, DP noise. • Brain MRI tumor detection: 25 clients, 8 scans each, small CNN. • Chest X-ray pneumonia: 150 clients, 8 images each, ResNet-18. Baselines: centralized training, plain daisy-chaining, FEDAVG (b=1 & 10), FEDPROX, FedAdam, FedYogi, FedAdagrad, distributed minibatch SGD. Metrics: train/test accuracy, convergence curves, communication rounds. Additional ablations: dataset-size sweep, skewed data sizes, communication budget, DP impact.",
        "limitations": "1) Extra communication: daisy-chaining adds roughly T/d additional message rounds, which may be costly in bandwidth-constrained edge settings.\n2) Requires a trusted coordinator to orchestrate permutations; paper does not provide fully decentralized or secure aggregation implementation.\n3) Theoretical guarantees cover convex losses (Radon) and assume known sample-complexity n0; non-convex deep-learning results remain empirical.\n4) Privacy risks persist because clients receive raw models from others; defense relies on added noise or encryption, which may degrade accuracy/efficiency.\n5) Hyper-parameters (d, b) need tuning and trade-off between overfitting, convergence and communication not fully characterized.\n6) Experiments focus on medium-scale vision and tabular tasks; scalability to very large models, millions of clients, or real-time mobile FL is untested.",
        "future_research_directions": "• Develop communication-efficient variants (model compression, adaptive d and b, peer-to-peer permutations).\n• Extend theoretical analysis to non-convex objectives and establish convergence rates with momentum/Adam.\n• Integrate stronger privacy mechanisms (secure multi-party computation, homomorphic encryption, robust aggregation) for daisy-chaining.\n• Combine FEDDC with personalized or clustered federated learning to handle heterogeneous client objectives.\n• Explore automated scheduling strategies that adapt permutation and aggregation frequency based on detected overfitting or gradient diversity.\n• Validate on larger-scale real-world deployments (e.g., nationwide hospital networks or cross-device mobile settings) and assess fault tolerance and straggler effects.\n• Study robustness against poisoning/backdoor attacks specific to the daisy-chaining pattern.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Retrieval-Enhanced Contrastive Vision-Text Models",
      "full_text": "Published as a conference paper at ICLR 2024 RETRIEVAL -ENHANCED CONTRASTIVE VISION -TEXT MODELS Ahmet Iscen Mathilde Caron Alireza Fathi Cordelia Schmid Google Research ABSTRACT Contrastive image-text models such as CLIP form the building blocks of many state-of-the-art systems. While they excel at recognizing common generic concepts, they still struggle on fine-grained entities which are rare, or even absent from the pre-training dataset. Hence, a key ingredient to their success has been the use of large-scale curated pre-training data aiming at expanding the set of concepts that they can memorize during the pre-training stage. In this work, we explore an alternative to encoding fine-grained knowledge directly into the model’s parameters: we instead train the model to retrieve this knowledge from an external memory. Specifically, we propose to equip existing vision-text models with the ability to refine their embedding with cross-modal retrieved information from a memory at inference time, which greatly improves their zero-shot predictions. Remarkably, we show that this can be done with a light-weight, single-layer, fusion transformer on top of a frozen CLIP. Our experiments validate that our retrieval-enhanced contrastive (RECO) training improves CLIP performance substantially on several challenging fine-grained tasks: for example +10.9 on Stanford Cars, +10.2 on CUB-2011 and +7.3 on the recent OVEN benchmark, where we even outperform the fine-tuned models on unseen classes. 1 I NTRODUCTION In the recent years, we have witnessed a surge in the development of vision-language models highly adaptable to a broad spectrum of downstream tasks (Jia et al., 2021; Radford et al., 2021; Yu et al., 2022; Chen et al., 2023; Singh et al., 2022). These models work by pre-training two parallel encoders using contrastive learning (van den Oord et al., 2018) on large-scale, carefully curated, image-text data (Radford et al., 2021). These two-tower models learn to encode images and texts into an aligned latent space which enables appealing capabilities such as zero-shot transfer to different downstream applications, e.g. image classification (Radford et al., 2021), image-text retrieval (Plummer et al., 2015) or open-world recognition (Minderer et al., 2022; Liang et al., 2022). Although these models have achieved state-of-the-art results across various generic vision-language benchmarks, we observe that they tend to struggle on tasks requiring a more fine-grained understanding of visual or textual entities. Our hypothesis is that this disparity stems from the fact that it is hard to align the image and text modalities. While every image is metaphorically valued at a thousand words, it is often paired with a short, sometimes noisy, text that neither exclusively nor comprehensively describes it. For example, current vision-language models are good at associating images of cars with generic concepts such as “car”, “mechanics” or “road trip”, because these are common words paired with car images, but less at finegrained, instance-level, associations such as the specific brand, series or year of that car. This might therefore produce poor accuracy for zero-shot fine-grained car classification. The current path taken by the research community has been to ever scale and curate the pre-training dataset in the hope of covering more and more image-text associations (Radford et al., 2021; Schuh- mann et al., 2021; Alayrac et al., 2022; Chen et al., 2023). An orthogonal effort has focused instead on memory or knowledge-based approaches (Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard et al., 2022; Guu et al., 2020b; Liu et al., 2023; Shen et al., 2022). These methods, instead of statically ingesting and memorizing all the world knowledge into model parameters, propose to rely on the access to an external source of knowledge. For example, K-Lite (Shen et al., 2022) explores how to improve vision-text models by enhancing the text captions with more comprehensive text definitions 1 arXiv:2306.07196v2  [cs.CV]  21 Feb 2024Published as a conference paper at ICLR 2024 retrieved from an external dictionary, i.e. WordNet (Meyer & Gurevych, 2012) or Wiktionary (Miller, 1998). One caveat that we identify in this approach is that initial captions are augmented within their modality only, hence limiting the potential added-value brought by the retrieved items. To mitigate this issue, we put forth a retrieval-augmented approach that enhances the alignment between image and text representation. A critical observation of ours is that matching representations within the same modality is a significantly simpler task than matching representations across different modalities. To clarify, we observe that the image representation can be effectively utilized to identify images closely resembling the query image, or the text representation can be used to identify texts closely resembling the query text. However, when crossing modalities, these representations are less successful in identifying suitable matches, such as finding the text with the closest representation to a query image representation. We utilize the inherent strength of learned image and text representations within their respective modalities to aid the alignment across modalities. To improve their compatibil- ity, we convert these unimodal representations into a multi-modal format, as conceptually illustrated in Fig. 1. Utilizing a web-scale corpus of image-text pairs for retrieval, we use image representation as a query to identify the top-k most similar images and incorporate the associated text to create a multi-modal representation. In a parallel manner, given a text representation as a query, we find the top-k most similar texts and integrate the associated images to create a multi-modal representation. Through this process, we successfully transform the image and text representations into multi-modal versions, which significantly simplifies their alignment. Our approach does not presuppose any downstream knowledge and produces a single generic model that can be used effectively across different tasks. We show that our method improves over original CLIP (Radford et al., 2021) or LiT (Zhai et al., 2022) models on 11 challenging fine-grained downstream tasks. 2 R ELATED WORK Vision-text pre-training. While early works have shown the promise of representation learning from image-text paired data (Zhang et al., 2022; Gomez et al., 2017; Joulin et al., 2016; Desai & Johnson, 2021), recent popular papers such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have truly unleashed the potential of contrastive image-text pre-training. This paradigm simply works with two parallel uni-modal encoders that learn to distinguish between aligned and non-aligned image-text pairs through a cross-modal contrastive objective (van den Oord et al., 2018; Miech et al., 2020). Appealing properties of these models are simplicity, scalability and great zero-shot performance (Xian et al., 2018). As a result, vision-text contrastive models now form the basic building blocks of more powerful foundational models, such as CoCa (Yu et al., 2022), Flamingo (Alayrac et al., 2022), FLA V A (Singh et al., 2022), and PaLI (Chen et al., 2023) for example. In our work, we enhance the capabilities of the CLIP model (Radford et al., 2021), by adding a light-weight retrieval module. Nevertheless, our method is not specific to CLIP and can be applied to any vision-text model. Knowledge-based vision-text models. Several works have focused on ways of improving upon different aspects of the contrastive vision-text models, such as their training objectives (Gao et al., 2022; Zhai et al., 2022; Dong et al., 2022) or through scaling (Cherti et al., 2022; Pham et al., 2021). Yet, only little exploration has been done on their combination with memory or knowledge-based techniques (Dwibedi et al., 2021; Banani et al., 2023; Liu et al., 2023; Shen et al., 2022; Fan et al., 2023). REACT (Liu et al., 2023) retrieves image-text pairs from an external memory in order to build a training dataset specialized for a specific downstream task. Unlike REACT (Liu et al., 2023), our work does not require any pre-knowledge about the nature of the downstream task, and is hence applicable in a full zero-shot transfer. Another key difference is that our model can leverage items from the memory at inference time, while REACT uses retrieved items to automatically generate a training set to finetune their model. Closer to our work, K-LITE (Shen et al., 2022) learns vision-text models by leveraging external sources of knowledge (i.e. WordNet (Meyer & Gurevych, 2012) or Wiktionary (Miller, 1998)) to complete captions with more descriptive content. Unlike our approach, the retrieved knowledge is uni-modal (e.g. they complement text with more text) and the external memory is not used for the image tower. Also using a knowledge-based approach but for image-only representation learning, NNCLR (Dwibedi et al., 2021) finds the visual nearest-neighbor of each training image from a memory for contrastive learning. LGSimCLR (Banani et al., 2023) uses the language guidance to find most similar visual nearest-neighbor. Unlike our work, NNCLR and LGSimCLR only learn visual representations and use retrieval to enhance their supervision during training but not at inference. 2Published as a conference paper at ICLR 2024 Figure 1: RECO works by complementing the frozen representations of pre-trained image-text encoders (such as CLIP) with knowledge retrieved from an external memory. We use an image representation as a query to identify the k most similar images and integrate their associated text embeddings to create a multi-modal representation. Likewise, given a text representation as a query, we find the top-k most similar texts and incorporate their associated images. The fusion of original and retrieved embeddings is done by learning a shallow fusion model to produce improved, multi- modal and knowledge-enhanced versions of the original embeddings. We train for alignment between the refined embeddings, as well as between the refined and original embeddings. Retrieval-based methods. The main argument of the retrieval-based methods is that not all the world knowledge can be compiled into a model’s parameters. Thus, the model should also learn to rely on items retrieved from an external memory at inference. Retrieval-based methods have shown their promise in various NLP tasks (Khandelwal et al., 2020; Guu et al., 2020a; Lewis et al., 2020; Wang et al., 2022; Wu et al., 2022; Borgeaud et al., 2022). More recently, there is an increasing interest in the computer vision for retrieval-based methods as well (Blattmann et al., 2022; Fürst et al., 2022; Chen et al., 2022; Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard et al., 2022; Guu et al., 2020b; Liu et al., 2023; Shen et al., 2022; Iscen et al., 2023). Of particular interest, SuS-X (Udandarao et al., 2023) shows that by either retrieving similar samples to the query sample from a large data-bank like LAION can improve zero-shot classification performance of CLIP. Conceptually, SuS-X falls under the “Cross-modal search and cross-modal fusion”variant explored in this paper (see second scenario in Fig. 2). RA-CLIP (Xie et al., 2023) enriches the CLIP visual representation by retrieved image and text. However, their attempt to enrich the text representation degrades the performance, whereas we show that the retrieved data can also help produce better text representations. 3 M ETHOD Our goal is to equip powerful pre-trained vision-language models (such as CLIP) with the ability to complement their representations with cross-modal knowledge retrieved from an external memory. We aim to do this without requiring such models to be retrained from scratch, but by simply learning a light-weight retrieval fusion module on top of them. We emphasize that this work does not propose a new model or loss but rather a new way of adapting pre-trained models to use relevant retrieved knowledge at inference time. An overview of our approach, RECO, is shown in Fig. 1. Preliminaries. We are given a pre-trained frozen dual-encoder vision-text model f, where v = fimage(I) is the embedding of image I, and t = ftext(T) is the embedding of text T. We say that these embeddings are uni-modal since they are obtained purely from a single modality, either image or text. We assume that image and text embedding spaces are already aligned, meaning that they have been trained to produce similar representations for matching image-text pairs and dissimilar representations for non-matching pairs (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2022; van den Oord et al., 2018). This alignment is usually obtained by minimizing the InfoNCE loss (or contrastive loss) (van den Oord et al., 2018) between embeddings of different modalities: LNCE(V, T) = − nX i=1  log ev⊤ i ti/τ P j ev⊤ i tj /τ + log ev⊤ i ti/τ P j ev⊤ j ti/τ  , (1) where V (resp. T) is the matrix composed of the n visual (resp. text) embeddings in the minibatch and τ is the temperature parameter. We propose to augment the text and visual embeddings, i.e. t and v, with external cross-modal knowledge in order to enhance both their expressiveness and their cross-modality alignment. In the following of this section, we first detail how we retrieve relevant 3Published as a conference paper at ICLR 2024 Uni-modal search Cross-modal search Uni-modal search Cross-modal search & cross-modal fusion & cross-modal fusion & uni-modal fusion & uni-modal fusion (Ours) Figure 2: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion.We illustrate the different scenarios for an input imageI while the scenarios for text inputT are shown in Appendix. cross-modal knowledge based on within-modality search. Second, we present how we learn to fuse the retrieved information into the original embeddings. 3.1 R ETRIEVING CROSS -MODAL EXTERNAL KNOWLEDGE Memory. We define the external source of knowledge by a memory M = {(Ii, Ti)}M i=1 of M image-text pairs. We assume that M is very large and covers a broad coverage of concepts. In practice, only a small-subset of M is relevant for a given input query. Thus, we only consider the k most relevant items from M for each input obtained by the nearest neighbour search. We denote by KNN(v, M) and KNN(t, M) the sets formed by the embeddings of the k most relevant items to the queries v and t from the memory, where KNN refers to the nearest-neighbour retrieval module. Cross-modal fusion. Our goal is to augment the text and visual original embeddings with cross- modal knowledge, not necessarily learned during the pre-training stage. For example, given the class name Yellow bellied flycatcher in a fine-grained bird classification problem such as CUB (Wah et al., 2011), we first look for captions in the memory that are semantically similar to the given class name. We then augment the class name representation with the visual representations of the retrieved similar captions, i.e. with what an Yellow bellied flycatcher looks like. Likewise, given a visual representation of a bird, we look for similar images in M and use their corresponding captions in the hope that some of them might contain useful information for our problem such as the species of that bird. Specifically, for a given text or image input, the retrieval module KNN(., M) returns items with the opposite modality than that of the input. We use the subscripts v or t to specify the modality of the retrieved embeddings. That is, KNNt(v, M) returns text embeddings from an image input and KNNv(t, M) returns image embeddings for text input. Note that we also evaluate uni-modal fusion in our experiments, i.e. complementing visual repre- sentations with the retrieved visual knowledge and text representation with the retrieved captions. However, we find in practice that this variant leads to poorer performance than cross-modal fusion, as shown in Tab. 3. Intuitively, we hypothesize that this is because the signal brought by cross-modal fusion is richer due to the complementarity of the different modalities (Iscen et al., 2023). Uni-modal search. We choose to search relevant items in the memory M based on within-modality similarities, which we refer to as “uni-modal search” as opposed to “cross-modal search”. Specifically, we use text-to-text similarity (t → t) to identify suitable content from a text embedding t and image- to-image similarity (v → v) to retrieve relevant matches from a visual embedding v. Formally, let us denote by VM and TM all the image and text embeddings from M given by our pretrained vision- text model f, i.e. we have VM = [fimage(I1), . . . , fimage(IM )] and TM = [ftext(T1), . . . , ftext(TM )]. The retrieval module is hence finally denoted as KNNv→v t (v, M) = TM NN(v;VM), i.e. for an input image embedding v, the k-NN search is done between v and VM, but the corresponding k-NN indices from the text embeddings TM are selected. Similarly, we denote the retrieval process as KNNt→t v (t, M) = VM NN(t;TM) for an input text embedding t. 4Published as a conference paper at ICLR 2024 We also evaluate cross-modal search but find that this leads to much poorer performance, especially in fine-grained problems, as shown in Tab. 3. An explanation is that the uni-modal search is an easier task, hence the retrieved elements are more relevant (because more similar) to the input. On the other hand, cross-modal search suffers from the pre-trained CLIP model’s lack of fine-grained alignment between the different modalities, resulting in noisier retrieval. Note that another advantage of uni- versus cross- modal search is that the latter requires the pre-trained image and text encoders to be already aligned while we can potentially let go of this hypothesis with uni-modal search. 3.2 L EARNING HOW TO FUSE THE RETRIEVED KNOWLEDGE Our goal is to refine the original image and text embeddings v and t with the cross-modal knowledge gathered from M. We denote these refined image and text embeddings by v and t, defined as v = ϕimage(v, KNNv→v t (v, M)) and t = ϕtext(t, KNNt→t v (t, M)), where ϕ is the fusion model. Transformer fusion. We model ϕimage and ϕtext as one-layer multi-head self-attention transformer encoders (Vaswani et al., 2017; Dosovitskiy et al., 2021). Intuitively, this choice allows the original embedding to attend to all the retrieved elements in the fusion process. Note that while the fusion models for text and image encoders have identical architectures, they do not share parameters. In practice, the fusion module has a total of 3.16M parameters, which corresponds to only 2% of the total parameter count when using CLIP-B/32 as the backbone f. We have experimented with bigger fusion modules (see Appendix) but find that this light-weight solution works well in practice. We have also tried mean fusion of retrieved and original elements by simply averaging their embeddings but have found in practice that it performs poorly (see Tab. 3). Intuitively, the model needs to learn how to incorporate this new information, by, for example, learning how to omit or enhance some of the retrieved elements. Learning. We train the fusion model ϕ on a dataset D = {(Ii, Ti)}N i=1 by performing retrieval at training time from the memory M. The pre-trained encoder f is kept frozen. We minimize the alignment loss between the refined embeddings which formally amounts to minimizing the InfoNCE loss of Eq. (1) with the refined embeddings instead of original embeddings, i.e. minimizing LNCE(V, T). We find that it is also sometimes beneficial to perform retrieval for only one of the branches (text or image) at inference time depending on the nature of the downstream task (see Tab. 4). Therefore, we also align the original and refined embeddings by minimizing the following “cross” loss terms: LNCE(V, T) and LNCE(V, T). This allows to disable one of branches at inference time, since refined and original embeddings are now also aligned. Overall, we minimize: L = LNCE(V, T) + LNCE(V, T) + LNCE(V, T). (2) 4 E XPERIMENTS 4.1 E XPERIMENTAL SETUP Training details. We train the fusion model on top of a frozen CLIP (-B/32 or -L/14 version) model (Radford et al., 2021). We also present a variant of RECO on top of a frozen LiT-L16L (Zhai et al., 2022) model. We train on Conceptual Captions 12M (“ CC12M”) (Changpinyo et al., 2021), an image-text dataset containing about 10M pairs. We use a batch size of 4096, learning rate of 1e−3 decayed with a cosine schedule and weight decay of 1e−5. The temperature parameter is learned (Radford et al., 2021). Training is done for 10 epochs, which lasts about 10 hours on a 4x4 TPUv2 pod. For the memory, we use the subset of WebLI (Chen et al., 2023) containing 1B image-text pairs. We remove the near-duplicates of the test images from the memory. We have also explored using smaller but publicly available memory such as LAION-400M dataset (Schuhmann et al., 2021) and show the results in Appendix. Evaluation datasets. We consider the following six image classification datasets: Stanford Cars (“Cars”) (Krause et al., 2013), CUB-200-2011 (“CUB”) (Wah et al., 2011), Oxford Flowers (“Flow- ers”) (Nilsback & Zisserman, 2008), ImageNet-1k (“Im1k”) (Russakovsky et al., 2015), Places365 (“Pl365”) (Zhou et al., 2017) and Stanford Dogs (“Dogs”) (Khosla et al., 2011). We also consider the recent Open-domain visual entity recognition (OVEN) benchmark (Hu et al., 2023), containing 729K test images possibly belonging to 6M entity candidates. Finally, we also report performance on text-to-image (“T→I”) and image-to-text (“I→T”) retrieval on Flickr30k (“Flickr”) (Plummer et al., 2015) and MS COCO (“COCO”) (Lin et al., 2014) in Appendix. More details about these datasets can be found in Appendix or in their corresponding publication. 5Published as a conference paper at ICLR 2024 Table 1: Zero-shot transfer to image classification. We report top-1 accuracy for classification. We show the improvements obtained with RECO on top of CLIP-R50, CLIP-B/32, CLIP-L/14 and LiT-L16L: absolute performance gains are between brackets. For reference, we also include the performance of K-Lite (Shen et al., 2022) and RA-CLIP (Xie et al., 2023) (other retrieval-augmented methods) and other image-text models (Align-base (Jia et al., 2021) and PaLI-17B (Chen et al., 2023)). We also report the total parameter count (“# par.”) of the different models (in Million). Method # par. Cars CUB Flowers Im1k Pl365 Dogs CLIP-R-50 102 38.6 52.0 47.2 59.2 53.1 60.6 + RECO 114 39.8(+1.2) 62.8(+10.8) 56.2(+9.0) 59.4(+0.2) 54.0(+0.9) 64.4(+3.8) CLIP-B/32 151 57.2 52.8 62.1 63.5 40.6 58.6 + RECO 154 68.1(+10.9) 63.0(+10.2) 67.9(+5.8) 64.6(+1.1) 42.2(+1.6) 59.7(+1.1) CLIP-L/14 428 75.6 61.7 75.6 75.5 42.0 72.7 + RECO 435 82.8(+7.2) 73.4(+11.7) 79.5(+3.9) 76.1(+0.6) 43.6(+1.6) 73.9(+1.2) LiT-L16L 638 90.5 54.5 77.4 80.2 45.2 75.7 + RECO 652 90.8(+0.3) 74.8(+20.3) 84.1(+6.7) 80.9(+0.7) 45.4(+0.2) 81.3(+5.8) Other approaches K-Lite 151 10.0 – 78.6 52.3 – – RA-CLIP 151 – – – 53.5 – 26.1 Align 247 78.7 38.2 64.9 67.6 44.0 56.3 PaLI 17,000 – – – 72.1 – – Table 2: Zero-shot performance on OVEN.We report top-1 accuracy on seen and unseen categories and their harmonic mean. We also indicate the total number of parameters of each model (“# params”). Method # params (M) Seen Unseen Harmonic mean Zero-shot PaLI-17B (Chen et al., 2023) 17,000 4.4 1.2 1.9 CLIP-L/14 (Radford et al., 2021) 428 5.6 4.9 5.3 CLIP-L/14 (Radford et al., 2021) + RECO (Ours) 435 11.5 (+5.9) 13.3 (+8.4) 12.3 (+7.0) Fine-tuning on the OVEN Seen categories CLIP-L/14 Fusion (Hu et al., 2023) 880 33.6 4.8 8.4 PaLI-3B (Chen et al., 2023) 3,000 19.1 6.0 9.3 CLIP-L/14 CLIP2CLIP (Hu et al., 2023) 860 12.6 10.5 11.5 PaLI-17B (Chen et al., 2023) 17,000 28.3 11.2 16.1 Evaluation protocol. We evaluate in the zero-shot setting for all the considered benchmarks, meaning that no adaptation is done to the downstream task. As common in the literature (Radford et al., 2021; Jia et al., 2021; Singh et al., 2022; Zhai et al., 2022), we add prompts to the text of the downstream tasks, following (Zhai et al., 2022). All evaluation protocols are in Appendix. 4.2 Z ERO -SHOT TRANSFER Image classification. In Tab. 1, we observe that RECO boosts the zero-shot performance of CLIP and LiT on zero-shot image classification with large improvements especially on the fine-grained datasets. For example, we improve the original CLIP-B/32 accuracy by +10.9 on Cars, +10.2 on CUB and +5.8 on Flowers. The performance is also improved on less fine-grained benchmarks such as ImageNet or Places, though by more moderate margins (i.e. respectively +1.1 and +1.6). Secondly, we see in Tab. 1 that the performance gains are consistent across all vision-text backbones (CLIP-R-50, CLIP-B/32, CLIP-L/14, and LiT-L16L). Note that LiT-L16L is pre-trained on Webli, which is our memory bank, and we still observe the benefits of RECO. For reference, we also report in Tab. 1 the numbers from other popular vision-text approaches (Jia et al., 2021; Chen et al., 2023). Overall, the experiment in Tab. 1 confirms our initial motivation that retrieval from an external memory improves zero-shot recognition tasks, especially in fine-grained settings. Open-domain visual entity recognition (OVEN). In Tab. 2, we show the zero-shot performance of RECO on the OVEN benchmark. We see that our method improves greatly over CLIP-L/14 on this challenging task, with an impressive relative improvement of+132%. Note that we do not train or fine-tune our model on the OVEN training set. Remarkably, we observe in Tab. 2 that RECO also 6Published as a conference paper at ICLR 2024 Table 3: Uni-modal search for cross-modal fusion. We report top-1 accuracy for zero-shot image classification. We evaluate the impact of uni-modal versus cross-modal search and uni-modal versus cross-modal fusion. These different mechanisms are conceptually illustrated in Fig. 2. We report absolute improvement between brackets and the averagerelative improvement over not using retrieval (i.e. CLIP performance) in the last row (“Avg. rel. ∆”). Search Fusion Cars CUB Flowers Im1k Pl365 Avg. rel. ∆ – – 57.2 52.8 62.1 63.5 40.6 – ϕ = Transformer fusion 1 Uni-modal Cross-modal 68.1 (+10.9) 63.0 (+10.2) 67.9 (+5.8) 64.6 (+1.1) 42.5 (+1.9) + 9.0 % 2 Cross-modal Cross-modal 56.6 (-0.6) 53.8 (+1.0) 64.3 (+2.2) 64.3 (+0.8) 42.4 (+1.8) + 1.7 % 3 Uni-modal Uni-modal 57.3 (+0.1) 51.2 (-1.6) 62.2 (+0.1) 62.1 (-1.4) 41.7 (+1.1) − 0.4 % 4 Cross-modal Uni-modal 54.0 (-3.2) 50.7 (-2.1) 61.4 (-0.7) 62.3 (-1.2) 41.2 (+0.6) − 1.9 % ϕ = Mean fusion 5 Uni-modal Cross-modal 46.9 (-10.3) 44.9 (-7.9) 50.5 (-11.6) 40.1 (-23.4) 23.7 (-16.9) − 21.7 % 6 Cross-modal Cross-modal 43.7 (-13.5) 45.3 (-7.5) 58.7 (-3.4) 55.2 (-8.3) 32.7 (-7.9) − 11.0 % 7 Uni-modal Uni-modal 44.0 (-13.2) 47.2 (-5.6) 61.3 (-0.8) 55.1 (-8.4) 36.2 (-4.4) − 9.8 % 8 Cross-modal Uni-modal 33.4 (-23.8) 30.2 (-22.6) 38.9 (-23.2) 40.0 (-23.5) 24.7 (-15.9) − 33.0 % significantly outperforms much bigger models which are directly fine-tuned for this task, for example CLIP2CLIP (Hu et al., 2023) or PaLI-3B (Chen et al., 2023) while using respectively 2 × and 7 × less parameters. It even comes close to the performance of PaLI-17B while being 39 × smaller and not using any fine-tuning. 4.3 D ESIGN CHOICE ANALYSES In this section, we validate several components of our model, namely the uni-modal search and cross-modal fusion, training of the fusion module and the number of retrieved elements from the memory. We also propose some qualitative examples to help understanding why RECO improves over CLIP performance. We use ViT-CLIP-B/32 throughout this section. Uni-modal search and cross-modal fusion. In Tab. 3, we evaluate different alternatives for our method, namely (i) performing cross-modal search in the memory instead of uni-modal search and (ii) fusing uni-modal items (i.e. combining text with text and image with image) instead of cross- modal fusion. These different scenarios (uni- versus cross- modal search and fusion) are detailed in Section 3.1 and conceptually illustrated in Fig. 2. Firstly, we observe in Tab. 3 that uni-modal search (row 1) leads to a better performance compared to cross-modal search (row 2), with +9.0 versus +1.7 average relative improvement over CLIP. We remark that the gap is especially important for fine-grained datasets such as Cars, CUB and Flowers. This agrees with our hypothesis that cross-modal search suffers from the pre-trained CLIP model’s lack of fine-grained alignment between different modalities. By contrast, using the inherent strength of image and text representations within their respective modalities allows to retrieve relevant matches, as qualitatively observed in Fig. 4. Secondly, we observe in Tab. 3 that uni-modal fusion (rows 3 and 4) works substantially worse than cross-modal fusion (rows 1 and 2). Indeed, we see that augmenting text embeddings with other text embeddings and image embeddings with other image embeddings does not bring any significant improvement over the baseline, and even tends to hurt the performance. Intuitively, a possible explanation is that cross-modal fusion allows us to inject complementary signal into the original embeddings (Iscen et al., 2023). By contrast, uni-modal provides signal that is already similar to the input, hence not as much additional information. Finally, we see in Tab. 3 that all the variants (rows 5, 6, 7 and 8) fail when simply averaging retrieved and original embeddings instead of learning the fusion with a transformer. This highlights the importance of learning to incorporate the retrieved items to the original embeddings before deploying the model at inference. Image and text retrieval fusion modules. In Tab. 4, we compare models trained to fuse only text original embeddings (row 1), only image original embeddings (row 2) or both (row 3). We observe that while models trained to fuse only image or text perform reasonably well on some benchmarks, they typically lag behind on other benchmarks. For example, the model trained for only image fusion (row 2) is strong on zero-shot Dogs benchmark but behind on CUB and COCO. Secondly, as shown in Tab. 4, unlike the vision-only or text-only variants, our model can be used in different modes at inference time in a flexible manner. Indeed, because we have trained it to align the refined embeddings 7Published as a conference paper at ICLR 2024 Table 4: Image and text retrieval fusion modules. We report zero-shot top-1 accuracy for image classification and recall@1 for image retrieval. We compare models trained only for text fusion (row 1), image fusion (row 2) or both (row 3). Our model can be used in different modes at inference: retrieval only for image (v), retrieval only for text (t) or retrieval for both image and text (v&t). CUB Dogs COCO T →I ϕimage ϕtext fusion training loss v t v&t Best v t v&t Best v t v&t Best 1 ✓ LNCE(V, T) ✗ 59.3 ✗ 59.3 ✗ 59.6 ✗ 59.6 ✗ 33.3 ✗ 33.3 2 ✓ LNCE(V, T) 59.7 ✗ ✗ 59.7 59.2 ✗ ✗ 59.2 31.3 ✗ ✗ 31.3 3 ✓ ✓ LNCE(V, T)+LNCE(V, T)+LNCE(V, T) 60.0 58.4 63.0 63.0 59.7 59.7 59.4 59.7 31.9 33.6 31.7 33.6 Method Train data CUB CLIP-B/32 – 52.8 + CLIP-style CC 12M 44.8 + CLIP-style CC 12M + RWebli 46.8 + RECO CC12M + RWebli 63.0 110 30 50 70 100 % of memory (training) 52 56 60 64CUB  train memory full memory 1 5 10 20 k retrieved items (training) 52 56 60 64 k'=20 k'=10 k'=5 k'=2 k'=1 Figure 3: (left) Disantangling the effect of additional training and RECO. (middle) Effect of updating the memory after training. (right) Effect of the number k of retrieved elements. We report zero-shot top-1 accuracy on CUB. The CLIP baseline is shown with symbol 8. with the original ones (see the cross terms in the loss 2), we can choose to disable the retrieval for one of the branches at inference time, depending on the task. Therefore, we compare in Tab. 4 different options at inference time: using retrieval only for the image input, only for the text input or for both of them, denoted respectively by v, t and v&t. We observe in Tab. 4 that depending on the nature of the task, one of these options might be preferable over the others. For example, for image classification we see that augmenting the image embeddings with retrieved text has more positive impact than augmenting the text embeddings, though the best of performance is obtained with both. On the other hand, text and image retrievals seem to benefit more from augmenting the text rather than the image side. This intuitively can be explained by the fact that text descriptions in retrieval benchmarks are typically highly specific compared to the class names in image classification and so augmenting with visual examples of what they refer to greatly helps the alignment. We demonstrate qualitative examples of this hypothesis in Appendix. Overall, at inference time, one can choose the best inference mode for a particular downstream task by validation on a held-out set. Is the performance boost merely due to additional training?We replace RECO with an MLP layer of the same capacity initialized from scratch. We train it in a CLIP-style manner on the subset of Webli that we use when training RECO. We denote this subset by RWebli: it contains the k = 10 nearest- neighbors for each CC12M datapoint retrieved from the Webli dataset, and contains 61M examples. We observe in Fig. 3 (left) that training an extra layer on top of CLIP does not bring any gains and even deteriorates its performance. Indeed, CLIP was extensively trained on a large dataset (Radford et al., 2021) and additional training on a relatively small dataset deteriorates the general-purpose property of its representations. Overall, this experiment validates that the performance gains are due to our method and not to training an additional layer on top of CLIP. Updating the memory after training. A clear advantage of the retrieval-based models is that the external memory can be updated with additional, and more contemporary information. We evaluate the effectiveness of RECO when using a larger memory that is not observed during the training. We first create various random subsets of Webli by randomly removing a percentage of data. Then, we train separate RECO models with each Webli subset as its memory. At inference, we evaluate each RECO model either with the subset of memory that it was trained with, or the full Webli memory. Results are shown in Fig. 3 (center). We observe that training and evaluating RECO with only1% of Webli as the memory does not show improvements compared to the CLIP baseline. However, we observe a significant improvement when evaluating the same model with full Webli memory at inference. This confirms that RECO is capable of utilizing an updated memory without re-training. Effect of the number of retrieved elements. In Fig. 3 (right), we study the effect of the number of retrieved elements in the memory. We evaluate different numbers ofk-NN during the training and inference time, i.e. we train our model with k items from the memory but use k′ at inference. We 8Published as a conference paper at ICLR 2024 Query Uni-modal search (Ours) Cross-modal search Figure 4: Qualitative examples on CUB and Cars datasets. We compare uni- versus cross- modal search for two image queries (top) and two text queries (bottom). Uni-modal search allows to find more suitable matches to the query, which improves the relevancy of the fused elements. We frame in red (resp. green) the unrelevant (resp. relevant) retrieved items to be fused with the query. see in Fig. 3 (right) that RECO generally obtains a higher performance when k′ > kat inference. Interestingly, the performance saturates after k = 10. An explanation is that increasing the number of retrieved elements goes with a reduction of the relevancy of the retrieved items. Qualitative study. In Fig. 4, we provide illustrative examples of why RECO can be useful for fine-grained image classification on CUB or Cars datasets. We compare our method with a variant using cross-modal search instead of uni-modal search to illustrate the importance of using the inherent strength of image-only and text-only representations. We observe in Fig. 4 that uni-modal search allows to retrieve better matches for the query. This is because image-to-image or text-to-text search retrieves more similar items to the query than crossing modalities. As a result, retrieved items are more accurate, which leads to a higher accuracy for fine-grained tasks. Limitations. A limitation of this work is that it assumes to have access to a large and rich source of image-text pairs knowledge. While we show in Appendix that public datasets , e.g. LAION (Schuh- mann et al., 2021), can serve this purpose, the best of performance is obtained with a large private memory. Alternatively, one could use search engine APIs as the memory. Another limitation is that the performance gains of RECO come at the cost of increased inference time. In practice, we use a highly-optimized approximate k-NN algorithm (Guo et al., 2020). It takes about 14ms to query Webli (956M examples) with a single 512-d ViT-B/32 CLIP embedding. Using retrieval at inference time incurs an overhead of 25% compared to not using any retrieval at inference time (e.g. baseline CLIP model), but improves the accuracy by up to 10.9. 5 C ONCLUSION In this paper, we introduce RECO, a method that enhances the fine-grained recognition capabilities of pre-trained vision-text models. Our approach shows the importance of uni-modal retrieval, yet cross- modal fusion for image and text inputs. We show that RECO consistently improves the performance on 11 zero-shot tasks and that the gains are especially important in challenging fine-grained tasks. 9Published as a conference paper at ICLR 2024 REFERENCES Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716– 23736, 2022. Mohamed El Banani, Karan Desai, and Justin Johnson. Learning visual representations via language- guided sampling. arXiv preprint arXiv:2302.12248, 2023. Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Semi-parametric neural image synthesis. arXiv preprint arXiv:2204.11824, 2022. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Im- proving language models by retrieving from trillions of tokens. In Proceedings of the International Conference on Machine Learning (ICML), 2022. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative com- ponents with random forests. In Proceedings of the European Conference on Computer Vision (ECCV), 2014. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. International Conference on Learning Representations (ICLR), 2023. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022. Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition work for everyone? In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked self-distillation advances contrastive language-image pretraining. arXiv preprint arXiv:2208.12262, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088, 2023. Andreas Fürst, Elisabeth Rumetshofer, Johannes Lehner, Viet T Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto, et al. Cloob: Modern hopfield networks with infoloob outperform clip. Advances in Neural Information Processing Systems (NeurIPS), 2022. 10Published as a conference paper at ICLR 2024 Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramid- clip: Hierarchical feature alignment for vision-language model pretraining. Advances in Neural Information Processing Systems (NeurIPS), 2022. Gerry. Sports100: 100 sports image classification. https://www.kaggle.com/datasets/ gpiosenka/sports-classification/metadata, 2021. Accessed: 2023-05-23. Lluis Gomez, Yash Patel, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised learning of visual features through embedding images into text topic spaces. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. Kat: A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614, 2021. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of the International Conference on Machine Learning (ICML), 2020. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. REALM: Retrieval augmented language model pre-training. In Proceedings of the International Conference on Machine Learning (ICML), 2020a. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In Proceedings of the International Conference on Machine Learning (ICML), 2020b. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. Proceedings of the International Conference on Computer Vision (ICCV), 2023. Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. arXiv preprint arXiv:2212.05221, 2022. Ahmet Iscen, Alireza Fathi, and Cordelia Schmid. Improving image recognition by retrieving from web-scale image-text data. arXiv preprint arXiv:2304.05173, 2023. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In Proceedings of the European Conference on Computer Vision (ECCV), 2016. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2015. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. International Conference on Learning Representations (ICLR), 2020. 11Published as a conference paper at ICLR 2024 Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In First Workshop on Fine-Grained Visual Categorization, CVPR, 2011. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the International Conference on Computer Vision (ICCV), 2013. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123, 2017. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented gen- eration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems (NeurIPS), 2020. Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. arXiv preprint arXiv:2210.04150, 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), 2014. Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint arXiv:2301.07094, 2023. Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography. na, 2012. Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis- serman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020. George A Miller. WordNet: An electronic lexical database. MIT press, 1998. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008. Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv:2111.10050, 2021. 12Published as a conference paper at ICLR 2024 Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the International Conference on Computer Vision (ICCV), 2015. Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer vision? arXiv preprint arXiv:2006.16923, 2020. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 2015. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge. Advances in Neural Information Processing Systems (NeurIPS), 2022. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. ICCV, 2023. Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. arXiv preprint arXiv:2203.08773, 2022. Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans- formers. In ICLR, 2022. 13Published as a conference paper at ICLR 2024 Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2010. Chen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, and Jingren Zhou. Ra-clip: Retrieval augmented contrastive language-image pre-training. In CVPR, 2023. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Con- trastive learning of medical visual representations from paired images and text. In Machine Learning for Healthcare Conference, 2022. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 14Published as a conference paper at ICLR 2024 Table 5: Zero-shot transfer to retrieval. We report recall@1 for retrieval. We show the improve- ments obtained with RECO on top of CLIP-B/32 and CLIP-L/14: absolute performance gains are between brackets. For reference, we also include the performance of other standard image-text foundation models (Flava (Singh et al., 2022) and Align-base (Jia et al., 2021). We also report the total parameter count (“# par.”) of the different models (in Million). T→I I →T Method # par. COCO Flickr COCO Flickr CLIP-B/32 151 30.2 61.1 51.2 80.9 + RECO 154 33.6(+3.4) 65.7(+4.6) 52.2(+1.1) 81.8(+0.9) CLIP-L/14 428 35.2 68.6 57.2 87.5 + RECO 435 38.7(+3.5) 72.6(+4.0) 58.0(+0.8) 88.5(+1.0) Other approaches Flava 172 38.4 65.2 42.7 67.7 Align 247 40.2 72.6 55.1 86.7 A A PPENDIX A.1 I MAGE /TEXT RETRIEVAL Tab. 5 shows that RECO allows to boost the zero-shot performance of CLIP for image and text retrieval even further. We observe that we can improve the recall@1 up to4.6 points for text-to-image retrieval. We also see smaller, but consistent gains for image-to-text retrieval. A.2 U SING LAION-400M AS THE MEMORY In Table 6, we show that our method also works when using a public dataset as the memory bank instead of our private source of knowledge. Indeed, we observe that using LAION-400M (Schuhmann et al., 2021) as the memory bank for RECO gives substantial gains of performance compared to the CLIP baseline across our different zero-shot tasks: for example +6.9 on Cars and +6.1 on CUB. This validates that our method is generic and can work with different choices of external knowledge. Table 6: Choice of memory bank. We report zero-shot top-1 accuracy on different image clas- sification tasks. We evaluate RECO when using two different sources of knowledge: the non publicly available WebLI (Chen et al., 2023) dataset (our default) and the publicly available LAION- 400M (Schuhmann et al., 2021) dataset. Memory bank Public Cars CUB Flowers Im1k Pl365 None – 57.2 52.8 62.1 63.5 40.6 WebLI (Chen et al., 2023) (default) ✗ 68.1 (+10.9) 63.0 (+10.2) 67.9 (+5.8) 64.6 (+1.1) 42.2 (+1.6) LAION-400M (Schuhmann et al., 2021) ✓ 64.1 (+6.9) 58.9 (+6.1) 63.7 (+1.6) 63.4 (-0.1) 42.3 (+1.7) A.3 M ORE COMPLEX FUSION MODULE In Table 7, we experiment with fusion modules of varying sizes. We observe that using a fusion module of one or two layers works comparatively well. However, using larger fusion modules with more layers, e.g. four, six or eight, deteriorates the performance. We hypothesize that this is because increasing the capacity of the fusion creates overfitting. Overall, using a single-layer fusion module brings large gains of performance on top of CLIP while being very light-weight to train. A.4 E ND-TO-END FINETUNING VERSUS FROZEN BACKBONE In Table 8, we evaluate the behavior of RECO when finetuning the original encoders at the same time as the fusion module. We observe in Table 8 that the performance is comparable to keeping the encoders frozen as in our default setting. Freezing the encoders has the advantage of requiring less compute resources. In our implementation and using the same hardware, finetuning CLIP-B/32 along 15Published as a conference paper at ICLR 2024 Table 7: Size of the fusion module. For each variant, we report the total number of parameters (“# params”) in millions and the percentage of the total parameter count which is part of the fusion modules (“% fusion params”). We report zero-shot top-1 accuracy on three image classification tasks. # fusion layer # params (M) % fusion params Cars Flowers Im1k 0 151.3 0% 57.2 62.1 63.5 1 (default) 154.4 2.0% 68.1 67.9 64.6 2 157.6 4.0% 67.8 69.1 64.3 4 163.9 7.7% 61.9 62.8 59.8 6 170.2 11.1% 60.6 64.5 59.6 8 176.5 14.3% 61.1 64.4 59.6 Table 8: Full finetuning versus frozen encoders. Both mechanisms produce good performance but freezing the backbone allows for 1.6× faster training. We report zero-shot top-1 accuracy. CLIP encoders f Cars Flowers Im1k Frozen (default) 68.1 67.9 64.6 Finetuned 68.5 67.1 64.2 with the fusion module has a training step 1.6× longer than working with frozen CLIP-B/32 and training only the fusion module. A.5 Q UALITATIVE EXAMPLES Zero-shot retrieval. In Figure 5, we show some qualitative examples when applying RECO to zero-shot retrieval tasks on COCO dataset. Specifically, we aim to gain an understanding about why the model benefits more from augmenting the text rather than the image side when applied to retrieval downstream tasks (see Table 4 of the main paper). In Figure 5, we look at three different image-text input pairs from the validation set of COCO and display the retrieved captions fused with the query image as well as the retrieved images fused with the query text. We observe in Figure 5 that the text captions of an image in COCO retrieval task usually focus on one specific aspect of the image (for example the towel in the image of the bathroom or the British flag on the train). We observe that the retrieved images from the input text are likely to also contain this particular aspect of interest and hence match well with the original caption. For example, the retrieved images from the train with a British flag caption (bottom of Figure 5) all contain representations of vehicles with painted British flags, which is more likely to help the alignment with the original input. On the contrary, the captions retrieved from the original image may focus on another particularity of the image, not mentioned in the original caption. For example, the captions retrieved from the train image contain information about train numbers, train station locations or operators. This information is not useful for this task, because the ground-truth caption focuses on the fact that the train carriage has a British flag on its side. This brings distracting signal instead of helping the alignment. Overall, we think these qualitative examples help us understand why disabling retrieval for the image input and enabling it for the text side results in a better performance in this task. Zero-shot image classification. In Figure 6, we display some qualitative examples of RECO for zero- shot image classification downstream tasks. Specifically, we consider several query images and their class names and show the corresponding elements (captions for query image and images for query class name) retrieved by our model. We observe in Figure 6 that in majority of cases, given a visual input, searching for similar images and retrieving their corresponding captions effectively returns descriptions containing useful information for fine-grained classification problem. For example, the captions retrieved from the cat image at the top of Figure 6 contain the breed of that cat (siamese). Likewise, given the class name “siamese cat”, RECO look for similar captions, for example “picture of a siamese cat”, and returns their corresponding images. These all contain visual examples of what a siamese cat looks like. Figure 6 show several successful examples of this mechanism and helps giving intuition about why RECO helps for zero-shot image classification. 16Published as a conference paper at ICLR 2024 Figure 5: Qualitative examples of RECO for image and text retrieval. We display image and text queries on the left panel and retrieved captions and images on the right panel. We observe that retrieved images tend to match better with the input original image than retrieved captions with the input original text. For example, the retrieved captions from the aerial view do not mention a lot “mountains” while this is present in the original text. Instead, they mention many specific locations, for example lima, cuzco, arizona or afghanistan, which are not relevant to the original text description. On the contrary, the retrieved images from the text query are semantically similar to the original image. This qualitatively explains why the best of performance of RECO for zero-shot retrieval is achieved by disabling retrieval on the query image and enabling it on the query text (see Table 4 of the main paper). Interestingly, we observe some failure cases when retrieving from a query class name which is ambiguous in the sense that it can refer to several things. For example, the retrieved images from the class name “prince of wales feathers” in Flowers dataset returns non useful information such as the emblem of prince of wales or a picture of a feather. This is because “prince of wales feathers” can refer to many things other than a flower species. We observe this behavior for several class names of the Flowers classification benchmark which have a meaning outside of the flower species they refer to; for example “bird of paradise”, or “bishop of llandaff” where one of the retrieved image is from the actual person who used to be the Bishop of Llandaff, a community in Wales. B E VALUATION DETAILS B.1 E VALUATION DATASETS We report the details of each image classification dataset that we use to evaluate our model. Note that we only use test or validation splits of each of these datasets, training sets are disregarded. Stanford Cars (“Cars”) (Krause et al., 2013) contains 8, 041 test images of 196 fine-grained car classes. Each 17Published as a conference paper at ICLR 2024 Figure 6: Qualitative examples of RECO for image classification. We consider several query images and their class names (“query text”) and show the retrieved items to be fused with them. For a given a class name, looking for similar captions and using their corresponding images usually returns relevant visual examples. For example, the images retrieved from the class name “dungeness crab” show examples of what dungeness crabs visually look like. However, when the class name can refer to several things, for example “bird of paradise” which is both a flower and a bird species, then the visual retrieved examples are not always relevant to the finegrained classification problem at hand. class name consists of make, model and year of a car, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe. CUB-200-2011 (“CUB”) (Wah et al., 2011) consists of 5, 794 test images of 200 bird species. The dataset contains additional annotations, such as part locations, binary attibutes, and bounding boxes, but we do not use any of them. Oxford Flowers (“Flowers”) (Nilsback & Zisserman, 2008) has 6, 149 test images of 102 flower categories commonly found in the United Kingdom. The dataset images contain strong visual variations, such as scale, pose, and light changes. ImageNet-1k (“Im1k”) (Russakovsky et al., 2015), or also referred to as ILSVRC 2012, contains 50, 000 validation images from 1, 000 classes. Class names are obtained from the synsets in WordNet (Miller, 1998), and come from a large variety of generic concepts. Places365 (“Pl365”) (Zhou et al., 2017) has 18Published as a conference paper at ICLR 2024 Table 9: Standard deviation of RECO results. We run five RECO training, each one with a different random seed. We show zero-shot image classification and retrieval results, and their standard deviation across 5 runs. Image classification T →I I →T Method Cars CUB Flowers Im1k Pl365 Dogs COCO Flickr COCO Flickr CLIP-B/32 57.2 52.8 62.1 63.5 40.6 58.6 30.2 61.1 51.2 80.9 RECO 68.1 ±0.3 63.0 ±0.3 67.9 ±0.4 64.6 ±0.1 42.2 ±0.1 59.7 ±0.2 33.6 ±0.1 65.7 ±0.3 52.2 ±0.3 81.8 ±0.3 36, 500 validation images from 365 generic scene categories. Some examples of class names include living room, cottage, lecture room, pier etc. Finally, Stanford Dogs (“Dogs”) (Khosla et al., 2011) consists of 8, 580 test images from 120 dog breeds. We use two datasets for our retrieval experiments. Flickr30k (“Flickr”) (Plummer et al., 2015) contains 1000 image-text pairs. Each image contains 5 sentence-level descriptions, or captions. Similarly, MS COCO (“COCO”) (Lin et al., 2014) test set, as defined by Karpathy and Li (Karpathy & Fei-Fei, 2015), contains 5, 000 image-text pairs, where each image contains 5 captions. We report the performance for text-to-image (“T→I”) and image-to-text (“I→T”) retrieval on both datasets. B.2 E VALUATION PROTOCOLS Zero-shot image classification. We follow the standard setup (Radford et al., 2021) of embedding each class name with the text encoder. We classify an image to the class which has the highest cosine similarity between the image embedding and the corresping class name embedding. We report top-1 accuracy in all the image classification benchmarks. There is no variance at zero-shot evaluation time since the inference for both text and vision encoders are deterministic. Image and text retrieval. For image-to-text retrieval, given an input image, we rank all the text embeddings according to their similarity to this image embedding. We report the proportion of images that ranks the correct text within the first R positions as the recall@R. The process is the symetric for text-to-image retrieval by switching the role of text and image. We report recall@1 in all the retrieval tasks. There is no variance at zero-shot evaluation time since the inference for both text and vision encoders are deterministic. Variance and error bars. We report the performance variance on our small CLIP-B/32 setting to make sure that observed gains are significant. We train RECO with CLIP-B/32 backbone 5 times with different random seeds. We perform the evaluation for each model separately and report the accuracy, averaged over5 run, with the variance in Table 9. We observe that the standard deviation is small across 5 runs, always below 0.4 across all benchmarks. OVEN benchmark. OVEN benchmark (Hu et al., 2023) is created by combining 14 existing datasets (ImageNet21k-P (Ridnik et al., 2021; Russakovsky et al., 2015), iNaturalist2017 (Van Horn et al., 2018), Cars196 (Krause et al., 2013), SUN397 (Xiao et al., 2010), Food101 (Bossard et al., 2014), Sports100 (Gerry, 2021), Aircraft (Maji et al., 2013), Oxford Flowers (Nilsback & Zisserman, 2008), Google Landmarks v2 (Weyand et al., 2020), and various VQA (visual question answering) datasets (Goyal et al., 2017; Zhu et al., 2016; Krishna et al., 2017; Marino et al., 2019; Singh et al., 2019; Gerry, 2021)) and grounding their categories to Wikipedia entities. The benchmark consists of two splits. Entity Split measures the image recognition or retrieval capabilities of a model, whereas the Query Split is designed as a VQA task. We focus on the Entity Split in this paper. The Entity Splits contains training, validation, and test splits. However, since we focus on zero-shot image classification in this paper, we ignore the training and validation splits, and evaluate our model (trained on CC12M as discussed in Sec. 4) directly on the test set. The test set contains 729, 259 examples from 20, 549 entities. Each example belongs to a single entity. Nevertheless, total number of candidate entities during inference is 6, 084, 494, i.e. there are more than 6M distractor entities at inference. Note that unlike other image classification datasets, each example is an image-text pair in OVEN. The so-called intent text accompanies each image, and clarifies the question at hand, e.g. what is the model of this vehicle? Similarly, each entity is also an image-text pair, containing the entity name 19Published as a conference paper at ICLR 2024 and entity image. We simply follow the same protocol as other image classification datasets in this paper, and only consider the example image and entity name. C I LLUSTRATIVE COMPARISON OF UNI -/CROSS - MODAL SEARCH AND UNI -/CROSS - FUSION We give a conceptual comparison of uni-/cross- modal search and uni-/cross- fusion for an image input I in the paper. We now show in Figure 7 this comparison for a text inputT. Uni-modal search Cross-modal search Uni-modal search Cross-modal search & cross-modal fusion & cross-modal fusion & uni-modal fusion & uni-modal fusion (Ours) Figure 7: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion.We illustrate the different scenarios for a text input T while the scenarios for image input I are shown in the main paper. D N EAR -DUPLICATE FILTERING 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Cosine Similarity 0 2000 4000 6000 8000 nearest\tneighborquery\timage 0.95\tsimilarity nearest\tneighborquery\timage 0.95\tsimilarity nearest\tneighborquery\timage 0.95\tsimilarity Figure 8: Left: Cosine similarity distribution between the test image queries of CUB2011 and their 10 kNN. Right: Some of the queries and their nearest neighbors from the memory with 0.95 similarity. Figure 8 (Left) of the PDF shows the cosine similarity distribution between the queries and memory images on CUB2011 dataset. It is shown that most of the retrieved items have a similarity around 0.9, which indicates that most of the retrieved elements are similar but not identical. Note that there is no cosine similarity above 0.95. This is because we remove any memory image (and their corresponding caption) if they have a similarity of 0.95 or above with any of the test images. Figure 8 (Right) shows some of the kNNs with 0.95 cosine similarity. We see that the retrieved examples are not near-duplicates, but they are conceptually similar as they should be. E B ROADER IMPACT We propose a retrieval-based recognition approach, where we search for similar images and text in a large-scale memory. Data retrieved from such uncurated sources may be biased against certain populations across the world (Prabhu & Birhane, 2020; De Vries et al., 2019). Furthermore, it is important that the privileged user data does not exist in such data collections, in order to avoid using the data without the consent of its owner. We acknowledge these potential misuses, and encourage the community to utilize more fair and responsible data collections. 20Published as a conference paper at ICLR 2024 retrieved\tcaptions\t1-5 hoary  redpoll hoary  redpoll immature  female  northern  cardinals rose  breasted  grosbeak purple finch query\timage image-to-image\t search male house  finch birding not  sure what's  going on with  this house  finch any  thoughts  milwaukee wi purple  finch  dsc_2669 pinson ann elias's signature  teal  stacked  bird feeder retrieved\tcaptions\t6-10 Label: purple  finch retrieved\tcaptions\t1-5 saltator  gros bec lazuli  bunting  pair by  brooke miller lazuli  bunting  pair by  brooke miller whiskered  treeswift whiskered  treeswift  oriental bird  club image  database query\timage image-to-image\t search 北 印旛 沼 ルリビタキ の 画像 lesser  striped  swallow ツバメ yellow  bellied  flycatcher  empidonax flaviventris ルリビタキ iso1600 retrieved\tcaptions\t6-10 Label: lazuli  bunting retrieved\tcaptions\t1-5 house finch beautiful  pink colored bird brown  capped rosy  finch chechetka чечётка  acanthis flammea  redpoll gray crowned  rosy finch  taylor abbott;grey crowned rosy  finch house finch  emily gorda query\timage image-to-image\t search tallbit all about  birds  purple  finch red  crossbill house finch зяблик  самец;обыкн овенный  зяблик retrieved\tcaptions\t6-10 Label: gray crowned rosy  finch Figure 9: Noisy examples. We display image queries and their labels on the left panel and 10 retrieved captions and images on the right panel. Retrieved captions are in green if they contain useful information, red otherwise. RECO learns F N OISY EXAMPLES We show the 10 retrieved captions for some of the test queries on the CUB2011 dataset on Figure 9. We observe that RECO predicts the correct class even if the retrieved captions contain irrelevant examples. This is because the Transformer module learns to aggregate the retrieved captions, implicitly keeping the relevant ones and disregarding the irrelevant ones. Note that Webli contains captions in languages other than English, which are considered noisy for CLIP, because it is trained in an English corpus. 21",
      "references": [
        "Flamingo: a visual language model for few-shot learning",
        "Learning visual representations via language-guided sampling",
        "Semi-parametric neural image synthesis",
        "Improving language models by retrieving from trillions of tokens",
        "Food-101–mining discriminative components with random forests",
        "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
        "Re-imagen: Retrieval-augmented text-to-image generator",
        "Pali: A jointly-scaled multilingual language-image model",
        "Reproducible scaling laws for contrastive language-image learning",
        "Does object recognition work for everyone?",
        "Virtex: Learning visual representations from textual annotations",
        "Maskclip: Masked self-distillation advances contrastive language-image pretraining",
        "An image is worth 16x16 words: Transformers for image recognition at scale",
        "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations",
        "Improving clip training with language rewrites",
        "Cloob: Modern hopfield networks with infoloob outperform clip",
        "Pyramid-clip: Hierarchical feature alignment for vision-language model pretraining",
        "Sports100: 100 sports image classification",
        "Self-supervised learning of visual features through embedding images into text topic spaces",
        "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
        "Kat: A knowledge augmented transformer for vision-and-language",
        "Accelerating large-scale inference with anisotropic vector quantization",
        "REALM: Retrieval augmented language model pre-training",
        "Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities",
        "Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory",
        "Improving image recognition by retrieving from web-scale image-text data",
        "Few-shot learning with retrieval augmented language models",
        "Scaling up visual and vision-language representation learning with noisy text supervision",
        "Learning visual features from large weakly supervised data",
        "Deep visual-semantic alignments for generating image descriptions",
        "Generalization through memorization: Nearest neighbor language models",
        "Novel dataset for fine-grained image categorization",
        "3d object representations for fine-grained categorization",
        "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "Open-vocabulary semantic segmentation with mask-adapted clip",
        "Microsoft coco: Common objects in context",
        "Learning customized visual models with retrieval-augmented knowledge",
        "Retrieval augmented classification for long-tail visual recognition",
        "Fine-grained visual classification of aircraft",
        "Ok-vqa: A visual question answering benchmark requiring external knowledge",
        "Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography",
        "End-to-end learning of visual representations from uncurated instructional videos",
        "WordNet: An electronic lexical database",
        "Simple open-vocabulary object detection with vision transformers",
        "Automated flower classification over a large number of classes",
        "Combined scaling for open-vocabulary image classification",
        "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
        "Large image datasets: A pyrrhic win for computer vision?",
        "Learning transferable visual models from natural language supervision",
        "Imagenet-21k pretraining for the masses",
        "Imagenet large scale visual recognition challenge",
        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
        "K-lite: Learning transferable visual models with external knowledge",
        "Towards vqa models that can read",
        "Flava: A foundational language and vision alignment model",
        "Sus-x: Training-free name-only transfer of vision-language models",
        "Representation learning with contrastive predictive coding",
        "The inaturalist species classification and detection dataset",
        "Attention is all you need",
        "The caltech-ucsd birds-200-2011 dataset",
        "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
        "Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval",
        "Memorizing transformers",
        "Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly",
        "Sun database: Large-scale scene recognition from abbey to zoo",
        "Ra-clip: Retrieval augmented contrastive language-image pre-training",
        "Coca: Contrastive captioners are image-text foundation models",
        "Lit: Zero-shot transfer with locked-image text tuning",
        "Contrastive learning of medical visual representations from paired images and text",
        "Places: A 10 million image database for scene recognition",
        "Visual7w: Grounded question answering in images"
      ],
      "meta_data": {
        "arxiv_id": "2306.07196v2",
        "authors": [
          "Ahmet Iscen",
          "Mathilde Caron",
          "Alireza Fathi",
          "Cordelia Schmid"
        ],
        "published_date": "2023-06-12T15:52:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces RECO, a retrieval-enhanced framework that equips frozen contrastive vision-language models (e.g., CLIP) with the ability to pull complementary cross-modal knowledge from a large external memory at inference time. Using a lightweight single-layer fusion transformer and uni-modal nearest-neighbour search, RECO markedly boosts zero-shot fine-grained recognition and retrieval, achieving double-digit gains on datasets like Stanford Cars, CUB-2011, and surpassing much larger or fine-tuned models on the OVEN benchmark.",
        "methodology": "1. Start from a frozen dual-encoder image-text model.\n2. Maintain a web-scale memory M of image–text pairs encoded once by the same model.\n3. Uni-modal search: for an image query, retrieve top-k similar images in embedding space (v→v); for a text query, retrieve top-k similar texts (t→t).\n4. Cross-modal fusion: replace each retrieved item by its opposite-modality embedding (texts for images, images for texts) and concatenate with the original embedding.\n5. Pass the sequence through a single-layer multi-head self-attention transformer (≈3 M parameters) to obtain refined multimodal embeddings.\n6. Train only the fusion modules on CC12M for 10 epochs with an InfoNCE loss that aligns (i) refined image vs refined text, (ii) refined vs original embeddings for both modalities.\n7. At inference the same retrieval-fusion is applied; retrieval can be enabled on either or both modalities.",
        "experimental_setup": "• Backbones: CLIP-R50, CLIP-B/32, CLIP-L/14 and LiT-L16L kept frozen.\n• Training data: Conceptual Captions 12 M (≈10 M pairs).\n• Memory: WebLI subset (~1 B image-text pairs, near-duplicates of test images removed); LAION-400M used in ablation.\n• Hyper-params: batch 4096, lr 1e-3 cosine decay, weight-decay 1e-5, learnt temperature, 10 epochs on 4×4 TPUv2 (≈10 h).\n• Evaluation tasks (zero-shot):\n  – Image classification: Stanford Cars, CUB-200-2011, Oxford Flowers-102, ImageNet-1k, Places365, Stanford Dogs (top-1 accuracy).\n  – Open-domain Visual Entity Recognition (OVEN) seen/unseen split (top-1 & harmonic mean).\n  – Text↔Image retrieval: MS-COCO & Flickr30k (Recall@1).\n• Baselines & ablations: original CLIP/LiT, K-Lite, RA-CLIP, ALIGN, PaLI; variants with cross-modal search, uni-modal fusion, mean fusion, different k, memory sizes.",
        "limitations": "1. Dependence on very large external memory; best results use a private 1-B-pair WebLI corpus.\n2. Retrieval adds ~25 % inference latency and requires efficient ANN infrastructure.\n3. Performance degrades when memory is small or retrieval quality is low; ambiguous queries can fetch noisy items.\n4. Assumes access to the same encoder for off-line indexing; updates need re-encoding.\n5. Retrieved web data may contain biases, noise, or privacy-sensitive content that propagate to predictions.",
        "future_research_directions": "• Explore adaptive or online memory updates and lifelong learning without re-training fusion.\n• Jointly learn retrieval and fusion, e.g. differentiable k-NN or retrieval-aware encoders.\n• Extend to other tasks (detection, segmentation, VQA) and multimodal generative settings.\n• Investigate hierarchical or deeper fusion architectures and dynamic k selection for better use of retrieved evidence.\n• Develop methods for bias mitigation, privacy preservation, and memory filtering in web-scale corpora.\n• Apply the framework to multilingual or domain-specific memories, and study cross-domain generalization.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "ConR: Contrastive Regularizer for Deep Imbalanced Regression",
      "full_text": "Published as a conference paper at ICLR 2024 CONR: C ONTRASTIVE REGULARIZER FOR DEEP IM- BALANCED REGRESSION Mahsa Keramati1,2∗, Lili Meng1, R. David Evans1 1 Borealis AI, 2 School of Computing Science, Simon Fraser University mkeramat@sfu.ca, lili.meng@gmail.com,dave.evans@borealisai.com ABSTRACT Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categor- ical label spaces but fail to effectively extend to regression problems where the label space is continuous. Local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. ConR discerns the disagreements between the label space and feature space, and imposes a penalty on these disagreements. ConR addresses the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penal- ized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly ex- tends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on four large-scale deep imbalanced regression benchmarks. Our code is publicly available in https://github.com/BorealisAI/ConR. 1 I NTRODUCTION Imbalanced data distributions, which are common in real-world contexts, introduce substantial challenges in generalizing conventional models due to variance across minority labels and bias to the majority ones (Wang et al., 2021b; Gong et al., 2022; Buda et al., 2018). Although there are numerous works on learning from imbalanced data (Chawla et al., 2002; Cui et al., 2021; Jiang et al., 2021), these studies mainly focus on categorical labels. Continuous labels are potentially infinite, high-dimensional and hard to bin semantically (Ren et al., 2022). These characteristics impede the performance of imbalanced classification approaches on Deep Imbalanced Regression (DIR) (Yang et al., 2021). Continuous labels result in underlying local and global correlations, which yields valuable perspec- tives towards effective representation learning of imbalanced data (Gong et al., 2022; Shin et al., 2022). For instance, regression training on imbalanced data fails to model appropriate relation- ships for minority labels in the feature space (Yang et al., 2021). Yang et al. (2021) established an empirical example of this phenomenon on the age estimation task where the learned features of under-represented samples collapse to majority features. Several approaches tackle this issue by en- couraging local dependencies (Yang et al., 2021; Steininger et al., 2021). However, these methods fail to exploit global relationships and are biased toward only learning representative features for majority samples, especially when minority examples do not have majority neighbours. RankSim (Gong et al., 2022) leverages global and local dependencies by exploiting label similarity orders in feature space. Yet, RankSim does not generalize to all regression tasks, as not all continuous label spaces convey order relationships. For example, for depth-map estimation from scene pictures, the complicated ∗Work done during an internship at Borealis AI. 1 arXiv:2309.06651v4  [cs.LG]  13 Mar 2024Published as a conference paper at ICLR 2024 relationships in the high-dimensional label space are not trivially convertible to a linearly ordered feature space. Given the importance of the correspondence between the label space and feature space for imbalanced regression, can we effectively transfer inter-label relationships, regardless of their complexity, to the feature space? a) Without ConR input + labelPred. Age1 21 80 25 positive anchornegative c) With ConR Pred. Age repulsedattracted 3. Strongly mis-  labeled mpushed more negative b)Feature Space1.Lower weight  for majorityanchor . 2. Strength based  on degree of  mis-prediction Figure 1: Key insights of ConR. a) WithoutConR, it is common to have minority examples mixed with majority examples. b) ConR selects the sam- ple with confusion around it as an anchor and ad- justs the feature space with relative contrastive learning. c) Reduced prediction error. We propose a method to enforce this correspon- dence: ConR is a novel Contrastive Regularizer which is based on infoNCE loss (Oord et al., 2018) but adapted for multi-positive pairs. While similar extensions are performed for clas- sification tasks (Khosla et al., 2020b), ConR addresses continuous label space and penalizes minority sample features from collapsing into the majority ones. Contrastive approaches for imbalanced classification are mainly based on predefined anchors, decision boundary-based negative pair selection and imposing discrimina- tive feature space (Cui et al., 2021; Wang et al., 2021a; Li et al., 2022). These are not feasibly extendible to continuous label spaces. ConR introduces continuity to contrastive learning for regression tasks. Fig 1 illustrates an intuitive ex- ample for ConR from the task of age estimation. There are images of individuals of varying ages, including 1, 21, 25, and 80 years. Age 1 and 80 are the minority examples, reflecting the limited number of images available for these age groups within the datasets. While 21 is a majority example, given the abundance of images around this age within the datasets. Without using ConR, similarities in the feature space are not aligned with the relationships in the label space. Thus, the minority samples’ features collapse to the majority sample, leading to inaccurate predictions for minority ones that mimic the majority sample (Fig 1a). ConR regularizes the feature space by simultaneously encouraging locality via pulling together positive pairs and preserving global correlations by pushing negative pairs. The 21-year-old sample coexists within a region in the feature space alongside 1-year-old and 80-year-old samples. Thus, ConR 1) considers the 21-year-old sample as an anchor, and, 2) pushes negative pairs for minority anchors harder than majority examples to provide better separation in feature space. Furthermore, ConR 3) increases pushing power based on how heavily mislabelled an example is (Fig 1b.1 to Fig 1b.3). We demonstrate thatConR effectively translates label relationships to the feature space, and boosts the regression performance on minority samples (Fig 1c). Refer to A.1 for the empirical analysis on the motivation of ConR. ConR implicitly models the local and global correlations in feature space by three main contributions to the contrastive objective: 1) Dynamic anchor selection: Throughout the training, considering the learned proximity correspondences, ConR selects samples with the most collapses on the feature manifold as anchors. 2) Negative Pairs selection: Negative pairs are sampled to quantify the deviation they introduce to the correspondence of similarities in feature and label space, thereby compensating for under-represented samples. 3) Relative pushing: Negative pairs are pushed away proportional to their label similarity and the density of the anchor’s label. ConR is orthogonal to other imbalanced learning techniques and performs seamlessly on high- dimensional label spaces. Our comprehensive experiments on large-scale DIR benchmarks for facial age, depth and gaze estimation show thatConR strikes a balance between efficiency and performance, especially on depth estimation, which has a complicated and high-dimensional label space. 2 R ELATED WORK Imbalanced classification. The main focus of existing imbalanced approaches is on the classification task, while imbalanced regression is under-explored. The imbalanced classification methods are either data-driven or model-driven. Resampling (Chawla et al., 2002; Chu et al., 2020; Byrd & Lipton, 2019; Han et al., 2005; Jiang et al., 2021) is a data-driven technique that balances the input 2Published as a conference paper at ICLR 2024 data by either over-sampling (Chawla et al., 2002; Byrd & Lipton, 2019) the minority classes or under-sampling the majority ones (Han et al., 2005). Model-Aware K-center(MAK) (Jiang et al., 2021) over-samples tail classes using an external sampling pool. Another branch of data-driven approaches is augmentation-based methods. Mixup-based approaches linearly interpolate data either in input space (Zhang et al., 2017) or feature space (Verma et al., 2019) for vicinal risk minimization. RISDA (Chen et al., 2022) shares statistics between classes considering a confusion-based knowledge graph and implicitly augments minority classes. Model-driven methods such as focal loss (Lin et al., 2017) and logit-adjustment (Tian et al., 2020) are cost-sensitive approaches that regulate the loss functions regarding the class labels. To encourage learning an unbiased feature space, several training strategies including two-stage training (Kang et al., 2020), and transfer learning (Yin et al., 2019) are employed. As discussed further, though there has been much success in this area, there are many issues in converting imbalanced classification approaches to regression. Imbalanced regression. Unlike classification tasks with the objective of learning discriminative representation, effective imbalanced learning in continuous label space is in lieu of modelling the label relationships in the feature space (Yang et al., 2021; Gong et al., 2022). Therefore, imbalanced classification approaches do not feasibly extend to the continuous label space. DenseLoss (Steininger et al., 2021) and LDS (Yang et al., 2021) encourage local similarities by applying kernel smoothing in label space. Feature distribution smoothing (FDS) (Yang et al., 2021) extends the idea of kernel smoothing to the feature space. Ranksim (Gong et al., 2022) proposes to exploit both local and global dependencies by encouraging a correspondence of similarity order between labels and features. Balanced MSE (Ren et al., 2022) prevents Mean Squared Error (MSE) from carrying imbalance to the prediction phase by restoring a balanced prediction distribution. The empirical observations in (Yang et al., 2021) and highlighted in VIR (Wang & Wang, 2023), demonstrate that using empirical label distribution does not accurately reflect the real label density in regression tasks, unlike classification tasks. Thus, traditional re-weighting techniques in regression tasks face limitations. Consequently, LDS (Yang et al., 2021) and the concurrent work, VIR propose to estimate effective label distribution. Despite their valuable success, two main issues arise, specifically for complicated label spaces. These approaches rely on binning the label space to share local statistics. However, while effective in capturing local label relationships, they disregard global correspondences. Furthermore, in complex and high-dimensional label spaces, achieving effective binning and statistics sharing demands in- depth domain knowledge. Even with this knowledge, the task becomes intricate and may not easily extend to different label spaces. Contrastive learning. Contrastive Learning approaches are pairwise representation learning tech- niques that push away semantically divergent samples and pull together similar ones (He et al., 2020; Chen et al., 2020a; Khosla et al., 2020a; Kang et al., 2021a). Momentum Contrast (Moco) (He et al., 2020) is an unsupervised contrastive learning approach that provides a large set of negative samples via introducing a dynamic queue and a moving-averaged encoder; while SupCon (Khosla et al., 2020b) incorporates label information to the contrastive learning. Kang et al. (2021a) argue that in case of imbalanced data, SupCon is subject to learning a feature space biased to majority samples and proposed k-positive contrastive loss (KCL) to choose the same number of positive samples for each instance to alleviate this bias. Contrastive long-tailed classification methods train parametric learnable class centres (Cui et al., 2021; Wang et al., 2021a) or encourage learning a regular simplex (Li et al., 2022). However, these approaches cannot be used for regression tasks where handling label-wise prototypes is potentially complex. Contrastive Regression (CR) (Wang et al., 2022) adds a contrastive loss to Mean Squared Error (MAE) to improve domain adaptation for the gaze estimation task. CR assumes there is a correspondence between similarities in label space and the feature space. Regardless, when learning from the imbalanced data, the correspondence can’t be assumed. Instead of the assumption of correspondence, ConR translates the relative similarities from the label space to the feature space. Barbano et al. (2023) proposed a relative pushing for the task of brain age prediction using MRI scans. This relative pushing doesn’t consider imbalanced distribution, and the weights are learnable kernels that can introduce complexities to complex label spaces. Rank-N-Contrast (Zha et al., 2023) ranks samples and contrasts them against each other regarding their relative rankings to impose continuous representations for regression tasks. Like RankSim Gong et al. (2022), Rank-N-Contrast is designed based on the order assumption and cannot be used in all regression tasks. 3Published as a conference paper at ICLR 2024 a) Pair Selection encoder Yes Yes No positive pair negative pair b) Relative Pushing Label No. of samples anchor positive negativefeature spaceregression function 1 2 close predictions?1 2 close labels?1 2 No pair-upX label similarity No potential pairlabelsample anchorsamplelabel Figure 2: The framework of ConR is to translate label similarities to the feature space. a) Per each augmented sample, ConR selects positive and negative pairs with regard to the label similarities and prediction similarities. b) ConR pulls positive pairs together while pushing away negative pairs regarding their label similarities and label distribution for the anchor. In this way, the minority anchor pushes negative samples harder. The pushing weight is inversely relative to the label similarities. 3 M ETHOD 3.1 P ROBLEM DEFINITION Consider a training dataset consisting N examples, which we denote as {(xi, yi)}N i=0, where xi ∈ Rd is an example input, and yi ∈ Rd′ is its corresponding label. d and d′ are the dimensions of the input and label, respectively. We additionally enforce that the distribution of the labelsDy deviates significantly from a uniform distribution. Given a model that consists of a feature encoder E(·), and a regression function R(·), the objective is to train a regression model R(E(·)), such that the model output ˆyi = R(E(xi)) is similar to the true label yi. 3.2 I MBALANCED CONTRASTIVE REGRESSION In regression tasks, inter-label relationships unveil meaningful associations in feature space. However, learning from imbalanced data harms this correspondence since the features learned for minority samples share statistics with the majority samples, despite dissimilar labels (Yang et al., 2021). By incorporating label and prediction relationships into contrastive learning, ConR enforces appropriate feature-level similarities for modelling a balanced feature space. ConR is a continuous variation of infoNCE. Initially, for creating diversity in examples, we perform problem-specific augmentations on each input xi to produce two augmented samples. We define the set of augmented examples from the input to be {(xa j , yj)}2N j=0, where xa j is an augmented input. As illustrated in Fig. 2, ConR is a collaboration of pair selection and relative pushing. For each augmented sample, ConR first selects pairs. Next, in the case of at least one negative pairing, the sample is considered an anchor and contributes to the regularizing process of ConR by pulling together positive pairs and relatively repelling negative pairs. 3.2.1 P AIR SELECTION Given a pair of examples (xa i , yi) and (xa j , yj) from the augmented inputs ( labels and samples, Fig. 2-a), each example is passed to the feature encoder E(·) to produce feature vectors zi and zj, and then to the regression function R(·) for predictions ˆyi and ˆyj (sample through regression function, Fig. 2-a). The values of the predicted and actual labels are used to determine if the examples should be a positive pair, a negative pair, or unpaired (righthand side, Fig. 2-a). To measure similarity between labels (or predictions) of augmented examples, ConR defines a similarity threshold ω. Given a similarity function Sim(·, ·) ∈ R (e.g., inverse square error), we define two labels (or two predictions) yi and yj as similar if Sim(yi, yj) ≥ ω. We denote this as yi ≃ yj. Iff two examples have similar labels yi ≃ yj, then they are treated as a positive pair. The 4Published as a conference paper at ICLR 2024 examples have dissimilar labels, but similar predictions ˆyi ≃ ˆyj, then they are treated as a negative pair. Otherwise, examples are unpaired. Anchor selection. For each example j, (xa j , yj) We denote the sets of positive and negative pairs for this example, and their feature vectors as K+ j = {(zp)} N+ j p and K− j = {(zq)} N− j q , respectively, where N+ j is the number of positive examples andN− j is the number of negative samples for example j. If N− j > 0, (xa j , yj) is selected as an anchor and contributes to the regularization process of ConR. 3.2.2 C ONTRASTIVE REGULARIZER For each example j, ConR introduces a loss function LConRj. If example j is not selected as an anchor, LConRj = 0. Otherwise, LConRj pulls together positive pairs while pushing away negative pairs proportional to the similarity of their labels. As shown in Fig. 2-b, ConR pushes away negative samples with less label similarity to yj harder than negative samples with labels closer to the anchor yj: LConRj = −log 1 N+ j X zi∈K+ j exp(zj · zi/τ)P zp∈K+ j exp(zj · zp/τ) + P zq∈K− j Sj,q exp(zj · zq/τ) (1) where τ is a temperature hyperparameter and Sj,n is a pushing weight for each negative pair: Sj,q = fS(ηj, Sim(yj, yq)), (2) and yq is the label of xq where zq = E(xq). ηj is a pushing power for each sample (xa j , yj) that depends on label distributionDy to boost the pushing power for minority samples (Fig. 2-b):ηj ∝ wj, where wj is a density-based weight for input j derived from the empirical label distribution (e.g., inverse frequency). The function fS computes the Sj,q to be proportionate to ηj and inversely related to Sim(yj, yq). Please refer to Appendix A.4 for the definition of fS. Finally, LConR is the ConR’s regularizer value for the augmented example set: LConR = 1 2N 2NX j=0 LConRj. (3) To prevent the representations of minority labels from collapsing to the majority ones in deep imbalanced regression, Lsum is optimized. Lsum is weighed sum of LConR and a regression loss LR (e.g., mean absolute error) as below: Lsum = αLR + βLConR (4) 3.3 T HEORETICAL INSIGHTS We theoretically justify the effectiveness of ConR in Appendix B. We derive the upper bound LConR + ϵ on the probability of incorrect labelling of minority samples to be: 1 4N2 2NX j=0,xj∈A K− jX q=0 log Sj,qp( ˆYj|xq) ≤ LConR + ϵ, ϵ N→∞ → 0 (5) where A is the set of anchors and xq is a negative sample. p( ˆYj|xq) is the likelihood of sample xq with an incorrect prediction yq ∈ ˆYj = (ˆyj − ω, ˆyj + ω). ˆyj a prediction that is mistakenly similar to its negative pair ˆyq. We refer to p( ˆYj|xq) as the probability of collapse for xq. The Left-Hand Side (LHS) contains the probability of all collapses for all negative samples, which is bounded above by LConR. Minimizing the loss, consequently minimizes the LHS, which causes either 1) the number of anchors decreases or 2) the degree of collapses is reduced. Each p( ˆYj|xq) is weighted by Sj,q, which penalizes incorrect predictions proportional to severity. Thus, optimizing ConR decreases the probability of mislabelling proportional to the degree, and emphasizes minority samples. 5Published as a conference paper at ICLR 2024 4 E XPERIMENTS 4.1 M AIN RESULTS Datasets and baselines. We use three datasets curated by Yang et al. (2021) for the deep im- balanced regression problem: AgeDB-DIR is a facial age estimation benchmark, created based on AgeDB (Moschoglou et al., 2017). IMDB-WIKI-DIR is an age estimation dataset originated from IMDB-WIKI (Rothe et al., 2018). NYUD2-DIR is created based on NYU Depth Dataset V2 (Silberman et al., 2012) to predict the depth maps from RGB indoor scenes. Moreover, we create MPIIGaze-DIR based on MPIIGaze, which is an appearance-based gaze estimation benchmark. Refer to Appendix A.2 and A.4 for more dataset and implementation details. Please refer to Appendix A.3 for baseline details. Evaluation process and metrics. Following the standard procedure for imbalanced learning (Yang et al., 2021; Gong et al., 2022), we report the results for four shots: All, few, median and many. The whole test data is denoted by All. Based on the number of samples in the training dataset, few, median and many that have less than 20, between 20 and 100, and more than 100 training samples per label, respectively. For AgeDB-DIR and IMDB-WIKI-DIR, the metrics are Mean-Absolute-Error (MAE), and Geometric Mean (GM). For NYUD2-DIR we use Root Mean Squared Error (RMSE) and Threshold accuracy (δ1) as metrics as in (Yang et al., 2021; Ren et al., 2022). Threshold accuracy δi is the percentage of di that satisfies max(d1 g1 , g1 d1 ) < 1.25, where for each pixel, g1 is the ground truth depth value and d1 is the predicted depth value. For MPIIGaze-DIR, we use Mean Angle Error (degrees). To calculate relative improvement of ConR, we compare the performance of each combination of methods including ConR, against the same combination without ConR. Each ”Ours vs. ...” entry shows the average of these improvements (e.g. ”Ours vs. LDS” is the average of the improvements of adding ConR to each combination of baselines that has LDS). Main results for age estimation. Table 1 and Table 2 show the results on AgeDB-DIR and IMDB- WIKI-DIR benchmarks, respectively. We compare the performance of DIR methods: FDS, LDS and RankSim with their regularized version by ConR. All results are the averages of 5 random runs. We observe that the performances of all the baselines are considerably boosted when they are regularized with ConR. In addition, ConR results in the best performance across both metrics and all shots for the AgeDB-DIR benchmark with leading MAE results of 6.81 and 9.21 on the overall test set and few-shot region, respectively. For IMDB-WIKI-DIR, ConR achieves the highest performance on 7 out of 8 shot/metric combinations with the best MAE results of 7.29 and 21.32 on the overall test set and few-shot region, respectively. Main results for depth estimation. To assess the effectiveness of ConR in more challenging settings where the label space is high-dimensional with non-linear inter-label relationships, we compare its performance with baselines: LDS, FDS and Balanced MSE on NYUD2-DIR depth estimation benchmark. For this task, the label similarity is measured by the difference between the average depth value of two samples. As shown in table 3, ConR alleviates the bias of the baseline toward the majority samples. ConR significantly outperforms LDS, FDS and Balanced MSE across all shots and both metrics with leading RMSE results of 1.265 on the overall test set and 1.667 on the few-shot region. Notably, RankSim cannot be used on the depth estimation task; however, ConR can smoothly be extended to high-dimensional label space with high performance. The reason is that order relationships are not semantically meaningful for all regression tasks with high-dimensional label space, such as depth estimation. Main results for gaze estimation. Table 4 compares the performance of ConR with three baseline methods: LDS, FDS and Balanced MSE on the MPIIGaze-DIR benchmark. As shown in table 4, ConR consistently improve the performance of all baselines for all shots and achieves the best Mean angular error of 5.63 (degrees) on the overall test set and 5.21 (degrees) on the few-shot region. Error reduction. In Fig. 3, we show the comparison between three strong baselines (LDS, FDS and Balanced MSE) and by adding ConR for NYUD2-DIR benchmark. It shows a consistent and notable error reduction effect by adding our ConR to the deep imbalanced learning. For more results and analysis on other datasets, please refer to Appendix A.5. 6Published as a conference paper at ICLR 2024 Table 1: Main results for AgeDB-DIR benchmark. Results are reported for the whole test data (all) and three other shots: many, median, and few. Each section compares a baseline with its regularized version of ConR. At the bottom of the table, the average improvements with respect to corresponding baselines without ConR are reported in Green. The best result of either a baseline or its regularized version by ConR is in bold (in column) and Red (across column). Baselines: (Gong et al., 2022). Metrics MAE↓ GM↓ Methods/Shots All Many Median Few All Many Median Few ConR-only(Ours) 7.20 6.50 8.04 9.73 4.59 3.94 4.83 6.39 LDS 7.42 6.83 8.21 10.79 4.85 4.39 5.80 7.03 LDS +ConR (Ours) 7.16 6.61 7.97 9.62 4.51 4.21 4.92 5.87 FDS 7.55 6.99 8.40 10.48 4.82 4.49 5.47 6.58 FDS +ConR (Ours) 7.08 6.46 7.89 9.80 4.31 4.01 5.25 6.92 RankSim 6.91 6.34 7.79 9.89 4.28 3.92 4.88 6.89 RankSim +ConR (Ours) 6.84 6.31 7.65 9.72 4.21 3.95 4.75 6.28 LDS + FDS 7.37 6.82 8.25 10.16 4.72 4.33 5.50 6.98 LDS + FDS +ConR (Ours) 7.21 6.88 7.63 9.59 4.63 4.45 5.18 5.91 LDS + RankSim 6.99 6.38 7.88 10.23 4.40 3.97 5.30 6.90 LDS + RankSim +ConR (Ours) 6.88 6.35 7.69 9.99 4.43 3.87 4.70 6.51 FDS + RankSim 7.02 6.49 7.84 9.68 4.53 4.13 5.37 6.89 FDS + RankSim+ConR (Ours) 6.97 6.33 7.71 9.43 4.19 3.92 4.67 6.14 LDS + FDS + RankSim 7.03 6.54 7.68 9.92 4.45 4.07 5.23 6.35 LDS + FDS + RankSim +ConR (Ours)6.81 6.32 7.45 9.21 4.39 3.81 5.01 6.02 Oursvs.LDS 2.58% 1.51% 3.97% 6.55% 2.39% 2.43% 9.15% 10.78% Oursvs.FDS 3.07% 3.14% 4.57% 5.47% 5.34% 4.85% 6.78 % 6.56% Oursvs.RankSim 1.62% 1.70% 2.24% 3.49% 2.45% 3.31% 8.14% 7.79% Table 2: Main results on IMDB-WIKI-DIR benchmark. Metrics MAE↓ GM↓ Methods/Shots All Many Median Few All Many Median Few ConR-only(Ours) 7.33 6.75 11.99 22.22 4.02 3.79 6.98 12.95 LDS 7.83 7.31 12.43 22.51 4.42 4.19 7.00 13.94 LDS +ConR (Ours) 7.43 6.84 12.38 21.98 4.06 3.94 6.83 12.89 FDS 7.83 7.23 12.60 22.37 4.42 4.20 6.93 13.48 FDS +ConR (Ours) 7.29 6.90 12.01 21.72 4.02 3.83 6.71 12.59 RankSim 7.42 6.84 12.12 22.13 4.10 3.87 6.74 12.78 RankSim +ConR (Ours) 7.33 6.69 11.87 21.53 3.99 3.81 6.66 12.62 LDS + FDS 7.78 7.20 12.61 22.19 4.37 4.12 7.39 12.61 LDS + FDS +ConR (Ours) 7.37 7.00 12.31 21.81 4.07 3.96 6.88 12.86 LDS + RankSim 7.57 7.00 12.16 22.44 4.23 4.00 6.81 13.23 LDS + RankSim +ConR (Ours) 7.48 6.79 12.03 22.31 4.04 3.86 6.77 12.80 FDS + RankSim 7.50 6.93 12.09 21.68 4.19 3.97 6.65 13.28 FDS + RankSim+ConR (Ours) 7.44 6.78 11.79 21.32 4.05 3.88 6.53 12.67 LDS + FDS + RankSim 7.69 7.13 12.30 21.43 4.34 4.13 6.72 12.48 LDS + FDS + RankSim +ConR (Ours)7.46 6.98 12.25 21.39 4.19 4.01 6.75 12.54 Oursvs.LDS 3.67% 3.54% 1.07% 1.21% 5.75% 4.07% 2.37% 2.08% Oursvs.FDS 4.00% 2.91% 2.49% 1.62% 5.68% 4.47% 2.86 % 2.18% Oursvs.RankSim 1.55% 2.37% 1.51% 1.29% 3.50% 2.56% 0.79% 2.16% Table 3: Main results on NYUD2-DIR benchmark. Metrics RMSE↓ δ1↑ Methods/Shots All Many Median Few All Many Median Few ConR-only(Ours) 1.304 0.682 0.889 1.885 0.675 0.699 0.753 0.648 FDS 1.442 0.615 0.940 2.059 0.681 0.760 0.695 0.596 FDS +ConR (Ours) 1.299 0.613 0.836 1.825 0.696 0.800 0.819 0.701 LDS 1.387 0.671 0.913 1.954 0.672 0.701 0.706 0.630 LDS +ConR (Ours) 1.323 0.786 0.823 1.852 0.700 0.632 0.827 0.702 RankSim - - - - - - - - Balanced MSE (BNI) 1.283 0.787 0.870 1.736 0.694 0.622 0.806 0.723 Balanced MSE (BNI) +ConR (Ours) 1.265 0.772 0.809 1.689 0.705 0.631 0.832 0.698 Balanced MSE (BNI) + LDS 1.319 0.810 0.920 1.820 0.681 0.601 0.695 0.648 Balanced MSE (BNI) + LDS +ConR (Ours)1.271 0.723 0.893 1.667 0.699 0.652 0.761 0.752 Oursvs.LDS 4.13% 4.25% 10.41% 6.81% 3.41% 0.07% 13.31% 13.74% Oursvs.FDS 9.92% 0.03% 11.06% 11.37% 2.20% 5.27% 17.84% 17.62% Oursvs.Balanced MSE (BNI) 2.52% 6.33% 8.99% 18.42% 2.11% 4.97% 6.36% 6.30% Time consumption Analysis. Table 5 provides the time consumption of ConR in comparison to other baselines and V ANILLA for the age estimation and depth estimation tasks. V ANILLA is a regression model with no technique for imbalanced learning. The reported time consumptions are 7Published as a conference paper at ICLR 2024 Table 4: Main results of on MPIIGaze-DIR benchmark. Metric Mean angular error (degrees)↓ Method/Shot All Many Median Few ConR-only(Ours) 6.16 5.73 6.85 6.17 LDS 6.48 5.93 7.28 6.83 LDS +ConR (Ours) 6.08 5.76 6.81 5.98 FDS 6.71 6.01 7.50 6.64 FDS +ConR (Ours) 6.32 5.69 6.56 5.65 Balanced MSE (BNI) 5.73 6.34 6.41 5.36 Balanced MSE (BNI)+ConR (Ours) 5.63 6.02 6.17 5.21 Oursvs. LDS 6.17 % 2.87% 6.46% 12.45 % Oursvs. FDS 5.81 % 5.32% 12.53% 14.91 % Oursvs. Balanced MSE 1.75 % 5.05% 3.75% 2.80 % Figure 3: comparison on RMSE results by adding ConR on top of the baselines for NYUD2-DIR benchmark. expressed in seconds for AgeDB-DIR and in minutes for NYUD2-DIR, representing the average forward pass and training time, and were measured using four NVIDIA GeForce GTX 1080 Ti GPUs. Table 5’s findings demonstrate that even with a high-dimensional label space, ConR’s training time is considerably lower than FDS while remaining comparable to the time complexity of LDS, RankSim, and Balanced MSE. This indicates that ConR is an efficient alternative to FDS without compromising efficiency compared to other well-established methods. This result highlights ConR’s ability to handle complex tasks without introducing significant computational overhead. Table 5: Time consumption for AgeDB-DIR and NYU2D-DIR benchmarks. Benchmark AgeDB-DIR NYUD2-DIR Method/Metric Forward pass (s) Training time (s) Forward pass (m) Training time (m) V ANILLA 12.2 31.5 7.5 28.3 LDS 12.7 34.3 7.9 30.2 FDS 38.4 60.5 10.8 66.3 RankSim 16.8 38.8 - - Balanced MSE 16.2 36.3 7.9 29.4 ConR(Ours) 15.7 33.4 8.1 30.4 Feature visualizations. We evaluate ConR by comparing its learned representations with V ANILLA, FDS, and LDS. Using t-SNE visualization, we map ResNet-50’s features to a 2D space for the AgeDB-DIR dataset. Fig. 4 demonstrates the feature-label semantic correspondence exploited by ConR, compared to V ANILLA, FDS and LDS. V ANILLA fails to effectively model the feature space regarding three key observations: a) high occurrences of collapses: features of minority samples are considerably collapsed to the majority ones. b) low relative spread: contradicting the linearity in the age estimation task’s label space, the learned representation exhibits low feature variance across the label spread (across the colour spectrum) compared to the variance across a single label (same colour). c) Noticeable gaps within the feature space: contradicts the intended continuity in regression tasks. Compared to V ANILLA, FDS and LDS slightly alleviate the semantic confusion in feature space. However, as shown in Fig. 4d, ConR learns a considerably more effective feature space with fewer collapses, higher relative spread and semantic continuity. 4.2 A BLATION STUDIES ON DESIGN MODULES Negative sampling, pushing weight and pushing power analysis. Here we assess the significance of our method’s contributions through the evaluation of several variations ofConR and investigating the impact of different choices of the hyperparameters in ConR. We define four versions of ConR: Contrastive-ConR: contrastive regularizer where negative peers are selected only based on label 8Published as a conference paper at ICLR 2024 (a) V ANILLA  (b) FDS  (c) LDS  (d) ConR 3 9595 3 Figure 4: Feature visualization on AgeDB-DIR for (a) V ANILLA, (b) FDS, (c) LDS and (d)ConR. similarities. ConR-S: ConR with no pushing power assigned to the negative pairs. ConR-η: ConR with pushing powers that are not proportionate to the instance weights. ConR-Sim: ConR with pushing powers that are not proportionate to the label similarities. Table 6 describes the comparison between these three versions on the AgeDB-DIR benchmark and shows the crucial role of each component in deep imbalanced regression. Contrastive-ConR is the continuous version of SupCon (Khosla et al., 2020b) that is biased to majority samples (Kang et al., 2021b). Thus, Contrastive-ConR shows better results on many shot that is due to the higher performance for majority samples, while it degrades the performance for minority ones. However, ConR results in a more balanced performance with significant improvement for the minoirty shots. Table 6: Ablation results on AgeDB-DIR benchmark. Metrics MAE↓ Methods/Shots All Many Median Few Contrastive-ConR 7.69 6.12 8.73 12.42 ConR-S 7.51 6.49 8.23 10.86 ConR-Sim 7.54 6.43 8.19 10.69 ConR-η 7.52 6.55 8.11 10.51 ConR 7.48 6.53 8.03 10.42 Similarity threshold analysis. We investigate the choice of similarity thresholdω by exploring the learned features and model performance employing different values for the similarity threshold. Fig. 5a and Fig. 5b compare the feature space learnt with similarity threshold ω = 2 and ω = 1 for ConR on the AgeDB-DIR benchmark. ConR implicitly enforces feature smoothness and linearity in feature space. A high threshold (ω = 2) is prone to encouraging feature smoothing in a limited label range and bias towards majority samples. As illustrated in Fig. 5b, choosing a lower similarity threshold leads to smoother and more linear feature space. Fig. 5c demonstrates the ablation results for the similarity threshold on the Age-DB-DIR dataset. For AgeDB-DIR, ω = 1 produces the best performance. An intuitive explanation could be the higher thresholds will impose sharing feature statistics to an extent that is not in correspondence with the heterogeneous dynamics in the feature space. For example, in the task of facial age estimation, the ageing patterns of teenagers are different from people in their 20s. For more details on this study please refer to Appendix A.6. (a) Visualization for 1 ω = 0.5 3 9595 3  (b) Visualization for 1 ω = 1 3 9595 3 6810 0.51 2 4 6 AllManyMedianFewMAE 1ω (c) Similarity threshold analysis. Figure 5: Ablation study on the similarity threshold ω. (a) and (b) compares the learnt feature space for similarity threshold of 2 and 1 respectively. (c) Comparison of different choices of ω in terms of MAE. 5 C ONCLUSION In this work, we propose ConR, a novel regularizer for DIR that incorporates continuity to contrastive learning and implicitly encourages preserving local and global semantic relations in the feature space without assumptions about inter-label dependencies. The novel anchor selection proposed in this work consistently derives a balanced training focus across the imbalanced training distribution. ConR is orthogonal to all regression models. Our experiments on uni- and multi-dimensional DIR benchmarks show that regularizing a regression model with ConR considerably lifts its performance, especially for the under-represented samples and high-dimensional label spaces. ConR opens a new perspective to contrastive regression on imbalanced data. 9Published as a conference paper at ICLR 2024 REFERENCES Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, and Pietro Gori. Contrastive learning for regression in multi-site brain age prediction. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pp. 1–4. IEEE, 2023. Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural networks, 106:249–259, 2018. Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In International conference on machine learning, pp. 872–881. PMLR, 2019. Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for con- trastive learning of visual representations. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020a. Xiaohua Chen, Yucan Zhou, Dayan Wu, Wanqian Zhang, Yu Zhou, Bo Li, and Weiping Wang. Imagine by reasoning: A reasoning-based implicit semantic data augmentation for long-tailed classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 356–364, 2022. Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b. Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for long-tailed data. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX 16, pp. 694–710. Springer, 2020. Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 715–724, 2021. Yu Gong, Greg Mori, and Frederick Tung. Ranksim: Ranking similarity regularization for deep imbalanced regression. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 7634–7649. PMLR, 2022. Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In Advances in Intelligent Computing: International Conference on Intelligent Computing, ICIC 2005, Hefei, China, August 23-26, 2005, Proceedings, Part I 1, pp. 878–887. Springer, 2005. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020. Junjie Hu, Mete Ozay, Yan Zhang, and Takayuki Okatani. Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries. In 2019 IEEE winter conference on applications of computer vision (W ACV), pp. 1043–1051. IEEE, 2019. Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Improving contrastive learning on imbalanced data via open-world sampling. Advances in Neural Information Processing Systems, 34:5997–6009, 2021. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations, 2020. Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2021a. 10Published as a conference paper at ICLR 2024 Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2021b. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661–18673, 2020a. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In H. Larochelle, M. Ran- zato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18661–18673. Curran Associates, Inc., 2020b. Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted supervised contrastive learning for long-tailed recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6918–6928, 2022. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999–3007, 2017. doi: 10.1109/ICCV .2017.324. Edward James McShane. Jensen’s inequality. 1937. Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 51–59, 2017. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Jiawei Ren, Mingyuan Zhang, Cunjun Yu, and Ziwei Liu. Balanced mse for imbalanced visual regres- sion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7926–7935, 2022. Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision, 126(2-4): 144–157, 2018. Nyeong-Ho Shin, Seon-Ho Lee, and Chang-Su Kim. Moving window regression: a novel approach to ordinal regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 18760–18769, 2022. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12, pp. 746–760. Springer, 2012. Michael Steininger, Konstantin Kobs, Padraig Davidson, Anna Krause, and Andreas Hotho. Density- based weighting for imbalanced regression. Machine Learning, 110:2187–2211, 2021. Junjiao Tian, Yen-Cheng Liu, Nathaniel Glaser, Yen-Chang Hsu, and Zsolt Kira. Posterior re- calibration for imbalanced datasets. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8101–8113. Curran Associates, Inc., 2020. Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International conference on machine learning, pp. 6438–6447. PMLR, 2019. Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, and Lei Wang. Contrastive learning based hybrid networks for long-tailed image classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 943–952, 2021a. 11Published as a conference paper at ICLR 2024 Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu. Long-tailed recognition by rout- ing diverse distribution-aware experts. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id=D9I3drBz4UC. Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19376–19385, 2022. Ziyan Wang and Hao Wang. Variational imbalanced regression: Fair uncertainty quantification via probabilistic smoothing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbal- anced regression. In International Conference on Machine Learning, pp. 11842–11851. PMLR, 2021. Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for face recognition with under-represented data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5704–5713, 2019. Kaiwen Zha, Peng Cao, Jeany Son, Yuzhe Yang, and Dina Katabi. Rank-n-contrast: Learning continuous representations for regression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 12Published as a conference paper at ICLR 2024 A E XPERIMENTS A.1 E MPIRICAL ANALYSIS OF THE MOTIVATION The key motivation of the design of ConR is that features of minority samples tend to collapse to their majority neighbours. ConR highlights these situations by defining a penalty based on the misalignments between prediction similarities and label similarities. Further, ConR regularizes a regression model by minimizing the defined penalty in a contrastive manner. Here we first define the penalty P(y) for each prediction value y ∈ Dy. P(y) denotes the av- erage of the regression errors of the samples with the similar prediction value of y: P(y) = 1 Ny PNy i=0 LR(xi, yi), where R(E(xi)) ≃ y, yi ̸≃ y and Ny is the number of the samples collapsed to the feature space of samples labelled with y. To empirically confirm the motivation behindConR, we investigate the importance of the penalty term in regression tasks. In addition, we show that regularizing a regression model with ConR consistently alleviate the penalty term defined byConR and this optimization considerably contributes to imbalanced regression. Fig. 6a and Fig. 6b demonstrate the comparison between LDS and ConR in terms of the training loss curve and validation loss curve, respectively. Moreover, Fig. 6c shows the trend of the expected value of P(y) over the label space, throughout the training. Comparing Fig. 6c with Fig. 6a and Fig. 6b, P(y) follows the same decreasing pattern as training loss and validation loss. This observation show that penalizing the penalty term is highly coupled with the learning process. In addition, Fig. 6 shows that ConR outperform LDS with a considerable gap, particularly in terms of P(y); showing that ConR regularizer consistently alleviates the penalty and significantly contributes to the imbalanced regression. 0 5 10 15 20 0 20406080 ConRLDS MAE epoch (a) Training loss 0 5 10 15 20 0 20406080 ConRLDS MAE epoch (b) Validation loss 0 10 20 30 0 20406080 ConRLDS 𝐸!(𝑃) epoch (c) Expected value of penalty Figure 6: Comparison of the performance of LDS and ConR on AgeDB-DIR benchmark in terms of (a)training loss, (b)validation loss, and (c)regularizer penalty. Fig. 7a shows the training label distribution and Fig. 7b depicts the difference of P(y) across the label space between ConR and LDS. It empirically shows the considerable improvement of ConR over LDS in terms of the defined penalty. More improvement over the majority of samples is intuitive because due to the imbalanced distribution, most of the collapses in feature space happen in the majority areas and decreasing the penalty in these areas contributes the most to the imbalanced regression. Finally, Fig. 7c compares the regression error of ConR and LDS and we observe ConR results in a considerable improvement over LDS, especially for minority samples. A.2 D ATASET DETAILS Age estimation. We evaluated our method on two DIR benchmarks for age estimation curated by Yang et al. (2021): IMDB-WIKI-DIR and AgeDB-DIR. IMDB-WIKI (Rothe et al., 2018) has 191.5K images for training, and 11.0K images for validation and testing, respectively. To structure IMDB-WIKI-DIR, Yang et al. (2021) bin the label space with a bin length of 1 year, where the minimum age is 0 and the maximum age is 186. The bin density varies between 1 and 7,149. The AgeDB dataset (Moschoglou et al., 2017) has 16,488 samples.Yang et al. (2021) constructed AgeDB- DIR in a similar manner as IMDB-WIKI-DIR, where the minimum age is 0 and the maximum age is 101. The maximum number of samples per bin is 353 images and the minimum bin density is 1. 13Published as a conference paper at ICLR 2024 0 4020 60 801000 2K 4K Number of Training samplesTarget Value (a) Training loss 0 4020 6080100 0 2K 4K -50 30 Penalty difference Number of Training samples Target Value (b) Validation loss 0 4020 6080100 0 2K 4K -50 15 MAEdifference Number of Training samples Target Value (c) Expected value of penalty Figure 7: Qualitative analysis of ConR in terms of regularizer penalty on AgeDB-DIR benchmark. (a) training label distribution. (b) Penalty difference (LDS minus ConR) on balanced test data. (c) MAE/ difference (LDS minus ConR) on balanced test data. There are 12,208 training samples and the validation set and test set are made balanced with 2,140 samples each. Depth estimation. Yang et al. (2021) created NYUD2-DIR based on the NYU Depth Dataset V2 Silberman et al. (2012). NYU Depth Dataset V2 has images and corresponding depth maps for different indoor scenes and the task is to predict the depth maps from the RGB scene images. The upper bound of the depth maps is 10 meters and the lower bound is 0.7 meters. Following standard practices. There are 50K images for training and 654 images for testing. Yang et al. (2021) use the bin length of 0.1-meter bin density varies between 1.13 × 106 and 1.46 × 108. For a balanced test set, Yang et al. (2021) randomly select 9,357 test pixels for each bin from 654 test images with a total of 8.70 × 105 test pixels in the NYUD2-DIR test set. Gaze estimation. We used a subset of the MPIIGaze dataset comprising 45,000 training samples from 15 individuals, with 3,000 samples per person. The dataset is naturally imbalanced over the 2D training label distribution. Here we provide the details of deriving the proposed MPIIGaze-DIR benchmark from the MPIIGaze dataset. the label space of MPIIGaze is 2-dimensional with one dimension ranging from -0.39 to 0.08 and the other from -0.72 to 0.67. Fig. 8a shows the imbalanced distribution across each dimension separately and Fig. 8b shows the joint distribution of the label space with a grid size of 10. Considering the joint distribution, we define thresholds on the bin densities to curate shots (i.e. many, median, and few) as follows: 1300 or more for the many-shot, 700 to 1300 for the median-shot and less than 700 for the few-shot. Next, with the code snippet provided in Fig. 9, We assign the samples to their corresponding shots. (a) Individual distribution  (b) Joint distribution Figure 8: Distribution of the 2-dimensional label space of MPIIGaze benchmark. For training, we follow a leave-one-out scheme where each time one person is dedicated to validation, one person for testing and 13 people for training. The reported results are in terms of average test results among all the 15 people. To create a balanced test set, we take 200 samples from each shot similar to the methods used in the FDS work. Follow the baselines (Gong et al., 2022), only the training data for these tasks is imbalanced; the test dataset is balanced. 14Published as a conference paper at ICLR 2024 Figure 9: Code snippet for MPIIGaze-DIR creation. A.3 B ASELINES ConR is orthogonal to state-of-the-art imbalanced learning methods, thus we examine the improve- ment from ConR when added on top of existing methods, which we refer to as baselines for our technique: Label- and Feature-Distribution Smoothing (LDS and FDS) encourage local similarities in label and feature space (Yang et al., 2021). RankSim imposes a matching between the order of similarities in label space with these similarities in feature space (Gong et al., 2022). Balanced MSE encourages a balanced prediction distribution (Ren et al., 2022). To investigate the effect of contrastive learning on deep imbalanced regression, we also regularize using infoNCE (Oord et al., 2018) and contrastive architecture of MoCo (He et al., 2020), MoCo V2 (Chen et al., 2020b). Refer to Appendix A.6 for contrastive analysis. Results for RankSim on depth estimation and gaze estimation are omitted as RankSim is not suitable for these tasks. A.4 I MPLEMENTATION DETAILS We use four NVIDIA GeForce GTX 1080 Ti GPU to train all models. For a fair comparison, we follow (Yang et al., 2021) for all standard train/val/test splits. The rest of this section provides the implementation details and choices of hyperparameters for all three datasets. Age estimation. For AgeDB-DIR benchmark and IMDB-WIKI-DIR benchmark, we use Resnet50 for encoder E(·) and a one-layer fully connected network for the regression module R(·). The batch size is 64 and the learning rate is 2.5 ∗ 10−4 and decreases by 10× at epoch 60 and epoch 80. We use the Adam optimizer with a momentum of 0.9 and a weight decay of 1e-4. Following the baselines (Yang et al., 2021) the loss function for regression LR is Mean Absolute Error(MAE). All the models are trained for 90 epochs. The augmentations in the age estimation task are random crop and random horizontal flip. ηj = (0.01)wj and the similarity functionSim(·, ·) is inverse Mean Absolute Error(MAE). To resolve divide by zero and infinite numbers, a pair of samples with MAE distance < 1 ω are considered similar. Further, the pushing weight Sj,q is defined as: Sj,q = fS(ηj, 1 MAE(yj,yq) ) = ηjMAE (yj, yq). Finally, the similarity threshold ω is 1, τ = 0.2, α = 1, and β = 4. Depth estimation. For NYUD2-DIR benchmark, we use ResNet-50-based encoder-decoder archi- tecture (Hu et al., 2019). The output size is 114 × 152. The batch size is 32 and the learning rate is 1 ∗ 10−4. All models are trained for 90 epochs with an Adam optimizer. The momentum of the optimizer is 0.9 and its weight decay is 1e − 4. Following the baselines (Yang et al., 2021; Hu et al., 2019) the loss function for regression LR is root-mean-square(RMSE). The augmentations in the depth estimation task are random rotate, color jitter, and random horizontal flip. To measure the similarity in label space, first, for each sample (zj, yj), we take the average value of the depth map, denoted as ¯yj. Then, for each pair of samples, we use the root-mean-square(RMSE) to quantify the similarity of this pair in the label space. If the RMSE( ¯yi, ¯yj) is less than 1 ω , sample i and sample j are considered similar and otherwise dissimilar. In addition, the pushing weight Sj,q is defined as: Sj,q = fS(ηj, 1 RMSE (yj,yq) ) = ηjRMSE (yj, yq). The similarity threshold ω is 5, β = 0.2, τ = 0.7 and ηj = (0.2)wj. 15Published as a conference paper at ICLR 2024 Gaze estimation. We reported our results on a balanced test set, containing 600 samples in total and 200 samples per each ”many,” ”median,” and ”few” shots. Our results were averaged over five random runs to ensure statistical significance. Each run in the evaluation incorporated a leave-one-out scheme, where we performed 15 runs with a single individual as the designated test set. The final results are the Mean Angle Error (in degrees) for all the individuals. The backbone is LeNet,β = 0.4, α = 1, ω = 1 . ηj = (0.01)wj and Sj,q = fS(ηj, 1 MAE(yj,yq) ) = ηjMAE (yj, yq). The batch size and the base learning rate are 32 and 0.01, respectively. The augmentations in the gaze estimation task are random crop, random resize, and colour jitter. A.5 P ERFORMANCE ANALYSIS All the experimental results are reported as the averages of 5 random runs. More results for age estimation. For a more extensive empirical confrimation that ConR is orthogonal to DIR baselines, Table 7 shows the performance improvements when RRT Yang et al. (2021), Focal-R Yang et al. (2021) and Balanced MSE are regularized byConR. Table 7: MAE results of ConR on AgeDB-DIR Benchmark and IMDB-WIKI-DIR benchmark. Metric MAE↓ Benchmark AgeDB-DIR IMDB-WIKI-DIR Methods/Shots All Many Median Few All Many Median Few RRT 7.74 6.98 8.79 11.99 7.81 7.07 14.06 25.13 RRT +ConR(Ours) 7.53 6.79 7.60 10.30 7.41 6.89 13.20 23.30 Focal-R 7.64 6.68 9.22 13.00 7.97 7.12 15.14 26.96 Focal-R +ConR(Ours) 7.23 6.63 8.30 11.89 7.85 7.01 14.31 25.23 Balabced MSE (GAI) 7.57 7.46 8.40 10.93 8.12 7.58 12.27 23.05 Balabced MSE (GAI) +ConR(Ours) 7.22 6.71 7.99 9.88 7.84 7.20 12.09 22.20 Oursvs. RRT 2.71 % 2.72% 13.54% 14.10 % 5.12 % 2.55% 6.12% 7.28 % Oursvs. Focal-R 5.37 % 0.75% 9.98% 8.54 % 1.51 % 1.54% 5.48% 6.42 % Oursvs. Balanced MSE (GAI) 4.62 % 10.05% 4.88% 9.61 % 3.45 % 5.01% 1.47% 3.69 % Error Reduction. Here we show the comparison of the Error reduction resulting from adding ConR to the deep imbalanced regression baselines (LDS, FDS and RankSim) for age estimation benchmarks(e.g. AgeDB-DIR and IMDB-WIKI-DIR) and (LDS, FDS and Balanced MSE) for gaze estimation benchmark. Fig. 10, Fig. 11 and Fig. 12 empirically confirm significant perfor- mance consistency ConR introduces to DIR for AgeDB-DIR, IMDB-WIKI-DIR and MPIIGaze-DIR benchmarks, respectively. Figure 10: comparison on MAE results by adding ConR on top of the baselines for AgeDB-DIR benchmark. Feature visualization. Fig. 13 compares the learned representations by RankSim and Balanced MSE. RanKSim by imposing order relationships encourage high relative spread while Balanced MSE suffer from low relative spread. Both RankSim and Balanced MSE have high occurrences of collapses and noticeable gaps in their feature space. Comparing Fig. 4-d with Fig. 13 shows that ConR learns the most effective representations. 16Published as a conference paper at ICLR 2024 Figure 11: Comparison of the performance gain by regularizing the DIR baselines (LDS, FDS, RankSim) with ConR on IMDB-WIKI-DIR benchmark. Figure 12: Comparison of the performance gain by regularizing the DIR baselines (LDS, FDS, Balanced MSE) with ConR on MPIIGaze-DIR benchmark. A.6 A BLATION STUDY Negative Sampling, Pushing Weight and Power Analysis. Table 8, table 9 and table 10 show the significance of the main contributions of ConR for IMDB-WIKI-DIR, NYUD2-DIR and MPIIGaze- DIR benchmarks, respectively. Table 8: Ablation results of the design modules of ConR on IMDB-WIKI-DIR benchmark. Metric MAE↓ Method/Shot All Many Median Few Contrastive-ConR 8.10 6.79 15.87 26.51 ConR-S 7.79 6.99 14.61 25.64 ConR-Sim 7.83 6.87 14.30 25.59 ConR-η 7.76 7.10 14.25 25.33 ConR (Ours) 7.84 7.09 14.16 25.15 Table 9: Ablation results of the design modules of ConR on NYUD2-DIR benchmark. Metric RMSE↓ Method/Shot All Many Median Few Contrastive-ConR 1.518 0.586 1.124 2.412 ConR-S 1.410 0.670 0.941 1.954 ConR-Sim 1.383 0.667 0.935 1.929 ConR-η 1.318 0.693 0.892 1.910 ConR (Ours) 1.304 0.682 0.889 1.885 Similarity Threshold Selection. Fig. 14 shows the ablation study on the similarity threshold for IMDB-WIKI-DIR and NYUD2-DIR benchmarks. ω = 1 and ω = 5 are the best similarity threshold choices for IMDB-WIKI-DIR and NYUD2-DIR benchmarks, respectively. 17Published as a conference paper at ICLR 2024 (a) RankSim (b) Balanced MSE Figure 13: Feature visualization on AgeDB-DIR dataset for (a) RankSim, (b) Balanced MSE. Table 10: Ablation results of the design modules of ConR on MPIIGaze-DIR benchmark. Metric Mean Angle Error (degrees)↓ Method/Shot All Many Median Few Contrastive-ConR 7.11 5.00 7.73 9.89 ConR-S 6.47 5.94 6.91 6.71 ConR-Sim 6.39 5.69 7.09 6.48 ConR-η 6.24 5.87 6.96 6.27 ConR (Ours) 6.16 5.73 6.85 6.17 6111621 0.51 2 4 6 AllManyMedianFewMAE 1ω (a) IMDB-WIKI-DIR 0.511.52 0.10.20.40.8 AllManyMedianFewRMSE 1ω (b) NYUD2-DIR 5.566.57 0.51 2 4 AllManyMedianFew 1ω Degree (c) NYUD2-DIR Figure 14: Comparison of different choices of ω (a) in terms of MAE for IMDB-WIKI-DIR bench- mark, (b) in terms of RMSE for NYUD2-DIR and (c) in terms of Mean angular error (degrees) for MPIIGaze-DIR. Hyperparameter selection. Here we present the selection process of hyperparameters α, β and η for all the benchmarks. Table 11, Table 12, Table 13 and Table 14 show the ablation study of ConR on α and β in Eq. 4 for AgeDB-DIR, IMDB-WIKI-DIR, NYUD2-DIR and MPIIGaze-DIR benchmarks, respectively. In these tables, hyperparameter η is set to the values mentioned in A.4. Additionally, Table 15, Table 16, Table 17 and Table 18 show the ablation study ofConR on η in Eq. 2 for AgeDB-DIR, IMDB-WIKI-DIR, NYUD2-DIR and MPIIGaze-DIR benchmarks, respectively. In these tables, hyperparameter η is set to the values mentioned in A.4. Contrastive Regression. To evaluate the impact of contrastive learning on deep imbalanced regression, we use two contrastive regularizers: MoCo: We regularize the baselines with infoNCE loss, using the architecture of with MoCo V1 (He et al., 2020), MoCo V2 (Chen et al., 2020b), and ConR. Here we regularized a regression model with both Moco v1 and MoCo v2. Our experiments shows that MoCo v2 degrades the regression performance in some cases. Table 19 compares the performance of a regression model on AgeDB-DIR, IMDB-WIKI-DIR and NYUD2-DIR benchmarks when it is regularized in a contrastive manner with MoCo V1, MoCo V2, and ConR. The results 18Published as a conference paper at ICLR 2024 Table 11: Ablation study on α and β in Eq. 4 for AgeDB-DIR benchmark α β MAE↓ All Many Median Few 0.5 1 7.31 6.53 8.08 10.51 1 0.5 7.30 6.59 8.10 10.46 1 1 7.31 6.55 8.11 10.48 1 2 7.35 6.58 8.07 10.46 1 4 7.28 6.53 8.03 10.42 1 5 7.25 6.47 8.12 10.54 Table 12: Ablation study on α and β in Eq. 4 for IMDB-WIKI-DIR benchmark α β MAE↓ All Many Median Few 0.5 1 7.98 7.11 14.26 25.19 1 0.5 7.90 7.17 14.34 25.21 1 1 7.94 7.29 14.28 25.19 1 2 7.80 7.21 14.23 25.17 1 4 7.84 7.09 14.16 25.15 1 5 7.98 7.25 14.24 25.13 Table 13: Ablation study on α and β in Eq. 4 for NYUD2-DIR benchmark α β RMSE↓ All Many Median Few 0.5 1 1.344 0.722 0.897 1.918 1 0.5 1.324 0.712 0.891 1.945 1 0.2 1.304 0.682 0.889 1.885 1 0.4 1.316 0.673 0.909 1.905 Table 14: Ablation study on α and β in Eq. 4 for MPIIGaze-DIR benchmark α β Mean Angle Error (degrees)↓ All Many Median Few 0.5 1 6.18 5.79 6.89 6.19 1 0.2 6.14 5.85 6.84 6.22 1 0.4 6.16 5.73 6.85 6.17 1 1 6.09 5.64 7.05 6.24 Table 15: Ablation study of η (pushing power) for AgeDB-DIR benchmark. η MAE↓ All Many Median Few 0.009 7.31 6.63 7.99 10.5 0.01 7.28 6.53 8.03 10.42 0.05 7.36 6.51 8.11 10.51 0.1 7.38 6.49 8.09 10.48 are reported in terms of MAE for AgeDB-DIR benchmark and IMDB-WIKI-DIR benchmark and in terms of RMSE for NYUD2-DIR dataset. Moco considerably boost the performance of V ANILLA and shows that contrastive training significantly improves the regression performance, especially for minority samples. ConR incorporate unbiased supervision into the contrastive regression and significantly boost the performance on minority samples with no harm to the learning process for majority samples. As shown in Fig. 15, Moco and ConR provide more consistent performance compared to the baseline. In addition, Moco is consistently outperformed by ConR and empirically 19Published as a conference paper at ICLR 2024 Table 16: Ablation study of η (pushing power) for IMDB-WIKI-DIR benchmark. η MAE↓ All Many Median Few 0.009 7.88 7.07 14.25 25.16 0.01 7.84 7.09 14.16 25.15 0.05 7.88 6.99 14.20 25.16 0.1 7.91 7.14 14.18 25.21 Table 17: Ablation study of η (pushing power) for NYUD2-DIR benchmark. η RMSE↓ All Many Median Few 0.1 1.312 0.674 0.886 1.891 0.2 1.304 0.682 0.889 1.885 0.4 1.295 0.619 0.92 1.911 Table 18: Ablation study of η (pushing power) for MPIIGaze-DIR benchmark. η Mean angular error (degrees)↓ All Many Median Few 0.5 6.15 5.77 6.83 6.21 1 6.16 5.73 6.85 6.17 2 6.18 5.74 6.89 6.23 confirms ConR improves the self-supervised contrastive regularizer by incorporating supervision in an unbiased manner. Table 19: Results of contrastive learning analysis on AgeDB-DIR, IMDB-WIKI-DIR and NYUD2-DIR benchmarks. Results are reported for the whole test data (all) and three other shots: many, median, few. At the bottom of the table the improvements of ConR with respect to Moco are reported in green for each benchmark, shot and metric. In each column, the best result is in bold. Benchmark AgeDB-DIR IMDB-WIKI-DIR NYUD2-DIRMetric MAE↓ MAE↓ RMSE↓ Method Shot All Many Median Few All Many Median Few All Many Median Few V ANILLA 7.35 6.56 8.23 12.37 8.06 7.23 15.12 26.33 1.477 0.591 0.952 2.123+ Moco V1 7.33 6.50 8.19 11.72 7.89 7.13 14.78 26.11 1.370 0.601 0.902 1.912+ Moco V2 7.47 6.21 8.75 12.75 8.12 6.99 15.02 26.01 1.404 0.632 0.978 2.207+ConR 7.28 6.53 8.03 10.42 7.84 7.09 14.16 25.1 5 1.304 0.682 0.889 1.885 ConRvs.Moco 0.68% -0.46% 1.96% 11.10%0.63% 0.56% 4.20% 3.68%4.82% -1.63% 1.44% 4.41% VANILLAMocoConR (a) AgeDB-DIR VANILLAMocoConR (b) IMDB-WIKI-DIR RMSE VANILLAMocoConR (c) NYUD2-DIR Figure 15: Comparison of the performance gain of regularizing V ANILLA regression model with con- trastive regularizers: Moco V1 and ConR on (a) AgeDB-DIR, (b) IMDB-WIKI-DIR anf (c) NYUD2- DIR benchmarks. It shows contrastive regularizer consistently lifts the performance of the baseline, particularly on minority samples. 20Published as a conference paper at ICLR 2024 B M ORE THEORETICAL INSIGHTS Here we theoretically justify the effectiveness of ConR by deriving a upper bound on the probability of incorrect labelling of minority samples. We show that minimizing LConR robustly minimizes this probability of mislabeling for minority samples and consequently improves the generalizability. In the following, we’ll derive the upper bound: following (Oord et al., 2018; Wang et al., 2022) we define the density ratio f(ˆy, x) in InfoNCE Oord et al. (2018) to be p(ˆy|x) p(ˆy) (Oord et al., 2018; Wang et al., 2022) that is estimated by exp(zj · zi/τ) in Eq. 1 like other contrastive objective functions (He et al., 2020; Chen et al., 2020a). p(ˆy|x) is the desired prediction distribution and ˆYj = (ˆyj − ω, ˆyj + ω). For each anchor xj the LConRj in Eq. 1 is defined to be: LConRj = −log 1 N+ j X zi∈K+ j exp(zj · zi/τ)P zp∈K+ j exp(zj · zp/τ) + P zq∈K− j Sj,q exp(zj · zq/τ) (6) Then, LConRj can be rewritten as: LConRj = − log 1 N+ j N+ jX i=0 f(ˆyj, xi) PK+ j p=0 f(ˆyj, xp) + PK− j q=0 Sj,qf( ˆYj, xq) (7) Following Jensen’s inequality we have: LConRj = − log 1 N+ j N+ jX i=0 f(ˆyj, xi) PN+ j p=0 f(ˆyj, xp) + PK− j q=0 Sj,qf( ˆYj, xq) (8) McShane (1937) ≥ − 1 N+ j N+ jX i=0 log f(ˆyj, xi) PK+ j p=0 f(ˆyj, xp) + PN− j q=0 Sj,qf( ˆYj, xq) (9) where xq are the negative sample, and ˆyj is the prediction of positive samples. prediction of xq mistakenly fall in the range of (ˆyj ± ω). Further, we have: LConRj ≥ −1 N+ j K+ jX i=0 log f(ˆyj, xi) PN+ j p=0 f(ˆyj, xp) + PN− j q=0 Sj,qf( ˆYj, xq) (10) = 1 N+ j N+ jX i=0 log \"PN+ j p=0 f(ˆyj, xp) f(ˆyj, xi) + PN− j q=0 Sj,qf( ˆYj, xq) f(ˆyj, xi) # (11) ≥ 1 N+ j N+ jX i=0 log \"PN+ j p=0 f(ˆyj, xp) PN+ j i=0 f(ˆyj, xi) + PK− j q=0 Sj,qf( ˆYj, xq) f(ˆyj, xi) # (12) ≥ 1 N+ j N+ jX i=0 log \" 1 + PN− j q=0 Sj,qf( ˆYj, xq) f(ˆyj, xi) # (13) ≥ 1 N+ j N+ jX i=0 log \"PN− j q=0 Sj,qf( ˆYj, xq) f(ˆyj, xi) # (14) = 1 N+ j \u0002 log N− jX q=0 Sj,qf( ˆYj, xq) − log N+ jX i=0 f(ˆyj, xi) \u0003 (15) 21Published as a conference paper at ICLR 2024 Next, the LConR is: LConR = 1 2N 2NX j=0 LConRj (16) ≥ 1 2N 2NX j=0 1 N+ j \u0002 log N− jX q=0 Sj,qf( ˆYj, xq) − log N+ jX i=0 f(ˆyj, xi) \u0003 (17) ≥ 1 2N 2NX j=0 1 2N \u0002 log N− jX q=0 Sj,qf( ˆYj, xq) − log N+ jX i=0 f(ˆyj, xi) \u0003 (18) = 1 4N2 2NX j=0 \u0002 log N− jX q=0 Sj,qf( ˆYj, xq) − log N+ jX i=0 f(ˆyj, xi) \u0003 (19) = 1 4N2 ( 2NX j=0 log N− jX q=0 Sj,qf( ˆYj, xq) − 2NX j=0 log N+ jX i=0 f(ˆyj, xi)) (20) = 1 4N2 ( 2NX j=0 log N− jX q=0 Sj,qf( ˆYj, xq) − 2NX j=0 log N+ jX i=0 p(ˆyj|xi) p(ˆyj) ) (21) p(ˆyj|xi) p(ˆyj) <1 ≥ 1 4N2 ( 2NX j=0 log N− jX q=0 Sj,qf( ˆYj, xq) − 2NX j=0 log N+ jX i=0 1) (22) = 1 4N2 ( 2NX j=0 log N− jX q=0 Sj,qf( ˆYj, xq) − 2N log N+ j ). (23) Further, we have: LConR = 1 4N2 ( 2NX j=0 log N− jX q=0 Sj,qf( ˆYj, xq) − 2N log N+ j ) (24) ≥ 1 4N2 ( 2NX j=0 N− jX q=0 log Sj,qf( ˆYj, xq) − 2N log N+ j ) (25) = 1 4N2 ( 2NX j=0 N− jX q=0 log Sj,q p( ˆYj|xq) p( ˆYj) − 2N log N+ j ) (26) p( ˆYj)<1 ≥ 1 4N2 ( 2NX j=0 N− jX q=0 log Sj,qp( ˆYj|xq) − 2N log N+ j ) (27) ≥ 1 4N2 ( 2NX j=0 N− jX q=0 log Sj,qp( ˆYj|xq) − 2N log(2N)). (28) Then: 1 4N2 2NX j=0 N− jX q=0 log Sj,qp( ˆYj|xq) ≤ LConR + log(2N) 2N . (29) 22Published as a conference paper at ICLR 2024 As explained in 3, among the 2N augmented samples only the ones with the confusion around them are chosen as anchors. Assuming A is the set of selected anchors that is a subset of the 2N augmented samples we have: 1 4N2 2NX j=0,xj∈A K− jX q=0 log Sj,qp( ˆYj|xq) ≤ LConR + ϵ, ϵ N→∞ → 0 (30) In Eq. 30, p( ˆYj|xq) is the likelihood of sample xq with an incorrect prediction yq ∈ ˆYj. We refer to p( ˆYj|xq) as the probability of collapse for xq. The left-hand side presents this probability for all the negative pairs. Regarding the empirical study by Yang et al. (2021), when learning a regression function from the imbalanced data, the representations of minority samples tend to collapse to the majority ones. Since in the definition of LConRj in Eq. 1, xq show the collapsed minority samples, minimizing the left side of the inequality in Eq. 30 is the intended optimization in deep imbalanced regression. The left-hand side is the probability of all collapses during the training and regarding Eq. 30 Convergence of LConR tightens the upper bound for it. Minimizing the left-hand side can be explained with two non-disjoint scenarios: either the number of anchors or the degree of collapses is reduced. Here the degree of collapse for each negative sample refers to the quantified disagreement between the label similarity and prediction similarity as discussed in section 3. In addition, each collapse probability is weighted with Sj,q, leading to penalizing the incorrect predictions with regard to their severity. In other words, ConR penalizes the bias probabilities with a re-weighting scheme, where the weights are defined based on the agreement of the predictions and labels. C A LGORITHM OF CONR Algorithm 1 shows the pseudo-code of regularizing a regression model usingConR. In this algorithm, Sj,q is the pushing weights for selected anchor zj and its negative pair zq (Eq. 2). K+ j and K− j are positive pairs of zj, and negative pairs of zj, respectively. 23Published as a conference paper at ICLR 2024 Algorithm 1 ConR: Contrastive regularizer for deep imbalanced regression Require: input samples {(xi, yi)}N i=0 feature encoder E(.) regression function R(.) for e in epochs do for b = {(xi, yi)}Nb i=0 in Batches do {xa j , yj}2Nb j=0 Augmentations ← { xi, yi}Nb i=0 {zj}2N j=0 ← E({xa j }2Nb j=0) ▷ zj : feature embeddings of augmented input xa j . {ˆyj}2Nb j=0 ← R(E({zj}2Nb j=0)) ▷ ˆyj : prediction of zj. {K+ j , K− j }2Nb j=0 ← {zj, yj, ˆyj}2Nb j=0 for j ∈ (0, 2Nb) do if N− j == 0 then LConRj ← 0 ▷ Samples with no negative pairs are not selected as anchors. else {Sj,q} N− j q=0 ← fS(yj, {yq} N− j q=0) ▷ Sj,q: pushing weight (Eq. 2) LConRj ← computeConR(zj, K+ j , K− j , {Sj,q} N− j q=0) ▷ Eq. 1 end if end for LConR Average ← {LConRj}2Nb j=0 ▷ Eq. 3 LR ← computeRegressionLoss({ˆyj, yj}Nb j=0) Lsum ← αLR + βLConR Lsum.backPropagate() end for end for Ensure: Trained regression function R(.) and trained feature encoder E(.) 24",
      "references": [
        "Contrastive learning for regression in multi-site brain age prediction",
        "A systematic study of the class imbalance problem in convolutional neural networks",
        "What is the effect of importance weighting in deep learning?",
        "Smote: synthetic minority over-sampling technique",
        "A simple framework for contrastive learning of visual representations",
        "Imagine by reasoning: A reasoning-based implicit semantic data augmentation for long-tailed classification",
        "Feature space augmentation for long-tailed data",
        "Parametric contrastive learning",
        "Moving window regression: a novel approach to ordinal regression",
        "Indoor segmentation and support inference from rgbd images",
        "Momentum contrast for unsupervised visual representation learning",
        "Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries",
        "Improving contrastive learning on imbalanced data via open-world sampling",
        "Decoupling representation and classifier for long-tailed recognition",
        "Supervised contrastive learning",
        "Targeted supervised contrastive learning for long-tailed recognition",
        "Focal loss for dense object detection",
        "Jensen’s inequality",
        "Agedb: the first manually collected, in-the-wild age database",
        "Representation learning with contrastive predictive coding",
        "Balanced mse for imbalanced visual regression",
        "Deep expectation of real and apparent age from a single image without facial landmarks",
        "Manifold mixup: Better representations by interpolating hidden states",
        "Posterior re-calibration for imbalanced datasets",
        "Ranksim: Ranking similarity regularization for deep imbalanced regression",
        "Density-based weighting for imbalanced regression",
        "Contrastive learning based hybrid networks for long-tailed image classification",
        "Long-tailed recognition by routing diverse distribution-aware experts",
        "Contrastive regression for domain adaptation on gaze estimation",
        "Variational imbalanced regression: Fair uncertainty quantification via probabilistic smoothing",
        "Delving into deep imbalanced regression",
        "Feature transfer learning for face recognition with under-represented data",
        "Rank-n-contrast: Learning continuous representations for regression",
        "mixup: Beyond empirical risk minimization",
        "Borderline-smote: a new over-sampling method in imbalanced data sets learning",
        "Model-Aware K-center",
        "Exploring balanced feature spaces for representation learning",
        "Improved baselines with momentum contrastive learning"
      ],
      "meta_data": {
        "arxiv_id": "2309.06651v4",
        "authors": [
          "Mahsa Keramati",
          "Lili Meng",
          "R. David Evans"
        ],
        "published_date": "2023-09-13T00:30:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces ConR, a simple yet effective contrastive regularization term for Deep Imbalanced Regression (DIR). ConR translates both local and global relationships of continuous labels into feature space, preventing minority‐label features from collapsing onto majority ones. It defines dynamic anchor selection, label-aware positive/negative pairing, and density- & similarity-weighted pushing, and is orthogonal to existing imbalance remedies. Extensive experiments on four large-scale benchmarks show consistent, state-of-the-art gains across age, depth, and gaze estimation tasks.",
        "methodology": "ConR modifies supervised InfoNCE for regression. Two stochastic augmentations per sample are encoded; pairs with similar labels (within threshold ω) are positives, while pairs with dissimilar labels but similar predictions are negatives. Samples that possess at least one negative become anchors. The loss pulls positives together and pushes negatives apart with a weight S_{j,q}=η_j/Sim(y_j,y_q), where η_j is inverse label density (stronger for minorities). Similarity can be any task-specific metric (e.g., inverse MAE, RMSE). The regularizer L_ConR is summed with the base regression loss (MAE or RMSE): L = α L_R + β L_ConR. The approach scales to multi-dimensional labels and is independent of model architecture.",
        "experimental_setup": "Benchmarks: (1) AgeDB-DIR (12k train, 2.1k val/test) and (2) IMDB-WIKI-DIR (191k train, 11k val/test) for facial age estimation; (3) NYUD2-DIR (50k train, 654 test images, millions of depth pixels) for single-image depth estimation; (4) newly curated MPIIGaze-DIR (45k train, leave-one-subject-out) for 2-D gaze estimation. Data are strongly long-tailed; test sets are balanced. Metrics: MAE & Geometric Mean for age; RMSE & δ1 for depth; mean angular error for gaze. Baselines include LDS, FDS, RankSim, Balanced MSE, RRT, Focal-R, MoCo v1/v2, and vanilla models. Models: ResNet-50 backbones (age, depth), ResNet-50 encoder-decoder (depth), LeNet (gaze). Training uses Adam, standard augmentations, 90 epochs. Few/median/many shot analyses quantify minority performance. ConR consistently improves every baseline (e.g., AgeDB-DIR MAE 7.42→6.81; NYUD2-DIR RMSE 1.319→1.265).",
        "limitations": "1) Requires a task-specific label similarity metric and threshold ω; performance is sensitive to tuning. 2) Uses empirical label densities for weighting, which may be inaccurate when data do not reflect true distributions. 3) Adds extra computation (pair selection and contrastive loss), though lighter than some methods. 4) Assumes availability of meaningful augmentations to create positive pairs. 5) Effectiveness not validated on extremely high-dimensional or sequential regression tasks, nor on cases with noisy labels or severe domain shift.",
        "future_research_directions": "• Automatic or learned determination of similarity thresholds, label metrics, and weights to remove manual tuning.\n• Extension to self-supervised or semi-supervised settings where labels are scarce or noisy.\n• Investigating alternative weighting schemes that rely on estimated true label densities or uncertainty.\n• Scaling and optimizing ConR for very high-dimensional regression outputs (e.g., 3-D occupancy grids, medical volumes).\n• Combining ConR with generative resampling/augmentation or distribution calibration techniques.\n• Applying ConR to temporal and sequential regression problems (e.g., forecasting) and multimodal data.\n• Formal analysis of convergence properties and robustness under label noise or adversarial imbalance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Interventional Contrastive Learning with Meta Semantic Regularizer",
      "full_text": "Interventional Contrastive Learning with Meta Semantic Regularizer Wenwen Qiang* 1 2 3 Jiangmeng Li * 1 2 3 Changwen Zheng 1 3 Bing Su 4 5 Hui Xiong 6 7 Abstract Contrastive learning (CL)-based self-supervised learning models learn visual representations in a pairwise manner. Although the prevailing CL model has achieved great progress, in this pa- per, we uncover an ever-overlooked phenomenon: When the CL model is trained with full images, the performance tested in full images is better than that in foreground areas; when the CL model is trained with foreground areas, the performance tested in full images is worse than that in fore- ground areas. This observation reveals that back- grounds in images may interfere with the model learning semantic information and their inﬂuence has not been fully eliminated. To tackle this is- sue, we build a Structural Causal Model (SCM) to model the background as a confounder. We propose a backdoor adjustment-based regulariza- tion method, namely Interventional Contrastive Learning with Meta Semantic Regularizer (ICL- MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can be incorporated into any existing CL methods to alleviate back- ground distractions from representation learning. Theoretically, we prove that ICL-MSR achieves a tighter error bound. Empirically, our experiments on multiple benchmark datasets demonstrate that ICL-MSR is able to improve the performances of different state-of-the-art CL methods. *Equal contribution 1Science & Technology on Integrated Infor- mation System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China 2University of Chinese Academy of Sciences, Beijing, China 3Southern Marine Science and Engi- neering Guangdong Laboratory (Guangzhou), Guangdong, China. 4Gaoling School of Artiﬁcial Intelligence, Renmin University of China, Beijing, China 5Beijing Key Laboratory of Big Data Man- agement and Analysis Methods, Beijing, China 6Thrust of Artiﬁ- cial Intelligence, The Hong Kong University of Science and Tech- nology (Guangzhou), Guangzhou, China 7Department of Com- puter Science & Engineering, The Hong Kong University of Sci- ence and Technology, Hong Kong SAR, China. Correspondence to: Bing Su <subingats@gmail.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1. Introduction Learning robust and generic representations without human annotation is a long-standing and important topic in machine learning. Contrastive learning (CL)-based self-supervised learning, an innovative unsupervised representation learn- ing (SSL) method, has recently demonstrated superiority in computer vision tasks such as classiﬁcation (Chen et al., 2020a), object identiﬁcation (Grill et al., 2020), and trans- fer learning (Zbontar et al., 2021). The success of CL is partly due to its instance-based learning paradigm, e.g., CL assumes that each sample in the training dataset as a dis- tinct class. This paradigm can be applied to any type of data to capture common semantic information applicable to different tasks. In general, for a sample X in a mini-batch of training data, two augmented samples X1 and X2 are generated by per- forming random augmentation transformations T to X, i.e.,{ X1,X2} = T(X). Then, using one of two augmented samples as the anchor, most existing CL frameworks treat the remaining augmented sample as a positive sample and the augmented samples generated by the other samples in the mini-batch as negative samples. The contrastive loss (Chen et al., 2020a) is used to train the feature extractor. According to the instance-based learning paradigm, mini- mizing contrastive loss entails pulling the positive sample closer to the anchor and pushing the negative samples fur- ther away from the anchor in the learnt feature space (Wang & Isola, 2020). Also, the contrastive loss can be considered as a way to assess the mutual information between the pos- itive sample and the anchor from an information theoretic standpoint (Oord et al., 2018). The high similarity or mutual information between the positive sample and the anchor should be due to shared semantic or foreground-related in- formation. Observations from several toy experiments, on the other hand, contradict this. To be more speciﬁc, we run the toy experiments on the COCO dataset (Lin et al., 2014) with four different experi- mental settings: 1) training and testing the CL model on full images; 2) training and testing the CL model on full images and foreground images, respectively; 3) training and testing the CL model on foreground images; and 4) training and testing the CL model on foreground images and full images, respectively. SimCLR (Chen et al., 2020a) and BYOL (Grill arXiv:2206.14702v1  [cs.CV]  29 Jun 2022Interventional Contrastive Learning with Meta Semantic Regularizer Figure 1.The experimental results for two CL models. ”1” repre- sents training and testing on full images, ”2” represents training on full images and testing on foreground images, ”3” represents train- ing and testing on foreground images, and ”4” represents training on foreground images and testing on full images. et al., 2020), two CL models, were chosen as baselines. Fig- ure 1 shows an often-overlooked characteristic in the current CL model: Comparing the results produced under settings 1) and 2), where the model is trained on full images, the per- formance evaluated on full images is clearly superior than the performance tested on foreground images. In addition, when comparing the results obtained under conﬁgurations 1) and 4), the model trained with full images outperforms the model trained with foreground images when both are tested on full images. That is, when the backdrop is removed from the full image during training or testing, the performance of the two CL models suffers. This discovery suggests that background-related information can inﬂuence the CL models’ learning process. However, comparing the results obtained under settings 3) and 4), we discover that when the model is trained with foreground images, the performance tested in foreground images is considerably better than the performance tested in full images. In addition, when all vari- ables are considered, we ﬁnd that training and testing with only foreground images produce the best results. We can conﬁdently conclude that background-related information degrades the performance of the CL models based on this. As we can see, the two conclusions are mutually exclusive. A plausible explanation is that a feature extractor trained on full images extracts background-dependent semantic fea- tures. During the test phase, because the full image con- tains both foreground and background parts, besides the foreground parts, the background part also plays a certain role in promoting the classiﬁcation. CL, on the other hand, strives to be adaptable to a variety of downstream tasks, such as object detection, object segmentation, and so on. Only foreground-related semantic information can ensure the ro- bustness of the learned features to various tasks. This is in accordance with the second observation. For this purpose, we develop a Structural Causal Model (SCM) to describe the causal relationships between semantic information, pos- itive sample, and anchor in this paper. We can represent the background as a confounder based on this, which ﬁts with the explanation given above. Then, to execute causal inter- vention towards the proposed SCM, we present a backdoor adjustment-based regularization approach called Interven- tional Contrastive Learning with Meta Semantic Regularizer (ICL-MSR). To eliminate background distractions from rep- resentation learning, ICL-MSR can be simply implemented into most existing CL approaches. We show that ICL-MSR achieves a tighter error bound than CL methods that merely minimize the contrastive loss. Our experiments on multiple benchmark datasets show that ICL-MSR can improve the performance of the state-of-the-art CL-based self-supervised learning approaches. The following is a list of our major contributions: • We discover a paradox: under different setting, back- ground information can both improve and prevent per- formance of the learned feature representations from improving. • To capture the causal links between semantic infor- mation, positive sample, and anchor, we establish a Structural Causal Model (SCM). We can simply de- duce that the background is effectively a confounder that produces misleading correlations between the pos- itive sample and the anchor based on this. • We propose a new method called Interventional Con- trastive Learning with Meta Semantic Regularizer (ICL-MSR) by implementing backdoor adjustments to the planned SCM. • We provide theoretical guarantee on the error bound and empirical evaluations to demonstrate that ICL- MSR can improve the performances of different state- of-the-art CL methods. 2. Related Works Self-supervised learning, such as contrastive learning (CL), aims to learn a generalized feature extractor that can be well applied to downstream tasks. The objective of contrastive learning is mainly based on the InfoNCE loss. It is ﬁrst proposed in (Oord et al., 2018) and can be seen as a lower bound of the mutual information between the feature and the context. SimCLR (Chen et al., 2020a;b; Sordoni et al., 2021; Wen & Li, 2021) extends the InfoNCE loss to maximize the similarity between two different data augmentations. To bet- ter prompt the performance of contrastive learning, InfoMin (Tian et al., 2020b) proposes a set of stronger augmentations that reduce the mutual information between views while keeping task-relevant information intact. AlignUniform (Wang & Isola, 2020) relates the contrastive loss to two crit- ical properties, including alignment and uniformity, to formInterventional Contrastive Learning with Meta Semantic Regularizer a new objective. The CL model has shown its superiority in many vision tasks. However, there are still some challenges worth mentioning. Firstly, CL is sensitive to batch size. To solve this problem, MoCo (He et al., 2020; Chen et al., 2020c; 2021b) increases the number of negative examples by using a memory bank. Secondly, some negative samples may contain similar se- mantic information as the positive sample. To tackle this issue, SwA V (Caron et al., 2020) and PCL (Li et al., 2020) learn good-quality negative samples by introducing clus- tering methods in the training process, thereby reducing the number of negative samples. BYOL (Grill et al., 2020) proposes learning feature representation without negative samples. However, this also brings a new challenge: degen- erate solutions. So, BYOL proposes learning feature repre- sentations with a structurally asymmetric feature extractor and adopting the moving average as an optimization method for model parameters. Then, DirectPred (Tian et al., 2021) provides theoretical analysis to verify the effectiveness of BYOL. At the same time, a series of effective works such as SimSiam (Chen & He, 2021), Barlow Twins (Zbontar et al., 2021), W-MSE (Ermolov et al., 2021), and SSL-HSIC (Sordoni et al., 2021) are proposed to avoid degenerate so- lutions. Among them, W-MSE and Barlow Twin do not require asymmetric networks and are conceptually simpler. In addition to the improvement of the model, the contrastive learning theory is also attracting increasing attention. Some works (Nozawa & Sato, 2021; Arora et al., 2019) provide a bound on the CL. RELIC (Mitrovic et al., 2021) gives a causal explanation for the objective of CL. The goal of this paper is to explore the impact of back- ground information on the representation learning process in contrastive learning. Our proposed ICL-MSR can be incorporated into most existing CL methods to alleviate background distractions. Also, there are three differences between ICL-MSR and Causal3DIdent (V on K¨ugelgen et al., 2021). The ﬁrst is that ICL-MSR is motivated by causal in- tervention, while Causal3DIdent is motivated by counterfac- tual. The second is that ICL-MSR is mainly concerned with the objective function of contrastive learning and regards the background-dependent semantic features as confounding factors, while Causal3DIdent focuses on data augmentation and regards it as counterfactual under soft style intervention. The third is that ICL-MSR implements backdoor adjustment, and Causal3DIdent can be a part of ICL-MSR. 3. Problem Formulations 3.1. Contrastive Learning In this paper, we mainly focus on CL-based representation learning approaches. The primary goal of the CL methods is to learn a generic feature extractor f, which projects the Figure 2.The proposed SCM between semantic information Xs, positive sample X3−j i , and anchor (or label) Y(X3−j i ). sample from the original input space to a latent space for extracting intrinsic features. Formally, we ﬁrst randomly sample a minibatch of training data and denote it as Xtr = {Xi}N i=1, where Xi represents the i-th sample, and N represents the number of samples in the minibatch. We perform stochastic data augmentation (e.g., random crop) to transform each sample Xi into two augmented views X1 i and X2 i. Since there are N samples in Xtr, we ﬁnally obtain 2N augmented samples denoted as Xaug tr = { X1 i,X2 i }N i=1. Then, we feed all training samples into the feature extractor f to get their feature representa- tions, i.e., Zj i = f(Xj i),i ∈{1,...,N },j ∈{1,2}. The general objective of CL is formulated as: Lct = N∑ i=1 2∑ j=1 −log exp ( sim(Zj i,Z3−j i ) τ ) N∑ k=1, 2∑ l=1,l̸=j exp ( sim(Zj i,Zl k) τ ) (1) where τ represents the temperature hyper-parameter and sim (u,v) = uTv / ∥u∥∥v∥denotes the dot product be- tween l2 normalized u and v (i.e., the cosine similarity). For a sample Xj i randomly selected from the dataset Xaug tr , CL regards it as the anchor, the pair{Xj i,X3−j i }as positive, the pairs {Xj i,Xl k}k=N,l=2 k=1,k̸=i,l=1 as negatives. The loss Lct in objective (1) is computed across all positive pairs. 3.2. Structural Causal Model Minimizing the objective (1) is to make the sample X3−j i in the positive pair close to the anchor and the sample Xl k in the negative pairs far away from the anchor. From this perspective, the anchor can be seen as the label, then mini- mizing the objective (1) equals to predict X3−j i to the label Xj i. Then, the SCM implicated in CL can be formalized as Figure 2. The nodes in SCM represent the abstract data variables and the directed edges represent the (functional) causality, e.g., X3−j i →Y(X3−j i ) represents that X3−j i is the cause and Y(X3−j i ) is the effect. In the following, we will describe the proposed SCM and the rationale behind its construction in detail at a high level. Please refer to Section 4 for the detailed functional implementations.Interventional Contrastive Learning with Meta Semantic Regularizer Figure 3.The framework of the proposed ICL-MSR. X3−j i →Y(X3−j i ). Y(X3−j i ) denotes the corresponding label of X3−j i . As mentioned above, the label equals to the anchor. Thus, we have Y(X3−j i ) =Xj i. This link assumes that X3−j i should be similar with the anchor Xj i. X3−j i ←Xs →Y(X3−j i ). Xs represents the semantic information and can be regarded as the convolution kernel of the feature extractor f. Therefore, the link X3−j i ←Xs and Xs →Y(X3−j i ) assume that the feature representations of X3−j i and Y(X3−j i ) in the latent space is extracted by using f, and each feature representation channel corresponds to a semantic information. An ideal contrastive learning model should capture the true causality between X3−j i and Y(X3−j i ) and can generalize to unseen samples well. For example, we expect the label prediction of Y(X3−j i ) to be caused by the foreground fea- ture, not the background information. However, from the proposed SCM, the increased likelihood of Y(X3−j i ) given X3−j i is not only due to X3−j i →Y(X3−j i ), but also the spurious correlation via Xs →X3−j i →Y(X3−j i ), e.g., the background of X3−j i generates the background feature, which provides useful context for predicting the anchor, this corresponds to our ﬁrst observation in Figure 1: when the model is trained on full images, the performance evalu- ated on full images is clearly superior to the performance tested on foreground images. Therefore, to pursue the true causality between X3−j i and Y(X3−j i ), we need to use the causal intervention P(Y(X3−j i )|do(X3−j i )) instead of the P(Y(X3−j i )|X3−j i ) to measure the causal relation. 3.3. Causal Intervention via Backdoor Adjustment Before we introduce the backdoor adjustment, we ﬁrst give an intuition of why the feature extractor f trained by previous contrastive learning models extracts background- dependent semantic features. The fact that the size of the positive sample is too tiny, i.e. only one, could explain this problem. As shown in objective (1), randomly given an anchor, there is only one positive sample, but 2N −2 negative samples. Also, there are some samples (false negative samples) in negative pairs that are with the same foreground as the positive sample. Due to the randomness of the sampling process, the background simi- larity between false negative samples and anchor is lower than the foreground similarity between them. However, the anchor and the sample in the positive pair are generated by the same original image, so that the foreground and back- ground between the two positive samples are similar. When minimizing the objective (1), the shared foreground and background between the positive pair will likely together drag the positive sample towards the anchor if there are too few positive samples. Observation is also carried out on false negative samples that have semantically equivalent information to the anchor (Tian et al., 2020a). As a result, shifting the false negative samples further from the anchor will mainly focus on lowering the foreground similarity. In other word, this in turn may promote ”drag” operation to pay greater attention to the backdrop to some extent. As a result, the role of the background is enhanced. This can lead to the previous contrastive learning models extracting background-dependent semantic features. Above, we have shown that the feature extractorf can ex- tract background-dependent semantic features and analyze how these semantic features affect the training process of the contrastive learning model. Below, we will propose to use the backdoor adjustment (Glymour et al., 2016) to eliminate the interference of the background-dependent semantics to achieve P(Y(X3−j i )|do(X3−j i )). Speciﬁcally, the backdoor adjustment assumes that we can observe and stratify the confounder. In the proposed SCM, the confounder is contained in Xs, we can layer it into different semantic features, e.g., Xs = {Zi s}i=n i=1 , where Zi sInterventional Contrastive Learning with Meta Semantic Regularizer represents a stratiﬁcation of semantic features. Formally, the backdoor adjustment for the proposed SCM is presented as: P(Y(X3−j i )|do(X3−j i )) = n∑ i=1 P(Y(X3−j i )|X3−j i ,Zi s)P(Zi s) (2) where P(Y(X3−j i )|do(X3−j i )) represents the true causality between Y(X3−j i ) and X3−j i . See appendix B for detailed derivation of equation (2). 4. Methodology 4.1. Meta Semantic Regularizer In this subsection, we present the implementation of the backdoor adjustment during the training phase. As shown in equation (2), ﬁrstly, we need to give detailed functional implementations for Zi s. Without loss of generality, we denote the dimension of the output Zj i of the feature extractor f as w×h×c, where w is the width, his the height, and cis the number of feature channels. To be more speciﬁc, we denote Zj i as Zj i = [Zj i,1,...,Z j i,c], where Zj i,r ∈Rw×hrepresents a feature map of Zj i, r ∈{1,...,c }. Note that for any pre-trained CNN- based feature extractor, each channel corresponds to a kind of semantic information or visual concepts (Zeiler & Fergus, 2014; Zhou et al., 2016). However, it is difﬁcult to encode one visual concept by a single channel. So, this motivates us to ﬁnd a weight vector for each semantic information. Our idea is that each semantic information corresponds to one subset of channels. So the weights for this related subset of channels should be large, and the weights for channels that are outside of the subset should be small. Given a weight vector at = [a1,t,...,a c,t]T, we represent the functional implementations of the Zt s as Zt s = at, and P(Zt s) = 1/n. Then, we represent the functional imple- mentations of the P(Y(X3−j i )|X3−j i ,Zt s) as P(Y(X3−j i )|X3−j i ,Zt s) = exp ( sim(Zj i,at⊙Z3−j i ) τ ) exp ( sim(Zj i,at⊙Z3−j i ) τ ) + N∑ k=1, k̸=i 2∑ l=1, l̸=j exp ( sim(Zj i,Zl k) τ ), (3) where at ⊙Z3−j i = [a1,t ·Z3−j i,1 ,...,a c,t ·Z3−j i,c ]. As a re- sult, the overall backdoor adjustment is presented as: P(Y(X3−j i )|do(X3−j i )) = n∑ t=1 exp ( sim(Zj i,at⊙Z3−j i ) τ ) ×1 n exp ( sim(Zj i,at⊙Z3−j i ) τ ) + N∑ k=1, k̸=i 2∑ l=1, l̸=j exp ( sim(Zj i,Zl k) τ ). (4) The proposed meta semantic regularizer can be thought of as a learnable module fmsr, which is to generate the se- mantically relevant weight matrix As and is implemented by convolutional neural network, where As = [a1,...,a n]. Speciﬁcally, for a input sample X, we ﬁrst obtain two aug- mentated samples { X1,X2} by feeding X to a stochastic data augmentation module. Then we feed X to the module fmsr to obtain the weight matrixAs, and the two augmented samples { X1,X2} share only one weight matrix As. 4.2. Model Objectives To this end, we introduce the objective of the proposed interventional contrastive learning with meta semantic regu- larizer (ICL-MSR). The whole learning framework of ICL- MSR is shown in Figure 3, and the training process is shown in Appendix. ICL-MSR consists of three modules, includ- ing the feature extractor f, the meta semantic regularizer fmsr, and the projective head fph. The meta semantic reg- ularizer is trained alongside the feature extractor, with two stages per epoch. In the ﬁrst stage, f and fph are learned using the two augmented training dataset Xaug tr and the se- mantically relevant weight matrix As. In the second stage, fmsr is updated by computing its gradients with respect to the contrastive loss. We train both modules in an iterative manner until convergence. In the ﬁrst stage of each epoch, the parameters of f and fph are updated by minimizing the objective Lto, which can be presented as: min f,fph Lto = Lct + λLmsr, (5) where Lct is shown in the objective (1), λ is the hyper- parameter, and Lmsr is presented as: Lmsr = N∑ i=1 2∑ j=1 −log P(Y(X3−j i )|do(X3−j i )). (6) In the second stage of each epoch, to learn the parameters of fmsr, we propose a meta learning-based training mecha- nism. That is, fmsr is updated by encouraging the weight matrix to be chosen such that, if f and fph are trained using the weight matrix, the performance of the primary con- trastive learning would be maximized on this same training data. Speciﬁcally, We ﬁrst update f and fph once with the learning rate αby the follows: f1 = f −α∇fLto, f1 ph = fph −α∇fphLto, (7) These two updates can be seen as to learn a good f and a good fph. After this step, f1 and f1 ph can be seen as the function of fmsr, because ∇fLto and ∇fphLto is related to fmsr. Then, we update fmsr by minimizing the following: min fmsr Lct ( f1,f1 ph ) + γLuni (8)Interventional Contrastive Learning with Meta Semantic Regularizer Table 1.Classiﬁcation accuracy (top 1) of a linear classiﬁer and a 5-nearest neighbors classiﬁer for methods and datasets with the ResNet-18 feature extractor. Methods CIFAR-10 CIFAR-100 STL-10 Tiny ImageNet linear 5-nn linear 5-nn linear 5-nn linear 5-nn SimCLR (Chen et al., 2020a) 91.80 88.42 66.83 56.56 90.51 85.68 48.84 32.86 BYOL (Grill et al., 2020) 91.73 89.45 66.60 56.82 91.99 88.64 51.00 36.24 W-MSE (Ermolov et al., 2021) 91.99 89.87 67.64 56.45 91.75 88.59 49.22 35.44 ReSSL (Zheng et al., 2021) 90.20 88.26 63.79 53.72 88.25 86.33 46.60 32.39 LMCL (Chen et al., 2021a) 91.91 88.52 67.01 56.86 90.87 85.91 49.24 32.88 SSL-HSIC (Li et al., 2021) 91.95 89.99 67.23 57.01 92.09 88.91 51.37 36.03 RELIC (Mitrovic et al., 2021) 91.96 89.35 67.24 56.88 91.15 86.21 49.17 32.97 ICL-MSR(SimCLR + MSR) 92.34 89.47 67.59 57.64 92.03 86.94 50.12 32.88 ICL-MSR(BYOL + MSR) 92.26 90.12 66.97 57.97 93.22 89.36 52.54 37.54 ICL-MSR(LMCL + MSR) 92.45 89.38 67.99 57.71 91.56 87.73 52.61 32.35 ICL-MSR(ReSSL + MSR) 91.77 89.06 65.12 55.07 89.91 88.06 47.17 33.03 where γis a hyper-parameter, Lct(f1,f1 ph) represents that the loss Lct is calculated based on the parameters f1 and f1 ph, and Luni is the uniformity loss that aims to constrain the distribution of elements in As to approximate a uniform distribution, so that the resulting visual semantics can be as inconsistent as possible. Based on the Gaussian potential kernel (Borodachov et al., 2019; Cohn & Kumar, 2007; Wang & Isola, 2020), the Luni can be represented as : Luni = log ∑ ai,aj∈As Gt(ai,aj,t) (9) where Gt(ai,aj,t) ∆ = exp (2t·aiTaj −2t), and t is a ﬁxed hyper-parameter. Note that the average pairwise Gaus- sian potential is nicely tied with the uniform distribution, for more details please refer to (Wang & Isola, 2020). A problem is that why minimizing objective (8) can make fmsr to learn semantic information. Note that only the semantic information shared between the positive pair can prompt X3−j i and Y(X3−j i ) to be similar, thus minimizing the contrastive loss. Based on the SCM, we can see that the shared semantic information contains both background- related and foreground-related information. The idea behind objective (8) is that minimizing the contrastive loss so that fmsr learns the shared semantic information. Also, this step can be seen as promoting the learning of Lct once again on the basis of Lct, which is similar to learning to learn. This is also the reason why we call it a meta semantic regularizer. 5. Error Bound for Downstream Classiﬁcation The classiﬁcation task is often used to evaluate the per- formance of most CL methods. Therefore, we present the generalization error bound (GEB) of the proposed ICL-MSR based on the classiﬁcation task which trains a softmax classiﬁer by minimizing the traditional cross en- tropy loss (Zhang & Sabuncu, 2018), e.g., LSM(f; T) = infWLCE(W ·f; T), where W is the linear classiﬁer, T is the label. For a feature embedding f(X), the generalization error is deﬁned by LT SM(f) =EX[LSM(f; T)]. Then we investigate how such a generalization error LT SM(f) is far from the contrastive learning objective Lct. Theorem 5.1. Let f∗∈arg minfLcl + λLmsr. Then with probability at least 1 −δ, we have that ⏐⏐LT SM (f∗) −Lcl(f∗) ⏐⏐≤O ( Q1RH (λ) M + √ Q2 M ) (10) where M is the total number of training samples, N is the size the mini-batch, and Q1 = √ 1 + 1/N, Q2 = log (1/δ) ·log2 (M), RH (λ) is the rademacher complexity. Also, RH (λ) is monotonically decreasing w.r.t.λ. The detailed proof can be found in Appendix C. As shown in equation (16), we can obtain that the error bound gradually decreases with the increase of the training sample size M. Note that this observation is consistent with the traditional supervised learning method. Also, we can see that the mini- batch size N in the error term √ Q2/N is negligible for the large sample size N. In this case, the relative large size N will effectively reduce the ﬁrst error term Q1RH (λ), and thereby tightening the error bound. Finally, when we enlarge the regularization parameter λ, the Rademacher complexity RH will also be decreased, and thus further reducing the error bound and improving the generalizability of the contrastive learning algorithm.Interventional Contrastive Learning with Meta Semantic Regularizer Figure 4.The experimental results for two kinds of ICL-MSR mod- els. ”1” represents training and testing on full images, ”2” repre- sents training on full images and testing on foreground images. 6. Experiments 6.1. Benchmark Datasets The following datasets are utilized to evaluate the perfor- mance of the proposed ICL-MSR: CIFAR-10 and CIFAR- 100 (Krizhevsky et al., 2009) are two small-scale datasets, which consist of images of size 32 ×32 from 10 classes and 100 classes, respectively. STL-10 (Coates et al., 2011) is derived from ImageNet and consists of more than 100K training samples with 96 ×96 resolution. Tiny ImageNet (Le & Yang, 2015) can be seen as a simpliﬁed version of ImageNet, which contains 100K training samples and 10K testing samples from 200 classes and an image scale of 64 ×64. ImageNet-100 (Tian et al., 2020a) is a randomly sampled subset of ImageNet with a total of 100 classes. ImageNet (Deng et al., 2009) is a well-known large-scale dataset. It consists of about 1.3M training images and 50K test images with over 1000 classes. 6.2. Implementation Details The experiments we carried out are to evaluate the effec- tiveness of the proposed meta semantic regularizer. So, we ignore the inﬂuence of secondary factors, e.g., the neural network architecture. To this end, for CIFAR-10, CIFAR- 100, STL-10, and Tiny ImageNet, we use the same feature extractor for all compared methods, e.g., ResNet-18. For Im- ageNet and ImageNet-100, we use ResNet-50 as the feature extractor. For all datasets, the obtained feature represen- tations are L2 normalized, unless otherwise speciﬁed. We set t = 2and n = 6. For SimCLR, we set τ = 0.5. For BYOL, we use the exponential moving average with cosine increasing, starting from 0.99. For all the compared meth- ods, the Adam optimizer (Kingma & Ba, 2014) is used for the datasets with small and medium sizes. Also, for CIFAR- 10 and CIFAR-100, the number of epochs is set to 1,000 and the learning rate is set to 3 ×10−3, for Tiny ImageNet, ImageNet-100, the number of epochs is set to 1,000 and the learning rate is set to 2 ×10−3, for STL-10, the number of epochs is set to 2,000 and the learning rate is set to2×10−3. Also, for all datasets, we use learning rate warm-up for the ﬁrst 500 iterations of the optimizer, and a 0.2 learning rate drop 50 and 25 epochs before the end. The dimension of output of the projection head fphis set to 1024. The weight decay is set to 10−6. The output dimension of the f is set to 64 for CIFAR-10 and CIFAR-100, 128 for STL-10 and Tiny ImageNet. Finally, for ImageNet, we set the implementation and hyperparameters to be the same as (Chen et al., 2020b; Chuang et al., 2020). As for image transformation details, on CIFAR-10 and CIFAR-100, and Tiny ImageNet, the extracted crops are varied with a random size from 0.2 to 1.0 and a random as- pect ratio from 3:4 to 4:3 of the input image. The horizontal mirroring is implemented with a probability of 0.5. The probability of the color jittering with conﬁguration (0.4, 0.4, 0.4, 0.1) is set to 0.8. The probability of grayscaling is set to 0.1. For ImageNet and ImageNet-100, the crop size is set to be varied from 0.08 to 1.0, we use stronger jittering (0.8; 0.8; 0.8; 0.2) and set the probability of grayscaling to 0.2 and the probability of Gaussian blurring to 0.5. As for evaluation protocol, we ﬁrst freeze the f after training phase and then train a supervised linear classiﬁer on top of it. The linear classiﬁer is a fully-connected layer followed by soft- max. We set the epochs for training the linear classiﬁer to 500 with the Adam optimizer. We also report the accuracy of a k-nearest neighbors classiﬁer (k-nn), and set k= 5. For toy experiments, we run four kinds of methods on COCO dataset (Lin et al., 2014), including SimCLR, BYOL, SimCLR-MSR, and BYOL-MSR. During train- ing, we eliminated the samples containing multiple tar- gets in the coco dataset to ensure that the samples in the training set are only one target. Meanwhile, we observe that some classes in the coco dataset contain only a few or no samples. Therefore, we only selected 30 of these categories, including: {’airplane’:0, ’banana’:1, ’bear’:2, ’bed’:3, ’bench’:4, ’bird’:5, ’boat’:6, ’broccoli’:7, ’bus’:8, ’cat’:9, ’clock’:10, ’cow’:11, ’dog’:12, ’elephant’:13, ’ﬁre hydrant’:14, ’giraffe’:15, ’horse’:16, ’motorcycle’:17, ’per- son’:18, ’pizza’:19, ’scissors’:20, ’sink’:21, ’stop sign’:22, ’teddy bear’:23, ’toilet’:24, ’trafﬁc light’:25, ’train’:26, ’truck’:27, ’vase’:28, ’zebra’:29}. The CoCo dataset pro- vides ground-truth bounding boxes of objects in images, so we take the area inside the bounding box of each image as the foreground, and the area outside the bounding box of each image as the background. 6.3. Evaluation Results Figure 4 shows the experimental results of two kinds of proposed ICL-MSR including SimCLR+MSR and BYOL+MSR. When training ICL-MSR on the full images,Interventional Contrastive Learning with Meta Semantic Regularizer Figure 5.Impacts of the hyperparameters (a) λ, (b) γ, and (c) the number of semantic weight vectors n. Table 2.Classiﬁcation accuracy (top 1 and top 5) of a linear classi- ﬁer for methods with the ResNet-50 feature extractor and negative sample size 4096 on ImageNet-100. Methods Top 1 Top 5 SimCLR (Chen et al., 2020a) 70.15 89.75 MoCo (He et al., 2020) 72.80 91.64 CMC (Tian et al., 2020a) 73.58 92.06 SwA V (Caron et al., 2020) 75.78 92.86 DCL (Chuang et al., 2020) 74.60 92.08 LMLC (Ermolov et al., 2021) 75.89 92.89 ICL-MSR (SimCLR + MSR) 72.08 91.81 ICL-MSR (CMC + MSR) 74.60 92.87 ICL-MSR (CMC + SwA V) 75.91 92.88 ICL-MSR (DCL + MSR) 75.68 93.17 ICL-MSR (LMLC + MSR) 76.45 93.88 we can observe that the testing results on full images are comparable with those on foreground images. Also, the testing results on full images have increased compared with those in Figure 1. This indicates that the proposed backdoor adjustment-based regularization method is effective and the background information is indeed a confounding factor that can interference with the learning process of the feature extractor. Table 1 shows the experimental results (linear and 5-nn) of the compared methods with a ResNet-18 feature ex- tractor on small and medium size datasets. The com- pared methods include SimCLR, BYOL, Barlow Twins, W-MSE, ReSSL, LMCL, SSL-HSIC, and RELIC. We in- corporate the Meta Semantic Regularizer into four mod- els, resulting in four kinds of ICL-MSR (SimCLR+MSR, BYOL+MSR, LMCL+MSR, ReSSL+MSR). The hyper- parameters λand γ are set to 1 and 1 for SimCLR+MSR, 0.1 and 1 for BYOL+MSR and LMCL+MSR, 0.01 and 1 for ReSSL+MSR, respectively. Table 2 shows the experi- mental results (Top 1 and Top 5) of the compared methods with a ResNet-50 feature extractor on ImageNet-100. The Table 3.Classiﬁcation accuracy (top 1 and top 5) of a linear classi- ﬁer for methods with the ResNet-50 feature extractor on ImageNet. Methods Top 1 Top 5 SimCLR (Chen et al., 2020a) 69.3 89.0 MoCo (He et al., 2020) 71.1 - CMC (Tian et al., 2020a) 66.2 87.0 CPC (Henaff, 2020) 63.8 85.3 InfoMin Aug. (Tian et al., 2020b) 73.0 91.1 SwA V (Caron et al., 2020) 75.3 - BYOL (Chuang et al., 2020) 74.3 91.6 RELIC (Ermolov et al., 2021) 74.8 92.2 SSL-HSIC (Li et al., 2021) 72.2 90.7 ICL-MSR (SimCLR + MSR) 70.9 90.5 ICL-MSR (SwA V + MSR) 75.5 - ICL-MSR (BYOL + MSR) 75.4 92.6 compared methods include SimCLR, MoCo, CMC, SwA V , DCL, and LMLC. We incorporate the Meta Semantic Regu- larizer into four models, resulting in four kinds of ICL-MSR (SimCLR+MSR, CMC+MSR, DCL+MSR, LMLC+MSR). The hyper-parameters λand γare set to 0.1 and 1 for Sim- CLR+MSR, 0.1 and 0.1 for CMC+MSR, and 1 and 1 for DCL+MSR and LMLC+MSR, respectively. Table 3 shows the experimental results (Top 1 and Top 5) of the compared methods a ResNet-50 feature extractor on ImageNet. The compared methods include SimCLR, MoCo, CMC, CPC, InfoMin Aug., SwA V , BYOL, SSL-HSIC, and RELIC. We incorporate the Meta Semantic Regularizer into two mod- els, resulting in two kinds of ICL-MSR (SimCLR+MSR, BYOL+MSR). The hyper-parameters λand γare set to 0.1 and 1, respectively. From the three Tables, we can observe that the performances of our proposed ICL-MSR are all better than those of the baseline, For Table 1, the best results always appear in BYOL+MSR and LMCL+MSR. For Table 2, the best re- sults are located in LMCL+MSR. For Table 3, LMCL+MSR outperforms all compared methods. This indicates the ef-Interventional Contrastive Learning with Meta Semantic Regularizer Table 4.Semi-supervised training with a fraction of ImageNet la- bels (1% and 10%). Classiﬁcation accuracy (top 1 and top 5) of a linear classiﬁer for methods with the ResNet-50 feature extractor. Methods 1% 10% Top 1 Top 5 Top 1 Top 5 SimCLR (Chen et al., 2020a) 48.3 75.5 65.6 87.8 BYOL (Chuang et al., 2020) 53.2 78.4 68.8 89.0 ICL-MSR (SimCLR + MSR)50.7 77.1 66.9 89.6 ICL-MSR (BYOL + MSR) 55.5 80.6 70.5 90.7 fectiveness of the proposed ICL-MSR. Note that RELIC is a causal-related method and focuses on the different aug- mentations. We can observe that, in Table 1, three of the four proposed methods outperform RELIC, and in Table 2, BYOL+MSR outperforms RELIC. This indicates that background information is indeed a confounding factor for learning a good feature extractor. We summarize two possi- ble reasons why the improvement is less than 1% in most cases. The ﬁrst is that ICL-MSR is more suitable for image data with larger resolutions. We ﬁnd that improvement of less than 1% mostly occurred on datasets with small reso- lutions, e.g., 32 ×32 for cifar-10 and cifar-100 datsets. An image with a smaller resolution already contains less back- ground information. The second is that ICL-MSR may be sensitive to the hyperparameter n. To reduce computational complexity, we set n= 6for all datasets. We evaluate the performance obtained when ﬁne-tuning the representation learned by ICL-MSR on a classiﬁcation task with a small subset of ImageNet’s training dataset. We follow the semi-supervised protocol of (Chen et al., 2020a; Chuang et al., 2020) and use the same ﬁxed splits of respectively 1% and 10% of ImageNet labeled training dataset. We report both top-1 and top-5 accuracies on the test dataset in Table 4. We set n= 14. We can observe that the proposed ICL-MSR outperforms the compared methods in all cases. This indicates that the proposed ICL-MSR is effective for the ﬁne-turning tasks. 6.4. Hyperparametric Analysis To understand the impacts of hyper-parameters, we se- lect SimCLR+MSR to conduct comparisons on CIFAR-10 dataset. Speciﬁcally, λcontrols the impact of the MSR, γ controls the impact of the uniformity loss, and nis the num- ber of the learned weight vectors. We ﬁrst set γ = 1,n = 6 and select λfrom the range of {10−3,10−2,..., 103}. The results are shown in Figure 5 (a). We observe that when λ= 1, ICL-MSR gets the best accuracy. This illustrates that the proposed MSR is effective. Then, we set λ= 1,n = 6 and select γfrom the range of {10−3,10−2,..., 103}. From Figure 5(b), we obtain that the best result corresponds to γ = 1. This indicates that constraining the distribution of the weight matrix to a uniform distribution is effective. Finally, we set λ = γ = 1 and select n from the range of {2,4,..., 12}. From Figure 5(c), we can obtain that a proper number of semantic weight vectors can prompt the performance of ICL-MSR. When γ = 103, the accuracy is the lowest. This indicates that releasing uniform constraint will discard the semantic information. 7. Conclusions In this paper, based on the toy experiments on the COCO dataset with four experimental settings, we ﬁnd two con- tradictory conclusions. Then, we build a Structural Causal Model (SCM) to give an explanation for this contradiction and propose to regard the background as a confounder. To tackle this problem, we propose a regularization method based on backdoor adjustment. Our method can be easily in- corporated into most existing CL methods. We demonstrate the effectiveness of the proposed method both theoretically and empirically. Acknowledgements The authors would like to thank the anonymous review- ers for their valuable comments. This work is sup- ported in part by National Natural Science Foundation of China No. 61976206 and No. 61832017, Key Special Project for Introduced Talents Team of South- ern Marine Science and Engineering Guangdong Labo- ratory (Guangzhou) No. GML2019ZD0603, National Key Research and Development Program of China No. 2019YFB1405100, Beijing Outstanding Young Scien- tist Program NO. BJJWZYJH012019100020098, Beijing Academy of Artiﬁcial Intelligence (BAAI), the Fundamen- tal Research Funds for the Central Universities, the Re- search Funds of Renmin University of China 21XNLG05, and Public Computing Cloud, Renmin University of China. This work is also supported in part by Intelligent Social Governance Platform, Major Innovation & Planning Inter- disciplinary Platform for the “Double-First Class” Initia- tive, Renmin University of China, and Public Policy and Decision-making Research Lab of Renmin University of China. Statement We ﬁnd that the positions of Xs and X3−j i in Figure 2 in the camera-ready version submitted to ICML is wrong. The Figure 2 in the blind-reviewed version is correct. Since the camera-ready deadline has passed, the 2022 Publications Chairs tell us it is impossible to update the camera ready version at this point and there is also no ofﬁcial errata. So, we release the correct version in arXiv. Also, we have carefully checked the whole paper.Interventional Contrastive Learning with Meta Semantic Regularizer References Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229, 2019. Borodachov, S. V ., Hardin, D. P., and Saff, E. B.Discrete energy on rectiﬁable sets. Springer, 2019. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. Unsupervised learning of visual fea- tures by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020. Chen, S., Niu, G., Gong, C., Li, J., Yang, J., and Sugiyama, M. Large-margin contrastive learning with distance po- larization regularizer. In International Conference on Machine Learning, pp. 1673–1683. PMLR, 2021a. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. Big self-supervised models are strong semi- supervised learners. arXiv preprint arXiv:2006.10029, 2020b. Chen, X. and He, K. Exploring simple siamese represen- tation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021. Chen, X., Fan, H., Girshick, R., and He, K. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020c. Chen, X., Xie, S., and He, K. An empirical study of train- ing self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021b. Chuang, C.-Y ., Robinson, J., Yen-Chen, L., Torralba, A., and Jegelka, S. Debiased contrastive learning. Advances in Neural Information Processing Systems, 2020. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011. Cohn, H. and Kumar, A. Universally optimal distribution of points on spheres. Journal of the American Mathematical Society, 20(1):99–148, 2007. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Ermolov, A., Siarohin, A., Sangineto, E., and Sebe, N. Whitening for self-supervised representation learning. In International Conference on Machine Learning, pp. 3015– 3024. PMLR, 2021. Glymour, M., Pearl, J., and Jewell, N. P. Causal inference in statistics: A primer. John Wiley & Sons, 2016. Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent- a new approach to self-supervised learning. Advances in Neural Information Processing Systems , 33:21271– 21284, 2020. He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729– 9738, 2020. Henaff, O. Data-efﬁcient image recognition with contrastive predictive coding. In International Conference on Ma- chine Learning, pp. 4182–4192. PMLR, 2020. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Le, Y . and Yang, X. Tiny imagenet visual recognition chal- lenge. CS 231N, 7(7):3, 2015. Li, J., Zhou, P., Xiong, C., and Hoi, S. C. Prototypical con- trastive learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020. Li, Y ., Pogodin, R., Sutherland, D. J., and Gretton, A. Self- supervised learning with kernel dependence maximiza- tion. arXiv preprint arXiv:2106.08320, 2021. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014. Mitrovic, J., McWilliams, B., Walker, J., Buesing, L., and Blundell, C. Representation learning via invariant causal mechanisms. In International Conference on Learning Representations (ICLR), 2021.Interventional Contrastive Learning with Meta Semantic Regularizer Nozawa, K. and Sato, I. Understanding negative samples in instance discriminative self-supervised representation learning. arXiv preprint arXiv:2102.06866, 2021. Oord, A. v. d., Li, Y ., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Sordoni, A., Dziri, N., Schulz, H., Gordon, G., Bachman, P., and Des Combes, R. T. Decomposed mutual information estimation for contrastive representation learning. In International Conference on Machine Learning, pp. 9859– 9869. PMLR, 2021. Tian, Y ., Krishnan, D., and Isola, P. Contrastive multiview coding. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part XI 16, pp. 776–794. Springer, 2020a. Tian, Y ., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P. What makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243, 2020b. Tian, Y ., Chen, X., and Ganguli, S. Understanding self- supervised learning dynamics without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021. V on K¨ugelgen, J., Sharma, Y ., Gresele, L., Brendel, W., Sch¨olkopf, B., Besserve, M., and Locatello, F. Self- supervised learning with data augmentations provably isolates content from style. Advances in Neural Informa- tion Processing Systems, 34, 2021. Wang, T. and Isola, P. Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020. Wen, Z. and Li, Y . Toward understanding the feature learn- ing process of self-supervised contrastive learning. arXiv preprint arXiv:2105.15134, 2021. Zbontar, J., Jing, L., Misra, I., LeCun, Y ., and Deny, S. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. Zeiler, M. D. and Fergus, R. Visualizing and understand- ing convolutional networks. In European conference on computer vision, pp. 818–833. Springer, 2014. Zhang, Z. and Sabuncu, M. R. Generalized cross entropy loss for training deep neural networks with noisy labels. In 32nd Conference on Neural Information Processing Systems (NeurIPS), 2018. Zheng, M., You, S., Wang, F., Qian, C., Zhang, C., Wang, X., and Xu, C. Ressl: Relational self-supervised learning with weak augmentation. Advances in Neural Information Processing Systems, 34, 2021. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. Learning deep features for discriminative localiza- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2921–2929, 2016.Appendix: Interventional Contrastive Learning with Meta Semantic Regularizer A. The Training Process Algorithm 1 ICL-MSR Input: #: N, minibatch size #: f, encoder function #: fmsr, meta semantic regularizer #: fph, projection head #: λ, γ, n, t, τ, hyperparameters #: α, β, learning rates 1: repeat 2: for t-th training iteration do 3: Iteratively sample minibatch Xtr = {Xi}N i=1. 4: # regularcontrastivetrainingstep 5: f ←f −α∇fLto 6: fph ←fph −α∇fphLto 7: end for 8: for t-th training iteration do 9: Iteratively sample minibatch Xtr = {Xi}N i=1. 10: # computefastweights 11: # retaincomputationalgraph 12: f1 ←f −α∇fLto 13: f1 ph ←fph −α∇fphLto 14: # metatrainingstepusingsecondderivative 15: fmsr ←fmsr −β∇fmsr [ Lct ( f1,f1 ph ) + γLuni ] 16: end for 17: until f, fph, and fmsr converge. B. Derivation of Equation 2 We ﬁrst give deﬁnitions to the path, the d-separation, and the backdoor criterion. From (Glymour et al., 2016), we can obtain that: Deﬁnition B.1. Path . A path consists of three components including the Chain Structure: A→B →Cor A←B ←C, the Bifurcate Structure: A←B →C, and the Collisions Structure: A→B ←C. Deﬁnition B.2. d-separation. A path pis blocked by a set of nodes Zif and only if: 1. pcontains a chain of nodes A →B →C or a fork A ←B ←C such that the middle node B is in Z (i.e., B is conditioned on), or 2. pcontains a collider A →B ←C such that the collision node B is not in Z, and no descendant of B is in Z. If Z blocks every path between two nodes X and Y, then X and Y are d-separated, conditional on Z, and thus are independent conditional on Z. Deﬁnition B.3. The Backdoor Criterion . Given on ordered pair of variables (X,Y ) in a directed acyclic graph G, a set of variables Zsatisﬁes the backdoor criterion relative to (X,Y ) if no node in Zis a descendant of X, and Zblocks everyInterventional Contrastive Learning with Meta Semantic Regularizer path between X and Y that contains an arrow into X. If a set of variables of Zsatisﬁes the backdoor criterion for X and Y, then the causal effect of X on Y is given by the formula: P(Y = y|do(X = x)) = ∑ z P(Y = y|X = x,Z = z)P(Z = z) (11) C. Proof for Theorem 5.1 First, we give a Lemma as follows: Lemma C.1. (Arora et al., 2019) Assume that f∗∈arg minfLcl + λLmsr. Then with probability at least 1 −δover the training data X = {X1,X2,...,X M}, for any f ∈H, LT SM (f∗) ≤Lcl(f∗) +O ( Q1RH (λ) M + √ Q2 M ) (12) where N is the size of negative pairs, Q1 = √ 1 + 1/M,Q2 = log (1/δ) ·log2 (M), the Rademacher Complexity is deﬁned as RH (λ) =Eσ∈{±1}3dN [ supf∈H(λ) ⟨σ,f⟩ ] , dis the dimension of the learned feature representation, and the restricted hypothesis space is deﬁned as H(λ) ={ϕ|ϕ∈H,andR1 (f) ≤4/λ}. Theorem C.2. Let f∗∈arg minfLcl + λLmsr. Then with probability at least 1 −δ, we have that ⏐⏐LT SM (f∗) −Lcl(f∗) ⏐⏐≤O ( Q1RH (λ) M + √ Q2 M ) (13) where M is the total number of training samples, N is the negative pair size, and Q1 = √ 1 + 1/N, RH (λ) is the rademacher complexity. Also, RH (λ) is monotonically decreasing w.r.t.λ. Proof. For the traditional cross entropy loss LT SM (f), we have that: LT SM (f) = EX [ inf W LCE (W ·f,T ) ] = EX [ −log ef(X)Tuc+ ef(X)Tuc+ +∑ef(X)Tuc− ] = EX [ −log e f(X)TEX+ [f(X+)] ef(X)TEX+ [f(X+)]+ME [ ef(X)TEX−[f(X−)]] ] ≥EX [ −log e f(X)TEX+ [f(X+)] ef(X)TEX+ [f(X+)]+MEX− [ ef(X)TEX−[f(X−)]] ] = Lcl(f) (14) Then, we can obtain: LT SM (f∗) −Lcl(f) ≤Lcl(f∗) −Lcl(f) ≤O ( Q1RH (λ) M + √ Q2 M ) (15) Therefore, we have ⏐⏐LT SM (f∗) −Lcl(f∗) ⏐⏐≤O ( Q1RH (λ) M + √ Q2 M ) (16)",
      "references": [
        "A theoretical analysis of contrastive unsupervised representation learning.",
        "Discrete energy on rectiﬁable sets.",
        "Unsupervised learning of visual fea- tures by contrasting cluster assignments.",
        "Large-margin contrastive learning with distance po- larization regularizer.",
        "A simple framework for contrastive learning of visual rep- resentations.",
        "Big self-supervised models are strong semi- supervised learners.",
        "Exploring simple siamese represen- tation learning.",
        "Improved baselines with momentum contrastive learning.",
        "An empirical study of train- ing self-supervised vision transformers.",
        "Debiased contrastive learning.",
        "An analysis of single- layer networks in unsupervised feature learning.",
        "Universally optimal distribution of points on spheres.",
        "Imagenet: A large-scale hierarchical image database.",
        "Whitening for self-supervised representation learning.",
        "Causal inference in statistics: A primer.",
        "Bootstrap your own latent- a new approach to self-supervised learning.",
        "Momentum contrast for unsupervised visual representation learning.",
        "Data-efﬁcient image recognition with contrastive predictive coding.",
        "Adam: A method for stochastic optimization.",
        "Learning multiple layers of features from tiny images.",
        "Tiny imagenet visual recognition chal- lenge.",
        "Prototypical con- trastive learning of unsupervised representations.",
        "Self- supervised learning with kernel dependence maximiza- tion.",
        "Microsoft coco: Common objects in context.",
        "Representation learning via invariant causal mechanisms.",
        "Understanding negative samples in instance discriminative self-supervised representation learning.",
        "Representation learn- ing with contrastive predictive coding.",
        "Decomposed mutual information estimation for contrastive representation learning.",
        "Contrastive multiview coding.",
        "What makes for good views for contrastive learning?",
        "Understanding self- supervised learning dynamics without contrastive pairs.",
        "Self- supervised learning with data augmentations provably isolates content from style.",
        "Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere.",
        "Toward understanding the feature learn- ing process of self-supervised contrastive learning.",
        "Barlow twins: Self-supervised learning via redundancy reduction.",
        "Visualizing and understand- ing convolutional networks.",
        "Generalized cross entropy loss for training deep neural networks with noisy labels.",
        "Ressl: Relational self-supervised learning with weak augmentation."
      ],
      "meta_data": {
        "arxiv_id": "2206.14702v1",
        "authors": [
          "Wenwen Qiang",
          "Jiangmeng Li",
          "Changwen Zheng",
          "Bing Su",
          "Hui Xiong"
        ],
        "published_date": "2022-06-29T15:02:38Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies that background regions act as a confounder in contrastive self-supervised visual representation learning, leading to a performance paradox; formulates a Structural Causal Model to describe this effect; introduces Interventional Contrastive Learning with Meta Semantic Regularizer (ICL-MSR), a back-door adjustment based regularization that can be plugged into existing CL frameworks to reduce background bias; provides a tighter theoretical generalization error bound; and empirically shows consistent performance gains across several state-of-the-art CL methods and datasets.",
        "methodology": "1) Build SCM with semantic features Xs as confounder linking anchor and positive. 2) Derive causal intervention P(Y|do(X)) via back-door adjustment, approximated by summing over n learnable semantic weight vectors. 3) Implement Meta Semantic Regularizer: CNN generates weight matrix As; each weight vector at re-weights feature channels of positive sample, inserted into contrastive loss to approximate intervention. 4) Joint objective Lto = Lct + λLmsr; additional uniformity loss on As. 5) Two-stage meta-learning optimization per epoch: update encoder and projection head, then update MSR using gradients through a one-step inner update. Compatible with SimCLR, BYOL, MoCo, etc.",
        "experimental_setup": "Toy study on COCO (30 single-object classes) comparing training/testing on full vs foreground crops. Main benchmarks: CIFAR-10/100 (32×32), STL-10 (96×96), Tiny-ImageNet (64×64) with ResNet-18; ImageNet-100 and full ImageNet with ResNet-50. Evaluate learnt encoders by linear probing and 5-NN classification; also semi-supervised fine-tuning on 1%/10% ImageNet labels. Compare baseline CL methods (SimCLR, BYOL, W-MSE, ReSSL, LMCL, SSL-HSIC, RELIC, MoCo, CMC, SwAV, DCL, LMLC) against their ICL-MSR variants. Common hyper-parameters: n=6 (or 14 for semi-supervised), τ=0.5, Adam optimizer, 1000–2000 epochs depending on dataset, standard data augmentations. Gains reported in top-1/5 accuracy.",
        "limitations": "Improvements are small (<1%) on low-resolution datasets, indicating limited impact when background information is scarce; performance sensitive to the number of semantic weight vectors n and hyper-parameters λ, γ; adds computational overhead due to meta-learning inner loop; assumes availability of sufficiently large images and single dominant foreground object (toy experiments exclude multi-object images); method validated only on classification – effect on detection, segmentation or non-vision modalities untested; relies on re-weighting feature channels which may not align cleanly with semantic factors in all architectures.",
        "future_research_directions": "1) Develop adaptive or learnable selection of the number of semantic weight vectors to reduce sensitivity. 2) Design more efficient optimization to cut meta-learning cost. 3) Extend causal intervention ideas to other downstream tasks (detection, segmentation, video) and to non-visual domains. 4) Investigate integration with high-resolution or multi-object scenes and automatic foreground estimation without annotations. 5) Explore alternative causal regularizers or contrastive objectives less reliant on hyper-parameters. 6) Apply approach to transformer-based encoders and study channel vs token level semantics.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Consistency Regularization for Generative Adversarial Networks",
      "full_text": "Published as a conference paper at ICLR 2020 CONSISTENCY REGULARIZATION FOR GENERATIVE ADVERSARIAL NETWORKS Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee Google Research {zhanghan,zizhaoz,augustusodena,honglak}@google.com ABSTRACT Generative Adversarial Networks (GANs) are known to be difﬁcult to train, de- spite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational over- heads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization—a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effec- tively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state- of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012. 1 I NTRODUCTION Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently demonstrated impressive results on image-synthesis benchmarks (Radford et al., 2016; Zhang et al., 2017; Miyato & Koyama, 2018; Zhang et al., 2018; Brock et al., 2018; Karras et al., 2019). In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and thediscriminator is trained to distinguish real samples from fake ones produced by the generator. One major problem with GANs is the instability of the training procedure and the general sensitivity of the results to various hyperparameters (Salimans et al., 2016). Because GAN training implicitly requires ﬁnding the Nash equilibrium of a non-convex game in a continuous and high dimensional parameter space, it is substantially more complicated than standard neural network training. In fact, formally characterizing the convergence properties of the GAN training procedure is mostly an open problem (Odena, 2019). Previous work (Arjovsky & Bottou, 2017; Miyato et al., 2018a; Odena et al., 2017; Chen et al., 2019; Wei et al., 2018) has shown that interventions focused on the discriminator can mitigate stability issues. Most successful interventions fall into two categories, normalization and regularization. Spectral normalization is the most effective normalization method, in which weight matrices in the discriminator are divided by an approximation of their largest singular value. For regularization, Gulrajani et al. (2017) penalize the gradient norm of straight lines between real data and generated data. Roth et al. (2017) propose to directly regularize the squared gradient norm for both the training data and the generated data. DRAGAN (Kodali et al., 2017) introduces another form of gradient penalty where the gradients at Gaussian perturbations of training data are penalized. One may anticipate simultaneous regularization and normalization could improve sample quality. However, most of these gradient based regularization methods either provide marginal gains or fail to introduce any improvement when normalization is used (Kurach et al., 2019), which is also observed in our experiments. These regularization methods and spectral normalization are motivated by controlling Lipschitz constant of the discriminator. We suspect this might be the reason that applying both does not lead to overlaid gain. 1 arXiv:1910.12027v2  [cs.LG]  18 Feb 2020Published as a conference paper at ICLR 2020 Image space Manifold space Semantic feature space Before consistency Afterconsistency Figure 1: An illustration of consistency regularization for GANs. Before consistency regularization, the zoomed-in dog and the zoomed-in cat (bottom left) can be closer than they are to their original images in feature space induced by the GAN discriminator. This is illustrated in the upper right (the semantic feature space), where the purple dot is closer to the blue dot than to the red dot, and so forth. After we enforce consistency regularization based on the implicit assumption that image augmentation preserves the semantics we care about, the purple dot pulled closer to the red dot. In this paper, we examine a technique called consistency regularization (Bachman et al., 2014; Saj- jadi et al., 2016; Laine & Aila, 2016; Zhai et al., 2019; Xie et al., 2019; Hu et al., 2017) in contrast to gradient-based regularizers. Consistency regularization is widely used in semi-supervised learning to ensure that the classiﬁer output remains unaffected for an unlabeled example even it is augmented in semantic-preserving ways. In light of this intuition, we hypothesize a well-trained discrimina- tor should also be regularized to have the consistency property, which enforces the discriminator to be unchanged by arbitrary semantic-preserving perturbations and to focus more on semantic and structural changes between real and fake data. Therefore, we propose a simple regularizer to the dis- criminator of GAN: we augment images with semantic-preserving augmentations before they are fed into the GAN discriminator and penalize the sensitivity of the discriminator to those augmentations. This technique is simple to use and surprisingly effective. It is as well less computationally expen- sive than prior techniques. More importantly, in our experiments, consistency regularization can always further improve the model performance when spectral normalization is used, whereas the performance gains of previous regularization methods diminish in such case. In extensive ablation studies, we show that it works across a large range of GAN variants and datasets. We also show that simply applying this technique on top of existing GAN models leads to new state-of-the-art results as measured by Frechet Inception Distance (Heusel et al., 2017). In summary, our contributions are summarized as follows: • We propose consistency regularization for GAN discriminators to yield a simple, effective regularizer with lower computational cost than gradient-based regularization methods. • We conduct extensive experiments with different GAN variants to demonstrate that our technique interacts effectively with spectral normalization. Our consistency regularized GAN (CR-GAN) achieves the best FID scores for unconditional image generation on both CIFAR-10 and CelebA. • We show that simply applying the proposed technique can further boost the performance of state-of-the-art GAN models. We improve FID scores for conditional image generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012. 2 M ETHOD 2.1 GAN S A GAN consists of a generator network and a discriminator network. The generator G takes a latent variable z ∼p(z) sampled from a prior distribution and maps it to the observation space X. The discriminator Dtakes an observation x ∈X and produces a decision output over possible observation sources (either from Gor from the empirical data distribution). In the standard GAN training procedure the generator Gand the discriminator Dare trained by minimizing the following 2Published as a conference paper at ICLR 2020 objectives in an alternating fashion: LD = −Ex∼pdata [log D(x)] −Ez∼p(z) [1 −log D(G(z))] , LG = −Ez∼p(z) [log D(G(z))] , (1) where p(z) is usually a standard normal distribution. This formulation is originally proposed by Goodfellow et al. (2014) as non-saturating (NS) GAN. A signiﬁcant amount of research has been done on modifying this formulation in order to improve the training process. A notable example is the hinge-loss version of the adversarial loss (Lim & Ye, 2017; Tran et al., 2017): LD = −Ex∼pdata [min(0,−1 +D(x))] −Ez∼p(z) [min(0,−1 −D(G(z)))] , LG = −Ez∼p(z) [D(G(z))] . (2) Another commonly adopted GAN formulation is the Wassertein GAN (WGAN) (Arjovsky et al., 2017), in which the authors propose clipping the weights of the discriminator in an attempt to enforce that the GAN training procedure implicitly optimizes a bound on the Wassertein distance between the target distribution and the distribution given by the generator. The loss function of WGAN can be written as LD = −Ex∼pdata [D(x)] +Ez∼p(z) [D(G(z))] , LG = −Ez∼p(z) [D(G(z))] . (3) Subsequent work has reﬁned this technique in several ways (Gulrajani et al., 2017; Miyato et al., 2018a; Zhang et al., 2019), and the current widely-used practice is to enforce spectral normaliza- tion (Miyato et al., 2018a) on both the generator and the discriminator. 2.2 C ONSISTENCY REGULARIZATION Consistency regularization has emerged as a gold-standard technique (Sajjadi et al., 2016; Laine & Aila, 2016; Zhai et al., 2019; Xie et al., 2019; Oliver et al., 2018; Berthelot et al., 2019) for semi- supervised learning on image data. The basic idea is simple: an input image is perturbed in some semantics-preserving ways and the sensitivity of the classiﬁer to that perturbation is penalized. The perturbation can take many forms: it can be image ﬂipping, or cropping, or adversarial attacks. The regularization form is either the mean-squared-error (Sajjadi et al., 2016; Laine & Aila, 2016) between the model’s output for a perturbed and non-perturbed input or the KL divergence (Xie et al., 2019; Miyato et al., 2018b) between the distribution over classes implied by the output logits. 2.3 C ONSISTENCY REGULARIZATION FOR GAN S The goal of the discriminator in GANs is to distinguish real data from fake ones produced by the generator. The decision should be invariant to any valid domain-speciﬁc data augmentations. For example, in the image domain, the image being real or not should not change if we ﬂip the image horizontally or translate the image by a few pixels. However, the discriminator in GANs does not guarantee this property explicitly. To resolve this, we propose a consistency regularization on the GAN discriminator during train- ing. In practice, we randomly augment training images as they are passed to the discriminator and penalize the sensitivity of the discriminator to those augmentations. We use Dj(x) to denote the output vector before activation of the jth layer of the discriminator given input x. T(x) denotes a stochastic data augmentation function. This function can be linear or nonlinear, but aims to preserve the semantics of the input. Our proposed regularization is given by min D Lcr = min D n∑ j=m λj Dj(x) −Dj(T(x)) 2 , (4) where j indexes the layers, m is the starting layer and n is the ending layer that consistency is enforced. λj is weight coefﬁcient for jth layer and ∥·∥denotes L2 norm of a given vector. This consistency regularization encourages the discriminator to produce the same output for a data point under various data augmentations. 3Published as a conference paper at ICLR 2020 Algorithm 1Consistency Regularized GAN (CR-GAN). We use λ= 10by default. Input: generator and discriminator parameters θG,θD, consistency regularization coefﬁcient λ, Adam hyperparameters α,β1,β2, batch size M, number of discriminator iterations per gen- erator iteration ND 1: for number of training iterations do 2: for t= 1,...,N D do 3: for i= 1,...,M do 4: Sample z∼p(z), x∼pdata(x) 5: Augment xto get T(x) 6: L(i) cr ← D(x) −D(T(x)) 2 7: L(i) D ←D(G(z)) −D(x) 8: end for 9: θD ←Adam( 1 M ∑M i=1(L(i) D + λL(i) cr ),α,β 1,β2) 10: end for 11: Sample a batch of latent variables {z(i)}M i=1 ∼p(z) 12: θG ←Adam( 1 M ∑M i=1(−D(G(z))),α,β 1,β2) 13: end for In our experiments, we ﬁnd that consistency regularization on the last layer of the discriminator before the activation function is sufﬁcient. Lcr can be rewritten as Lcr = D(x) −D(T(x)) 2 , (5) where from now on we will drop the layer index for brevity. This cost is added to the discriminator loss (weighted by a hyper-parameter λ) when updating the discriminator parameters. The generator update remains unchanged. Thus, the overall consistency regularized GAN (CR-GAN) objective is written as Lcr D = LD + λLcr, L cr G = LG. (6) Our design of Lcr is general-purpose and thereby can work with any valid adversarial lossesLG and LD for GANs (See Section 2.1 for examples). Algorithm 1 illustrates the details of CR-GAN with Wassertein loss as an example. In contrast to previous regularizers, our method does not increase much overhead. The only extra computational cost comes from feeding an additional (third) image through the discriminator forward and backward when updating the discriminator parameters. 3 E XPERIMENTS This section validates our proposed CR-GAN method. First we conduct a large scale study to com- pare consistency regularization to existing GAN regularization techniques (Kodali et al., 2017; Gul- rajani et al., 2017; Roth et al., 2017) for several GAN architectures, loss functions and other hyper- parameter settings. We then apply consistency regularization to a state-of-the-art GAN model (Brock et al., 2018) and demonstrate performance improvement. Finally, we conduct ablation studies to in- vestigate the importance of various design choices and hyper-parameters. All our experiments are based on the open-source code from Compare GAN (Kurach et al., 2019), which is available at https://github.com/google/compare_gan. 3.1 D ATASETS AND EVALUATION METRICS We validate our proposed method on three datasets: CIFAR-10 (Krizhevsky, 2009), CELEBA-HQ- 128 (Karras et al., 2018), and ImageNet-2012 (Russakovsky et al., 2015). We follow the procedure in Kurach et al. (2019) to prepare datasets. CIFAR-10 consists of 60K of 32 ×32 images in 10 classes; 50K for training and 10K for testing. CELEBA-HQ-128 (CelebA) contains 30K images of faces at a resolution of 128 ×128. We use 3K images for testing and the rest of images for training. ImageNet-2012 contains roughly 1.2 million images with 1000 distinct categories and we down-sample the images to 128 ×128 in our experiments. We adopt the Fréchet Inception distance (FID) (Heusel et al., 2017) as primitive metric for quantita- tive evaluation, as FID has proved be more consistent with human evaluation. In our experiments the 4Published as a conference paper at ICLR 2020 (a) (b) (c) (d) (e) (f) Figure 2: Comparison of our method with existing regularization techniques under different GAN losses. Techniques include no regularization (W/O), Gradient Penalty (GP) (Gulrajani et al., 2017), DRAGAN (DR) (Kodali et al., 2017) and JS-Regularizer (JSR) (Roth et al., 2017). Results (a-c) are for CIFAR-10 and results (d-f) are for CelebA. FID is calculated on the test dataset. In particular, we use 10K generated images vs. 10K test images on CIFAR-10, 3K vs. 3K on CelebA and 50K vs. 50K on ImageNet. We also provide the Inception Score (Salimans et al., 2016) for different methods in the Appendix F for supplementary results. By default, the augmentation used in consistency regularization is a combination of randomly shifting the image by a few pixels and randomly ﬂipping the image horizontally. The shift size is 4 pixels for CIFAR-10 and CelebA and 16 for ImageNet. 3.2 C OMPARISON WITH OTHER GAN REGULARIZATION METHODS In this section, we compare our methods with three GAN regularization techniques, Gradient Penalty (GP) (Gulrajani et al., 2017), DRAGAN Regularizer (DR) (Kodali et al., 2017) and JS-Regularizer (JSR) (Roth et al., 2017) on CIFAR-10 and CelebA. Following the procedures from (Kurach et al., 2019; Lucic et al., 2018), we evaluate these methods across different optimizer parameters, loss functions, regularization coefﬁcient and neural architec- tures. For optimization, we use the Adam optimizer with batch size of 64 for all our experiments. We stop training after 200k generator update steps for CIFAR-10 and 100k steps for CelebA. By default, spectral normalization (SN) (Miyato et al., 2018a) is used in the discriminator, as this is the most effective normalization method for GANs (Kurach et al., 2019) and is becoming the standard for ‘modern’ GANs (Zhang et al., 2019; Brock et al., 2018). Results without spectral normalization can be seen in the Appendix B. 3.2.1 I MPACT OF LOSS FUNCTION In this section, we discuss how each regularization method performs when the loss function is changed. Speciﬁcally, we evaluate regularization methods using three loss functions: the non- saturating loss (NS) (Goodfellow et al., 2014), the Wasserstein loss (W AS) (Arjovsky et al., 2017), and the hinge loss (Hinge) (Lim & Ye, 2017; Tran et al., 2017). For each loss function, we evaluate over 7 hyper-parameter settings of the Adam optimizer (more details in Section A of the appendix). For each conﬁguration, we run each model 3 times with different random seeds. For the regulariza- tion coefﬁcient, we use the best value reported in the corresponding paper. Speciﬁcally λis set to be 10 for both GP, DR and our method and 0.1 for JSR. In this experiment, we use the SNDCGAN network architecture (Miyato et al., 2018a) for simplicity. In the end, similar as Kurach et al. (2019), we aggregate all runs and report the FID distribution of the top 15% of trained models. The results are shown in Figure 2. The consistency regularization improves the baseline across all different loss functions and both datasets. Other techniques have more mixed results: For example, 5Published as a conference paper at ICLR 2020 Setting W/O GP DR JSR Ours (CR-GAN) CIFAR-10 (SNDCGAN) 24.73 25.83 25.08 25.17 18.72 CIFAR-10 (ResNet) 19.00 19.74 18.94 19.59 14.56 CelebA (SNDCGAN) 25.95 22.57 21.91 22.17 16.97 Table 1: Best FID scores for unconditional image generation on CIFAR-10 and CelebA. CIFAR-10 CelebA FID FID Figure 3: Comparison of FID scores with different values of the regularization coefﬁcient λ on CIFAR-10 and CelebA. The dotted line is a model without regularization. GP and DR can marginally improve the performance for settings (d) and (e) but lead to worse results for settings (a) and (b) (which is consistent with ﬁndings from Kurach et al. (2019)). In all cases, our consistency-regularized GAN models have the lowest (best) FID. This ﬁnding is especially encouraging, considering that the consistency regularization has lower computational cost (and is simpler to implement) than the other techniques. In our experiments, the consistency regularization is around 1.7 times faster than gradient based regularization techniques, including DR, GP and JSR, which need to compute the gradient of the gradient norm ∥∇x(D)∥. Please see Table C1 in the appendix for the actual training speed. 3.2.2 I MPACT OF THE REGULARIZATION COEFFICIENT Here we study the sensitivity of GAN regularization techniques to the regularization coefﬁcient λ. We train SNDCGANs with non-saturating losses and ﬁx the other hyper-parameters. λis chosen among {0.1, 1, 10, 100}. The results are shown in Figure 3. From this ﬁgure, we can see consistency regularization is more robust to changes in λthan other GAN regularization techniques (it also has the best FID for both datasets). The results indicate that consistency regularization can be used as a plug-and-play technique to improve GAN performance in different settings without much hyper- parameter tuning. 3.2.3 I MPACT OF NEURAL ARCHITECTURES To validate whether the above ﬁndings hold across different neural architectures, we conduct exper- iments on CIFAR-10 using a ResNet (He et al., 2016; Gulrajani et al., 2017) architecture instead of an SNDCGAN. All other experimental settings are same as in Section 3.2.1. The FID values are presented in Figure 4. By comparing results in Figure 4 and Figure 2, we can see that results on SNDCGAN and results on ResNet are comparable, though consistency regularization favors even better in this case: In sub-plot (c) of Figure 4, we can see that consistency regularization is the only regularization method that can generate satisfactory samples with a reasonable FID score (The FID scores for other methods are above 100). Please see Figure D3 for the actual generated samples in this setting. As in Section 3.2.1, consistency regularization has the best FID for each setting. In Table 1, we show FID scores for the best-case settings from this section. Consistency regulariza- tion improves on the baseline by a large margin and achieves the best results across different network architectures and datasets. In particular, it achieves an FID 14.56 on CIFAR-10 16.97 on CelebA. In fact, our FID score of 14.56 on CIFAR-10 for unconditional image generation is even lower than the 14.73 reported in Brock et al. (2018) for class-conditional image-synthesis with a much larger network architecture and much bigger batch size. 6Published as a conference paper at ICLR 2020 (a) (b) (c) Figure 4: Comparison of FID scores with ResNet structure on different loss settings on CIFAR-10. 3.3 C OMPARISON WITH STATE -OF-THE -ART GAN MODELS In this section, we add consistency regularization to the state-of-the-art BigGAN model (Brock et al., 2018) and perform class conditional image-synthesis on CIFAR-10 and ImageNet. Our model has exactly the same architecture and is trained under the same settings as BigGAN ⋆, the open-source implementation of BigGAN from Kurach et al. (2019). The only difference is that our model uses consistency regularization. In Table 2, we report the original FID scores without noise truncation. Consistency regularization improves the FID score of BigGAN⋆ on CIFAR-10 from 20.42 to 11.48. In addition, the FID on ImageNet is improved from 7.75 to 6.66. Generated samples for CIFAR-10 and ImageNet with consistency regularized models and baseline models are shown in Figures E1, E2 and E3 in the appendix. Dataset SNGAN SAGAN BigGAN BigGAN ⋆ CR-BigGAN⋆ CIFAR-10 17.5 / 14.73 20.42 11.48 ImageNet 27.62 18.65 8.73 7.75 6.66 Table 2: Comparison of our technique with state-of-the-art GAN models including SNGAN (Miy- ato & Koyama, 2018), SAGAN (Zhang et al., 2019) and BigGAN (Brock et al., 2018) for class conditional image generation on CIFAR-10 and ImageNet in terms of FID. BigGAN ⋆ is the Big- GAN implementation of Kurach et al. (2019). CR-BigGAN ⋆ has the exactly same architecture as BigGAN⋆ and is trained with the same settings. The only difference is CR-BigGAN ⋆ adds consis- tency regularization. 4 A BLATION STUDIES AND DISCUSSION 4.1 H OW MUCH DOES AUGMENTATION MATTER BY ITSELF ? Our consistency regularization technique actually has two parts: we perform data augmentation on inputs from the training data, and then consistency is enforced between the augmented data and the original data. We are interested in whether the performance gains shown in Section 3 are merely due to data augmentation, since data augmentation reduces the over-ﬁtting of the discriminator to the input data. Therefore, we have designed an experiment to answer this question. First, we train three GANs: (1) a GAN trained with consistency regularization, as in Algorithm 1, (2) a baseline GAN trained without augmentation or consistency regularization, and (3) a GAN trained with only data augmentation and no consistency regularization. We then plot (Figure 5) both their FID and the test accuracy of their discriminator on a held-out test set. The FID tells us how ‘good’ the resulting GAN is, and the discriminator test accuracy tells us how much the GAN discriminator over-ﬁts. Interestingly, we ﬁnd that these two measures are not well correlated in this case. The model trained with only data augmentation over-ﬁts substantially less than the baseline GAN, but has almost the same FID. The model trained with consistency regularization has the same amount of over-ﬁtting as the model trained with just data augmentation, but a much lower FID. This suggests an interesting hypothesis, which is that the mechanism by which the consistency regu- larization improves GANs is not simply discriminator generalization (in terms of classifying images into real vs fake). We believe that the main reason for the impressive gain from the consistency regularization is due to learning more semantically meaningful representation for the discriminator. More speciﬁcally, data augmentation will simply treat all real images and their transformed images 7Published as a conference paper at ICLR 2020 0 1000 2000 3000 4000 Epochs 0.4 0.5 0.6 0.7 0.8 0.9Traing accuracy GAN GAN w/ Aug. GAN w/ Cons. Reg. 0 1000 2000 3000 4000 Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Test accuracy GAN GAN w/ Aug. GAN w/ Cons. Reg. 0 1000 2000 3000 4000 Epochs 20 25 30 35 40 45 50FID GAN GAN w/ Aug. GAN w/ Cons. Reg. Figure 5: A study of how much data augmentation matters by itself. Three GANs were trained on CIFAR-10: one baseline GAN, one GAN with data augmentation only, and one GAN with consis- tency regularization. (Left) Training accuracy of the GAN discriminator. (Middle) Test accuracy of the GAN discriminator on the held out test set. The accuracy is low for the baseline GAN, which in- dicates it suffered from over-ﬁtting. The accuracy for the other two is basically indistinguishable for each other. This suggests that augmentation by itself is enough to reduce discriminator over-ﬁtting, and that consistency regularization by itself does little to address over-ﬁtting. (Right) FID scores of the three settings. The score for the GAN with only augmentation is not any better than the score for the baseline, even though its discriminator is not over-ﬁtting. The score for the GAN with consis- tency regularization is better than both of the others, suggesting that the consistency regularization acts on the score through some mechanism other than by reducing discriminator over-ﬁtting. Metric Gaussian Noise Random shift & ﬂip Cutout Cutout w/ random shift & ﬂip FID 21.91±0.32 16.04±0.17 17.10±0.29 19.46±0.26 Table 3: FID scores on CIFAR-10 for different types of image augmentation. Gaussian noise is the worst, and random shift and ﬂip is the best, consistent with general consensus on the best way to perform image optimization on CIFAR-10 (Zagoruyko & Komodakis, 2016). with the same label as real without considering semantics, whereas our consistency regularization further enforces learning implicit manifold structure in the discriminator that pulls semantically similar images (i.e., original real image and the transformed image) to be closer in the discriminator representation space. 4.2 H OW DOES THE TYPE OF AUGMENTATION AFFECT RESULTS ? To analyze how different types of data augmentation affect our results, we conduct an ablation study on the CIFAR-10 dataset comparing the results of using four different types of image augmentation: (1) adding Gaussian noise to the image in pixel-space, (2) randomly shifting the image by a few pixels and randomly ﬂipping it horizontally, (3) applying cutout (DeVries & Taylor, 2017) trans- formations to the image, and (4) cutout and random shifting and ﬂipping. As shown in Table 3, random ﬂipping and shifting without cutout gives the best results (FID 16.04) among all four meth- ods. Adding Gaussian noise in pixel-space gives the worst results. This result empirically suggests that adding Gaussian noise is not a good semantic preserving transformation in the image manifold. It’s also noteworthy that the most extensive augmentation (random ﬂipping and shifting with cutout) did not perform the best. One possible reason is that the generator sometimes also generates samples with augmented artifacts (e.g., cutout). If such artifacts do not exist in the real dataset, it might lead to worse FID performance. 5 C ONCLUSION In this paper, we propose a simple, effective, and computationally cheap method – consistency reg- ularization – to improve the performance of GANs. Consistency regularization is compatible with spectral normalization and results in improvements in all of the many contexts in which we evaluated it. Moreover, we have demonstrated consistency regularization is more effective than other regular- ization methods under different loss functions, neural architectures and optimizer hyper-parameter settings. We have also shown simply applying consistency regularization on top of state-of-the-art GAN models can further greatly boost the performance. Finally, we have conducted a thorough study on the design choices and hyper-parameters of consistency regularization. 8Published as a conference paper at ICLR 2020 ACKNOWLEDGMENTS We thank Colin Raffel for feedback on drafts of this article. We also thank Marvin Ritter, Michael Tschannen and Mario Lucic for answering our questions of using compare GAN codebase for large scale GAN evaluation. REFERENCES Martín Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017. Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017. Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In NeurIPS, 2014. David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv:1905.02249, 2019. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In CVPR, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im- proved training of wasserstein GANs. In NeurIPS, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, 2016. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study on regularization and normalization in gans. In ICML, 2019. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv:1705.02894, 2017. 9Published as a conference paper at ICLR 2020 Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? A large-scale study. In NeurIPS, 2018. Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018a. Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018b. Augustus Odena. Open questions about generative adversarial networks. Distill, 2019. doi: 10. 23915/distill.00018. https://distill.pub/2019/gan-open-problems. Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil- iary classiﬁer gans. In ICML, 2017. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In NeurIPS, pp. 3235–3246, 2018. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. Kevin Roth, Aurélien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In NeurIPS, 2017. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. IJCV, 115(3):211–252, 2015. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma- tions and perturbations for deep semi-supervised learning. In NeurIPS, 2016. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. Dustin Tran, Rajesh Ranganath, and David M. Blei. Deep and hierarchical implicit models. arXiv:1702.08896, 2017. Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. In ICLR, 2018. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S 4l: Self-supervised semi- supervised learning. arXiv preprint arXiv:1905.03670, 2019. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In ICML, 2019. Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic text-to-image synthesis with a hierarchically-nested adversarial network. In CVPR, 2018. 10Published as a conference paper at ICLR 2020 APPENDIX A H YPERPARAMETER SETTINGS OF OPTIMIZER Setting lr β 1 β2 Ndis A 0.0001 0.5 0.9 5 B 0.0001 0.5 0.999 1 C 0.0002 0.5 0.999 1 D 0.0002 0.5 0.999 5 E 0.001 0.5 0.9 5 F 0.001 0.5 0.999 5 G 0.001 0.9 0.999 5 Table A1: Hyper-parameters of the optimizer used in our experiments. Here, similar as the experiments in Miyato et al. (2018a); Kurach et al. (2019), we evaluate all reg- ularization methods across 7 different hyperparameters settings for (1) learning rate lr(2) ﬁrst and second order momentum parameters of Adam β1, β2 (3) number of the updates of the discriminator per generator update, Ndis. The details of all the settings are shown in Table A1. Among all these 7 settings, A-D are the \"good\" hyperparameters used in previous publications (Radford et al., 2016; Gulrajani et al., 2017; Kurach et al., 2019); E, F, G are the \"aggressive\" hyperparameter settings in- troduced by Miyato et al. (2018a) to test model performance under noticeably large learning rate or disruptively high momentum. In practice, we ﬁnd setting C generally works the best for SNDCGAN and setting D is the optimal setting for ResNet. These two settings are also the default settings in the Compare GAN codebase for the corresponding network architectures. CIFAR-10 CelebA Figure A1: Comparison of FID scores with different optimizer settings. Figure A1 displays the FID score of all methods with 7 settings A-G. We can observe that con- sistency regularization is fairly robust even for some of the aggressive hyperparameter settings. In general, the proposed consistency regularization can generate better samples with different optimizer settings compared with other regularization methods. 11Published as a conference paper at ICLR 2020 B C OMPARISON OF DIFFERENT REGULARIZATION METHODS WHEN SPECTRAL NORMALIZATION IS NOT USED CIFAR-10 SNDCGAN CIFAR-10 ResNet CelebA SNDCGAN (a) (b) (c) (d) (e) (f) (g) (h) (i) Figure B1: Comparison of FID scores when SN is not used. Here, we compare different regularization methods when spectral normalization (SN) is not used. As shown in Figure B1, our consistency regularization always improves the baseline model (W/O). It also achieves the best FID scores in most of the cases, which demonstrates that consistency reg- ularization does not depend on spectral normalization. By comparing with the results in Figure 2 and Figure 4, we ﬁnd adding spectral normalization will further boost the results. More importantly, the consistency regularization is only method that improve on top of spectral normalization without exception. The other regularization methods do not have this property. C T RAINING SPEED Here we show the actual training speed of discriminator updates for SNDCGAN on CIFAR-10 with NVIDIA Tesla V100. Consistency regularization is around 1.7 times faster than gradient based regularization techniques. Method W/O GP DR JSR Ours (CR-GAN) Speed (step/s) 66.3 29.7 29.8 29.2 51.7 Table C1: Training speed of discriminator updates for SNDCGAN on CIFAR-10. 12Published as a conference paper at ICLR 2020 D G ENERATED SAMPLES FOR UNCONDITIONAL IMAGE GENERATION Figure D1: Comparison of generated samples of CelebA. Ours (FID:14.56)  DR (FID:18.94) JSR (FID: 19.59) W/O (FID:19.00) Real Images GP(FID: 19.74) Figure D2: Comparison of generated samples for unconditional image generation on CIFAR-10 with a ResNet architecture. 13Published as a conference paper at ICLR 2020 W/O GP DR JSR Ours (CR-GAN) Figure D3: Comparison of unconditional generated samples on CIFAR-10 with a ResNet architec- ture, Wasserstein loss and spectral normalization. This is a hard hyperparameter setting where the baseline and previous regularization methods fail to generate reasonable samples. Consistency Reg- ularization is the only regularization method that can generate satisfactory samples in this setting. FID scores are shown in sub-plot (c) of Figure 4. E G ENERATED SAMPLES FOR CONDITIONAL IMAGE GENERATION BigGAN* (FID: 20.42)CR-BigGAN* (FID: 11.67) Figure E1: Comparison of generated samples for conditional image generation on CIFAR-10. Each row shows the generated samples of one class. 14Published as a conference paper at ICLR 2020 Figure E2: Comparison of conditionally generated samples of BigGAN* and CR-BigGAN* on ImageNet. (Left) Generated samples of CR-BigGAN*. (Right) Generated samples of BigGAN*. 15Published as a conference paper at ICLR 2020 Figure E3: More results for conditionally generated samples of BigGAN* and CR-BigGAN* on ImageNet. (Left) Generated samples of CR-BigGAN*. (Right) Generated samples of BigGAN*. 16Published as a conference paper at ICLR 2020 F C OMPARISON WITH INCEPTION SCORE Inception Score (IS) is another GAN evaluation metric introduced by Salimans et al. (2016). Here, we compare the Inception Score of the unconditional generated samples on CIFAR-10. As shown in Table F1, Figure F1 and Figure F2, consistency regularization achieves the best IS result with both SNDCGAN and ResNet architectures. Setting W/O GP DR JSR Ours (CR-GAN) CIFAR-10 (SNDCGAN) 7.54 7.54 7.54 7.52 7.93 CIFAR-10 (ResNet) 8.20 8.04 8.09 8.03 8.40 Table F1: Best Inception Score for unconditional image generation on CIFAR-10. Figure F1: Comparison of IS with a SNDCGAN architecture on different loss settings. Models are trained on CIFAR-10. Figure F2: Comparison of IS with a ResNet architecture on different loss settings. Models are trained on CIFAR-10. 17Published as a conference paper at ICLR 2020 G E FFECT OF THE NUMBER OF LAYERS REGULARIZED IN DISCRIMINATOR Here, we examine the effect of the number of layers regularized in discriminator. In this experiment, we use SNDCGAN architecture with NS loss on the CIFAR-10 dataset. There are 8 intermediate layers in the discriminator. To start, we add consistency only to the last layer (0 intermediate lay- ers). Then we gradually enforce consistency for more intermediate layers. We use two weighting variations to combine the consistency loss across different layers. In the ﬁrst setting, the weight of each layer is the inverse of feature dimension dj in that layer, which corresponds to λj = 1/dj in Equation 4. In the second setting, we give equal weight to each layer, which corresponds toλj = 1. The results for both settings are shown in Figure G1. In both settings, we observe that consistency regularization on the ﬁnal layer achieves reasonably good results. Adding the consistency to ﬁrst few layers in the discriminator harms the performance. For simplicity, we only add consistency regularization in the ﬁnal layer of the discriminator in the rest of our experiments. (a) (b) Figure G1: Comparison of consistency regularization on different number of intermediate layers: (a) ﬁrst weight setting, where the weight for each layer is the inverse of its feature dimension (b) second weight setting, where each layer has equal weight. H C ONSISTENCY REGULARIZATION ON THE GENERATED SAMPLES In this section, we investigate the effect of adding consistency regularization for the generated sam- ples. We compare four settings, no consistency regularization (W/O), regularization only on the real samples (CR-Real), consistency regularization only on the fake samples produced by the genera- tor (CR-Fake) and regularization on both real and fake samples (CR-All). CR-Real is presented in Algorithm 1. CR-Fake has similar computational cost as CR-Real and CR-All doubles the compu- tational cost, since both the augmented real and fakes samples need to be fed into the discriminator to calculate the consistency loss. As shown in Figure H1, CR-Real, CR-Fake and CR-All are always better than the baseline without consistency regularization. In addition, CR-Real is consistently better than CR-Fake. It is interesting to note that CR-All is not always better than CR-real given the extra computational costs and stronger regularization. For example, CR-All improves FID from 20.21 of CR-Real to 15.51 for SNDCGAN, but it also gives slightly worse results for ResNet (14.93 vs 15.07) and for CR-BigGAN* (11.48 vs 12.51). We observe that enforcing additional consistency on the generated samples gives more performance gain when the model capacity is small and that gain decreases when model capacity increases. For computational efﬁciency and simplicity of the training algorithm, we use consistency regularization on real samples for the rest of our experiments. 18Published as a conference paper at ICLR 2020 W/O CR-Real CR-Fake CR-All (a) 0 5 10 15 20 25 30FID W/O CR-Real CR-Fake CR-All (b) 0 5 10 15 20FID W/O CR-Real CR-Fake CR-All (c) 0 5 10 15 20 25 30FID Figure H1: Comparison of FID scores with no consistency regularization (W/O), regularization only on the real samples (CR-Real), consistency regularization only on the fake samples produced by the generator (CR-Fake) and regularization on both real and fake samples (CR-All) for (a) unconditional image generation on CIFAR-10 with SNDCGAN, (b) unconditional image generation on CIFAR-10 with ResNet, (c) conditional image generation on CIFAR-10 with CR-BigGAN*. 19",
      "references": [
        "Towards principled methods for training generative adversarial networks.",
        "Wasserstein GAN.",
        "Learning with pseudo-ensembles.",
        "Mixmatch: A holistic approach to semi-supervised learning.",
        "Large scale gan training for high ﬁdelity natural image synthesis.",
        "Self-supervised gans via auxiliary rotation loss.",
        "Improved regularization of convolutional neural networks with cutout.",
        "Generative adversarial nets.",
        "Improved training of wasserstein GANs.",
        "Deep residual learning for image recognition.",
        "Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
        "Learning discrete representations via information maximizing self-augmented training.",
        "Progressive growing of GANs for improved quality, stability, and variation.",
        "A style-based generator architecture for generative adversarial networks.",
        "On convergence and stability of gans.",
        "Learning multiple layers of features from tiny images.",
        "A large-scale study on regularization and normalization in gans.",
        "Temporal ensembling for semi-supervised learning.",
        "Geometric GAN.",
        "Are gans created equal? A large-scale study.",
        "cGANs with projection discriminator.",
        "Spectral normalization for generative adversarial networks.",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning.",
        "Open questions about generative adversarial networks.",
        "Conditional image synthesis with auxiliary classifier gans.",
        "Realistic evaluation of deep semi-supervised learning algorithms.",
        "Unsupervised representation learning with deep convolutional generative adversarial networks.",
        "Stabilizing training of generative adversarial networks through regularization.",
        "ImageNet large scale visual recognition challenge.",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning.",
        "Improved techniques for training gans.",
        "Deep and hierarchical implicit models.",
        "Improving the improved training of wasserstein gans: A consistency term and its dual effect.",
        "Unsupervised data augmentation for consistency training.",
        "Wide residual networks.",
        "S 4l: Self-supervised semi-supervised learning.",
        "StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks.",
        "Self-attention generative adversarial networks.",
        "Photographic text-to-image synthesis with a hierarchically-nested adversarial network."
      ],
      "meta_data": {
        "arxiv_id": "1910.12027v2",
        "authors": [
          "Han Zhang",
          "Zizhao Zhang",
          "Augustus Odena",
          "Honglak Lee"
        ],
        "published_date": "2019-10-26T09:06:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Consistency Regularized GAN (CR-GAN), a lightweight regularization scheme that penalizes the discriminator when its intermediate or final representations change under semantic-preserving data augmentations. This approach stabilizes GAN training, works seamlessly with spectral normalization, requires little additional computation, and achieves state-of-the-art FID on both unconditional (CIFAR-10, CelebA) and conditional (CIFAR-10, ImageNet) image generation.",
        "methodology": "For each real training image x (and optionally generated image), a stochastic augmentation T(x) (random horizontal flip and small spatial shift by 4–16 pixels) is produced. An L2 penalty L_cr = ||D(x) − D(T(x))||^2 is added to the discriminator loss, weighted by λ (default 10). Usually only the discriminator’s last pre-activation layer is regularized. The generator objective is unchanged. The technique is agnostic to the adversarial loss (tested with Non-Saturating, Wasserstein, and Hinge) and architectures (SNDCGAN, ResNet, BigGAN). Training uses Adam with various hyper-parameters; spectral normalization is applied to discriminator (and sometimes generator).",
        "experimental_setup": "Datasets: CIFAR-10 (50k train/10k test, 32×32), CelebA-HQ-128 (27k train/3k test, 128×128), ImageNet-2012 downsampled to 128×128 (1.2M images, 1000 classes).\nArchitectures: SNDCGAN and ResNet for unconditional generation; BigGAN* implementation for class-conditional generation.\nLosses: Non-saturating, Wasserstein, and Hinge; discriminator trained 1–5 times per generator step.\nBaselines: No regularization, Gradient Penalty (GP), DRAGAN, JS-Regularizer; with/without spectral normalization.\nEvaluation metrics: Frechet Inception Distance (FID) as primary, Inception Score as supplementary; computed on 10k (CIFAR-10), 3k (CelebA), and 50k (ImageNet) samples vs equal number of real images.\nValidation: extensive hyper-parameter sweep (7 Adam settings), ablations on λ, number of regularized layers, augmentation type, real vs fake consistency, and training speed.",
        "limitations": "Evaluations are limited to image data and relatively low resolutions (≤128×128); efficacy on higher-resolution or non-image modalities is untested. The method assumes availability of augmentation that preserves semantics, which may be domain-dependent. Gains diminish when model capacity is very large or when applying consistency to both real and fake samples can hurt. Theoretical understanding of why consistency helps GAN convergence remains informal. Additional forward pass adds some overhead, and hyper-parameter λ still needs tuning (though less sensitive than baselines).",
        "future_research_directions": "1) Extend consistency regularization to high-resolution images, video, 3D, text, and audio GANs.\n2) Develop adaptive or learned augmentation policies to maximize semantic preservation without harming generation quality.\n3) Investigate theoretical connections between consistency, Lipschitz constraints, and GAN equilibrium to provide convergence guarantees.\n4) Combine with other regularizers (e.g., gradient penalties) or normalization schemes in a principled way.\n5) Apply consistency not only to discriminator outputs but also to generator feature spaces or latent codes, possibly improving mode coverage and diversity.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Consistency Regularization for Variational Auto-Encoders",
      "full_text": "Consistency Regularization for Variational Auto-Encoders Samarth Sinha Vector Institute University of Toronto Adji B. Dieng Google Brain Princeton University Abstract Variationalauto-encoders( vaes)areapowerfulapproachtounsupervisedlearning. They enable scalable approximate posterior inference in latent-variable models usingvariationalinference( vi). Avaepositsavariationalfamilyparameterizedby adeepneuralnetwork—calledan encoder—thattakesdataasinput. Thisencoderis shared across all the observations, which amortizes the cost of inference. However the encoder of avae has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to diﬀerent latent representations. This“inconsistency\"oftheencoderlowersthequalityofthelearnedrepresentations, especially for downstream tasks, and also negatively aﬀects generalization. In this paper,weproposearegularizationmethodtoenforceconsistencyin vaes. Theidea is to minimize the Kullback-Leibler (kl) divergence between the variational distri- bution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to anyvae. In our experiments we apply it to four diﬀerent vae variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the nouveau variational auto-encoder (nvae), our regularizationmethodyieldsstate-of-the-artperformanceon mnist,cifar-10,and celeba. Wealsoappliedourmethodto3Ddataandfounditlearnsrepresentations of superior quality as measured by accuracy on a downstream classiﬁcation task. Finally, we show our method can even outperform the triplet loss, an advanced and popular contrastive learning-based method for representation learning.1 1 Introduction Variationalauto-encoders( vaes)havesigniﬁcantlyimpactedresearchonunsupervisedlearning. They have been used in several areas, including density estimation (Kingma & Welling, 2013; Rezende et al., 2014), image generation (Gregor et al., 2015), text generation (Bowman et al., 2015; Fang et al., 2019), music generation (Roberts et al., 2018), topic modeling (Miao et al., 2016; Dieng et al., 2019), and recommendation systems (Liang et al., 2018).Vaes have also been used for diﬀerent representation learning problems such as semi-supervised learning (Kingma et al., 2014), anomaly detection (An & Cho, 2015; Zimmerer et al., 2018), language modeling Bowman et al. (2015), active learning (Sinha et al., 2019), continual learning (Achille et al., 2018), and motion prediction of agents (Walker et al., 2016). This widespread application ofvae representations makes it critical that we focus on improving them. vaesextend deterministic auto-encoders to probabilistic generative modeling. The encoder of avae parameterizes an approximate posterior distribution over latent variables of a generative model. The encoder is shared between all observations, which amortizes the cost of posterior inference. Once ﬁtted, the encoder of avae can be used to obtain low-dimensional representations of data, (e.g. for 1Code for this work can be found athttps://github.Com/sinhAsAm/CRVAE 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2105.14859v2  [cs.LG]  6 Jun 2022(a)  (b)  (c) Figure 1:Illustration of theinconsistencyproblem invaesand howcr-vaesaddress this problem. Thered dotscorrespondtotherepresentationsoffewimagesfrom mnist. Theblue dotscorrespondto therepresentationsofthetransformedimages. Thetransformationsusedherearerotations,translations, and scaling; they are semantics-preserving. The arrows connect the representations of any two pairs of an image and its transformation. The shorter the arrow, the better.(a): Thevae maps the two sets of images to diﬀerent areas in the latent space.(b): Even when trained with the original dataset augmentedwiththetransformedimages, the vae stillmapsthetwosetsofimagestodiﬀerentpartsin the latent space.(c): Thecr-vaemaps an image and its transformation to nearby areas in the latent space. downstream tasks.) The quality of these representations is therefore very important to a successful application ofvaes. Researchers have looked at ways to improve the quality of the latent representations ofvaes, often tackling the so-calledlatent variable collapseproblem—in which the approximate posterior distribu- tion induced by the encoder collapses to the prior over the latent variables (Bowman et al., 2015; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Fu et al., 2019). In this paper, we focus on a diﬀerent problem pertaining to the latent representations ofvaes for image data. Indeed, the encoder of a ﬁttedvae tends to map an image and a semantics-preserving transformation of that image to diﬀerent parts in the latent space. This “inconsistency\" of the encoder aﬀects the quality of the learned representations and generalization. We propose a method to enforce consistencyin vaes. Theideaissimpleandconsistsinmaximizingthelikelihoodoftheimageswhile minimizing the Kullback-Leibler (kl) divergence between the approximate posterior distribution induced by the encoder when conditioning on the image, on one hand, and its transformation, on the other hand. This regularization technique can be applied to anyvae variant to improve the quality of the learned representations and boost generalization performance. We call avae with this form of regularization, a consistency-regularized variational auto-encoder (cr-vae). Figure 1 illustrates the inconsistency problem ofvaesand howcr-vaesaddress this problem on mnist. The red dots are representations of a few images and the blue dots are the representations of their transformations. We applied semantics-preserving transformations: rotation, translation, and scaling. Thevae maps each image and its transformation to diﬀerent parts in the latent space as evidencedbythelongarrowsconnectingeachpair(a). Evenwhenweincludethetransformedimages to the data and ﬁt thevae the inconsistency problem still occurs (b). Thecr-vaedoes not suﬀer from the inconsistency problem; it maps each image and its transformation to nearby areas in the latent space, as evidenced by the short arrows connecting each pair (c). In our experiments (see Section 4), we apply the proposed technique to fourvae variants, the original vae (Kingma & Welling, 2013), the importance-weighted auto-encoder (iwae) (Burda et al., 2015), theβ-vae (Higgins et al., 2017), and the nouveau variational auto-encoder (nvae) (Vahdat & Kautz, 2020). We found, on four diﬀerent benchmark datasets, thatcr-vaesalways yield better representations and generalize better than their basevaes. In particular, consistency-regularized nouveau variational auto-encoders (cr-nvaes) yield state-of-the-art performance onmnist and cifar-10. We also appliedcr-vaesto 3D data where these conclusions still hold. 2 Method We consider a latent-variable modelpθ(x,z) =pθ(x|z) ·p(z), wherex denotes an observation and z is its associated latent variable. The marginalp(z) is a prior over the latent variable andpθ(x|z) 2is an exponential family distribution whose natural parameter is a function ofz parameterized byθ, e.g. through a neural network. Our goal is to learn the parametersθand a posterior distribution over the latent variables. The approach ofvaesis to maximize the evidence lower bound (elbo), a lower bound on the log marginal likelihood of the data, Lvae = elbo = Eqφ(z|x) [ log (pθ(x,z) qφ(z|x) )] (1) whereqφ(z|x) is an approximate posterior distribution over the latent variables. The idea of avae is to let the parameters of the distributionqφ(z|x) be given by the output of a neural network, with parametersφ, that takesx as input. The parametersθandφare then jointly optimized by maximizing a Monte Carlo approximation of theelbo using the reparameterization trick (Kingma & Welling, 2013). Consider a semantics-preserving transformationt(˜x|x) of datax (e.g. rotation or translation for images.) A good representation learning algorithm should provide similar latent representations forx and ˜x. This is not the case for thevae that maximizes Equation 1 and its variants. Once ﬁt to data, theencoderofa vae isunabletoyieldsimilarlatentrepresentationsforadata x anditstranformation ˜x (see Figure 1). This is because there is nothing in Equation 1 that forces this desideratum. We now propose a regularization method that ensuresconsistency of the encoder of avae. We call avae with such a regularization acr-vae. The regularization proposed is applicable to many variants of thevae such as theiwae (Burda et al., 2015), theβ-vae (Higgins et al., 2017), and the nvae (Vahdat & Kautz, 2020). In what follows, we use the standardvae, the one that maximizes Equation 1, as the basevae to regularize to illustrate the method. Consider an imagex. Denote byt(˜x|x) the random process by which we generate˜x, a semantics- preserving transformation ofx. We draw˜x from t(˜x|x) as follows: ˜x ∼t(˜x|x) ⇐⇒ϵ∼p(ϵ) and ˜x = g(x,ϵ). (2) Hereg(x,ϵ) is a semantics-preserving transformation of the imagex, e.g. translation with random lengthϵdrawn fromp(ϵ) =U[−δ,δ] for some thresholdδ. Acr-vaethen maximizes Lcr-vae(x) =Lvae(x) +Et(˜x|x) [Lvae(˜x)] −λ·R(x,φ) (3) where the regularization termR(x,φ) is R(x,φ) =Et(˜x|x) [kl (qφ(z|˜x)||qφ(z|x))] . (4) Maximizing the objective in Equation 3 maximizes the likelihood of the data and their augmentations while enforcing consistency throughR(x,φ). MinimizingR(x,φ), which only aﬀects the encoder (with parametersφ), forces each observation and the corresponding augmentations to lie close to each other in the latent space. The hyperparameterλ≥0 controls the strength of this constraint. The objective in Equation 3 is intractable but we can easily approximate it using Monte Carlo with the reparameterization trick. In particular, we approximate the regularization term with one sample fromt(˜x|x) and make the dependence to this sample explicit using the notationR(x,˜x,φ). Algorithm 1 illustrates this in greater detail. Although we show the application of consistency regularization using thevae that maximizes theelbo,Lvae(·) in Equation 3 can be replaced with anyvae objective. 3 Related Work Applying consistency regularization tovaes, as we do in this paper, has not been previously explored. Consistency regularization is a widely used technique for semi-supervised learning (Bachman et al., 2014; Sajjadi et al., 2016; Laine & Aila, 2016; Miyato et al., 2018; Xie et al., 2019). The core idea behind consistency regularization for semi-supervised learning is to force classiﬁers to learn representations that are insensitive to semantics-preserving changes to images, so as to improve classiﬁcation of unlabeled images. Examples of semantics-preserving changes used in the literature include rotation, zoom, translation, crop, or adversarial attacks. Consistency is often enforced by minimizing theL2 distance between a classiﬁer’s logit output for an image and the logit output for its semantics-preserving transformation (Sajjadi et al., 2016; Laine & Aila, 2016), or by minimizing 3Algorithm 1:Consistency Regularization for Variational Autoencoders input : Datax, consistency regularization strengthλ, latent space dimensionality K Initialize parametersθ,φ for iteration t= 1,2,... do Draw minibatch of observations{xn}B n=1 for n= 1,...,B do Transform the data:ϵn ∼p(ϵn) and ˜xn = T(xn,ϵn) Get variational mean and variance for the data: µn = W⊤NN(xn; φ) +a and σn = softplus(Q⊤NN(xn; φ) +b) Get Ssamples from the variational distribution when conditioning onxn: η(s) ∼N(0,I) and z(s) n = µn + η(s) ·σn fors= 1,...,S Get variational mean and variance for the transformed data: ˜µn = W⊤NN(˜xn; φ) +a and ˜σn = softplus(Q⊤NN(˜xn; φ) +b) Get Ssamples from the variational distribution when conditioning on˜xn: η(s) ∼N(0,I) and ˜z(s) n = ˜µn + η(s) ·˜σn fors= 1,...,S end Compute Lvae(x): Lvae(x) ≈ 1 B ∑B n=1 1 S ∑S s=1 [ log pθ(xn,z(s) n ) −log qφ(z(s) n |xn) ] Compute Lvae(˜x): Lvae(˜x) ≈ 1 B ∑B n=1 1 S ∑S s=1 [ log pθ(˜xn,˜z(s) n ) −log qφ(˜z(s) n |˜xn) ] Compute KL consistency regularizer: R(x,˜x,φ) =1 2 ∑K k=1 ( ˜σ2 nk+(˜µnk−µnk)2 σ2 nk −1 + 2·log σnk ˜σnk ) Compute ﬁnal loss: Lcr-vae(x) =Lvae(x) +Lvae(˜x) −λ·R(x,˜x,φ) Backpropagate throughL(x,θ,φ ) =−Lcr-vae(x) and take a gradient step forθand φ end the kl divergence between the classiﬁer’s label distribution induced by the image and that of its tranformation (Miyato et al., 2018; Xie et al., 2019). More recently, consistency regularization has been applied to generative adversarial networks (gans)(Goodfellowetal.,2014). IndeedWeietal.(2018)andZhangetal.(2020)showthatapplying consistencyregularizationonthediscriminatorofa gan—alsoaclassiﬁer—cansubstantiallyimprove its performance. Theideawedevelopinthispaperdiﬀersfromtheworksaboveintwoways. First,itappliesconsistency regularization tovaesfor image data. Second, it leverages consistency regularization, not in the label or logit space, as done in the works mentioned above, but in the latent space. Although diﬀerent, consistency regularization forvaesrelates to works that study ways to constrain the sensitivity of encoders to various perturbations. For example, denoising auto-encoders (daes) and their variants (Vincent et al., 2008, 2010) corrupt an imagex intox′, typically using Gaussian noise, and then minimize the distance between the reconstruction ofx′and theun-corrupted imagex. The motivation is to learn representations that are insensitive to the added noise. Our work diﬀers in that we do not constrain the decoder to recover the original image from the corrupted image but, rather, to constrain the encoder to recover the latent representation of the original image from the corrupted image via akl divergence minimization constraint. Contractive auto-encoders (caes) (Rifai et al., 2011) share a similar goal withcr-vaes. Acae is an auto-encoder whose encoder is constrained by minimizing the norm of the Jacobian of the output of the encoder with respect to the input image. This norm constraint on the Jacobian forces the representations learned by the encoder to be insensitive to changes in the input. Our work diﬀers in several main ways. First,cr-vaesare not deterministic auto-encoders, contrary tocaes. We can easily sample from acr-vae, as for anyvae, which is not the case for acae. Second, acae does 4Table 1:cr-vaeslearn better representations than their basevaeson all three benchmark datasets. Although ﬁtting the basevae with augmentations does improve the representations, adding the consistency regularization further improves the quality of these learned representations. The value of βfor theβ-vae is inside the parentheses. mnist omniglot celeba Method MI AU MI AU MI AU vae 124.5 ±1.1 36 ±0.8 105 .4 ±1.2 50 ±0.0 33 .8 ±0.2 32 ±0.9 vae + Aug 125.9 ±0.2 42 ±0.5 105 .9 ±0.7 50 ±0.0 34 .1 ±0.8 33 ±0.9 cr-vae 126.3 ±0.9 47 ±0.5 107.8 ±1.1 50 ±0.0 34.9 ±0.5 33 ±1.2 iwae 127.1 ±0.7 39 ±0.5 110 .3 ±1.1 50 ±0.0 36 .9 ±0.5 36 ±1.6 iwae+Aug 129.0 ±0.9 45 ±0.8 112 .9 ±0.7 50 ±0.0 37 .0 ±0.2 36 ±1.2 cr-iwae 129.7 ±1.0 50 ±0.0 115.3 ±0.8 50 ±0.0 38.4 ±0.5 36 ±1.9 β-vae (0.5) 284.3 ±1.1 50 ±0.0 143 .4 ±1.0 50 ±0.0 75 .8 ±0.5 49 ±0.5 β-vae (0.5) + Aug 289.3 ±1.0 50 ±0.0 159 .6 ±1.3 50 ±0.0 75 .7 ±0.3 49 ±0.0 β-cr-vae(0.5) 291.9 ±0.7 50 ±0.0 169.5 ±0.5 50 ±0.0 77.1 ±0.1 50 ±0.0 β-vae (10) 6.3 ±0.6 8 ±1.7 1 .4 ±0.2 4 ±0.9 3 .6 ±0.3 7 ±0.8 β-vae (10) + Aug 6.5 ±0.5 9 ±1.1 1 .6 ±0.2 4 ±0.5 3 .7 ±0.1 7 ±0.0 β-cr-vae(10) 6.9 ±0.6 10 ±0.5 1.6 ±0.1 4 ±0.5 3.7 ±0.4 9 ±0.9 notapplytransformationstotheinputimage,whichlimitsthesensitivitiesitcanlearntolimittothose exhibited in the training set. Finally,caesuse the Jacobian to impose a consistency constraint, which are not as easy to compute as thekl divergence we use on the variational distribution induced by the encoder. 4 Empirical Study In this section we show that acr-vaeimproves the learned representations of its basevae and positively aﬀects generalization performance We also show that the proposed regularization method is amenable to diﬀerentvae variants by applying it not only to the originalvae but also to theiwae, the β-vae, and thenvae. We showcase the importance of the KL regularization term by conducting an ablation study. We found that only regularizing with data augmentation improves performance but that accounting for thekl term (λ> 0) further improves the quality of the learned representations and generalization. We will conduct three sets of experiments. In the ﬁrst experiment, we will apply the regularization method proposed in this paper to standardvaessuch as the originalvae, theiwae, and theβ-vae. We usemnist, omniglot, andceleba as datasets for this experiment. Forceleba, we choose the 32x32 resolution for this experiment. Our results show that adding consistency regularization always improves upon the basevae, both in terms of the quality of the learned representations and generalization. We conduct an ablation study and also report performance of the diﬀerentvae variants above when they are ﬁtted with the original data and their augmentations. The results from this ablation highlight the importance of settingλ> 0. Inthesecondsetofexperimentsweapplyourmethodtoalarge-scale vae, thelatestnvae (Vahdat& Kautz, 2020). We usemnist,cifar-10, andceleba as datasets for this experiment. We increased the resolution for theceleba dataset for this experiment to64x64. We reach the same conclusions as for the ﬁrst sets of experiments;cr-vaesimprove the learned representations and generalization of their basevaes. In this particular setting, thecr-nvaeachieves state-of-the-art generalization performanceonboth mnist andcifar-10. Thisstate-of-the-artperformancecouldn’tbereachsimply by training thenvae with augmentations, as our results show. Finally,inathirdsetofexperiments,weapplyourregularizationtechniquetoa3Dpoint-clouddataset called ShapeNet (Chang et al., 2015). We adapt a high-performing auto-encoding method called FoldingNet (Yang et al.,2018) to itsvae counterpart and apply the methodwe described in thispaper to thatvae variant on the ShapeNet dataset. We found that adding consistency regularization yields better learned representations. We next describe in great detail the set up for each of these experiments and the results showcasing the usefulness of the regularization method we propose in this paper. 5Table 2:cr-vaeslearn representations that yield higher accuracy on downstream classiﬁcation than their basevaes. These results correspond to the accuracy from a linear classiﬁer that was ﬁtted on the training. We fed this classiﬁer with the representations learned by each method. On bothmnist and cifar-10, cr-vaesyield higher accuracy. Method mnist cifar -10 vae 98.5 32.6 vae+Aug 98.9 40.1 cr-vae 99.4 44.7 iwae 98.6 35.8 iwae+Aug 99.9 37.1 cr-iwae 99.9 44.8 β-vae (0.5) 97.6 27.0 β-vae (0.5)+Aug 98.7 27.6 β-cr-vae(0.5) 98.9 30.0 β-vae (10) 99.4 36.5 β-vae (10)+Aug 99.6 42.1 β-cr-vae(10) 99.6 46.1 Table 3: cr-vaes generalize better than their basevaes on almost all cases; they achieve lower negative log-likelihoods. Although training the basevaeswith the augmented data improves general- ization, adding the consistency regularization term further improves generalization performance. Method mnist omniglot celeba vae 83.7 ±0.3 128 .2 ±0.8 66 .1 ±0.2 vae + Aug 82.8 ±0.4 125 .7 ±0.2 66 .0 ±0.2 cr-vae 81.2 ±0.2 124.1 ±0.1 65.9 ±0.2 iwae 81.7 ±0.3 127 .5 ±0.5 65 .3 ±0.1 iwae+Aug 80.4 ±0.2 125 .0 ±0.6 65 .3 ±0.1 cr-iwae 79.7 ±0.3 123.6 ±0.5 65.0 ±0.2 β-vae (0.5) 92.6 ±0.3 137 .1 ±0.2 68 .7 ±0.2 β-vae (0.5) + Aug 90.0 ±0.5 134 .6 ±0.5 68 .8 ±0.2 β-cr-vae(0.5) 85.7 ±0.6 132.5 ±0.3 68.2 ±0.1 β-vae (10) 126.1 ±1.8 157 .5 ±1.1 92 .7 ±0.5 β-vae (10) + Aug 127.1 ±1.0 157.3 ±0.5 92 .7 ±0.3 β-cr-vae(10) 126.2 ±0.5 157 .6 ±0.6 92.6 ±0.1 4.1 Application to standardvaes on benchmark datasets We apply consistency regularization, as described in this paper, to the originalvae, theiwae, and the β-vae. We now describe the set up and results for this experiment. Datasets. We study three benchmark datasets that we brieﬂy describe below. We ﬁrst consider mnist. mnist is a handwritten digit recognition dataset with60,000 images in the training set and 10,000 images in the test set (LeCun, 1998). We form a validation set of10,000 images randomly sampled from the training set. We also consideromniglot, a handwritten alphabet recognition dataset (Lake et al., 2011). This dataset is composed of19,280 images. We use16,280 randomly sampled images for training and 1,000 for validation and the remaining2,000 samples for testing. Finallyweconsider celeba. Itisadatasetoffaces,consistingof 162,770 imagesfortraining, 19,867 images for validation, and19,962 images for testing (Liu et al., 2018). We set the resolution to32x32 for this experiment. Transformations t(˜x|x). We consider three transformations variants for image datat(˜x|x). The ﬁrstrandomlytranslatesanimage [−2,2] pixelsinanydirection. Thesecondtransformationrandomly 6Table 4:The regularization termλaﬀects both generalization performance and the quality of the learned representations. Many values ofλperform better than the basevae. However a large enough value ofλ, e.g.λ= 1, can lead to worse performance than the basevae because for large values ofλ the regularization term takes over the data-term in the objective function. λ MI AU NLL vae −− 124.5 36 83 .7 cr-vae 0.001 125 .0 38 83 .5 cr-vae 0.01 125 .9 41 82 .4 cr-vae 0.1 126.3 47 81.2 cr-vae 1 124 .3 47 83 .9 Table 5:The choice of augmentation aﬀects both generalization performance and the quality of the learned representations. Jointly using all augmentations works best. Augmentation MI AU NLL Rotations only 125.8 45 82 .1 Translations only 126.1 45 81 .9 Scaling only 125.1 42 82 .7 All 126.3 47 81.2 rotates an image uniformly in[−15,15] degrees clockwise. Finally the third transformation randomly scales an image by a factor uniformly sampled from[0.9,1.1]. Evaluation metrics.Theregularizationmethodweproposeinthispaperismainlyaimedatimproving the learned representations ofvaes. To assess these representations we use three metrics: mutual information, number of active latent units, and accuracy on a downstream classiﬁcation task. We also evaluate the eﬀect of the proposed method on generalization to unseen data. For that we also report negative log-likelihood. We deﬁne each of these metrics next. Mutual information (MI).The ﬁrst quality metric is the mutual informationI(z; x) between the observations and the latents under the joint distribution induced by the encoder, I(z; x) =Epd(x) [KL(qφ(z|x)||p(z)) −kl(qφ(z)||p(z))] (5) where pd(x) is the empirical data distribution andqφ(z) is theaggregated posterior, the marginal overz induced by the joint distribution deﬁned bypd(x) and qφ(z|x). The mutual information is intractable but we can approximate it with Monte Carlo. Higher mutual information corresponds to more interpretable latent variables. Number of active latent units (AU).The second quality metrics we consider is the number of active latent units (AU). It is deﬁned in Burda et al. (2015) and measures the “activity\" of a dimension of the latent variablesz. A latent dimension is “active\" if Covx(Eu∼qφ(u|x)) >δ (6) where δ is a threshold deﬁned by the user. For our experiments we setδ = 0.01. The higher the number of latent active units, the better the learned representations. Accuracy on downstream classiﬁcation.This metric is calculated by ﬁtting a givenvae, taking the learned representations for each data in the test set and computing the accuracy from the prediction of the labels of the images in that same test set by a classiﬁer ﬁtted on the training set. This metric is only applicable to labelled datasets. Negative log-likelihood.We use negative held-out log-likelihood to assess generalization. Consider an unseen datax∗, its negative held-out log-likelihood under the ﬁtted model is log pθ(x∗) =−log ( Eqφ(z|x∗) [pθ(x∗,z) qφ(z|x∗) ]) . (7) 7Table 6: The cr-vaeoutperforms a popular and advanced contrastive learning technique called triplet losson both generalization performance and quality of learned representations. Method MI AU NLL vae 124.5 36 83 .7 vae + augmentations 125.9 42 82 .8 vae + triplet loss 124.9 39 83 .1 cr-vae 126.3 47 81.2 This is intractable and we approximate it using Monte Carlo, log pθ(x∗) ≈−log 1 S S∑ s=1 pθ(x∗,z(s)) qφ(z(s)|x∗) (8) where z(1),..., z(S) ∼qφ(z|x∗). Settings. The vaesare built on the same architecture as Tolstikhin et al. (2017). The networks are trainedwiththeAdamoptimizerwithalearningrateof 10−4 (Kingma&Ba,2014)andtrainedfor 100 epochs with a batch size of64. We set the dimensionality of the latent variables to50, therefore the maximumnumberofactivelatentunitsinthelatentspaceis 50. Wefoundλ= 0.1 tobebestaccording to cross-validation using held-out log-likelihood and exploring the range[1e−4,1.0] datasets. In an ablation study we exploreλ = 0. For theβ-vae we setλ = 0.1 ·β and study bothβ = 0.1 and β = 10, two regimes under which theβ-vae performs qualitatively very diﬀerently (Higgins et al., 2017). All experiments were done ona GPU cluster consisting ofNvidia P100 andRTX. The training took approximately 1 day for most experiments. Results. Table 1 shows that on all the three benchmark datasets all the diﬀerentvae variants we studied, consistency regularization as developed in this paper always improves the quality of the learned representations as measured by mutual information and the number of active latent units. These results are conﬁrmed by the numbers shown in Table 2 wherecr-vaesalways lead to better accuracy on downstream classiﬁcation. We proposed consistency regularization as a way to improve the quality of the learned representations. Incidentally, Table 3 also shows that it can improve generalization as measured by negative log- likelihood. Ablation Study. We now look at the impact of each factor that goes into the regularization method we introduced in this paper usingmnist. We test the impact of the regularization termλand the impact of the choice of augmentation on all metrics. Table 4 and Table 5 show the results. Table 4 shows that even small consistency regularization (a smallλvalue) results in improvement over the basevae but that a large enoughλvalue can hurt performance. Table 5 shows that rotations and translations are more important than scaling, but the combination of all three augmentations works best forcr-vaes. Comparison to Contrastive Learning.We look at howcr-vaes compare against a popular and advanced contrastive-learning-based technique, thetriplet loss(Schroﬀ et al., 2015) usingmnist. Table 6 shows that thecr-vaeoutperforms the triplet loss on both generalization performance and quality of learned representations. Table 6 also conﬁrms existing literature showing simply applying augmentations can outperform complex contrastive learning-based methods such as the triplet loss (Kostrikov et al., 2020; Sinha & Garg, 2021). 4.2 Application to the large-scalenvae on benchmark datasets Along with standard VAE variants, we also experiment with a large scale state-of-the-artvae, the nvae(Vahdat & Kautz, 2020). Similar to before, we simply add consistency regularization using the image-based augmentations techniques to the NVAE model and experiment on benchmark datasets: mnist (LeCun, 1998),cifar-10 (Krizhevsky et al., 2009) andceleba (Liu et al., 2018). The results for large scale generative modeling are tabulated in Table 8 and Table 7, where we see that usingcr-nvaewe are able to learn representations that yield better accuracy on downstream 8Table 7:The cr-nvaeslearns better representations than the basenvae as measured by accuracy on a downstream classiﬁcation on bothmnist and cifar-10. We get to this same conclusion when lookingatthenumberofactiveunitsasanindicatorforthequalityofthelearnedlatentrepresentations; cr-nvaerecovers226 units whereasnvae recovers211 units. Method mnist cifar -10 nvae 99.9 57.9 nvae+Aug 99.9 66.4 cr-nvae 99.9 71.4 Table 8: Large-scale experiments withnvaes with and without consistency-regularization on 3 benchmark datasets: dynamically binarizedmnist, cifar-10 andceleba. We report generalization using negative log-likelihood onmnist and bits per dim oncifar-10 andceleba. On all datasets consistency regularization improves generalization performance. In particularcr-nvae achieves state-of-the-art performance onmnist and cifar-10. mnist (28 ×28) cifar-10 (32 ×32) celeba (64 ×64) nvae 78.19 2.91 2.03 nvae+Aug 77.53 2.70 1.96 cr-nvae 76.93 2.51 1.86 Figure 2: Interpolation between two samples of a lamp, airplane and table using a trained CR- FoldingNet trained on the ShapeNet dataset. The CR-FoldingNet is able to learn an interpretable latent space. classiﬁcation and set new state-of-the-art values on each of the datasets, improving upon the baseline log-likelihood values. This shows the ability of consistency regularization to work at scale on challenging generative modeling tasks. 4.3 Application to the FoldingNet on 3D point-cloud data Along with working with image data, we additionally experiment with 3D point cloud data using a FoldingNet Yang et al. (2018) and the ShapeNet dataset Chang et al. (2015) which consists of 55 distinct object classes. FoldingNet learns a deep AutoEncoder to learn unsupervised representations from the point cloud data. To add consistency regularization, we ﬁrst substitute the AutoEncoder to a 9Table 9:The FoldingNet yields higher accuracy when paired with consistency regularization on the ShapeNet dataset. The results shown here correspond to a FoldingNet that was trained with augmented data, the same used to apply consistency regularization. As can be seen from these results, enforcing consistency through KL as we do in this paper leads to representations that perform well on a downstream classiﬁcation. Here the classiﬁer used is a linear SVM. We also report mean reconstruction error through Chamfer distance where the same conclusion holds. Method Accuracy Reconstruction Loss Folding Net (Aug) 82.5% 0.0355 CR-Folding Net 84.6% 0.0327 vae by adding the KL term from the ELBO to the baseline FoldingNet. We then add the additional consistency regularization KL term to the latent space of FoldingNet. For the ShapeNet point cloud data, we perform data augmentation using a similar scheme to what we did for the previous experiments, we randomly translate, rotate and add jitter to the(x,y,z ) coordinates of the point cloud data. We follow the same scheme detailed in FoldingNet (Yang et al., 2018). We train both the FoldingNet turned in avae and the CR-FoldingNet with these augmentations. To train CR-FoldingNet, we additionally apply the consistency regularization term as proposed in Equation 3. The results on the validation set for reconstruction (as measured by Chamfer distance) and accuracy are shown in Table 9. Wealsovisualizethepointcloudsreconstructionsandinterpolationsbetween3diﬀerentobjectclasses using a CR-FoldingNet in Figure 2. We perform 4 interpolation steps for each of the objects, to highlight the interpretable learned latent space. Additionally, we perform the same interpolation on the baseline FoldingNet model. We show these interpolations in the appendix. 5 Conclusion We proposed a simple regularization technique to constrain encoders ofvaesto learn similar latent representations for an image and a semantics-preserving transformation of the image. The idea consists in maximizing the likelihood of the pair of images while minimizing thekl divergence between the variational distribution induced by the encoder when conditioning on the image on one hand, and its transformation, on the other hand. We applied this technique to severalvae variants on severaldatasets,includinga3Ddataset. Wefounditalwaysleadstobetterlearnedrepresentationsand also better generalization to unseen data. In particular, when applied to thenvae, the regularization technique we developed in this paper yields state-of-the-art results onmnist and cifar-10. Broader Impact Inthispaper,weproposeasimplemethodthatperformsaKL-basedconsistencyregularizationscheme using data augmentation forvaes. The broader impact of the study includes practical applications such as graphics and computer vision applications. The method we propose improves the learned representations ofvaes, and as an artifact, also improves their generalization to unseen data. In this regard, any implications ofvaesalso apply to this work. For example, the generative model ﬁt by avae may be used to generate artiﬁcial data such as images, text, and 3D objects. Biases may arise as a result of poor data selection. Furthermore, text generated from generative systems may amplify harmful speech contained in the data. However, the method we propose can also improve the performance ofvaeswhen used in certain practical domains as we discussed in the introduction of the paper. 6 Acknowledgements We thank Kevin Murphy, Ben Poole, and Augustus Odena for their comments on this work. 10References Achille, A., Eccles, T., Matthey, L., Burgess, C. P., Watters, N., Lerchner, A., and Higgins, I. Life-long disentangled representation learning with cross-domain latent homologies.arXiv preprint arXiv:1808.06508, 2018. An, J. and Cho, S. Variational autoencoder based anomaly detection using reconstruction probability.Special Lecture on IE, 2(1), 2015. Bachman, P., Alsharif, O., and Precup, D. Learning with pseudo-ensembles. InAdvances in neural information processing systems, pp. 3365–3373, 2014. Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., and Bengio, S. Generating sentences from a continuous space.arXiv preprint arXiv:1511.06349, 2015. Burda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015. Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al. Shapenet: An information-rich 3d model repository.arXiv preprint arXiv:1512.03012, 2015. Dieng, A. B., Kim, Y., Rush, A. M., and Blei, D. M. Avoiding latent variable collapse with generative skip models. arXiv preprint arXiv:1807.04863, 2018. Dieng,A.B.,Ruiz,F.J.,andBlei,D.M. Topicmodelinginembeddingspaces. arXivpreprintarXiv:1907.04907 , 2019. Fang, L., Li, C., Gao, J., Dong, W., and Chen, C. Implicit deep latent variable models for text generation.arXiv preprint arXiv:1908.11527, 2019. Fu, H., Li, C., Liu, X., Gao, J., Celikyilmaz, A., and Carin, L. Cyclical annealing schedule: A simple approach to mitigating kl vanishing.arXiv preprint arXiv:1903.10145, 2019. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. InAdvances in neural information processing systems, pp. 2672–2680, 2014. Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. Draw: A recurrent neural network for image generation.arXiv preprint arXiv:1502.04623, 2015. Hadjeres,G.,Nielsen,F.,andPachet,F. Glsr-vae: Geodesiclatentspaceregularizationforvariationalautoencoder architectures. In2017 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1–7. IEEE, 2017. He, J., Spokoyny, D., Neubig, G., and Berg-Kirkpatrick, T. Lagging inference networks and posterior collapse in variational autoencoders.arXiv preprint arXiv:1901.05534, 2019. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., andLerchner, A. beta-vae: Learning basic visual concepts with a constrained variational framework.Iclr, 2(5):6, 2017. Jun, H., Child, R., Chen, M., Schulman, J., Ramesh, A., Radford, A., andSutskever, I. Distributionaugmentation for generative modeling. InInternational Conference on Machine Learning, pp. 5006–5019. PMLR, 2020. Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., and Rush, A. M. Semi-amortized variational autoencoders. arXiv preprint arXiv:1802.02550, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2014. Kingma, D. P. and Welling, M. Auto-encoding variational bayes.arXiv preprint arXiv:1312.6114, 2013. Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. Semi-supervised learning with deep generative models. arXiv preprint arXiv:1406.5298, 2014. Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.arXiv preprint arXiv:2004.13649, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Laine, S. and Aila, T. Temporal ensembling for semi-supervised learning.arXiv preprint arXiv:1610.02242, 2016. Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J. One shot learning of simple visual concepts. In Proceedings of the annual meeting of the cognitive science society, volume 33, 2011. 11LeCun, Y. The mnist database of handwritten digits.http://yann. lecun. com/exdb/mnist/, 1998. Liang, D., Krishnan, R. G., Hoﬀman, M. D., and Jebara, T. Variational autoencoders for collaborative ﬁltering. InProceedings of the 2018 World Wide Web Conference, pp. 689–698, 2018. Liu, Z., Luo, P., Wang, X., and Tang, X. Large-scale celebfaces attributes (celeba) dataset.Retrieved August, 15: 2018, 2018. Miao, Y., Yu, L., and Blunsom, P. Neural variational inference for text processing. InInternational conference on machine learning, pp. 1727–1736, 2016. Miyato, T., Maeda, S.-i., Ishii, S., and Koyama, M. Virtual adversarial training: a regularization method for supervised and semi-supervised learning.IEEE transactions on pattern analysis and machine intelligence, 2018. Osada, G., Ahsan, B., Bora, R. P., and Nishide, T. Regularization with latent space virtual adversarial training. InEuropean Conference on Computer Vision, pp. 565–581. Springer, 2020. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models.arXiv preprint arXiv:1401.4082, 2014. Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Contractive auto-encoders: Explicit invariance during feature extraction. 2011. Roberts, A., Engel, J., Raﬀel, C., Hawthorne, C., and Eck, D. A hierarchical latent vector model for learning long-term structure in music.arXiv preprint arXiv:1803.05428, 2018. Sajjadi, M., Javanmardi, M., and Tasdizen, T. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. InNeurIPS, 2016. Schroﬀ, F., Kalenichenko, D., and Philbin, J. Facenet: A uniﬁed embedding for face recognition and clustering. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 815–823, 2015. Sinha, S. and Garg, A. S4rl: Surprisingly simple self-supervision for oﬄine reinforcement learning.arXiv preprint arXiv:2103.06326, 2021. Sinha, S., Ebrahimi, S., and Darrell, T. Variational adversarial active learning. InProceedings of the IEEE International Conference on Computer Vision, pp. 5972–5981, 2019. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017. Vahdat, A. and Kautz, J. Nvae: A deep hierarchical variational autoencoder.arXiv preprint arXiv:2007.03898, 2020. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103, 2008. Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A. Stackeddenoisingautoencoders: Learning usefulrepresentationsinadeepnetworkwithalocaldenoisingcriterion. Journalofmachinelearningresearch , 11(Dec):3371–3408, 2010. Walker, J., Doersch, C., Gupta, A., and Hebert, M. An uncertain future: Forecasting from static images using variational autoencoders. InEuropean Conference on Computer Vision, pp. 835–851. Springer, 2016. Wei, X., Gong, B., Liu, Z., Lu, W., and Wang, L. Improving the improved training of wasserstein gans: A consistency term and its dual eﬀect.arXiv preprint arXiv:1803.01541, 2018. Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019. Yang, Y., Feng, C., Shen, Y., and Tian, D. Foldingnet: Point cloud auto-encoder via deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 206–215, 2018. Zhang, H., Zhang, Z., Odena, A., and Lee, H. Consistency regularization for generative adversarial networks. 2020. Zimmerer, D., Kohl, S. A., Petersen, J., Isensee, F., and Maier-Hein, K. H. Context-encoding variational autoencoder for unsupervised anomaly detection.arXiv preprint arXiv:1812.05941, 2018. 12",
      "references": [
        "Life-long disentangled representation learning with cross-domain latent homologies.",
        "Variational autoencoder based anomaly detection using reconstruction probability.",
        "Learning with pseudo-ensembles.",
        "Generating sentences from a continuous space.",
        "Importance weighted autoencoders.",
        "Shapenet: An information-rich 3d model repository.",
        "Avoiding latent variable collapse with generative skip models.",
        "Topic modeling in embedding spaces.",
        "Implicit deep latent variable models for text generation.",
        "Cyclical annealing schedule: A simple approach to mitigating kl vanishing.",
        "Generative adversarial nets.",
        "Draw: A recurrent neural network for image generation.",
        "GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures.",
        "Lagging inference networks and posterior collapse in variational autoencoders.",
        "beta-VAE: Learning basic visual concepts with a constrained variational framework.",
        "Distribution augmentation for generative modeling.",
        "Semi-amortized variational autoencoders.",
        "Adam: A method for stochastic optimization.",
        "Auto-encoding variational bayes.",
        "Semi-supervised learning with deep generative models.",
        "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.",
        "Learning multiple layers of features from tiny images.",
        "Temporal ensembling for semi-supervised learning.",
        "One shot learning of simple visual concepts.",
        "Variational autoencoders for collaborative filtering.",
        "Large-scale celebfaces attributes (celeba) dataset.",
        "Neural variational inference for text processing.",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning.",
        "Regularization with latent space virtual adversarial training.",
        "Stochastic backpropagation and approximate inference in deep generative models.",
        "Contractive auto-encoders: Explicit invariance during feature extraction.",
        "A hierarchical latent vector model for learning long-term structure in music.",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning.",
        "FaceNet: A unified embedding for face recognition and clustering.",
        "S4RL: Surprisingly simple self-supervision for offline reinforcement learning.",
        "Variational adversarial active learning.",
        "Wasserstein auto-encoders.",
        "Nvae: A deep hierarchical variational autoencoder.",
        "Extracting and composing robust features with denoising autoencoders.",
        "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.",
        "An uncertain future: Forecasting from static images using variational autoencoders.",
        "Improving the improved training of Wasserstein GANs: A consistency term and its dual effect.",
        "Unsupervised data augmentation for consistency training.",
        "FoldingNet: Point cloud auto-encoder via deep grid deformation.",
        "Consistency regularization for generative adversarial networks.",
        "Context-encoding variational autoencoder for unsupervised anomaly detection."
      ],
      "meta_data": {
        "arxiv_id": "2105.14859v2",
        "authors": [
          "Samarth Sinha",
          "Adji B. Dieng"
        ],
        "published_date": "2021-05-31T10:26:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Consistency-Regularized Variational Auto-Encoder (CR-VAE), a generic regularization that encourages an encoder to map an image and any semantics-preserving transformation of it to similar latent distributions by adding a KL penalty between the two posteriors. The method is plug-and-play for any VAE variant and consistently improves representation quality and generalization, achieving state-of-the-art likelihoods when applied to NVAE on MNIST and CIFAR-10.",
        "methodology": "Augment each training sample x with a randomly sampled transformation \\tilde{x}=g(x,ϵ) (translation, rotation, scaling, etc.). Optimize modified objective: ELBO(x)+ELBO(\\tilde{x})−λ·KL(q_φ(z|\\tilde{x}) || q_φ(z|x)). Monte-Carlo estimates with reparameterization; same encoder processes both inputs. Applicable to VAE, IWAE, β-VAE, NVAE and a VAE version of FoldingNet for 3-D point clouds.",
        "experimental_setup": "Datasets: MNIST, Omniglot, CelebA (32×32 and 64×64), CIFAR-10, ShapeNet point clouds. Models: vanilla VAE, IWAE, β-VAE (β=0.5 and 10), NVAE, FoldingNet-VAE. Augmentations: ±2-pixel translation, ±15° rotation, scaling 0.9–1.1 for images; translation, rotation and jitter for point clouds. Metrics: mutual information, number of active latent units, downstream linear-classifier accuracy, negative log-likelihood or bits-per-dimension, Chamfer distance for 3-D. Baselines include training with augmentations only and triplet-loss contrastive learning. Hyper-parameter λ chosen via validation (0.1 default). Training: Adam, LR 1e-4, 100 epochs, batch 64; GPUs (P100/RTX).",
        "limitations": "Effectiveness depends on availability of known semantics-preserving transformations; method tested only on images and 3-D point clouds. Choice of regularization weight λ is sensitive—too large degrades performance. Additional computation due to doubled forward passes and KL penalty. Does not address latent variable collapse explicitly and assumes the encoder can approximate both posteriors well. Evaluation limited to unsupervised settings; impact on conditional or sequential VAEs untested.",
        "future_research_directions": "1) Learn or adapt transformations instead of hand-crafted augmentations. 2) Adaptive or scheduled λ to balance ELBO and consistency automatically. 3) Extend consistency regularization to other modalities (audio, text) and sequential VAEs. 4) Combine with latent collapse mitigation techniques or contrastive objectives. 5) Theoretical analysis of consistency’s effect on disentanglement and generalization. 6) Apply to semi-supervised VAEs or other generative families such as normalizing flows and diffusion models.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Towards Better Robust Generalization with Shift Consistency Regularization",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Samples With Low Loss Curvature Improve Data Efficiency",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Provable Dynamic Fusion for Low-Quality Multimodal Data",
      "full_text": "Provable Dynamic Fusion for Low-Quality Multimodal Data Qingyang Zhang 1 Haitao Wu1 Changqing Zhang 1 2 Qinghua Hu 1 2 Huazhu Fu 3 Joey Tianyi Zhou 3 4 Xi Peng 5 Abstract The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dy- namic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still no- tably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion frame- work from the generalization perspective. We proceed to reveal that several uncertainty estima- tion solutions are naturally available to achieve robust multimodal fusion. Then a novel multi- modal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classifi- cation accuracy and model robustness. Extensive experimental results on multiple benchmarks can support our findings. 1. Introduction Our perception of the world is based on multiple modalities, e.g., touch, sight, hearing, smell and taste. With the devel- opment of sensory technology, we can easily collect diverse forms of data for analysis. For example, multi-sensor in autonomous driving and wearable electrical devices (Xiao et al., 2020; Wen et al., 2022), or various examinations in 1College of Intelligence and Computing, Tianjin University, Tianjin, China 2Tianjin Key Lab of Machine Learning, Tianjin Uni- versity, Tianjin, China 3Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore 4Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore5College of Computer Science, Sichuan University, Chengdu, China. Corre- spondence to: Changqing Zhang <zhangchangqing@tju.edu.cn>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). medical diagnosis and treatment (Qiu et al., 2022; Acosta et al., 2022). Intuitively, fusing information from different modalities offers the possibility of exploring cross-modal correlation and gaining better performance. However, con- ventional fusion methods have largely overlooked the un- reliable quality of multimodal data. In real-world settings, the quality of different modalities usually varies due to un- expected environmental issues. Some recent studies have shown both empirically and theoretically that multimodal fusion may fail on low-quality multimodal data, e.g., im- balanced (Wang et al., 2020a; Peng et al., 2022; Huang et al., 2022), noisy or even corrupted (Huang et al., 2021b) multimodal data. Empirically, it is recognized that multi- modal models cannot always outperform unimodal models especially in a high noise (Scheunders & De Backer, 2007; Eitel et al., 2015; Silva et al., 2022) or imbalanced modality quality (Wu et al., 2022; Peng et al., 2022) regime. Theo- retically, the previous study proves that the advantages of multimodal learning may vanish under the setting of lim- ited data volume (Huang et al., 2021a) which implies the exploitation of cross-modal relationship is not a free lunch. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, introducing dynamic fusion mechanism emerges as a promising way to obtain reliable predictions. As a concrete example, previ- ous work (Guan et al., 2019) proposes a dynamic weight- ing mechanism to depict illumination condition of scenes. By introducing dynamics, they can integrate reliable cues from multi-spectral data for around-the-clock applications (e.g., pedestrian detection in security surveillance and au- tonomous driving). Dynamic fusion has been used in diverse real-world multimodal applications, including multimodal classification (Han et al., 2021; Geng et al., 2021; Han et al., 2022b), regression (Ma et al., 2021), object detection (Li et al., 2022a; Zhang et al., 2019; Chen et al., 2022b) and semantic segmentation (Tian et al., 2020). While dynamic multimodal fusion shows excellent power in practice, theo- retical understanding is notably lack in this field with the fol- lowing fundamental open problem: Can we realize reliable multimodal fusion in practice with theoretical guarantee? This paper tries to shed light upon the theoretical advantage and criterion of robust multimodal fusion. Following pre- vious works in multimodal learning theory (Huang et al., 1 arXiv:2306.02050v2  [cs.LG]  6 Jun 2023Provable Dynamic Fusion for Low-Quality Multimodal Data Figure 1.Visualization of accuracy gap between multimodal learning methods (e.g., late fusion, align-based fusion, MMTM) and single- modal learning methods using the best modality on noisy multimodal data. Noted that the performance existing multimodal fusion methods degrade significantly of compared with their best unimodal counterparts in a high noise regime, while the proposed QMF consistently outperforms unimodal methods on low-quality data. 2021b; Wang et al., 2020a), the framework we study is also abstracted from decision-level multimodal fusion, which is one of the most fundamental research topics in multimodal learning (Baltrušaitis et al., 2018). In particular, we devise a novel Quality-aware Multimodal Fusion (QMF) framework for multimodal learning. Key to our framework, we leverage energy-based uncertainty to characterize the quality of each modality. Our contributions can be summarised as follows: • This paper provides a rigorous theoretical framework to understand the advantage and criterion of robust mul- timodal fusionas shown in Figure 2. Firstly, we charac- terize the generalization error bound of decision-level multimodal fusion methods from a Rademacher com- plexity perspective. Then, we identify under what con- ditions dynamic fusion outperforms static, i.e., when the fusion weights of multimodal fusion is negatively correlates to the unimodal generalization errors, dy- namic fusion methods provably outperform static. • Under the theoretical analysis, we proceed to reveal that the generalization ability of dynamic fusion coin- cides with the performance of uncertainty estimation. This directly implies a principle to design and evaluate new dynamic fusion algorithms. • Directly motivated by the above analysis, we pro- pose a novel dynamic multimodal fusion method termed Quality-aware Multimodal Fusion ( QMF), which serves as a realization for provably better gener- alization ability. As shown in Figure 1, extensive ex- periments on commonly used benchmarks are carried out to empirically validate the theoretical observations. 2. Related works 2.1. Multimodal Fusion Multimodal fusion is one of the most original and funda- mental topics in multimodal learning, which typically aims to integrate modality-wise features into a joint representa- tion for downstream multimodal learning tasks. Multimodal fusion can be classified into early fusion, intermediate fu- sion and late fusion. Although studies in neuroscience and machine learning suggest that intermediate fusion could benefit representation learning (Schroeder & Foxe, 2005; Macaluso, 2006), late fusion is still the most widely used method for multimodal learning due to its interpretation and practical simplicity. By introducing modality-level dy- namics based on various strategies, dynamic fusion practi- cally improves overall performance. As a concrete example, the previous work (Guan et al., 2019) proposes a dynamic weighting mechanism to depict illumination conditions of scenes. By introducing dynamics, they can integrate reliable cues from multi-spectral data for around-the-clock applica- tions (e.g., pedestrian detection in security surveillance and autonomous driving). Combining with additional dynamic mechanism (e.g., a simple weighting strategy or Dempster- Shafer Evidence Theory (Shafer, 1976)), recent uncertainty- based multimodal fusion methods show remarkable advan- tages in various tasks, including clustering (Geng et al., 2021), classification (Han et al., 2021; 2022b; Tellamekala et al., 2022; Subedar et al., 2019; Chen et al., 2022a), re- gression (Ma et al., 2021), object detection (Zhang et al., 2019; Li et al., 2022b) and semantic segmentation (Tian et al., 2020; Chang et al., 2022). 2.2. Uncertainty Estimation Multimodal machine learning has achieved great success in various real-world application. However, the reliabil- ity of current fusion methods is still notably unexplored, which limits their application in safety-critic field (e.g., financial risk, medical diagnosis). The motivation of un- certainty estimation is to indicate whether the predictions given by machine learning models are prone to be wrong. Many uncertainty estimation methods have been proposed 2Provable Dynamic Fusion for Low-Quality Multimodal Data Figure 2.Left: The generalization error upper bound of multimodal fusion method f can be characterized by its performance on each modality in terms of empirical loss, model complexity and uncertainty awareness.Right: Dynamic vs Static multimodal fusion hypothesis space, where the latter is a subset of the former. fstatic, fdynamic are the hypothesises of static and dynamic fusion methods respectively and f∗ is the true mapping. Informally, closer to the true mapping leads to less error. Under some certain conditions, dynamic multimodal fusion methods (e.g., the proposed QMF) can be well regularized and thus provably achieve better generalization ability. in the past decades, including Bayesian neural networks (BNNs) (Denker & LeCun, 1990; Mackay, 1992; Neal, 2012) and its varieties (Gal & Ghahramani, 2016; Han et al., 2022a), deep ensembles (Lakshminarayanan et al., 2017; Havasi et al., 2021), predictive confidence (Hendrycks & Gimpel, 2017), Dempster-Shafer thoery (Han et al., 2021) and energy score (Liu et al., 2020). Predictive confidence expects the predicted class probability to be consistent with the empirical accuracy, which is usually referred in clas- sification tasks. Dempster-Shafer theory (DST) is a gen- eralization of Bayesian theory to subjective probabilities and a general framework for modeling epistemic uncer- tainty. Energy score emerges as a promising way to cap- ture Out-of-Distribution (OOD) uncertainty, which arises when a machine learning model encounters an input that differs from its training data, and thus the output from the model is unreliable. A plethora of recent researches have studied the issue of OOD uncertainty (Ming et al., 2022; Chen et al., 2021; Meinke & Hein, 2019; Hendrycks et al., 2019). In this paper, we investigate predictive confidence, the Dempster-Shafer theory and energy score due to their theoretical interpretability and effectiveness. 3. Theory In this section, we first clarify the basic notations and the formal definition of multimodal fusion used in Section 3.1. Then we provide main theoretical results in Section 3.2 to rigorously demonstrate when and how dynamic fusion methods work from the perspective of generalization abil- ity (Bartlett & Mendelson, 2002). Due to space constraints, we defer the full details to Appendix A and only present a brief summary of the proofs. 3.1. Preliminaries We initialize by introducing the necessary notations for our theoretical frameworks. Considering a learning task on the data (x, y) ∈ X × Y, where x = {x(1), ··· , x(M)} has M modalities and y ∈ Ydenotes the data label. The multimodal training data is defined as Dtrain = {xi, yi}N i=1. Specifically, we use X, Y and Z to denote the input space, target space and latent space. Similar to the previous work in multimodal learning theory (Huang et al., 2021a), we define h : X 7→ Zis a multimodal fusion mapping from the input space to the latent space, and g : Z 7→ Yis a task mapping. Our goal is to learn a reliable multimodal model f = g ◦ h(x) performing well on the unknown multimodal test dataset Dtest. Dtrain and Dtest are both drawn from joint distribution D over X × Y. Here f = g ◦ h(x) represents the composite function of h and g. 3.2. When and How Dynamic Multimodal Fusion Help For simplicity, we provide analysis of ensemble-like late fusion strategy using logistic loss function in two-class clas- sification setting. Our analysis follows this roadmap: (1) we first characterize the generalization error bound of dy- namic late fusion using Rademacher complexity (Bartlett & Mendelson, 2002) and then separate the bound into three components (Theorem 1); (2) base on above separation, we further prove that dynamic fusion achieves better general- ization ability under certain conditions (Theorem 2). We initiate our analysis with the basic setting as follows. Basic setting. Under a M input modalities and two-class classification scenario, we define fm as the unimodal clas- sifier on modality x(m). The final prediction of late fusion multimodal method is calculated by weighting decisions 3Provable Dynamic Fusion for Low-Quality Multimodal Data from different modalities: f(x) =PM m=1 wm · fm(x(m)), where f(x) denotes the final prediction. In contrast to static late fusion, the weights in dynamic multimodal fusion are generated dynamically and vary for different samples. For clarity, we use subscript to distinguish them, i.e., wm static refers to the ensemble weight of modality m in static late fusion and wm dynamic refers to the weight in dynamic fu- sion. Specifically, wm static is a constant and wm dynamic(·) is a function of the input sample x. The generalization error of two-class multimodal classifier f is defined as: GError(f) =E(x,y)∼D[ℓ(f(x), y)], (1) where D is the unknown joint distribution, and ℓ is logis- tic loss function. For convenience, we simplify unimodal classifier loss ℓ(fm(xm), y) as lm and omit the inputs in the following analysis. Now we present our first main result regarding multi-modal fusion. Theorem 1 (Generalization Bound of Multimodal Fusion). Let Dtrain = {xi, yi}N i=1 be a training dataset of N samples, ˆE(fm) is the unimodal empirical errors of fm on Dtrain. Then for any hypothesis f in H (i.e., H : X → {−1, 1}, f ∈ H) and 1 > δ >0, with probability at least 1 − δ, it holds that GError(f) ≤ MX m=1 E(wm) ˆE(fm) | {z } Term-L (average empirical loss) + MX m=1 E(wm)Rm(fm) | {z } Term-C (average complexity) + MX m=1 Cov(wm, lm) | {z } Term-Cov (covariance) + M r ln(1/δ) 2N , (2) where E(wm) is the expectations of fusion weights on joint distribution D, Rm(fm) is Rademacher complexity, Cov(wm, ℓm) is the covariance between fusion weight and loss. Intuitively, Theorem 1 demonstrates that the generalization error of multimodal classifier is bounded by the weighted average performances of all the unimodal classifiers in terms of empirical loss, model complexity and the covariance be- tween fusion weight and unimodal loss. Having established the general error bound, our next goal is to verify when dy- namic multimodal late fusion indeed achieves tighter bound than that of static late fusion. Informally, in Eq. 1, Term-Cov measures the joint variability of wm and ℓm. Remember that in static multimodal fusion wm static is a constant, which means Term-Cov = 0for any static fusion methods. Thus the generalization error bound of static fusion methods re- duces to GError(fstatic) ≤ MX m=1 wm static ˆE(fm) | {z } Term-L (average empirical loss) + MX m=1 wm staticRm(fm) | {z } Term-C (average complexity) +M r ln(1/δ) 2N . (3) So when summation of Term-L , Term-C is invariant or smaller in dynamic fusion and Term-Cov ≤ 0, we can en- sure that dynamic fusion provably outperforms static fusion. This theorem is formally presented as Theorem 2. Let O(GError(fdynamic)), O(GError(fstatic)) be the upper bound of generalization error of multimodal classifier using dynamic and static fusion strategy respec- tively. ˆE(fm) is the unimodal empirical errors of fm on Dtrain defined in Theorem 1. Then for any hypothesis fdynamic, fstatic in H : X → {−1, 1} and 1 > δ >0, it holds that O(GError(fdynamic)) ≤ O(GError(fstatic)) (4) with probability at least 1 − δ, if we have E(wm dynamic) =wm static (5) and r(wm dynamic, ℓ(fm)) ≤ 0 (6) for all input modalities, where r is the Pearson correlation coefficient which measures the correlation between fusion weights wm dynamic and unimodal loss ℓm. Remark. Theoretically, optimizing over the same function class efficiently results in the same empirical loss. Suppose for each modality m, the unimodal classifier fm we used in dynamic and static fusion are of the same architecture, then the intrinsic complexity of unimodal classifierRm(fm) and empirical risk ˆE(fm) can be invariant. Thus in this case, it holds that MX m=1 E(wm dynamic) ˆE(fm) ≤ MX m=1 wm static ˆE(fm), (7) and MX m=1 E(wm dynamic)Rm(fm) ≤ MX m=1 wm staticRm(fm), (8) if Eq. 5 is satisfied for any modality m. According to The- orem 2, it is easy to derive the conclusion that the main 4Provable Dynamic Fusion for Low-Quality Multimodal Data challenge of achieving reliable dynamic multimodal fusion is to learn a reasonable wm dynamic(x) for each modality that satisfies Eq. 5 and Eq. 6. 4. Method Now we proceed to answer \"How to realize robust dy- namic fusion?\". In this section, we theoretically identify the connection between dynamic multimodal fusion and uncertainty estimation. Then, a unified dynamic multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed. We next show how to realize this frame- work in decision-level late fusion and classification tasks to support our findings. 4.1. Coincidence with Uncertainty Estimation Firstly, we focus on how to satisfy Eq. 6. As we discuss in Section 2.2, the common motivation of various uncertainty estimation methods is to provide an indicator of whether the predictions given by models are prone to be wrong. This motivation is inherently close to obtaining weights that satisfy Eq. 6. We formulate this claim with the following assumption Assumption 1. Given an effective uncertainty estima- tor um : X →R on modality m, the estimated uncer- tainty um(x) is positively correlated with its modal- specific loss ℓm(x): r(um, ℓm(x)) ≥ 0, where r is the Pearson correlation coefficient. This insight offers opportunity to explore novel dynamic fusion methods provably outperform conventional static fusion methods. Similar to previous dynamic fusion meth- ods (Blundell et al., 2015; Zhang et al., 2019; Han et al., 2022b), we deploy modal-level weighting strategy to intro- duce dynamics. Uncertainty-aware weighting. The uncertainty-aware fu- sion weighting wm : X →R is a function that linearly and negatively relates to the corresponding uncertainty wm(x) =αm um(x) +βm, (9) where αm < 0, βm ≥ 0 are modal-specific hyper- parameters. um(x) is the uncertainty of modality m. By tuning hyper-parameters αm, βm, we can ensure dynamic fusion weights satisfied Eq. 5 and 6 simultaneously. This lemma is formally presented as Lemma 1 (Satisfiability). With Assumption 1, for any wm static ∈ R, there always exist βm ∈ R such that E(wm dynamic) =wm static, r(wm dynamic, ℓ(fm)) ≤ 0. (10) Once we obtain the fusion weights, we can perform uncertainty-aware weighting fusion in decision-level ac- cording to the following rule f(x) = MX m=1 wm(x) · fm(x), (11) where fm(x) defined in Section 3.2 denotes unimodal pre- diction on modality m. 4.2. Enhance Correlation by Additional Regularization With the above analysis, the core challenges of robust dy- namic multimodal fusion present in Section. 3.2 have been reduced to obtain an effective uncertainty estimator in As- sumption 1. In our implementation, we leverage energy score (Liu et al., 2020), which is a widely accepted met- ric in the literature of uncertainty learning. Energy score 1 bridges the gap between the Helmholtz free energy of a given data point and its density. For multimodal data, the density functions of different modalities can be estimated by the corresponding energy function: log p(x(m)) =−Energy(x; fm)/T m − log Zm, (12) where x(m) is the m-th input modality and fm is the uni- modal classification model. Energy(·) is the energy func- tion and Zm is an intractable constant for all xm. The above equation suggests that−Energy(x(m); fm) is linearly aligned with density p(x(m)). The energy score for them-th modality of input x can be calculated as Energy(x(m)) =−T m · log KX k efm k (x(m))/T m , (13) where fm k (x(m)) is the output logits of classifier fm corre- sponding to the k-th class label and T m is a temperature parameter. Intuitively, more uniformly distributed predic- tion leads to higher estimated uncertainty. However, it has been shown experimentally that the uncer- tainty estimated in this way without additional regulariza- tion is not well enough to satisfy our Assumption 1. To address this, we propose a sampling-based regularization technology to enhance the original method in terms of corre- lation. The most simple and straightforward way to improve the correlation between estimated uncertainty and respec- tive loss is to leverage the sample-wise loss during training stage as supervision information. However, due to the over- parameterization phenomenon of deep neural networks, the 1While another line of previous works usually incorporate an auxiliary outlier dataset (e.g., random noised out-of-distribution data) during training for higher performance, for clarity and a strictly fair comparison, we conduct our experiments without the help of additional data. 5Provable Dynamic Fusion for Low-Quality Multimodal Data Algorithm 1 Training Pseudo Code of Quality-aware Multimodal Fusion (QMF) Input : Multimodal training dataset Dtrain, the number of sampling T, hyperparameters λ, temperature parameters {T m}M m=1, unimodal predictors {fm(·)}M i=m; Output : The multimodal classifier f; 1 for each iteration do 2 Obtain training sample (xi, yi) from dataset Dtrain and the decisions on each modality fm(x); 3 Calculate uncertainty-aware fusion weights [w1 i , ··· , wm i ] defined in Eq. 9; 4 Update the average training loss κm i of each modalities; 5 Obtain the multimodal decision by weighting unimodal predictions dynamically according to Eq. 11; 6 Update model parameters of each unimodal predictor by minimizing Loverall in Eq. 18. 7 end losses constantly reduce to zero during training. Inspired by recent works in Bayesian learning (Maddox et al., 2019) and uncertainty estimation (Moon et al., 2020; Han et al., 2022a), we propose to leverage the information from his- torical training trajectory to regularize the fusion weights. Specifically, given the m-th modality of a sample (xi, yi), the training average loss for xm i is calculated as: κm i = 1 T Ts+TX t=Ts ℓ(yi, fm θt (xi)), (14) where fm θt is the unimodal classifier on each iteration epoch t with parameters θt. After training Ts − 1 epochs, we sample T times and calculate the average training loss. Empirically, recent works (Geifman et al., 2019) shown that easy-to-classify samples are learned earlier during train- ing compared to hard-to-classify samples (e.g., noise sam- ples (Arazo et al., 2019)). It is desirable to regularize a dynamic fusion model by learning the following relation- ship during training κm i ≥ κm j ⇐⇒ wm i ≤ wm j . (15) We now present the full definition of our regularization term as follows Lreg = max(0, g(wm i , wm j )(κm i − κm j ) +|wm i − wm j |), (16) where g(wm i , wm j ) =    1 if wm i > wm j , 0 if wm i = wm j , −1 otherwise. (17) Inspired by multi-task learning, we define the total loss func- tion as a summation of standard cross-entropy classification losses of multiple modalities and the regularization term Loverall = LCE(y, f(x)) + MX m=1 LCE(y, fm(xm)) +λLreg, (18) where λ is a hyperparamter which controls the strength of regularization, LCE and Lreg are the cross-entropy loss and reguralization term respectively. The whole training process is shown in Algorithm 1. Intuitive explanation of the effectiveness of QMF.With- out loss of generality, we assume modality xA is clean and modality xB is noisy due to unknown environmental factors or sensor failure. At this time, xA is in the distri- bution of clean training data but xB deviates significantly from it. Accordingly, we have u(xA) ≤ u(xB) and thus wA ≥ wB. Therefore, for our QMF, the multimodal deci- sion will tend to more rely on the high-quality modality xA than the other modality xB. By dynamically determining the fusion weights of each modality, the influence of the unreliable modalities can be alleviated. 5. Experiment In this section, we conduct experiments on multiple datasets of diverse applications 2. The main questions to be verified are highlighted here: • Q1 Effectiveness I. Does the proposed method has bet- ter generalization ability than its counterparts? (Sup- port Theorem 1) • Q2 Effectiveness II. Under what conditions does uncertainty-aware dynamic multimodal fusion work? (Support Theorem 2) • Q3 Reliability. Does the proposed method have an effective perception for the uncertainty of modality? (Support Assumption 1) • Q4 Ablation study. What is the key factor of perfor- mance improvement in our method? 5.1. Experimental Setup We briefly present the experimental setup here, including the experimental datasets and comparison methods. Please 2Code is available at https://github.com/QingyangZhang/QMF. 6Provable Dynamic Fusion for Low-Quality Multimodal Data (a) NYU Depth V2  (b) SUN RGB-D Figure 3.Test accuracy and Pearson correlation coefficient achieved by different fusion methods over 10 times random experiments. The average and worst-case accuracy are highly consistency with uncertainty estimation ability. refer to Appendix B for more detailed setup. Tasks and datasets. We evaluate our method on two multimodal classification tasks. ◦ Scenes Recognition: NYU Depth V2 (Silberman et al., 2012) and SUN RGB- D (Song et al., 2015) are two public indoor scenes recogni- tion datasets, which are associated with two modalities, i.e., RGB and depth images. ◦ Image-text classification: The UPMC FOOD101 dataset (Wang et al., 2015) contains (pos- sibly noisy) images obtained by Google Image Search and corresponding textual descriptions. MVSA sentiment anal- ysis dataset (Niu et al., 2016) includes a set of image-text pairs with manual annotations collected from social media. Although the datasets above are all under the condition that M = 2, it is intuitive and easy to generalize to M ≥ 3. Evaluation metrics. Due to the randomness involved, we report the mean accuracy, standard deviation and worst-case accuracy on NYU Depth V2 and SUN RGB-D over 10 dif- ferent seeds. To be consistent with existing works (Han et al., 2022c; Kiela et al., 2019; Yadav & Vishwakarma, 2023), we repeat experiments over 3 times on UMPC FOOD101 and 5 times on MVSA. Compared methods. For scene recognition task, we com- pare the proposed method with three static fusion methods: Late fusion, Concatenate-based fusion, Alignment-based fusion methods (Wang et al., 2016) and two representative dynamic fusion methods, i.e., MMTM (Joze et al., 2020) and TMC3 (Han et al., 2021). For image-text classifica- tion, we compare against strong unimodal baselines (i.e., Bow, Bert and ResNet-152) as well as sophisticated multi- modal fusion methods, including Late fusion, ConcatBow, 3There are two variants in (Han et al., 2021): TMC and ETMC (with additional concatenated-based multimodal fusion strategy). TMC has comparable performance and is a more fair comparison. ConcatBERT and recent sota MMBT (Kiela et al., 2019). 5.2. Experimental Results Classification robustness (Q1). To validate the robust- ness of the uncertainty-aware weighting fusion, we evaluate QMF and the compared methods in terms of average and worst-case accuracy under Gaussian noise (for image modal- ity) and blank noise (for text modality) following previous works (Han et al., 2021; Ma et al., 2021; Verma et al., 2021; Hu et al., 2019; Xie et al., 2017). More results under dif- ferent types of noise (e.g. Salt-Pepper Noise) can be found in Appendix C.2. The experimental results are presented in Table 1. It is observed that QMF usually performs in the top three in terms of both average and worst-case accuracy. This observation indicates that QMF has better generalization ability than their counterparts experimentally. It is also note- worthy that the QMF outperforms the prior state-of-the-art methods (i.e., MMBT and TMC) on large-scale benchmark UPMC FOOD101, which illustrates the superiority of the proposed method. Connection to uncertainty estimation (Q2). We fur- ther conduct comparisons with QMF realized by various uncertainty estimation algorithms, i.e., prediction confi- dence (Hendrycks & Gimpel, 2017) and Dempster-Shafer evidence theory (DST) (Han et al., 2021). According to comparison results shown in the Figure 3, it is clear that (i) the generalization ability (i.e., average and worst-case accuracy) of dynamic fusion methods coincide with their uncertainty estimation ability and (ii) our QMF achieves the best performance in terms of classification accuracy and uncertainty estimation in the meantime. This comparison reveals the underlying reason of why QMF outperforms other fusion methods and supports Theorem 2. We show the results on NYU Depth V2 and SUN RGB-D under Gaussian 7Provable Dynamic Fusion for Low-Quality Multimodal Data Table 1.Classification comparison when 50% of the modalities are corrupted with Gaussian noise i.e., zero mean with variance of ϵ. The best three results are in bold brown and the best results are highlighted in bold blue. Full results with standard deviation are in Appendix. Dataset Dynamic Method ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 Avg. Worst. Avg. Worst. Avg. Worst. NYU Depth V2 ✗ RGB 63.30 62 .54 53 .12 50 .31 45 .46 42 .20 ✗ Depth 62.65 61 .01 50 .95 42 .81 44 .13 35 .93 ✗ Late fusion 69.14 68 .35 59 .63 53 .98 51 .99 44 .95 ✗ Concat 70.30 69 .4269.4269.42 59 .97 55 .89 53 .2053.2053.20 47 .7147.7147.71 ✗ Align 70.3170.3170.31 68 .50 59 .47 56 .27 51 .74 44 .19 ✓ MMTM 71.0471.0471.04 70 .1870.1870.18 60 .3760.3760.37 56 .7356.7356.73 52 .28 46 .18 ✓ TMC 71.0671.0671.06 69 .5769.5769.57 61 .0461.0461.04 58 .7258.7258.72 53 .3653.3653.36 49 .2349.2349.23 ✓ Ours 70.09 68 .81 61 .6261.6261.62 58 .8758.8758.87 55 .6055.6055.60 51 .0751.0751.07 SUN RGB-D ✗ RGB 56.78 56 .51 48 .40 47 .16 42 .94 41 .02 ✗ Depth 52.99 51 .32 37 .81 35 .63 33 .07 30 .41 ✗ Late fusion 62.0962.0962.09 60 .55 52 .4452.4452.44 50 .8350.8350.83 47 .3347.3347.33 44 .6044.6044.60 ✗ Concat 61.9061.9061.90 61 .1961.1961.19 52 .6952.6952.69 50 .61 45 .64 42 .95 ✗ Align 61.12 60 .12 50 .05 47 .63 44 .19 38 .12 ✓ MMTM 61.72 60 .9460.9460.94 51 .86 50 .8050.8050.80 46 .0346.0346.03 44 .2844.2844.28 ✓ TMC 60.68 60 .31 51 .24 49 .45 45 .66 41 .60 ✓ Ours 62.0962.0962.09 61 .3061.3061.30 53 .4053.4053.40 52 .0752.0752.07 48 .5848.5848.58 47 .5047.5047.50 FOOD 101 ✗ Bow 82.50 82 .32 61 .68 60 .98 41 .95 41 .41 ✗ Img 64.62 64 .22 34 .72 34 .19 33 .03 32 .67 ✗ Bert 86.46 86 .42 67 .38 67 .19 43 .88 43 .56 ✗ Late fusion 90.6990.6990.69 90 .5890.5890.58 68 .49 65 .05 58 .0058.0058.00 55 .77 ✗ ConcatBow 70.77 70 .68 38 .28 37 .95 35 .68 34 .92 ✗ ConcatBert 88.20 87 .81 61 .10 59 .25 49 .86 47 .79 ✓ MMBT 91.5291.5291.52 91 .3891.3891.38 72 .3272.3272.32 71 .7871.7871.78 56 .75 56 .2156.2156.21 ✓ TMC 89.86 89 .80 73 .9373.9373.93 73 .6473.6473.64 61 .3761.3761.37 61 .1061.1061.10 ✓ Ours 92.9292.9292.92 92 .7292.7292.72 76 .0376.0376.03 74 .6874.6874.68 62 .2162.2162.21 61 .7661.7661.76 MVSA ✗ Bow 48.79 35 .45 42 .20 32 .56 41 .57 32 .18 ✗ Img 64.12 62 .04 49 .36 45 .67 45 .00 39 .31 ✗ Bert 75.61 74 .7674.7674.76 69 .5069.5069.50 65 .7065.7065.70 47 .41 45 .86 ✗ Late fusion 76.8876.8876.88 74 .76 63 .46 58 .57 55 .16 47 .78 ✗ ConcatBow 64.09 62 .04 49 .95 45 .28 45 .40 40 .95 ✗ ConcatBert 65.59 64 .74 50 .70 44 .70 46 .12 41 .81 ✓ MMBT 78.5078.5078.50 78 .0478.0478.04 71 .9971.9971.99 69 .9469.9469.94 55 .3555.3555.35 52 .2252.2252.22 ✓ TMC 74.88 71 .10 66 .72 60 .1260.1260.12 60 .36 53 .3753.3753.37 ✓ Ours 78.0778.0778.07 76 .3076.3076.30 73 .8573.8573.85 71 .1071.1071.10 61 .2861.2861.28 57 .6157.6157.61 8Provable Dynamic Fusion for Low-Quality Multimodal Data Table 2.Ablation study on NYU Depth V2. Full results with standard deviation are in Appendix C.1. UAW Lreg ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 ϵ = 20.0 Avg. Worst. Avg. Worst. Avg. Worst. Avg. Worst. ✗ ✗ 69.14 68 .35 59 .62 53 .98 51 .94 44 .95 43 .76 36 .85 ✗ ✓ 69.68 67 .74 61 .35 58 .26 55 .44 51 .5351.5351.53 47 .32 42 .97 ✓ ✗ 70.06 69 .1169.1169.11 61 .59 57 .49 55 .14 50 .15 47 .46 42 .05 ✓ ✓ 70.0970.0970.09 68 .81 61 .6261.6261.62 58 .8758.8758.87 55 .8155.8155.81 51 .07 48 .2648.2648.26 43 .7343.7343.73 Table 3.Pearson correlation coefficient r between losses and fu- sion weights of test samples (a higher |r| indicates a better uncer- tainty estimation). ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 MSP 0.391 0 .433 0 .486 Energy score 0.272 0 .429 0 .510 Entropy 0.397 0 .420 0 .452 Evidence 0.157 0 .136 0 .265 Ours 0.4980.4980.498 0 .6520.6520.652 0 .7350.7350.735 noise with zero mean and variance of 10. Reliability of QMF (Q3). We calculate the fusion weights defined in Eq. 9 of different modalities in Table 3 on UPMC FOOD-101. It is observed that the fusion weights of QMF have the most effective perception of modal quality com- pared with other uncertainty estimation methods (in terms of correlation). This observation justifies our expectation of uncertainty-aware weights in Eq. 9. Ablation study (Q4). We compare different combinations of components (i.e., uncertainty-aware weighting and the regularization term Lreg). Here we also employ Gaussian noise on NYU Depth V2 in Table 2, and more results can be found in the Appendix C.1. It is easy to conclude that 1) adding Lreg is beneficial to obtaining more reasonable fusion weights; 2) the best performance could be expected with the full QMF. Please refer to Table. 4 in Appendix C.1 for full results with standard deviation. In summary, the empirical results can support our theoretical findings. These works identify the causes and conditions of performance gains of dynamic multimodal fusion methods. The proposed method can help to improve robustness on multiple datasets. 6. Limitations Even though the proposed method achieves superior per- formance, there are still some potential limitations. Firstly, the fusion weights of QMF are based on uncertainty esti- mation, which can be a challenging task in the real world. For example, in our experiments, we can only achieve mild Pearson’s r on NYU Depth V2 and SUN RGB-D dataset. Therefore, it is important and valuable to explore novel un- certainty estimation methods in the future work. Secondly, though we characterize the generalization error bound of the proposed method, our theoretical justifications are based on Assumption 1. However, previous work (Fang et al., 2022) reveals that OOD detection is not learnable under some scenarios. Thus it’s still a challenging open problem to further characterize the generalization ability of dynamic multimodal fusion. 7. Conclusions and Future works Introducing dynamics in multimodal fusion has yielded re- markable empirical results in various applications, including image classification, object detection and semantic segmen- tation. Many state-of-the-art multimodal models introduce dynamic fusion strategies, but the inductive bias provided by this technique is not well understood. In this paper, we provide rigorous analysis towards understanding when and what dynamic multimodal fusion methods are more robust on multimodal data in the wild. These findings demonstrate the connection between uncertainty learning and robust mul- timodal fusion, which further implies a principle to design novel dynamic multimodal fusion methods. Finally, we perform extensive experiments on multiple benchmarks to support our findings. In the work, the energy-based weight- ing strategy is devised, and other uncertainty estimation ways could be explored. Another interesting direction is proving the dynamic fusion under a more general setting. Acknowledgments This work is partially supported by the National Natural Science Foundation of China (Grant No. 61976151) and A*STAR Central Research Fund. We gratefully acknowl- edge the support of MindSpore and CAAI. The authors would like to thank Zhipeng Liang (Hong Kong University of Science and Technology) for checking on math details and Zongbo Han, Huan Ma (Tianjin University) for their comments on writing. The authors also appreciate the sug- gestions from ICML anonymous peer reviewers. 9Provable Dynamic Fusion for Low-Quality Multimodal Data References Acosta, J. N., Falcone, G. J., Rajpurkar, P., and Topol, E. J. Multimodal biomedical ai. Nature Medicine, 28(9):1773– 1784, 2022. Almalioglu, Y ., Turan, M., Trigoni, N., and Markham, A. Deep learning-based robust positioning for all-weather autonomous driving. Nature Machine Intelligence, 4(9): 749–760, 2022. Amrani, E., Ben-Ari, R., Rotman, D., and Bronstein, A. Noise estimation using density estimation for self- supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. Arazo, E., Ortego, D., Albert, P., O’Connor, N., and McGuinness, K. Unsupervised label noise modeling and loss correction. In International Conference on Machine Learning, pp. 312–321. PMLR, 2019. Baltrušaitis, T., Ahuja, C., and Morency, L.-P. Multimodal machine learning: A survey and taxonomy. IEEE trans- actions on pattern analysis and machine intelligence, 41 (2):423–443, 2018. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 2002. Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. InInternational Conference on Machine Learning, 2015. Caglayan, A., Imamoglu, N., and Nakamura, R. Mmsnet: Multi-modal scene recognition using multi-scale encoded features. Image and Vision Computing , 122:104453, 2022. Chang, Y ., Xue, F., Sheng, F., Liang, W., and Ming, A. Fast road segmentation via uncertainty-aware symmetric network. In International Conference on Robotics and Automation, 2022. Charpentier, B., Zügner, D., and Günnemann, S. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts. In Advances in Neural Information Processing Systems, 2020. Chen, J., Li, Y ., Wu, X., Liang, Y ., and Jha, S. Atom: Robus- tifying out-of-distribution detection using outlier mining. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2021. Chen, S., Yang, X., Chen, Y ., Yu, H., and Cai, H. Uncertainty-based fusion netwok for automatic skin le- sion diagnosis. In International Conference on Bioinfor- matics and Biomedicine, 2022a. Chen, Y .-T., Shi, J., Ye, Z., Mertz, C., Ramanan, D., and Kong, S. Multimodal object detection via probabilistic en- sembling. In European Conference on Computer Vision, 2022b. Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. Supervised learning of universal sentence rep- resentations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, 2009. Denker, J. and LeCun, Y . Transforming neural-net output levels to probability distributions. In Advances in Neural Information Processing Systems, 1990. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. D’Mello, S. K. and Westlund, J. K. A review and meta- analysis of multimodal affect detection systems. ACM Computing Surveys, 2015. Eitel, A., Springenberg, J. T., Spinello, L., Riedmiller, M., and Burgard, W. Multimodal deep learning for robust rgb-d object recognition. In International Conference on Intelligent Robots and Systems, 2015. Fang, Z., Li, Y ., Lu, J., Dong, J., Han, B., and Liu, F. Is out-of-distribution detection learnable? In Advances in Neural Information Processing Systems, 2022. Ferreri, A., Bucci, S., and Tommasi, T. Multi-modal rgb-d scene recognition across domains. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. Gal, Y . and Ghahramani, Z. Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, 2016. Gallo, I., Ria, G., Landro, N., and La Grassa, R. Image and text fusion for upmc food-101 using bert and cnns. In International Conference on Image and Vision Computing New Zealand, 2020. Geifman, Y ., Uziel, G., and El-Yaniv, R. Bias-reduced un- certainty estimation for deep neural classifiers. In Inter- national Conference on Learning Representations, 2019. Geng, Y ., Han, Z., Zhang, C., and Hu, Q. Uncertainty-aware multi-view representation learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. 10Provable Dynamic Fusion for Low-Quality Multimodal Data Girdhar, R., Singh, M., Ravi, N., van der Maaten, L., Joulin, A., and Misra, I. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition , 2022. Guan, D., Cao, Y ., Yang, J., Cao, Y ., and Yang, M. Y . Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection. Information Fusion, 50:148–157, 2019. Gunes, H. and Piccardi, M. Affect recognition from face and body: early fusion vs. late fusion. In International Conference on Systems, Man and Cybernetics, 2005. Guo, C., Pleiss, G., Sun, Y ., and Weinberger, K. Q. On calibration of modern neural networks. In International Conference on Machine Learning, 2017. Han, Z., Zhang, C., Fu, H., and Zhou, J. T. Trusted multi- view classification. In International Conference on Learn- ing Representations, 2021. Han, Z., Liang, Z., Yang, F., Liu, L., Li, L., Bian, Y ., Zhao, P., Wu, B., Zhang, C., and Yao, J. Umix: Im- proving importance weighting for subpopulation shift via uncertainty-aware mixup. In Advances in Neural Infor- mation Processing Systems, 2022a. Han, Z., Yang, F., Huang, J., Zhang, C., and Yao, J. Multi- modal dynamics: Dynamical fusion for trustworthy mul- timodal classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022b. Han, Z., Zhang, C., Fu, H., and Zhou, J. T. Trusted multi- view classification with dynamic evidential fusion. IEEE transactions on pattern analysis and machine intelligence, 2022c. Havasi, M., Jenatton, R., Fort, S., Liu, J. Z., Snoek, J., Lakshminarayanan, B., Dai, A. M., and Tran, D. Train- ing independent subnetworks for robust prediction. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Rep- resentations, 2017. Hendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019. Hu, Z., Tan, B., Salakhutdinov, R. R., Mitchell, T. M., and Xing, E. P. Learning data manipulation for augmenta- tion and weighting. In Advances in Neural Information Processing Systems, 2019. Huang, Y ., Du, C., Xue, Z., Chen, X., Zhao, H., and Huang, L. What makes multi-modal learning better than single (provably). In Advances in Neural Information Process- ing Systems, 2021a. Huang, Y ., Lin, J., Zhou, C., Yang, H., and Huang, L. Modality competition: What makes joint training of multi- modal network fail in deep learning?(provably). In Inter- national Conference on Machine Learning, 2022. Huang, Z., Niu, G., Liu, X., Ding, W., Xiao, X., Wu, H., and Peng, X. Learning with noisy correspondence for cross- modal matching. In Advances in Neural Information Processing Systems, 2021b. Joze, H. R. V ., Shaban, A., Iuzzolino, M. L., and Koishida, K. Mmtm: Multimodal transfer module for cnn fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. Katz-Samuels, J., Nakhleh, J. B., Nowak, R., and Li, Y . Training ood detectors in their natural habitats. In Inter- national Conference on Machine Learning, 2022. Kiela, D., Bhooshan, S., Firooz, H., Perez, E., and Testug- gine, D. Supervised multimodal bitransformers for classi- fying images and text. arXiv preprint arXiv:1909.02950, 2019. Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Process- ing Systems, 2017. Li, B., Han, Z., Li, H., Fu, H., and Zhang, C. Trust- worthy long-tailed classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022a. Li, Q., Zhang, C., Hu, Q., Fu, H., and Zhu, P. Confidence- aware fusion using dempster-shafer theory for multispec- tral pedestrian detection. IEEE Transactions on Multime- dia, 2022b. Liu, W., Wang, X., Owens, J., and Li, Y . Energy-based out- of-distribution detection. Advances in Neural Information Processing Systems, 2020. Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., and Yu, S. X. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 11Provable Dynamic Fusion for Low-Quality Multimodal Data Ma, H., Han, Z., Zhang, C., Fu, H., Zhou, J. T., and Hu, Q. Trustworthy multimodal regression with mixture of normal-inverse gamma distributions. In Advances in Neu- ral Information Processing Systems, 2021. Macaluso, E. Multisensory processing in sensory-specific cortical areas. The neuroscientist, 12(4):327–338, 2006. Mackay, D. J. C. Bayesian methods for adaptive models . California Institute of Technology, 1992. Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems, 2019. Malinin, A. and Gales, M. Predictive uncertainty estimation via prior networks. In Advances in Neural Information Processing Systems, 2018. Meinke, A. and Hein, M. Towards neural networks that provably know when they don’t know. arXiv preprint arXiv:1909.12180, 2019. Ming, Y ., Fan, Y ., and Li, Y . Poem: Out-of-distribution detection with posterior sampling. In International Con- ference on Machine Learning, 2022. Moon, J., Kim, J., Shin, Y ., and Hwang, S. Confidence- aware learning for deep neural networks. In International Conference on Machine Learning, 2020. Mroueh, Y ., Marcheret, E., and Goel, V . Deep multimodal learning for audio-visual speech recognition. In Inter- national Conference on Acoustics, Speech and Signal Processing, 2015. Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012. Niu, T., Zhu, S., Pang, L., and El Saddik, A. Sentiment analysis on multi-view social data. In MultiMedia Model- ing: 22nd International Conference, MMM 2016, Miami, FL, USA, January 4-6, 2016, Proceedings, Part II 22, pp. 15–27. Springer, 2016. Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., and Tran, D. Measuring calibration in deep learning. InCVPR workshops, 2019. Peng, X., Wei, Y ., Deng, A., Wang, D., and Hu, D. Balanced multimodal learning via on-the-fly gradient modulation. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, 2022. Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing, 2014. Perrin, R. J., Fagan, A. M., and Holtzman, D. M. Mul- timodal techniques for diagnosis and prognosis of alzheimer’s disease. Nature, 461(7266):916–922, 2009. Qiu, S., Miller, M. I., Joshi, P. S., Lee, J. C., Xue, C., Ni, Y ., Wang, Y ., Anda-Duran, D., Hwang, P. H., Cramer, J. A., et al. Multimodal deep learning for alzheimer’s disease dementia assessment. Nature communications, 13(1):1–17, 2022. Rudin, W. et al. Principles of mathematical analysis, vol- ume 3. McGraw-hill New York, 1976. Scheunders, P. and De Backer, S. Wavelet denoising of mul- ticomponent images using gaussian scale mixture models and a noise-free image as priors. IEEE Transactions on Image Processing, 2007. Schroeder, C. E. and Foxe, J. Multisensory contributions to low-level,‘unisensory’processing. Current opinion in neurobiology, 15(4):454–458, 2005. Seichter, D., Köhler, M., Lewandowski, B., Wengefeld, T., and Gross, H.-M. Efficient rgb-d semantic segmentation for indoor scene analysis. In International Conference on Robotics and Automation, 2021. Seichter, D., Fischedick, S. B., Köhler, M., and Groß, H.-M. Efficient multi-task rgb-d scene analysis for indoor envi- ronments. In International Joint Conference on Neural Networks, 2022. Shafer, G. A mathematical theory of evidence, volume 42. Princeton university press, 1976. Silberman, N., Hoiem, D., Kohli, P., and Fergus, R. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision, 2012. Silva, A., Luo, L., Karunasekera, S., and Leckie, C. Noise- robust learning from multiple unsupervised sources of inferred labels. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022. Snoek, C. G., Worring, M., and Smeulders, A. W. Early versus late fusion in semantic video analysis. In Proceed- ings of the 13th annual ACM international conference on Multimedia, 2005. Song, S., Lichtenberg, S. P., and Xiao, J. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2015. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014. 12Provable Dynamic Fusion for Low-Quality Multimodal Data Subedar, M., Krishnan, R., Meyer, P. L., Tickoo, O., and Huang, J. Uncertainty-aware audiovisual activity recog- nition using deep bayesian variational inference. In Pro- ceedings of the IEEE/CVF international conference on computer vision, 2019. Tellamekala, M. K., Amiriparian, S., Schuller, B. W., André, E., Giesbrecht, T., and Valstar, M. Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty- aware multimodal emotion recognition. arXiv preprint arXiv:2206.05833, 2022. Tian, J., Cheung, W., Glaser, N., Liu, Y .-C., and Kira, Z. Uno: Uncertainty-aware noisy-or multimodal fusion for unanticipated input degradation. In International Confer- ence on Robotics and Automation, 2020. Verma, V ., Qu, M., Kawaguchi, K., Lamb, A., Bengio, Y ., Kannala, J., and Tang, J. Graphmix: Improved training of gnns for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, 2021. Wang, J., Wang, Z., Tao, D., See, S., and Wang, G. Learn- ing common and specific features for rgb-d semantic seg- mentation with deconvolutional networks. In European Conference on Computer Vision, 2016. Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., and Huang, K. Mogonet integrates multi-omics data using graph convolutional networks allowing patient clas- sification and biomarker identification. Nature communi- cations, 12(1):3445, 2021. Wang, W., Tran, D., and Feiszli, M. What makes training multi-modal classification networks hard? InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020a. Wang, X., Kumar, D., Thome, N., Cord, M., and Precioso, F. Recipe recognition with large multimodal food dataset. In International Conference on Multimedia & Expo Work- shops. IEEE, 2015. Wang, Y ., Huang, W., Sun, F., Xu, T., Rong, Y ., and Huang, J. Deep multimodal fusion by channel exchanging. Ad- vances in Neural Information Processing Systems, 2020b. Wang, Y ., Chen, X., Cao, L., Huang, W., Sun, F., and Wang, Y . Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Wen, L., Zhou, Y ., He, L., Zhou, M., and Xu, Z. Mutual in- formation gradient estimation for representation learning. In International Conference on Learning Representations, 2020. Wen, L., Nie, M., Chen, P., Zhao, Y .-n., Shen, J., Wang, C., Xiong, Y ., Yin, K., and Sun, L. Wearable multimode sen- sor with a seamless integrated structure for recognition of different joint motion states with the assistance of a deep learning algorithm. Microsystems & nanoengineering, 8 (1):1–14, 2022. Widmann, D., Lindsten, F., and Zachariah, D. Calibration tests in multi-class classification: A unifying framework. In Advances in Neural Information Processing Systems, 2019. Wu, N., Jastrzebski, S., Cho, K., and Geras, K. J. Char- acterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. In International Conference on Machine Learning, 2022. Xiao, Y ., Codevilla, F., Gurram, A., Urfalioglu, O., and López, A. M. Multimodal end-to-end autonomous driv- ing. IEEE Transactions on Intelligent Transportation Systems, 2020. Xie, Z., Wang, S. I., Li, J., Lévy, D., Nie, A., Jurafsky, D., and Ng, A. Y . Data noising as smoothing in neural network language models. In International Conference on Learning Representations, 2017. Xu, B., Huang, S., Du, M., Wang, H., Song, H., Sha, C., and Xiao, Y . Different data, different modalities! reinforced data splitting for effective multimodal information extrac- tion from social media posts. In Proceedings of the 29th International Conference on Computational Linguistics, 2022. Xu, N., Mao, W., and Chen, G. Multi-interactive memory network for aspect based multimodal sentiment analy- sis. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019. Yadav, A. and Vishwakarma, D. K. A deep multi-level at- tentive network for multimodal sentiment analysis. ACM Transactions on Multimedia Computing, Communica- tions and Applications, 19(1), 2023. Zhang, L., Zhu, X., Chen, X., Yang, X., Lei, Z., and Liu, Z. Weakly aligned cross-modal learning for multispectral pedestrian detection. In Proceedings of the IEEE/CVF international conference on computer vision, 2019. Zhou, K., Chen, L., and Cao, X. Improving multispectral pedestrian detection by addressing modality imbalance problems. In European Conference on Computer Vision, 2020. 13Provable Dynamic Fusion for Low-Quality Multimodal Data Appendix A. Proofs A.1. Proof of Theorem 1 Proof. Let (x, y) ∼ Ddenotes the multimodal sample, then we have ℓ(f(x), y) =ℓ( MX m=1 wmfm(x(m)), y). (19) Noted that ℓ is a convex logistic loss function, which indicates that ℓ(f(x), y) =ℓ( MX m=1 wmfm(x(m)), y) ≤ MX m=1 wmℓ(fm(x(m)), y). (20) Then we take the expectation on both sides of the above equation E(x,y)∼Dℓ(f(x), y) ≤ E(x,y)∼D MX m=1 wmℓ(fm(x(m)), y), (21) since expectation is a linear operator and the expected value of the product is equal to the product of the expected values plus the covariance, we can further decompose the right-hand side of the equation into E(x,y)∼Dℓ(f, y) ≤ MX m=1 E(x,y)∼D[wmℓ(fm, y)] (22) = MX m=1 E(x,y)∼D(wm)E(x,y)∼D(ℓ(fm, y)) +Cov(wm, ℓ(fm, y)) (23) Next, we recap the Rademacher complexity measure for model complexity. We use complexity-based learning the- ory (Bartlett & Mendelson, 2002) (Theorem 8) to quantify the generalization error of unimodal models. Let Dtrain = {xi, yi}N i=1 be the training dataset of N samples, ˆE(fm) is the unimodal empirical error of fm. Then for any hypothesis fm in H (i.e., H : X → {−1, 1}, f ∈ H) and 1 > δ >0, with probability at least 1 − δ, we have E(x,y)∼D(fm) ≤ ˆE(fm) +Rm(H) + r ln(1/δ) 2N , where Rm(fm) is the Rademacher complexities. Finally, it holds with probability at least 1 − δ that GError(f) ≤ MX m=1 E(wm) ˆE(fm) +E(wm)Rm(H) +Cov(wm, ℓ(fm, y)) +M r ln(1/δ) 2N . (24) A.2. Proof of Theorem 2 Proof. Let O(GError(fdynamic)), O(GError(fstatic)) be the upper bound of generalization error of multimodal classifier using dynamic and static fusion strategy respectively, ˆE(fm) is the unimodal empirical errors of fm on Dtrain defined in Theorem. 1. Theoretically, optimizing over the same function class results in the same empirical risk. Therefore ˆE(fm static) = ˆE(fm dynamic). (25) 14Provable Dynamic Fusion for Low-Quality Multimodal Data Additionally, the intrinsic complexity of unimodal classifier Rm(fm) is also invariant Rm(fm static) =Rm(fm dynamic). (26) Thus in this special case, it holds that MX m=1 E(wm dynamic) ˆE(fm) ≤ MX m=1 wm static ˆE(fm), (27) and MX m=1 E(wm dynamic)Rm(fm) ≤ MX m=1 wm staticRm(fm), (28) if E(wm dynamic) =wm static. Since the covariance and correlation coefficient have the same sign, when r(wm, lm) ≤ 0, the covariance Cov(wm, lm) is also less than or equal to 0. Therefore, it holds that O(GError(fdynamic)) ≤ O(GError(fstatic)) (29) with probability at least 1 − δ, if we have E(wm dynamic) =wm static (30) and r(wm dynamic, ℓ(fm)) ≤ 0 (31) for all input modality m. B. Experimental details B.1. Datasets details ◦ Senses recognition. For NYUD-V2, following the standard split, we reorganize the 27 categories into 10 categories with 9 usual senes and one \"others\" category. For SUN RGB-D, following the previous work (Han et al., 2021), we use the 19 major scene categories of SUN RGB-D, each of which contains at least 80 images. ◦ Image-text classification. For FOOD-101, following the previous work (Kiela et al., 2019), there are 60101 image-text pairs in the training set, 5000 image-text pairs in the validation set, and 21695 image-text pairs in the test set. For MVSA, we conduct the division strategy presents in (Kiela et al., 2019). There are 1555 image-text pairs in the training set. The validation set contains 518 image-text pairs, and the test set contains 519 image-text pairs. B.2. Implementation details Senses recognition. For senses recognition task, we compare the proposed method with diverse multimodal fusion methods, including late fusion, align-based fusion, concatenated-based fusion and recent SOTA MMTM (attention-based fusion). Regarding late fusion, align-based fusion, concatenated-based fusion, we adopt the architecture of ResNet (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) as the backbone network for each modality. ◦ Concatenate-based fusion For concatenate-based fusion, we concatenate the representations extracted from different modalities by ResNet. Then a fully connection layer is deployed to map the multimodal representation to the target space. The dimensions of unimodal representation and common representation are 128 and 256 respectively.◦ Align-based fusion The alignment fusion method is a re-implementation of (Wang et al., 2016). We deploy cosine distance to measure the similarity of representations. ◦ MMTM We follow the authors’ implementation, where the squeeze ratio is set to 4. For all compared methods, we use Adam optimizer to train all above models with L2 regularization and dropout (Srivastava et al., 2014). The learning rate is 1e-4 and the dropout rate is 0.1. Image-text classification. For image-text classification, we compare the proposed method with diverse multimodal fusion methods, including late fusion, concatenated-bow fusion, concatenated-bert fusion and MMTM. For late fusion and concatenated-bert fusion, we adopt the architecutre of ResNet (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) as 15Provable Dynamic Fusion for Low-Quality Multimodal Data the backbone network for image modality and pre-trained Bert(Devlin et al., 2018) for text modality. For concatenated-Bow fusion, we use the Bow (Pennington et al., 2014) to replace BERT for text modality. For the Bert models, we use BertAdam and regular Adam for the other models. The learning rate is 1e-4 with a warmup rate of 0.1. We adopt the early stop strategy based on validation accuracy. For all above experiments, we conduct sampling during the whole training phase (Ts = 1). The hyperparameter λ is set to 0.1. Temperature parameters {T m}M m=1 are set to 1. C. Additional results C.1. Full results with standard deviation In this section, we present the full results with standard deviation in Tab. 5, and Tab. 4. C.2. Different type of noise We provide more results with different type of noise (i.e., salt-pepper noise with varying noise rate ϵ) in Tab. 6. The results validate that the proposed method can improve the performance of multimodal fusion methods under different type of noise. Table 4.Full ablation study on NYU Depth V2. UAW Lreg ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 ϵ = 20.0 ✗ ✗ 69.14 ± 0.69 68 .35 ± 0.82 59 .62 ± 1.17 53 .98 ± 1.08 ✗ ✓ 69.68 ± 0.39 67 .74 ± 0.40 61 .35 ± 0.34 58 .26 ± 0.1.13 ✓ ✗ 70.06 ± 0.1.03 69 .11 ± 0.8269.11 ± 0.8269.11 ± 0.82 61 .59 ± 0.0.72 57 .49 ± 1.41 ✓ ✓ 70.09 ± 0.3870.09 ± 0.3870.09 ± 0.38 68 .81 ± 0.62 61 .62 ± 0.3161.62 ± 0.3161.62 ± 0.31 58 .87 ± 0.4058.87 ± 0.4058.87 ± 0.40 16Provable Dynamic Fusion for Low-Quality Multimodal Data Table 5.Full comparison results when 50% of the modalities are corrupted with Gaussian noise. Dataset Dynamic Method ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 NYU Depth V2 ✗ RGB 62.65 ± 1.22 50 .95 ± 3.38 44 .13 ± 3.80 ✗ Depth 63.30 ± 0.48 53 .12 ± 1.52 45 .46 ± 2.07 ✗ Late fusion 69.14 ± 0.67 59 .63 ± 2.44 51 .99 ± 3.11 ✗ Concat 70.31 ± 0.80 59 .97 ± 2.14 53 .20 ± 3.50 ✗ Align 70.31 ± 1.28 59 .47 ± 1.84 51 .74 ± 3.41 ✓ MMTM 71.04 ± 0.41 60 .37 ± 2.61 52 .28 ± 3.77 ✓ TMC 71.06 ± 0.7671.06 ± 0.7671.06 ± 0.76 61 .04 ± 1.66 53 .36 ± 2.76 ✓ Ours 70.09 ± 0.97 61 .62 ± 1.8461.62 ± 1.8461.62 ± 1.84 55 .60 ± 2.0955.60 ± 2.0955.60 ± 2.09 SUN RGB-D ✗ RGB 52.99 ± 0.88 37 .81 ± 1.14 33 .07 ± 1.81 ✗ Depth 56.78 ± 0.19 48 .40 ± 1.11 42 .94 ± 1.63 ✗ Late fusion 62.00 ± 0.15 52 .52 ± 0.67 47 .48 ± 1.40 ✗ Concat 62.48 ± 0.5062.48 ± 0.5062.48 ± 0.50 53 .30 ± 0.39 48 .01 ± 0.96 ✗ Align 61.12 ± 0.61 50 .05 ± 1.59 44 .19 ± 2.18 ✓ MMTM 61.72 ± 0.67 51 .86 ± 1.14 46 .03 ± 1.47 ✓ TMC 60.68 ± 0.24 51 .24 ± 0.96 45 .66 ± 2.06 ✓ Ours 62.09 ± 0.56 53 .40 ± 0.8953.40 ± 0.8953.40 ± 0.89 48 .58 ± 0.8248.58 ± 0.8248.58 ± 0.82 UMPC FOOD101 ✗ Bow 82.50 ± 0.18 61 .68 ± 0.71 41 .95 ± 0.54 ✗ Img 64.62 ± 0.40 34 .72 ± 0.53 33 .03 ± 0.37 ✗ Bert 86.46 ± 0.05 67 .38 ± 0.19 43 .88 ± 0.32 ✗ Late fusion 90.69 ± 0.12 68 .49 ± 3.37 57 .99 ± 1.59 ✗ Concatbow 70.77 ± 0.09 38 .28 ± 0.26 35 .68 ± 0.69 ✗ Concatbert 88.20 ± 0.34 61 .10 ± 2.02 49 .86 ± 2.05 ✓ MMBT 91.52 ± 0.10 72 .32 ± 0.34 56 .75 ± 0.33 ✓ TMC 89.86 ± 0.07 73 .93 ± 0.34 61 .37 ± 0.21 ✓ Ours 92.92 ± 0.1192.92 ± 0.1192.92 ± 0.11 76 .03 ± 0.7076.03 ± 0.7076.03 ± 0.70 62 .21 ± 0.2562.21 ± 0.2562.21 ± 0.25 MVSA ✗ Bow 48.79 ± 7.05 42 .20 ± 6.40 41 .57 ± 6.28 ✗ Img 64.12 ± 1.23 49 .36 ± 2.02 45 .00 ± 2.63 ✗ Bert 75.61 ± 0.53 69 .50 ± 1.50 47 .41 ± 0.79 ✗ Late fusion 76.88 ± 1.30 63 .46 ± 3.46 55 .16 ± 3.60 ✗ ConcatBow 64.08 ± 1.54 49 .95 ± 2.29 45 .39 ± 3.03 ✗ ConcatBert 65.59 ± 1.33 50 .70 ± 2.65 46 .12 ± 2.44 ✓ MMBT 78.50 ± 0.40 71 .99 ± 1.04 55 .34 ± 2.84 ✓ TMC 74.87 ± 2.24 66 .72 ± 4.55 60 .35 ± 2.79 ✓ Ours 78.07 ± 1.1078.07 ± 1.1078.07 ± 1.10 73 .85 ± 1.4273.85 ± 1.4273.85 ± 1.42 61 .28 ± 2.1261.28 ± 2.1261.28 ± 2.12 17Provable Dynamic Fusion for Low-Quality Multimodal Data Table 6.Full comparison results when 50% of the modalities are corrupted with Salt-pepper noise. Dataset Dynamic Method ϵ = 0.0 ϵ = 5.0 ϵ = 10.0 NYU Depth V2 ✗ RGB 62.61 ± 1.21 49 .14 ± 1.40 34 .76 ± 1.59 ✗ Depth 63.32 ± 0.50 50 .99 ± 1.41 38 .56 ± 2.16 ✗ Late fusion 69.16 ± 0.68 56 .27 ± 2.40 41 .22 ± 2.78 ✗ Concat 70.44 ± 0.89 57 .98 ± 2.08 44 .51 ± 2.91 ✗ Align 70.31 ± 1.28 57 .54 ± 2.50 43 .01 ± 2.66 ✓ MMTM 71.04 ± 0.4171.04 ± 0.4171.04 ± 0.41 59 .45 ± 1.3859.45 ± 1.3859.45 ± 1.38 44 .59 ± 2.49 ✓ TMC 71.01 ± 0.75 59 .34 ± 1.03 44 .65 ± 2.30 ✓ Ours 70.06 ± 0.81 58 .50 ± 2.05 45 .69 ± 2.7945.69 ± 2.7945.69 ± 2.79 SUN RGB-D ✗ RGB 52.63 ± 0.89 40 .42 ± 0.99 28 .15 ± 1.00 ✗ Depth 56.81 ± 0.57 46 .36 ± 0.82 35 .66 ± 1.44 ✗ Late fusion 61.79 ± 0.57 51 .54 ± 2.12 39 .35 ± 2.89 ✗ Concat 62.06 ± 0.5362.06 ± 0.5362.06 ± 0.53 51 .09 ± 1.91 38 .61 ± 3.07 ✗ Align 61.02 ± 0.54 50 .45 ± 0.82 38 .70 ± 1.46 ✓ MMTM 61.80 ± 0.40 51 .09 ± 0.77 38 .38 ± 1.56 ✓ TMC 61.02 ± 0.39 50 .88 ± 1.28 39 .61 ± 2.30 ✓ Ours 61.89 ± 0.49 52 .49 ± 1.8152.49 ± 1.8152.49 ± 1.81 40 .53 ± 2.7940.53 ± 2.7940.53 ± 2.79 UMPC FOOD101 ✗ Bow 82.43 ± 0.18 60 .83 ± 0.54 41 .56 ± 0.33 ✗ Img 64.53 ± 0.47 50 .75 ± 0.44 36 .83 ± 0.92 ✗ Bert 86.44 ± 0.02 67 .41 ± 0.20 43 .89 ± 0.33 ✗ Late fusion 90.66 ± 0.16 77 .99 ± 0.54 58 .75 ± 0.99 ✗ Concatbow 70.68 ± 0.12 55 .01 ± 0.33 38 .81 ± 0.62 ✗ Concatbert 88.22 ± 0.36 72 .49 ± 0.75 52 .10 ± 0.97 ✓ MMBT 91.51 ± 0.10 76 .27 ± 0.22 54 .98 ± 0.55 ✓ TMC 89.86 ± 0.07 77 .86 ± 0.41 60 .22 ± 0.43 ✓ Ours 92.90 ± 0.1392.90 ± 0.1392.90 ± 0.13 80 .87 ± 0.4080.87 ± 0.4080.87 ± 0.40 61 .60 ± 0.2061.60 ± 0.2061.60 ± 0.20 MVSA ✗ Bow 48.82 ± 7.08 42 .23 ± 6.43 41 .60 ± 6.31 ✗ Img 64.12 ± 1.23 56 .72 ± 1.92 50 .71 ± 3.20 ✗ Bert 75.61 ± 0.53 69 .50 ± 1.50 47 .41 ± 0.79 ✗ Late fusion 76.88 ± 1.30 67 .88 ± 1.87 55 .43 ± 1.94 ✗ ConcatBow 64.08 ± 1.54 56 .66 ± 1.73 49 .35 ± 2.44 ✗ ConcatBert 65.59 ± 1.33 58 .69 ± 2.25 51 .16 ± 2.99 ✓ MMBT 78.50 ± 0.4078.50 ± 0.4078.50 ± 0.40 74 .07 ± 1.1274.07 ± 1.1274.07 ± 1.12 51 .26 ± 5.65 ✓ TMC 74.87 ± 2.24 68 .02 ± 3.07 56 .62 ± 3.67 ✓ Ours 78.07 ± 1.10 73 .90 ± 1.89 60 .41 ± 2.6360.41 ± 2.6360.41 ± 2.63 18",
      "references": [
        "Multimodal biomedical ai",
        "Deep learning-based robust positioning for all-weather autonomous driving",
        "Noise estimation using density estimation for self- supervised multimodal learning",
        "Unsupervised label noise modeling and loss correction",
        "Multimodal machine learning: A survey and taxonomy",
        "Rademacher and gaussian complexities: Risk bounds and structural results",
        "Weight uncertainty in neural network",
        "Mmsnet: Multi-modal scene recognition using multi-scale encoded features",
        "Fast road segmentation via uncertainty-aware symmetric network",
        "Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts",
        "Atom: Robus- tifying out-of-distribution detection using outlier mining",
        "Uncertainty-based fusion netwok for automatic skin le- sion diagnosis",
        "Multimodal object detection via probabilistic ensembling",
        "Supervised learning of universal sentence rep- resentations from natural language inference data",
        "Imagenet: A large-scale hierarchical image database",
        "Transforming neural-net output levels to probability distributions",
        "Bert: Pre-training of deep bidirectional transformers for lan- guage understanding",
        "A review and meta- analysis of multimodal affect detection systems",
        "Multimodal deep learning for robust rgb-d object recognition",
        "Is out-of-distribution detection learnable?",
        "Multi-modal rgb-d scene recognition across domains",
        "Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning",
        "Image and text fusion for upmc food-101 using bert and cnns",
        "Bias-reduced un- certainty estimation for deep neural classifiers",
        "Uncertainty-aware multi-view representation learning",
        "Omnivore: A single model for many visual modalities",
        "Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection",
        "On calibration of modern neural networks",
        "Trusted multi- view classification",
        "Umix: Im- proving importance weighting for subpopulation shift via uncertainty-aware mixup",
        "Multi- modal dynamics: Dynamical fusion for trustworthy multi- modal classification",
        "Trusted multi- view classification with dynamic evidential fusion",
        "Training independent subnetworks for robust prediction",
        "Deep residual learn- ing for image recognition",
        "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
        "Deep anomaly detection with outlier exposure",
        "What makes multi-modal learning better than single (provably)",
        "Modality competition: What makes joint training of multi- modal network fail in deep learning?(provably)",
        "Learning with noisy correspondence for cross- modal matching",
        "Mmtm: Multimodal transfer module for cnn fusion",
        "Training ood detectors in their natural habitats",
        "Supervised multimodal bitransformers for classi- fying images and text",
        "Simple and scalable predictive uncertainty estimation using deep ensembles",
        "Trust- worthy long-tailed classification",
        "Confidence- aware fusion using dempster-shafer theory for multispec- tral pedestrian detection",
        "Energy-based out- of-distribution detection",
        "Large-scale long-tailed recognition in an open world",
        "A simple baseline for bayesian uncertainty in deep learning",
        "Multisensory processing in sensory-specific cortical areas",
        "Bayesian methods for adaptive models",
        "Trustworthy multimodal regression with mixture of normal-inverse gamma distributions",
        "Predictive uncertainty esti- mation via prior networks",
        "Towards neural networks that provably know when they don’t know",
        "Poem: Out-of-distribution detection with posterior sampling",
        "Confidence- aware learning for deep neural networks",
        "Deep multimodal learning for audio-visual speech recognition",
        "Bayesian learning for neural networks",
        "Sentiment analysis on multi-view social data",
        "Measuring calibration in deep learning",
        "Glove: Global vectors for word representation",
        "Multimodal techniques for diagnosis and prognosis of alzheimer’s disease",
        "Multimodal deep learning for alzheimer’s disease dementia assessment",
        "Principles of mathematical analysis",
        "Wavelet denoising of mul- ticomponent images using gaussian scale mixture models and a noise-free image as priors",
        "Multisensory contributions to low-level,‘unisensory’processing",
        "Efficient rgb-d semantic segmentation for indoor scene analysis",
        "Efficient multi-task rgb-d scene analysis for indoor envi- ronments",
        "A mathematical theory of evidence",
        "Indoor segmentation and support inference from rgbd images",
        "Noise- robust learning from multiple unsupervised sources of inferred labels",
        "Early versus late fusion in semantic video analysis",
        "Sun rgb-d: A rgb-d scene understanding benchmark suite",
        "Dropout: a simple way to prevent neural networks from overfitting",
        "Uncertainty-aware audiovisual activity recog- nition using deep bayesian variational inference",
        "Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty- aware multimodal emotion recognition",
        "Uno: Uncertainty-aware noisy-or multimodal fusion for unanticipated input degradation",
        "Graphmix: Improved training of gnns for semi-supervised learning",
        "Learn- ing common and specific features for rgb-d semantic seg- mentation with deconvolutional networks",
        "Mogonet integrates multi-omics data using graph convolutional networks allowing patient clas- sification and biomarker identification",
        "What makes training multi-modal classification networks hard?",
        "Recipe recognition with large multimodal food dataset",
        "Multimodal token fusion for vision transformers",
        "Deep multimodal fusion by channel exchanging",
        "Wearable multimode sen- sor with a seamless integrated structure for recognition of different joint motion states with the assistance of a deep learning algorithm",
        "Mutual in- formation gradient estimation for representation learning",
        "Calibration tests in multi-class classification: A unifying framework",
        "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
        "Multimodal end-to-end autonomous driv- ing",
        "Data noising as smoothing in neural network language models",
        "Different data, different modalities! reinforced data splitting for effective multimodal information extrac- tion from social media posts",
        "Multi-interactive memory network for aspect based multimodal sentiment analy- sis",
        "A deep multi-level at- tentive network for multimodal sentiment analysis",
        "Weakly aligned cross-modal learning for multispectral pedestrian detection",
        "Improving multispectral pedestrian detection by addressing modality imbalance problems"
      ],
      "meta_data": {
        "arxiv_id": "2306.02050v2",
        "authors": [
          "Qingyang Zhang",
          "Haitao Wu",
          "Changqing Zhang",
          "Qinghua Hu",
          "Huazhu Fu",
          "Joey Tianyi Zhou",
          "Xi Peng"
        ],
        "published_date": "2023-06-03T08:32:35Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Provides the first rigorous generalization-theoretic analysis of dynamic multimodal fusion under low-quality or noisy modalities; derives a Rademacher-complexity bound showing that dynamic fusion surpasses static fusion when fusion weights are negatively correlated with unimodal errors; reveals that such weights can be obtained via uncertainty estimation; introduces Quality-aware Multimodal Fusion (QMF) that assigns sample-wise weights using energy-based uncertainty and an additional regularizer; demonstrates superior robustness and accuracy over prior fusion methods on several benchmarks.",
        "methodology": "1) Formalizes decision-level late fusion: final prediction is a weighted sum of unimodal classifiers. 2) Derives bounds for binary logistic loss, decomposing error into empirical loss, model complexity, and covariance between weights and losses. 3) Shows dynamic fusion is better if expected weights equal static weights and correlation between weights and losses is negative. 4) Uses uncertainty estimation to satisfy this criterion; assumes uncertainty positively correlates with loss. 5) Implements QMF: (a) compute per-modality energy score as uncertainty; (b) convert to weights via linear negative mapping with tunable parameters; (c) add sampling-based regularization that enforces inverse relation between historical loss and weight; (d) train unimodal networks jointly and fuse predictions on-the-fly.",
        "experimental_setup": "Tasks: multiclass scene recognition (NYU Depth V2 with 10 classes; SUN RGB-D with 19 classes, RGB+Depth), food classification (UPMC FOOD101, image+text), and sentiment analysis (MVSA, image+text).\nBaselines: unimodal models, static fusion (late, concat, align), and dynamic fusion (MMTM, TMC, MMBT). Backbones include ResNet-50 for images, BERT/BOW for text.\nNoise simulation: randomly corrupt 50% of modalities with Gaussian, salt-pepper, or blank noise to emulate low-quality data.\nMetrics: mean, standard deviation, and worst-case accuracy over multiple random seeds (10 for scene datasets, 3–5 for others). Also report Pearson correlation between fusion weights and losses.\nTraining: Adam/BertAdam optimizers; learning rate 1e-4; dropout 0.1; early stopping. QMF temperature T=1, regularization weight λ=0.1, sampling over the whole training trajectory.",
        "limitations": "1) Effectiveness hinges on accurate uncertainty estimation; current energy scores yield only moderate correlation on some datasets. 2) Theory restricted to binary classification, logistic loss, decision-level late fusion, and linear weight functions, which may not generalize to complex tasks or fusion strategies. 3) Assumes uncertainty correlates with error (Assumption 1); this may fail when OOD detection is unlearnable. 4) Experiments limited to two-modality settings and classification; scalability to more modalities or other tasks (detection, regression) not validated. 5) Additional hyper-parameters (temperature, λ) and sampling increase training complexity.",
        "future_research_directions": "• Develop more powerful uncertainty estimators (e.g., Bayesian ensembles, density models) to enhance weight–error correlation.\n• Extend theoretical analysis to multiclass settings, other losses, regression, and intermediate/early fusion architectures.\n• Investigate dynamic fusion with more than two modalities and real-time sensor failures.\n• Explore non-linear or learnable weighting functions and joint optimization of uncertainty and fusion.\n• Study robustness under distributional shift and formally address cases where OOD detection is provably hard.\n• Apply QMF framework to other domains such as medical imaging, autonomous driving, and multi-omics data.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Zero-Shot Reinforcement Learning from Low Quality Data",
      "full_text": "Zero-Shot Reinforcement Learning from Low Quality Data Scott Jeen University of Cambridge srj38@cam.ac.uk Tom Bewley University of Bristol tomdbewley@gmail.com Jonathan M. Cullen University of Cambridge jmc99@cam.ac.uk Abstract Zero-shot reinforcement learning (RL) promises to provide agents that can per- form any task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by conservatism, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via the project page https://enjeeneer.io/projects/zero-shot-rl/. 1 Introduction Today’s large pre-trained models generalise impressively to unseen vision [70] and language [9] tasks, but not to sequential decision-making problems. Zero-shot reinforcement learning (RL) attempts to correct this, asking, informally: can we pre-train an agent on a dataset of reward-free transitions such that it can perform any downstream task in an environment? Recently, methods leveraging successor features [5, 7] and successor measures [ 6, 82] have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks [83]. These works have assumed access to a large heterogeneous dataset of transitions for pre-training. In theory, such datasets could be curated by highly-exploratory agents during an upfront data collection phase [32, 10, 18, 62, 63, 35, 48]. However, in practice, deploying such agents in real systems can be time-consuming, costly or dangerous. To avoid these downsides, it would be convenient to skip the data collection phase and pre-train on historical datasets. Whilst these are common in the real world, they are usually produced by controllers that are not optimising for data heterogeneity [16], making them smaller and less diverse than current zero-shot RL methods expect. Can we still perform zero-shot RL using these datasets? This is the primary question this paper seeks to answer, and one we address in four parts. First, we investigate the performance of existing methods when trained on such datasets, finding their performance suffers because of out-of-distribution state-action value overestimation, a well-observed phenomenon in single-task offline RL. Second, we develop ideas from conservatism in single-task offline RL for use in the zero-shot RL setting, introducing a straightforward regularizer of OODvalues or measures that can be used by any zero-shot RL algorithm (Figure 1). Third, we conduct experiments across varied domains, tasks and datasets, showing our conservative zero-shot RL proposals outperform their non-conservative counterparts, and surpass the performance of methods that get to see the task in advance. Finally, we establish 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2309.15178v3  [cs.LG]  30 Oct 2024Figure 1: Conservative zero-shot RL.. ( Left) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task zcollect, yet generalise to new tasks zeval. Both tasks have associated optimal value functions Q∗ zcollect and Q∗ zeval for a given marginal state. (Middle) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (Right) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots (•) represent state-action samples present in the dataset. that our proposals do not hinder performance on large heterogeneous datasets, meaning adopting them presents little downside. We believe the ideas explored in this paper represent a step toward real-world deployment of zero-shot RL methods. 2 Preliminaries Markov decision processes A reward-freeMarkov Decision Process (MDP) is defined by M = {S, A, T , γ} where S is the state space, A is the action space, T : S × A →∆(S) is the transition function, where ∆(X) denotes the set of possible distributions over X, and γ ∈ [0, 1) is the discount factor [77]. Given (s0, a0) ∈ S × Aand a policy π : S →∆(A), we denote Pr(·|s0, a0, π) and E[·|s0, a0, π] the probabilities and expectations under state-action sequences (st, at)t≥0 starting at (s0, a0) and following policyπ, with st ∼ T(·|st−1, at−1) and at ∼ π(·|st). Given a reward function r : S →R≥0, the Q function of π for r is Qπ r := P t≥0 γtE[r(st+1)|s0, a0, π]. Zero-shot RL For pre-training, the agent has access to a static offline dataset of reward-free transitions D = {(si, ai, si+1)}|D| i=1 generated by an unknown behaviour policy, and cannot interact with the environment. At test time, a reward function reval specifying a task is revealed and the agent must return a policy for the task without any further planning or learning. Ideally, the policy should maximise the expected discounted return on the task E[P t≥0 γtreval(st+1)|s0, a0, π]. The reward function is specified either via a small dataset of reward-labelled statesDlabelled = {(si, reval(si))}k i=1 with k ≤ 10, 000 or as an explicit function s 7→ reval(s) (like 1 at a goal state and 0 elsewhere). Intuitively, the zero-shot RL problem asks: is it possible to train an agent using a pre-collected dataset of transitions from an environment such that, at test time, it can return the optimal policy for any task in that environment without any further planning or learning? State-of-the-art zero-shot RL methods leverage either successor measures [6] or successor features [5], with the former instantiated by forward backward representations[82] and the latter by universal successor features [7]. The remainder of this section introduces these ideas. Successor measures The successor measure Mπ(s0, a0, ·) over S is the cumulative discounted time spent in each future state st+1 after starting in state s0, taking action a0, and following policy π thereafter: Mπ(s0, a0, X) := P t≥0γt Pr(st+1 ∈ X|s0, a0, π) ∀ X ⊂ S. (1) The Q function of policy π for task r is the integral of r with respect to Mπ: Qπ r (s0, a0) := R s+∈S r(s+)Mπ(s0, a0, s+). (2) The forward-backward framework FB representations [82] approximate the successor measures of near-optimal policies for any task. Let ρ be an arbitrary state distribution, and Rd be a representation 2space. FB representations are composed of a forward model F : S × A ×Rd → Rd, a backward model B : S →Rd, and set of polices (πz)z∈Rd. They are trained such that Mπz (s0, a0, X) ≈ Z X F(s0, a0, z)⊤B(s)ρ(ds) ∀ s0 ∈ S, a0 ∈ A, X⊂ S, z∈ Rd, (3) and πz(s) ≈ arg max a F(s, a, z)⊤z ∀ (s, a) ∈ S × A, z∈ Rd. (4) Intuitively, Equation 3 says that the approximated successor measure under πz from (s0, a0) to s is high if their respective forward and backward embeddings are similar i.e. have large dot product. By comparing Equation 2 and Equation 3, we see that an FB representation can be used to approximate the Q function of πz with respect to any reward function r as: Qπz r (s0, a0) ≈ R s∈S r(s)F(s0, a0, z)⊤B(s)ρ(ds) = F(s0, a0, z)⊤Es∼ρ[r(s)B(s) ]. (5) Training of F and B is done with TD learning [71, 78] using transition data sampled from D: LFB = E(st,at,st+1,s+)∼D,z∼Z[(F(st, at, z)⊤B(s+) − γ ¯F(st+1, πz(st+1), z)⊤ ¯B(s+))2 − 2F(st, at, z)⊤B(st+1)], (6) where s+ is sampled independently of (st, at, st+1), ¯F and ¯B are lagging target networks, and Z is a task sampling distribution. The policy is trained in an actor-critic formulation [47]. See [82] for a full derivation of the TD update, and our Appendix B.1 for practical implementation details including the specific choice of task sampling distribution Z. By relating Equations 4 and 5, we find z = Es∼ρ[r(s)B(s)] for some reward function r. At test time, we can use this property to perform zero-shot RL. Using Dlabelled, we estimate the task as zeval ≈ Es∼Dlabelled [reval(s)B(s)] and pass it as an argument to πz. If zeval lies within the task sampling distribution Z used during pre-training, then πz(s) ≈ arg maxa Qπz reval (s, a), and hence this policy is approximately optimal for reval. (Universal) successor features Successor featuresassume access to a basic feature mapφ : S 7→Rd that embeds states into a representation space, and are defined as the expected discounted sum of future features ψπ(s0, a0) := E[P t≥0 γtφ(st+1)|s0, a0, π] [5]. They are made universal by conditioning their predictions on a family of policies πz ψ(s0, a0, z) = E  X t≥0 γtφ(st+1)|s0, a0, πz   ∀ s0 ∈ S, a0 ∈ A, z∈ Rd, (7) with πz(s) ≈ arg max a ψ(s, a, z)⊤z, ∀ (s0, a0) ∈ S × A, z∈ Rd. (8) Like FB, USFs are trained using TD learning on LSF = E(st,at,st+1)∼D,z∼Z[(ψ(st, at, z)⊤z − φ(st+1)⊤z − γ ¯ψ(st+1, πz(st+1), z)⊤z)2], (9) where ¯ψ is a lagging target network, and Z is the same z sampling distribution used for FB. We refer the reader to [7] for a derivation of the TD update and full learning procedure. Test time policy inference is performed similarly to FB. Using Dlabelled, the task is inferred by performing a linear regression of reval onto the features: zeval := arg minz Es∼Dlabelled [(reval(s) − φ(s)⊤z)2] before it is passed as an argument to the policy. 3 Zero-Shot RL from Low Quality Data In this section we introduce methods for improving the performance of zero-shot RL methods on low quality datasets. In Section 3.1, we explore the failure mode of existing methods on such datasets. Then, in Section 3.2, we propose straightforward amendments to these methods that address the failure mode. Finally, in Section 3.3, we illustrate the usefulness of our proposals with a controlled example. We develop our methods within the FB framework because of its superior empirical performance [83], but our proposals are also compatible with USF. We push their derivation to Appendix D for brevity. 3Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM . Q values predicted during training increase as both the size and “quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is “high\" quality, and the RANDOM dataset is “low\" quality–see Appendix A.2 for more details. 3.1 Failure Mode of Existing Methods To investigate the failure mode of existing methods we examine the FB loss (Equation 6) more closely. The TD target includes an action produced by the current policy at+1 ∼ πz(st+1). Equation 4 shows this is the policy’s current best estimate of the Q-maximising action in state s for task z. For finite datasets, this maximisation does not constrain the policy to actions observed in the dataset, and so it can become biased towards out-of-distribution (OOD) actions thought to be of high value. In such instances F and B are updated towards targets for which the dataset provides no support. This distribution shift is a well-observed phenomenon in single-task offline RL [42, 46, 44], and is exacerbated by small, low-diversity datasets as we explore in Figure 2. 3.2 Mitigating the Distribution Shift In the single-task setting, the distribution shift is addressed by applying constraints to either the policy, value function or model (see Section 6 for a summary of past work). Here we re-purpose single-task value function and model regularisation for use in the zero-shot RL setting. To avoid further complicating zero-shot RL methods, we only consider regularisation techniques that do not introduce new parametric functions. We discuss the implications of this decision in Section 5. Conservative Q-learning (CQL) [42, 44] regularises the Q function by querying OOD state-action pairs and suppressing their value. This is achieved by adding new term to the usual Q loss function LCQL = α · Es∼D,a∼µ(a|s)[Q(s, a)] − E(s,a)∼D[Q(s, a)]−H(µ) + LQ, (10) where α is a scaling parameter, µ(a|s) is a policy distribution selected to find the maximum value of the current Q function iterate, H(µ) is the entropy of µ used for regularisation, and LQ is the normal TD loss on Q. Equation 10 has the dual effect of minimising the peaks in Q under µ whilst maximising Q for state-action pairs in the dataset. We can replicate a similar form of regularisation in the FB framework, substitutingF(s, a, z)⊤z for Q in Equation 10 and adding the normal FB loss (Equation 6) LVC-FB = α · (Es∼D,a∼µ(a|s),z∼Z[F(s, a, z)⊤z]−E(s,a)∼D,z∼Z[F(s, a, z)⊤z] − H(µ)) + LFB. (11) The key difference between Equations 10 and 11 is that the former suppresses the value of OOD actions for one task, whereas the latter does so for all task vectors drawn from Z. We call models learnt with this loss value-conservative forward-backward representations(VC-FB). Because FB derives Q functions from successor measures (Equation 5), and because (by assumption) rewards are non-negative, suppressing the predicted measures for OOD actions provides an alternative route to suppressing their Q values. As we did with VC-FB, we can substitute FB’s successor measure approximation F(s, a, z)⊤B(s+) into Equation 10, which yields: 4Figure 3: Ignoring out-of-distribution actions. The agents are tasked with learning separate policies for reaching ⊛ and ⊛. (a) RND dataset with all “left\" actions removed; quivers represent the mean action direction in each state bin. (b) Best FB rollout after 1 million learning steps. (c) Best VC-FB performance after 1 million learning steps. FB overestimates the value of OOD actions and cannot complete either task; VC-FB synthesises the requisite information from the dataset and completes both tasks. LMC-FB = α · (Es∼D,a∼µ(a|s),z∼Z,s+∼D[F(s, a, z)⊤B(s+)] −E(s,a)∼D,z∼Z,s+∼D[F(s, a, z)⊤B(s+)] − H(µ)) + LFB. (12) Equation 12 has the effect of suppressing the expected visitation count to goal states+ when taking an OOD action for all task vectors drawn from Z, which says, informally, if we don’t know where OOD actions take us in the MDP, we assume they have low probability of taking us to any future states for all tasks. This is analogous to works that regularise model predictions in the single-task offline RL setting [37, 96, 69]. As such, we call this variant a measure-conservative forward-backward representation (MC-FB). Since it is not obvious a priori whether the VC-FB or MC-FB form of conservatism would be more effective in practice, we evaluate both in Section 4. Implementing these proposals requires two new model components: 1) a conservative penalty scaling factor α and 2) a way of obtaining policy distribution µ(a|s) that maximises the current value or measure iterate. For 1), we observe fixed values of α leading to fragile performance, so dynamically tune it at each learning–see Appendix B.1.4. For 2), the choice of maximum entropy regularisation following [44]’s CQL(H) allows µ to be approximated conveniently with a log-sum exponential across Q values derived from the current policy distribution and a uniform distribution. That this is true is not obvious, so we refer the reader to the detail and derivations in Section 3.2, Appendix A, and Appendix E of [44], as well as our adjustments to [44]’s theory in Appendix B.1.3. Code snippets demonstrating the required changes to a vanilla FB implementation are provided in Appendix G. We emphasise these additions represent only a small increase in the number of lines required to implement existing methods. 3.3 A Didactic Example To understand situations in which a conservative zero-shot RL methods may be useful, we introduce a modified version of Maze from the ExORL benchmark [ 95]. Episodes begin with a point-mass initialised in the upper left of the maze ( ⊚), and the agent is tasked with selecting x and y tilt directions such that the mass is moved towards one of two goal locations ( ⊛ and ⊛). The action space is two-dimensional and bounded in [−1, 1]. We take the RND dataset and remove all “left\" actions such that ax ∈ [0, 1] and ay ∈ [−1, 1], creating a dataset that has the necessary information for solving the tasks, but is inexhaustive (Figure 3 (a)). We train FB and VC-FB on this dataset and plot the highest-reward trajectories–Figure 3 (b) and (c). FB overestimates the value of OOD actions and cannot complete either task. Conversely, VC-FB synthesises the requisite information from the dataset and completes both tasks. 4 Experiments In this section we perform an empirical study to evaluate our proposals. We seek answers to four questions: (Q1) Can our proposals from Section 3 improve FB performance on small and/or low- quality exploratory datasets? (Q2) How does the performance of VC-FB and MC-FB vary with respect to task type and dataset diversity? (Q3) Do we sacrifice performance on full datasets for 5Figure 4: Aggregate zero-shot performance on ExORL.(Left) IQM of task scores across datasets and domains, normalised against the performance of CQL, our baseline. (Right) Performance profiles showing the distribution of scores across all tasks and domains. Both conservative FB variants stochastically dominate vanilla FB–see [1] for performance profile exposition. The black dashed line represents the IQM of CQL performance across all datasets, domains, tasks and seeds. performance on small and/or low-quality datasets? (Q4) If our pre-training dataset only covers behaviour related to one downstream task (i.e. the dataset distribution is narrow and not exploratory), can our proposals from Section 3 improve FB performance on that task? 4.1 Setup Domains We respond to Q1-Q3 using the ExORL benchmark [ 95]. ExORL provides datasets collected by unsupervised exploratory algorithms on the DeepMind Control Suite [80]. We select three of the same domains as [83]: Walker, Quadruped and Maze, but substitute Jaco for Cheetah. This provides two locomotion domains and two goal-reaching domains. Within each domain, we evaluate on all tasks provided by the DeepMind Control Suite for a total of 17 tasks across four domains. Full details are provided in Appendix A.1. We respond to Q4 using the D4RL benchmark [21]. We select the two MuJoCo [81] environments from the Open AI gym [8] that closest resemble those from ExORL: Walker2D and HalfCheetah. Datasets For Q1-Q3 we pre-train on three datasets of varying quality from ExORL. There is no unambiguous metric for quantifying dataset quality, so we use the reported performance of offline TD3 on Maze for each dataset as a proxy 1. We choose datasets collected via Random Network Distillation (RND) [10], Diversity is All You Need ( DIAYN) [18], and RANDOM policies, where agents trained on RND are the most performant, on DIAYN are median performers, and on RANDOM are the least performant. As well as selecting for quality, we also select for size by uniformly sub- sampling 100,000 transitions from each dataset. For Q4 we choose the “medium\", “medium-replay\", and “medium-expert\" datasets from D4RL, each providing different fractions of random, medium and expert task-directed trajectories. More details on the datasets are provided in Appendix A.2. 4.2 Baselines We compare our proposals to baselines from three categories: 1) zero-shot RL methods, 2) goal- conditioned RL (GCRL) methods, and 3) single-task offline RL methods. From category 1), we use the state-of-the-art successor measure based method, FB, and the state-of-the-art successor feature based method, SF with features from Laplacian eigenfunctions (SF-LAP) [83]. From category 2), we use goal-conditioned IQL (GC-IQL) [60], a state-of-the-art GCRL method that, like our proposals, regularises the value function at OOD state-actions. We condition GC-IQL on the goal state on Maze and Jaco, and on the state in Dlabelled with highest reward on Walker and Quadruped in lieu of a 1We note that [ 75] propose metrics that describe dataset quality as a function of the behaviour policy’s exploration and exploitation w.r.t. one downstream task. However, since we are interested in generalising to any downstream task we cannot use these proposals directly, nor can we easily re-purpose them. We acknowledge that our proxy is imperfect, and that more work is required to better understand what dataset quality means in the context of zero-shot RL. 6Zero-shot Performance (abs.) Random 100k RND 100k DIAYN 100k Walker Maze Quadruped Jaco FB MC-FB VC-FB CQLSF-LAP GC-IQL 0 20 40 0 200 400  0 200 0 200 400 0 20 40 0 200 400  0 200 0 200 400 0 20 40 0 200 400  0 200 0 200 400 Figure 5: Performance by dataset/domain on ExORL.IQM scores across tasks/seeds with 95% conf. intervals. well-defined goal state. From category 3), we use CQL and offline TD3 trained on the same datasets relabelled with task rewards. CQL approximates what an algorithm with similar mechanistics can achieve when optimising for one task in a domain rather than all tasks. Offline TD3 exhibits the best aggregate single-task performance on the ExORL benchmark, so it should be indicative of the maximum performance we could expect to extract from a dataset. Full implementation details for all algorithms are provided in Appendix B. The full evaluation protocol is described in Appendix A.5. Appendix A.6 provides a breakdown of the computational resources used in this work. 4.3 Results Q1 We report the aggregate performance of our baselines and proposals on ExORL in Fig- ure 4. Both MC-FB and VC-FB outperform the zero-shot RL and GCRL baselines, achiev- ing 150% and 137% of FB’s IQM performance respectively. The performance gap between FB and SF-LAP is consistent with the results in [ 83]. MC-FB and VC-FB outperform our single-task baseline in expectation, reaching 111% and 120% of CQL’s IQM performance Figure 6: Performance by dataset size. Aggregate IQM scores across all domains and tasks as RND size is varied. The performance delta between vanilla FB and the conservative variants increases as dataset size decreases. respectively despite not having access to task- specific reward labels and needing to fit policies for all tasks. This is a surprising result, and to the best of our knowledge, the first time a multi- task offline agent has been shown to outperform a single-task analogue. CQL outperforms offline TD3 in aggregate, so we drop offline TD3 from the core analysis, but report its full results in Appendix C alongside all other methods. We note FB achieves 80% of single-task offline TD3, which roughly aligns with the 85% performance on the full datasets reported by [83]. Q2 We decompose the methods’ performance with respect to domain and dataset diversity in Figure 5. The largest gap in performance be- tween the conservative FB variants and FB is on RND. VC-FB and MC-FB reach 2.5× and 1.8× of FB performance respectively, and outperform CQL on three of the four domains. On DIAYN, the conservative variants outperform all methods and reach 1.3 × CQL’s score. On the RANDOM dataset, all methods perform similarly poorly, except for CQL on Jaco, which outperforms all meth- ods. However, in general, these results suggest the RANDOM dataset is not informative enough to extract valuable policies–discussed further in response to Q3. There appears to be little correlation between the type of domain (Appendix A.1) and the score achieved by any method. GC-IQL performs particularly well on the goal-reaching domains as expected, but worse than all zero-shot methods on the locomotion tasks, irrespective of whether they are conservative or not. This is presumably because the goal-state used to condition the policy (i.e. the state with highest reward in Dlabelled) is a poor proxy for the true, dense reward function. 7Table 1: Aggregate performance on full ExORL datasets. IQM scores aggregated over domains and tasks for all datasets, averaged across three seeds. Both VC-FB and MC-FB maintain the performance of FB; the largest relative performance improvement is on RANDOM . Dataset Domain Task FB VC-FB MC-FB RND all all 389 390 396 DIAYN all all 269 280 283 RANDOM all all 111 131 133 ALL all all 256 267 271 Q3 We report the aggregated performance of all FB methods across domains when trained on the full datasets in Table 1 (a full breakdown of results in provided in Appendix C). Both conservative FB variants slightly exceed the performance of vanilla FB in expectation. The largest relative performance improvement is on the RANDOM dataset–MC-FB performance is 20% higher than FB, compared to 5% higher on DIAYN and 2% higher on RND. This corrob- orates the hypothesis that RANDOM -100 K was not informative enough to extract valuable policies. Figure 7: Aggregate zero-shot performance on D4RL. Aggregate IQM scores across all domains and datasets, normalised against the performance of CQL. Table 1 and Figure 4 suggest the performance gap between the conservative FB variants and vanilla FB changes as dataset size is varied. We further explore this effect in Figure 6 where we scale the RND dataset size from 105 through 107 and plot aggregate IQM performance of FB, VC- FB and MC-FB across all domains. We find that the performance gap decreases as dataset size increases. This result is to be expected: a larger dataset size for a fixed exploration algorithm means at+1 ∼ πz(st+1) in the FB TD update (Equation 6) is more likely to be in the dataset, the policy is less likely to become biased toward OOD actions, and conservatism is less needed. Q4 We report the aggregate performance of all zero-shot RL methods and CQL on our D4RL domains in Figure 7. FB fails all domain-dataset tasks, and reaches only 10% of CQL’s aggregate performance. MC-FB and VC-FB improve on FB’s considerably (by 5.6 × and 6.8 × respectively) but under-perform CQL. SF-LAP outperforms FB, but under-performs VC-FB, MC-FB and CQL. 5 Discussion and Limitations Performance discrepancy between conservative variants Why does VC-FB outperform MC-FB on both ExORL and D4RL? To understand, we inspect the regularising effect of both models more closely. VC-FB regularises OOD actions on F(s, a, z)⊤z, with s ∼ D, and z ∼ Z, whilst MC-FB regularises OOD actions on F(s, a, z)⊤B(s+), with (s, s+) ∼ Dand z ∼ Z. Note the trailing z in VC-FB is replaced with B(s+) in MC-FB which ties its updates to D further. We hypothesised that as |D| reduces, B(s+) provides poorer task coverage than z ∼ Z, hence the comparable performance on full datasets and divergent performance on 100k datasets. To test this, we evaluate a third conservative variant called directed (D)VC-FB which replaces all z ∼ Zin VC-FB with B(s+) such that OOD actions are regularised on F(s, a, B(s+))⊤B(s+) with (s, s+) ∼ D. This ties conservative updates entirely to D, and according to our above hypothesis, DVC-FB should perform worse than VC-FB and MC-FB on the 100k ExORL datasets. See Appendix B.1.6 for implementation details. We evaluate this variant on all 100k ExORL datasets, domains and tasks and compare with FB, VC-FB and MC-FB in Table 2. See Appendix C for a full breakdown. We find the aggregate relative performance of each method is as expected i.e.DVC-FB < MC-FB < VC-FB. As a consequence we conclude that VC-FB should be preferred for small datasets with 8Table 2: Aggregated performance of conservative variants employing differing z sampling procedures on ExORL. DVC-FB derives all zs from the backward model; VC-FB derives allzs from Z; and MC-FB combines both. Performance correlates with the degree to which z ∼ Z. Dataset Domain Task FB DVC-FB MC-FB VC-FB ALL (100k) all all 99 108 136 148 no prior knowledge of the dataset or test tasks. Of course, for a specific domain-dataset pair, B(s+) with s+ ∼ Dmay happen to cover the tasks well, and MC-FB may outperform VC-FB. We suspect this was the case for all datasets on the Jaco domain for example. Establishing whether this will be true a priori requires either relaxing the restrictions imposed by the zero-shot RL setting, or better understanding of the distribution of tasks in z-space and their relationship to pre-training datasets. The latter is important future work. Avoiding new parametric functions State-of-the-art zero-shot RL methods are complex, and we wanted to avoid further complicating them with new parametric functions. This limited our solution- space to CQL-style regularisation techniques, but had we relaxed this constraint, other options become available. Methods like AW AC [58], IQL [40], and X-QL [25] all require an estimate of the state-value function which is not immediately accessible in the FB or USF frameworks. In theory, we could learn an action-independent USF of the form V (s, z) = E[P t≥0 γtφ(st+1)|s0, πz] ∀ s0 ∈ S, z∈ Rd concurrently to F and B (or ψ for USFs). If learnt with expectile regression, this function could be used to implement IQL and X-QL style regularisation; without expectile regression it could be used to compute the advantage weighting required for AWAC. It’s possible that implementing these methods could improve downstream performance and reduce computational overhead at the cost of increased training complexity. We leave this worthwhile investigation for future work. We provide detail of negative results related to downstream finetuning of FB models in Appendix E to help inform future research. D4RL Performance Unlike the ExORL results, VC-FB and MC-FB do not outperform CQL on the D4RL benchmark. We believe these narrower data distributions require a more careful selection of the conservative penalty scaling factor α. We explore this further in Appendix F, and note this is corroborated by findings in the original CQL paper [ 44]. Methods described above, like IQL, have been shown to be more robust than CQL partly because they bypass α tuning. We expect that exploring the integration of these methods may improve D4RL performance. 6 Related Work Zero-shot RL Zero-shot RL methods build upon successor representations [15], universal value function approximators [74], successor features [5] and successor measures [6]. The state-of-the-art methods instantiate these ideas as either universal successor features (USFs) [7] or forward-backward (FB) representations [82, 83], with recent work showing the latter can be used to perform a range of imitation learning techniques efficiently [ 65]. A representation learning method is required to learn the features for USFs, with past works using inverse curiosity modules [62], diversity methods [49, 29], Laplacian eigenfunctions [87], or contrastive learning [13]. No works have yet explored the issues arising when training these methods on low quality offline datasets, and only one has investigated applying these ideas to real-world problems [34]. Goal-conditioned RL methods train policies to reach any goal state from any other state, and so can be used to perform zero-shot RL in goal-reaching environments [60, 54, 93, 19, 85]. However, they have no principled mechanism for conditioning policies on “dense” reward functions (as such tasks are not solved by simply reaching a particular state), and so are not full zero-shot RL methods. A concurrent line of work trains policies using sequence models conditioned on reward-labelled histories [12, 33, 45, 68, 99, 11, 24, 76, 91, 90], but, unlike zero-shot RL methods, these works do not have a robust mechanism for generalising to different reward functions as test-time. Offline RL Offline RL algorithms require regularisation of policies, value functions, models, or a combination to manage the offline-to-online distribution shift [46]. Past works regularise policies with explicit constraints [88, 20, 23, 22, 27, 64, 43, 86, 94], via important sampling [66, 79, 50, 57, 26], by leveraging uncertainty in predictions [ 89, 98, 4, 36], or by minimising OOD action queries [84, 14, 40], a form of imitation learning [72, 73]. Other works constrain value function approximation 9so OOD action values are not overestimated [ 44, 42, 52, 53, 51, 92]. Offline model-based RL methods use the model to identify OOD states and penalise predicted rollouts passing through them [97, 37, 96, 2, 55, 67, 69]. All of these works have focused on regularising a finite number of policies; in contrast we extend this line of work to the zero-shot RL setting which is concerned with learning an infinite family of policies. 7 Conclusion In this paper, we explored training agents to perform zero-shot reinforcement learning (RL) from low quality data. We established that the existing methods suffer in this regime because they overestimate the value of out-of-distribution state-action values, a well-observed pheneomena in single-task offline RL. As a resolution, we proposed a family ofconservative zero-shot RL algorithms that regularise value functions or dynamics predictions on out-of-distribution state-action pairs. In experiments across various domains, tasks and datasets, we showed our proposals outperform their non-conservative counterparts in aggregate and sometimes surpass our task-specific baseline despite lacking access to reward labels a priori. In addition to improving performance when trained on sub-optimal datasets, we showed that performance on large, diverse datasets does not suffer as a consequence of our design decisions. Our proposals represent a step towards the use of zero-shot RL methods in the real world. Acknowledgements We thank Sergey Levine for helpful feedback on the core and finetuning experiments, and Alessandro Abate and Yann Ollivier for reviewing earlier versions of this manuscript. We also thank the anonymous reviewers whose suggestions significantly improved this work. Computational resources were provided by the Cambridge Centre for Data-Driven Discovery (C2D3) and Bristol Advanced Computing Research Centre (ACRC). This work was supported by an EPSRC DTP Studentship (EP/T517847/1) and Emerson Electric. References [1] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Belle- mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021. [2] Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint arXiv:2008.05556, 2020. [3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [4] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. arXiv preprint arXiv:2202.11566, 2022. [5] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. [6] Léonard Blier, Corentin Tallec, and Yann Ollivier. Learning successor states and goal-dependent values: A mathematical viewpoint. arXiv preprint arXiv:2101.07123, 2021. [7] Diana Borsa, André Barreto, John Quan, Daniel Mankowitz, Rémi Munos, Hado Van Hasselt, David Silver, and Tom Schaul. Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018. [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 10[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [10] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. [11] Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. arXiv preprint arXiv:2309.10150, 2023. [12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021. [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020. [14] Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best- action imitation learning for batch deep reinforcement learning.Advances in Neural Information Processing Systems, 33:18353–18363, 2020. [15] Peter Dayan. Improving generalization for temporal difference learning: The successor repre- sentation. Neural computation, 5(4):613–624, 1993. [16] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforce- ment learning. arXiv preprint arXiv:1904.12901, 2019. [17] Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution, pages 569–593. Springer, 1992. [18] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. [19] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Process- ing Systems, 35:35603–35620, 2022. [20] Rasool Fakoor, Jonas W Mueller, Kavosh Asadi, Pratik Chaudhari, and Alexander J Smola. Continuous doubly constrained batch reinforcement learning. Advances in Neural Information Processing Systems, 34:11260–11273, 2021. [21] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. [22] Scott Fujimoto, Wei-Di Chang, Edward Smith, Shixiang Shane Gu, Doina Precup, and David Meger. For sale: State-action representation learning for deep reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [23] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning , pages 2052–2062. PMLR, 2019. [24] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsight information matching. arXiv preprint arXiv:2111.10364, 2021. [25] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent rl without entropy. International Conference on Learning Representations, 2023. [26] Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3647–3655, 2019. 11[27] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In International Conference on Machine Learning, pages 3682–3691. PMLR, 2021. [28] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861–1870, 2018. [29] Steven Hansen, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and V olodymyr Mnih. Fast task inference with variational intrinsic successor features.arXiv preprint arXiv:1906.05030, 2019. [30] Charles R Harris, K Jarrod Millman, Stéfan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Array programming with numpy. Nature, 585(7825):357–362, 2020. [31] John D Hunter. Matplotlib: A 2d graphics environment. Computing in science & engineering, 9(03):90–95, 2007. [32] Max Jaderberg, V olodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016. [33] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273– 1286, 2021. [34] Scott Jeen, Alessandro Abate, and Jonathan M Cullen. Low emission building control with zero- shot reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 14259–14267, 2023. [35] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, pages 4870–4879, 13–18 Jul 2020. [36] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084–5096. PMLR, 2021. [37] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. Advances in neural information processing systems, 33:21810–21823, 2020. [38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [39] Yehuda Koren. On spectral graph drawing. In International Computing and Combinatorics Conference, pages 496–508. Springer, 2003. [40] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pages 5774–5783. PMLR, 2021. [41] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q- learning on diverse multi-task data both scales and generalizes.arXiv preprint arXiv:2211.15144, 2022. [42] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [43] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off- policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019. 12[44] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. arXiv preprint arXiv:2006.04779, 2020. [45] Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Ser- gio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. Advances in neural information processing systems, 35, 2022. [46] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [47] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR (Poster), 2016. [48] Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pages 6736–6747. PMLR, 2021. [49] Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pages 6736–6747. PMLR, 2021. [50] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state distribution correction. arXiv preprint arXiv:1904.08473, 2019. [51] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 35:1711–1724, 2022. [52] Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, and Bin Liang. Offline reinforcement learning with value-based episodic memory. arXiv preprint arXiv:2110.09796, 2021. [53] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. Advances in Neural Information Processing Systems, 34:19235–19247, 2021. [54] Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i’ll go: Offline goal-conditioned reinforcement learning via f-advantage regression. arXiv preprint arXiv:2206.03023, 2022. [55] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. arXiv preprint arXiv:2006.03647, 2020. [56] Wes McKinney et al. pandas: a foundational python library for data analysis and statistics. Python for high performance and scientific computing, 14(9):1–9, 2011. [57] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in neural information processing systems, 32, 2019. [58] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020. [59] Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. arXiv preprint arXiv:2303.05479, 2023. [60] Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goal- conditioned rl with latent states as actions. Advances in Neural Information Processing Systems, 37, 2023. [61] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 13[62] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778– 2787. PMLR, 2017. [63] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagree- ment. In International conference on machine learning, pages 5062–5071. PMLR, 2019. [64] Zhiyong Peng, Changlin Han, Yadong Liu, and Zongtan Zhou. Weighted policy constraints for offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9435–9443, 2023. [65] Matteo Pirotta, Andrea Tirinzoni, Ahmed Touati, Alessandro Lazaric, and Yann Ollivier. Fast imitation via behavior foundation models. In International Conference on Learning Representa- tions, 2024. [66] Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with function approximation. In ICML, pages 417–424, 2001. [67] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. In Learning for Dynamics and Control, pages 1154–1168. PMLR, 2021. [68] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. [69] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based offline reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 16082–16097. Curran Associates, Inc., 2022. [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. [71] Arthur L Samuel. Some studies in machine learning using the game of checkers. IBM Journal of research and development, 3(3):210–229, 1959. [72] Stefan Schaal. Learning from demonstration. Advances in neural information processing systems, 9, 1996. [73] Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):233–242, 1999. [74] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function ap- proximators. In International conference on machine learning , pages 1312–1320. PMLR, 2015. [75] Kajetan Schweighofer, Andreas Radler, Marius-Constantin Dinu, Markus Hofmarcher, Vihang Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A dataset perspective on offline reinforcement learning. arXiv preprint arXiv:2111.04714, 2021. [76] Max Siebenborn, Boris Belousov, Junning Huang, and Jan Peters. How crucial is transformer in decision transformer? arXiv preprint arXiv:2211.14655, 2022. [77] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. [78] Richard S Sutton. Learning to predict by the methods of temporal differences.Machine learning, 3:9–44, 1988. [79] Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):2603–2631, 2016. 14[80] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [81] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE, 2012. [82] Ahmed Touati and Yann Ollivier. Learning one representation to optimize all rewards.Advances in Neural Information Processing Systems, 34:13–23, 2021. [83] Ahmed Touati, Jérémy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In The Eleventh International Conference on Learning Representations, 2023. [84] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. Advances in Neural Information Processing Systems, 31, 2018. [85] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In International Conference on Machine Learning, pages 36411–36430. PMLR, 2023. [86] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. Advances in Neural Information Processing Systems, 35:31278–31291, 2022. [87] Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586, 2018. [88] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019. [89] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140, 2021. [90] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In Proceedings of the 39th International Conference on Machine Learning, pages 24631–24645, 17–23 Jul 2022. [91] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline RL. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 38989–39007, 23–29 Jul 2023. [92] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. Rorl: Ro- bust offline reinforcement learning via conservative smoothing.Advances in Neural Information Processing Systems, 35:23851–23866, 2022. [93] Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang, and Tong Zhang. What is essential for unseen goal generalization of offline goal-conditioned rl? In International Conference on Machine Learning, pages 39543–39571. PMLR, 2023. [94] Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, and Mingyuan Zhou. A behavior regularized implicit policy for offline reinforcement learning. arXiv preprint arXiv:2202.09673, 2022. [95] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don’t change the algorithm, change the data: Exploratory data for offline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022. [96] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954–28967, 2021. 15[97] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020. [98] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in neural information processing systems, 34:13626–13640, 2021. [99] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In international conference on machine learning, pages 27042–27059. PMLR, 2022. 16Appendices A Experimental Details 18 A.1 ExORL Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 ExORL Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.3 D4RL Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.4 D4RL Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.5 Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.6 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B Implementation Details 20 B.1 Forward-Backward Representations . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Universal Successor Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3 GC-IQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.4 CQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.5 TD3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.6 Code References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C Extended Results 25 D Value Conservative Universal Successor Features 28 E Negative Results 29 E.1 Downstream Finetuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 F Learning Curves & Hyperparameter Sensitivity 31 G Code Snippets 37 G.1 Update Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 G.2 Value-Conservative Penalty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 G.3 Measure-Conservative Penalty . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.4 α Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 H NeurIPS Paper Checklist 43 17A Experimental Details A.1 ExORL Domains We consider two locomotion and two goal-directed domains from the ExORL benchmark [ 95] which is built atop the DeepMind Control Suite [ 80]. Environments are visualised here: https: //www.youtube.com/watch?v=rAai4QzcYbs. The domains are summarised in Table 3. Walker. A two-legged robot required to perform locomotion starting from bent-kneed position. The state and action spaces are 24 and 6-dimensional respectively, consisting of joint torques, velocities and positions. ExORL provides four tasks stand, walk, run and flip. The reward function for stand motivates straightened legs and an upright torse; walk and run are supersets of stand including reward for small and large degrees of forward velocity; andflip motivates angular velocity of the torso after standing. Rewards are dense. Quadruped. A four-legged robot required to perform locomotion inside a 3D maze. The state and action spaces are 78 and 12-dimensional respectively, consisting of joint torques, velocities and positions. ExORL provides five tasks stand, roll, roll fast, jump and escape. The reward function for stand motivates a minimum torse height and straightened legs; roll and roll fast require the robot to flip from a position on its back with varying speed; jump adds a term motivating vertical displacement to stand; and escape requires the agent to escape from a 3D maze. Rewards are dense. Maze. A 2D maze with four rooms where the task is to move a point-mass to one of the rooms. The state and action spaces are 4 and 2-dimensional respectively; the state space consists of x, ypositions and velocities of the mass, the action space is the x, ytilt angle. ExORL provides four reaching tasks top left, top right, bottom left and bottom right. The mass is always initialised in the top left and the reward is proportional to the distance from the goal, though is sparse i.e. it only registers once the agent is reasonably close to the goal. Jaco. A 3D robotic arm tasked with reaching an object. The state and action spaces are 55 and 6-dimensional respectively and consist of joint torques, velocities and positions. ExORL provides four reaching tasks top left, top right, bottom left and bottom right. The reward is proportional to the distance from the goal object, though is sparse i.e. it only registers once the agent is reasonably close to the goal object. A.2 ExORL Datasets We train on 100,000 transitions uniformly sampled from three datasets on the ExORL benchmark collected by different unsupervised agents: RANDOM , DIAYN, and RND. The state coverage on Maze is depicted in Figure 8. Though harder to visualise, we found that state marginals on higher- dimensional tasks (e.g. Walker) showed a similar diversity in state coverage. RND. An agent whose exploration is directed by the predicted error in its ensemble of dynamics models. Informally, we say RND datasets exhibit high state diversity. DIAYN. An agent that attempts to sequentially learn a set of skills. Informally, we sayDIAYN datasets exhibit medium state diversity. Table 3: ExORL domain summary. Dimensionality refers to the relative size of state and action spaces. Type is the task categorisation, either locomotion (satisfy a prescribed behaviour until the episode ends) or goal-reaching (achieve a specific task to terminate the episode). Reward is the frequency with which non-zero rewards are provided, where dense refers to non-zero rewards at every timestep and sparse refers to non-zero rewards only at positions close to the goal. Green and red colours reflect the relative difficulty of these settings. Domain Dimensionality Type Reward Walker Low Locomotion Dense Quadruped High Locomotion Dense Maze Low Goal-reaching Sparse Jaco High Goal-reaching Sparse 18RANDOM . A agent that selects actions uniformly at random from the action space. Informally, we say RANDOM datasets exhibit low state diversity. Figure 8: Maze state coverage by dataset. (left) RANDOM ; (middle) DIAYN; (right) RND. A.3 D4RL Domains We consider two MuJoCo [81] locomotion tasks from the D4RL benchmark [21], which is built atop the v2 Open AI Gym [8]. The below environment descriptions are taken from [8]. Walker2D-v2. A two-dimensional two-legged figure that consist of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs in the bottom below the thighs, and two feet attached to the legs on which the entire body rests. The goal is to walk in the in the forward (right) direction by applying torques on the six hinges connecting the seven body parts. HalfCheetah-v2. A 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply a torque on the joints to make the cheetah run forward (right) as fast as possible, with a positive reward allocated based on the distance moved forward and a negative reward allocated for moving backward. A.4 D4RL Datasets We consider three goal-directed datasets from D4RL, each providing a different proportion of expert trajectories. The below dataset descriptions are taken from [21]. Medium. Generated by training an SAC policy, early-stopping the training, and collecting 1M samples from this partially-trained policy. Medium-replay. Generated by recording all samples in the replay buffer observed during training until the policy reaches the “medium” level of performance. Medium-expert. Generated by mixing equal amounts of expert demonstrations and suboptimal data, either from a partially trained policy or by unrolling a uniform-at-random policy. A.5 Evaluation Protocol We evaluate the cumulative reward (hereafter called score) achieved by VC-FB, MC-FB and our baselines on each task across five seeds. We report task scores as per the best practice recommenda- tions of [1]. Concretely, we run each algorithm for 1 million learning steps, evaluating task scores at checkpoints of 20,000 steps. At each checkpoint, we perform 10 rollouts, record the score of each, and find the interquartile mean (IQM). We average across seeds at each checkpoint to create the learning curves reported in Appendix F. From each learning curve, we extract task scores from the learning step for which the all-task IQM is maximised across seeds. Results are reported with 95% confidence intervals obtained via stratified bootstrapping [17]. Aggregation across tasks, domains and datasets is always performed by evaluating the IQM. Full implementation details are provided in Appendix B.1. 19A.6 Computational Resources We train our models on NVIDIA A100 GPUs. Training a single-task offline RL method to solve one task on one GPU takes approximately 4 hours. FB and SF solve one domain (for all tasks) on one GPU in approximately 4 hours. Conservative FB variants solve one domain (for all tasks) on one GPU in approximately 12 hours. As a result, our core experiments on the 100k datasets used approximately 110 GPU days of compute. B Implementation Details Here we detail implementations for all methods discussed in this paper. The code required to reproduce our experiments is available via https://github.com/enjeeneer/zero-shot-rl. B.1 Forward-Backward Representations B.1.1 Architecture The forward-backward architecture described below follows the implementation by [ 83] exactly, other than the batch size which we reduce from 1024 to 512. We did this to reduce the computational expense of each run without limiting performance. The hyperparameter study in Appendix J of [83] shows this choice is unlikely to affect FB performance. All other hyperparameters are reported in Table 4. Forward Representation F(s, a, z). The input to the forward representation F is always prepro- cessed. State-action pairs (s, a) and state-task pairs (s, z) have their own preprocessors which are feedforward MLPs that embed their inputs into a 512-dimensional space. These embeddings are con- catenated and passed through a third feedforward MLP F which outputs a d-dimensional embedding vector. Note: the forward representation F is identical to ψ used by USF so their implementations are identical (see Table 4). Backward Representation B(s). The backward representation B is a feedforward MLP that takes a state as input and outputs a d-dimensional embedding vector. Actor π(s, z). Like the forward representation, the inputs to the policy network are similarly preprocessed. State-action pairs (s, a) and state-task pairs (s, z) have their own preprocessors which feedforward MLPs that embed their inputs into a 512-dimensional space. These embeddings are concatenated and passed through a third feedforward MLP which outputs a a-dimensional vector, where a is the action-space dimensionality. A Tanh activation is used on the last layer to normalise their scale. As per [23]’s recommendations, the policy is smoothed by adding Gaussian noise σ to the actions during training. Note the actors used by FB and USFs are identical (see Table 4). Misc. Layer normalisation [ 3] and Tanh activations are used in the first layer of all MLPs to standardise the inputs. B.1.2 Task Sampling Distribution Z FB representations require a method for sampling the task vector z at each learning step. [83] employ a mix of two methods, which we replicate: 1. Uniform sampling of z on the hypersphere surface of radius √ d around the origin of Rd, 2. Biased sampling ofz by passing statess ∼ Dthrough the backward representationz = B(s). This also yields vectors on the hypersphere surface due to the L2 normalisation described above, but the distribution is non-uniform. We sample z 50:50 from these methods at each learning step. B.1.3 Maximum Value Approximator µ The conservative variants of FB require access to a policy distribution µ(a|s) that maximises the value of the current Q iterate in expectation. Recall the standard CQL loss 20Table 4: Hyperparameters for zero-shot RL methods. The additional hyperparameters for Conservative FB representations are highlighted in blue . Hyperparameter Value Latent dimension d 50 (100 for maze) F / ψ dimensions (1024, 1024) B / φ dimensions (256, 256, 256) Preprocessor dimensions (1024, 1024) Std. deviation for policy smoothing σ 0.2 Truncation level for policy smoothing 0.3 Learning steps 1,000,000 Batch size 512 Optimiser Adam [38] Learning rate 0.0001 Discount γ 0.98 (0.99 for maze) Activations (unless otherwise stated) ReLU Target network Polyak smoothing coefficient 0.01 z-inference labels 10,000 z mixing ratio 0.5 Conservative budget τ 50 (45 for D4RL) OOD action samples per policy N 3 LCQL = α · \u0000 Es∼D,a∼µ(a|s)[Q(s, a)] − E(s,a)∼D[Q(s, a)] − R(µ) \u0001 + LQ, (13) where α is a scaling parameter, µ(a|s) the policy distribution we seek, R regularises µ and LQ represents the normal TD loss onQ. [44]’s most performant CQL variant (CQL(H)) utilises maximum entropy regularisation on µ i.e. R = H(µ). They show that obtaining µ can be cast as a closed-form optimisation problem of the form: max µ Ex∼µ(x)[f(x)] + H(µ) s.t. X x µ(x) = 1, µ(x) ≥ 0 ∀x, (14) and has optimal solution µ∗(x) = 1 Z exp(f(x)), where Z is a normalising factor. Plugging Equation 14 into Equation 13 we obtain: LCQL = α ·   Es∼D[log X a exp(Q(s, a))] − E(s,a)∼D[Q(s, a)] ! + LQ. (15) In discrete action spaces the logsumexp can be computed exactly; in continuous action spaces [44] approximate it via importance sampling using actions sampled uniformly at random, actions from the current policy conditioned on st ∼ D, and from the current policy conditioned on st+1 ∼ D2: 2Conditioning on next states st+1 ∼ Dis not mentioned in the paper, but is present in their official implementation. 21log X a exp Q(st, at) = log(1 3 X a exp Q(st, at)) + 1 3 X a exp Q(st, at)) + 1 3 X a exp(exp Q(st, at)), = log(1 3Eat∼Unif(A) \u0014exp(Q(st, at) Unif(A) \u0015 + 1 3Eat∼π(at|st) \u0014exp(Q(st, at)) π(at|st) \u0015 1 3Eat+1∼π(at+1|st+1) \u0014exp(Q(st, at)) π(at+1|st+1) \u0015 ), = log( 1 3N NX ai∼Unif(A) \u0014exp(Q(st, at)) Unif(A) \u0015 + 1 6N 2NX ai∼π(at|st) \u0014exp(Q(st, at)) π(ai|st) \u0015 1 3N NX ai∼π(at+1|st+1) \u0014exp(Q(st, at)) π(ai|st+1) \u0015 ), (16) with N a hyperparameter defining the number of actions to sample across the action-space. We can substitute F(s, a, z)⊤z for Q(s, a) in the final expression of Equation 17 to obtain the equivalent for VC-FB: log X a exp F(st, ai, z)⊤z = log( 1 3N NX ai∼Unif(A) \u0014exp(F(st, ai, z)⊤z) Unif(A) \u0015 + 1 6N 2NX ai∼π(at|st) \u0014exp(F(st, ai, z⊤z) π(ai|st) \u0015 1 3N NX ai∼π(at+1|st+1) \u0014exp(F(st, ai, z)⊤z) π(ai|st+1) \u0015 ). (17) In Appendix F, Figure 16 we show how the performance of VC-FB varies with the number of action samples. In general, performance improves with the number of action samples, but we limit N = 3 to limit computational burden. The formulation for MC-FB is identical other than each value F(s, a, z)T z being replaced with measures F(s, a, z)T B(s+). B.1.4 Dynamically Tuning α A critical hyperparameter is α which weights the conservative penalty with respect to other losses during each update. We initially trialled constant values of α, but found performance to be fragile to this selection, and lacking robustness across environments. Instead, we follow [44] once again, and instantiate their algorithm for dynamically tuningα, which they call Lagrangian dual gradient-descent on α. We introduce a conservative budget parameterised by τ, and set α with respect to this budget: min FB max α≥0 α · \u0000 Es∼D,a∼µ(a|s)z∼Z[F(s, a, z)⊤z] − E(s,a)∼D,z∼Z[F(s, a, z)⊤z] − τ \u0001 + LFB. (18) Intuitively, this implies that if the scale of overestimation ≤ τ then α is set close to 0, and the conservative penalty does not affect the updates. If the scale of overestimation ≥ τ then α is set proportionally to this gap, and thus the conservative penalty is proportional to the degree of overestimation above τ. As above, for the MC-FB variant values F(s, a, z)⊤z are replaced with measures F(s, a, z)⊤B(s+). B.1.5 Algorithm We summarise the end-to-end implementation of VC-FB as pseudo-code in Algorithm 1. MC-FB representations are trained identically other than at line 10 where the conservative penalty is computed for M instead of Q, and in line 12 where Ms are lower bounded via Equation 12. 22Algorithm 1 Pre-training value-conservative forward-backward representations Require: D: dataset of trajectories FθF , BθB , π: randomly initialised networks N, Z, ν, b: learning steps, z-sampling distribution, polyak momentum, batch size 1: for learning step n = 1...N do 2: {(si, ai, si+1)} ∼ Di∈|b| ◁ Sample mini-batch of transitions 3: {zi}i∈|b| ∼ Z ◁ Sample zs (Appendix B.1.2) 4: 5: // FB Update 6: {ai+1} ∼π(si+1, zi) ◁ Sample batch of actions at next states from policy 7: Update F Bgiven {(si, ai, si+1, ai+1, zi)} ◁ Equation 6 8: 9: // Conservative Update 10: Qmax(si, ai) ≈ log P a exp F(si, ai, zi)⊤zi ◁ Compute conservative penalty (Equation 17) 11: Compute α given Qmax via Lagrangian dual gradient-descent ◁ Equation 18 12: Lower bound Q ◁ Equation 11 13: 14: // Actor Update 15: {ai} ∼π(si, zi) ◁ Sample actions from policy 16: Update actor to maximise E[F(si, ai, zi)⊤zi] ◁ Standard actor-critic formulation 17: 18: // Update target networks via polyak averaging 19: θ− F ← νθ− F + (1−ν)θF ◁ Forward target network 20: θ− B ← νθ− B + (1 − ν)θB ◁ Backward target network 21: end for B.1.6 Directed Value-Conservative Forward Backward Representations VC-FB applies conservative updates using task vectors z sampled from Z (which in practice is a uniform distribution over the √ d-hypersphere). This will include many vectors corresponding to tasks that are never evaluated in practice in downstream applications. Intuitively, it may seem reasonable to direct conservative updates to focus on tasks that are likely to be encountered downstream. One simple way of doing this would be consider the set of all goal-reaching tasks for goal states in the training distribution, which corresponds to sampling z = B(sg) for some sg ∼ D. This leads to the following conservative loss function: LDVC-FB = α · \u0010 Es∼D,a∼µ(a|s),sg∼D[F(s, a, B(sg))⊤B(sg)] − E(s,a)∼D,sg∼D[F(s, a, B(sg))⊤B(sg)] − H(µ) \u0011 + LFB. (19) We call models learnt via this loss directed-VC-FB (DVC-FB). While we were initially open to the possibility that DVC-FB updates would be better targeted than those of VC-FB, and would lead to improved downstream task performance, this turns out not to be the case in our experimental settings as discussed in Section 5. We report scores obtained by the DVC-FB method across all 100k datasets, domains and tasks in Appendix C. B.2 Universal Successor Features We directly reimplement USFs, with basic features φ(s) provided by Laplacian eigenfunctions [87], from [83]. B.2.1 Architecture USF ψ(s, a, z). The input to the USF ψ is always preprocessed. State-action pairs (s, a) and state-task pairs (s, z) have their own preprocessors which are feedforward MLPs that embed their inputs into a 512-dimensional space. These embeddings are concatenated and passed through a third 23feedforward MLP ψ which outputs a d-dimensional embedding vector. Note this is identical to the implementation of F as described in Appendix B.1. All other hyperparameters are reported in Table 4. Feature Embedding φ(s). The feature map φ(s) is a feedforward MLP that takes a state as input and outputs a d-dimensional embedding vector. The loss function for learning the feature embedding is provided in Appendix B.2.2. Actor π(s, z). Like the forward representation, the inputs to the policy network are similarly preprocessed. State-action pairs (s, a) and state-task pairs (s, z) have their own preprocessors which are feedforward MLPs that embed their inputs into a 512-dimensional space. These embeddings are concatenated and passed through a third feedforward MLP which outputs a a-dimensional vector, where a is the action-space dimensionality. A Tanh activation is used on the last layer to normalise their scale. As per [23]’s recommendations, the policy is smoothed by adding Gaussian noise σ to the actions during training. Note this is identical to the implementation of π(s, z) as described in Appendix B.1. Misc. Layer normalisation [ 3] and Tanh activations are used in the first layer of all MLPs to standardise the inputs. z sampling distribution Z is identical to FB’s (Appendix B.1.2). B.2.2 Laplacian Eigenfunctions Loss Laplacian eigenfunction features φ(s) are learned as per [87]. They consider the symmetrized MDP graph Laplacian created by some policy π, defined as L = Id − 1 2 (Pπdiagρ−1 + diagρ−1(Pπ)T ). They learn the eigenfunctions of L with the following: min φ E(st,st+1)∼D \u0002 ||φ(st) − φ(st+1)||2\u0003 +λE(st,s+)∼D \u0002 (φ(s)⊤φ(s+))2 − ||φ(s)||2 2 − ||φ(s+)||2 2 \u0003 , (20) which comes from [39]. B.3 GC-IQL B.3.1 Architecture We implement GC-IQL following [60]’s codebase. GC-IQL inherits all functionality from a base soft actor-critic agent [28], but adds a soft conservative penalty to the goal-conditioned critic’sV (s, g) updates. We refer the reader to paper that introduces GC-IQL [60] for details on the loss function used to train V (s, g). Hyperparameters are reported in Table 5. Critic(s). GC-IQL trains double goal-conditioned value functionsV (s, g). The critics are feedforward MLPs that take a state-goal pair (s, g) as input and output a value ∈ R1. During training the goals are sampled from the prior G described in Section B.3.2. Actor. The actor is a standard feedforward MLP taking the state s as input and outputting an 2a- dimensional vector, where a is the action-space dimensionality. The actor predicts the mean and standard deviation of a Gaussian distribution for each action dimension; during training a value is sampled at random, during evaluation the mean is used. B.3.2 Goal Sampling Distribution G Following [60], goals are sampled from either random states, future states, or the current state with probabilities 0.3, 0.5 and 0.2 respectively. A geometric distribution Geom(1 − γ) is used for the future state distribution, and the uniform distribution over the offline dataset is used for sampling random states. B.4 CQL B.4.1 Architecture We adopt the same implementation and hyperparameters as is used on the ExORL benchmark. CQL inherits all functionality from a base soft actor-critic agent [28], but adds a conservative penalty to the critic updates (Equation 10). Hyperparameters are reported in Table 5. 24Critic(s). CQL employs double Q networks, where the target network is updated with Polyak averaging via a momentum coefficient. The critics are feedforward MLPs that take a state-action pair (s, a) as input and output a value ∈ R1. Actor. The actor is a standard feedforward MLP taking the state s as input and outputting an 2a- dimensional vector, where a is the action-space dimensionality. The actor predicts the mean and standard deviation of a Gaussian distribution for each action dimension; during training a value is sampled at random, during evaluation the mean is used. B.5 TD3 B.5.1 Architecture We adopt the same implementation and hyperparameters as is used on the ExORL benchmark. Hyperparameters are reported in Table 5. Critic(s). TD3 employs double Q networks, where the target network is updated with Polyak averaging via a momentum coefficient. The critics are feedforward MLPs that take a state-action pair (s, a) as input and output a value ∈ R1. Actor. The actor is a standard feedforward MLP taking the state s as input and outputting an a- dimensional vector, where a is the action-space dimensionality. The policy is smoothed by adding Gaussian noise σ to the actions during training. Misc. As is usual with TD3, layer normalisation [3] is applied to the inputs of all networks. Table 5: Hyperparameters for Non-zero-shot RL. Hyperparameter CQL Offline TD3 GC-IQL Critic dimensions (1024, 1024) (1024, 1024) (1024, 1024) Actor dimensions (1024, 1024) (1024, 1024) (1024, 1024) Learning steps 1,000,000 1,000,000 1,000,000 Batch size 1024 1024 1024 Optimiser Adam Adam Adam Learning rate 0.0001 0.0001 0.0001 Discountγ 0.98 (0.99 for maze) 0.98 (0.99 for maze) 0.98 (0.99 for maze) Activations ReLU ReLU ReLU Target network Polyak smoothing coefficient 0.01 0.01 0.01 Sampled Actions Number 3 - - CQLα 0.01 - - CQL Lagrange False - - Std. deviation for policy smoothingσ - 0.2 - Truncation level for policy smoothing - 0.3 - IQL temperature - - 1 IQL Expectile - - 0.7 B.6 Code References This work was enabled by: NumPy [30], PyTorch [61], Pandas [56] and Matplotlib [31]. C Extended Results In this section we report a full breakdown of our experimental results on the ExORL benchmark by dataset, domain and task. Table 6 reports results for methods trained on the 100k sub-sampled datasets, and Table 7 reports results for methods trained on the full datasets. 25Table 6: 100k dataset experimental results on ExORL. For each dataset-domain pair, we report the score at the step for which the all-task IQM is maximised when averaging across 5 seeds, and the constituent task scores at that step. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap. Dataset Domain Task CQL Offline TD3 GC-IQL SF-LAP FB VC-FB (ours)DVC-FB (ours) MC-FB (ours) RND-100k Walker Walk 138(122−140) 210(205−231) 118(114−126) 58(33−104) 184(123−278) 446(435−460) 394(166−512) 247(164−299) Stand 386(375−391) 362(335−378) 284(260−313) 190(128−233) 558(498−637) 624(604−639) 590(557−622) 480(402−517) Run 71(64−75) 84(79−88) 51(42−56) 34(27−41) 101(90−135) 179(165−197) 134(77−191) 106(72−137) Flip 153(135−174) 162(148−171) 157(152−173) 70(56−84) 163(90−212) 325(292−350) 250(215−286) 164(131−192) Quadruped Stand 167(73−268) 119(9−338) 43(14−173) 108(51−192) 134(86−176) 331(190−410) 269(152−385) 171(71−372) Roll Fast 93(27−219) 63(4−223) 66(14−92) 80(21−169) 83(55−127) 141(87−182) 146(85−207) 81(19−199) Roll 251(147−320) 96(8−272) 224(123−399) 100(22−277) 139(71−224) 141(107−212) 209(123−295) 132(40−267) Jump 128(82−223) 85(6−248) 152(39−247) 94(28−189) 121(78−186) 159(110−212) 167(100−234) 97(41−172) Escape 3(2−4) 3(0−9) 1(0−3) 1(1−4) 7(3−12) 8(4−14) 13(6−19) 5(1−12) Maze Reach Top Right433(275−558) 457(0−728) 308(123−494) 1(0−368) 0(0−26) 0(0−406) 0(0−0) 99(16−432) Reach Top Left561(503−758) 921(897−936) 628(384−872) 302(18−602) 384(0−735) 662(218−899) 244(10−477) 723(363−895) Reach Bottom Right0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Reach Bottom Left253(69−419) 85(22−295) 25(4−46) 0(0−34) 0(0−0) 479(56−725) 250(0−501) 384(125−653) Jaco Reach Top Right37(21−54) 0(0−1) 0(0−0) 0(0−1) 0(0−3) 1(0−4) 6(3−9) 17(8−29) Reach Top Left21(12−33) 0(0−0) 0(0−1) 2(0−5) 2(1−4) 2(1−2) 11(7−16) 9(2−21) Reach Bottom Right37(21−53) 0(0−0) 0(0−2) 0(0−0) 0(0−12) 5(2−21) 7(3−11) 16(7−23) Reach Bottom Left20(18−28) 0(0−0) 0(0−1) 1(0−4) 7(4−15) 4(1−21) 3(1−5) 11(1−41) RANDOM-100k Walker Walk 126(112−139) 132(105−166) 24(21−26) 129(118−139) 76(55−116) 122(86−140) 38(32−43) 119(58−210) Stand 246(199−287) 295(251−326) 141(115−162) 206(161−266) 237(212−278) 223(206−241) 223(201−246) 209(190−239) Run 31(23−47) 57(38−65) 20(16−23) 49(38−62) 37(33−48) 40(36−46) 31(25−36) 32(27−37) Flip 115(102−128) 72(47−83) 22(19−24) 100(82−119) 47(40−62) 62(40−99) 47(43−52) 44(40−55) Quadruped Stand 186(70−294) 264(68−472) 76(16−235) 285(146−432) 278(154−440) 269(48−618) 196(100−284) 172(78−284) Roll Fast 161(99−223) 151(31−283) 99(71−103) 64(26−112) 96(16−195) 43(17−132) 155(89−220) 78(53−126) Roll 326(218−430) 260(41−463) 165(37−264) 111(66−169) 105(63−185) 130(74−185) 183(120−246) 178(115−452) Jump 213(136−293) 189(82−380) 139(18−397) 128(14−221) 75(33−155) 78(23−226) 94(67−121) 147(53−261) Escape 6(2−8) 4(2−9) 1(0−5) 2(0−5) 5(3−7) 2(1−11) 3(2−5) 6(1−10) Maze Reach Top Right0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Reach Top Left0(0−0) 0(0−2) 925(915−929) 3(0−5) 18(0−54) 26(4−129) 52(0−104) 10(0−32) Reach Bottom Right0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Reach Bottom Left0(0−0) 0(0−4) 37(0−233) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Jaco Reach Top Right53(47−60) 34(17−89) 0(0−0) 0(0−0) 3(0−15) 0(0−8) 0(0−0) 4(0−11) Reach Top Left52(34−88) 2(1−5) 4(2−5) 2(0−5) 0(0−0) 12(7−25) 26(10−42) 23(11−53) Reach Bottom Right53(45−60) 34(15−78) 0(0−0) 0(0−0) 0(0−4) 1(0−1) 30(0−59) 1(0−6) Reach Bottom Left32(19−37) 3(1−4) 0(0−0) 0(0−0) 2(1−12) 0(0−0) 16(0−33) 0(0−8) DIAYN-100k Walker Walk 147(123−198) 150(132−164) 23(21−26) 93(61−106) 251(132−299) 262(174−344) 248(243−255) 260(175−347) Stand 406(365−441) 262(234−300) 142(117−173) 276(189−292) 497(381−651) 455(401−491) 387(352−423) 423(370−594) Run 38(33−42) 45(44−47) 19(15−23) 53(37−59) 98(79−114) 82(76−96) 87(82−92) 81(71−107) Flip 149(116−178) 163(152−179) 23(20−32) 144(89−159) 193(136−205) 228(193−244) 180(155−205) 182(151−237) Quadruped Stand 299(160−435) 848(722−885) 251(213−404) 313(167−492) 459(396−525) 430(393−481) 447(413−482) 457(396−511) Roll Fast 164(75−195) 446(350−499) 111(84−160) 185(162−319) 287(256−328) 260(232−280) 290(285−296) 293(275−299) Roll 264(126−369) 709(619−799) 117(41−209) 189(98−306) 460(411−485) 415(396−434) 429(407−452) 456(408−490) Jump 196(135−267) 410(350−517) 171(141−213) 240(102−350) 363(337−418) 357(324−397) 391(371−411) 372(329−403) Escape 6(3−11) 23(15−32) 6(3−9) 16(6−28) 45(35−56) 31(24−43) 45(42−48) 42(37−50) Maze Reach Top Right760(494−787) 796(520−799) 705(402−777) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 27(0−97) Reach Top Left943(941−950) 943(941−945) 901(889−915) 764(383−940) 576(156−876) 910(620−928) 557(270−887) 853(580−906) Reach Bottom Right0(0−0) 0(0−0) 0(0−18) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Reach Bottom Left0(0−0) 799(537−806) 139(0−288) 0(0−0) 0(0−0) 0(0−0) 0(0−0) 0(0−0) Jaco Reach Top Right17(10−29) 0(0−0) 0(0−0) 8(1−21) 2(0−8) 5(2−11) 0(0−0) 9(6−19) Reach Top Left10(5−18) 0(0−0) 0(0−0) 0(0−1) 2(0−5) 7(0−14) 27(2−53) 0(0−0) Reach Bottom Right17(11−33) 0(0−0) 0(0−0) 3(2−7) 4(2−13) 5(2−14) 0(0−0) 11(1−36) Reach Bottom Left2(0−12) 0(0−0) 0(0−0) 2(0−3) 10(5−19) 5(0−8) 15(0−39) 9(4−16) 26Table 7: Full dataset experimental results on ExORL. For each dataset-domain pair, we report the score at the step for which the all-task IQM is maximised when averaging across 5 seeds, and the constituent task scores at that step. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap. Dataset Domain Task FB VC-FB (ours) MC-FB (ours) RND Walker Walk 821(758−883) 864(850−879) 792(728−857) Stand 928(925−930) 878(854−903) 873(812−934) Run 281(242−320) 351(328−374) 343(323−366) Flip 525(452−598) 542(513−571) 598(538−657) Quadruped Stand 957(952−963) 863(777−950) 949(939−958) Roll Fast 574(553−599) 512(471−553) 565(555−575) Roll 920(895−944) 831(741−920) 890(874−906) Jump 736(721−751) 630(570−690) 705(703−707) Escape 94(63−125) 59(50−68) 66(56−86) Maze Reach Top Right0(0−0) 425(153−698) 270(9−533) Reach Top Left 612(313−911) 454(138−769) 773(611−934) Reach Bottom Right0(0−0) 0(0−0) 0(0−0) Reach Bottom Left268(0−536) 270(2−539) 1(0−2) Jaco Reach Top Right48(39−56) 24(0−47) 51(23−79) Reach Top Left 23(6−40) 14(4−25) 20(7−33) Reach Bottom Right60(56−65) 5(0−10) 47(15−79) Reach Bottom Left27(12−42) 88(33−143) 20(9−30) RANDOM Walker Walk 148(70−225) 145(109−182) 129(80−178) Stand 318(281−355) 255(202−308) 285(262−309) Run 51(45−60) 47(44−50) 45(36−55) Flip 57(49−67) 83(49−117) 103(65−140) Quadruped Stand 417(393−453) 295(165−424) 210(153−267) Roll Fast 110(51−170) 271(252−290) 215(139−292) Roll 231(116−346) 154(53−255) 303(160−530) Jump 287(123−450) 67(44−90) 164(135−194) Escape 10(6−14) 7(4−10) 12(9−17) Maze Reach Top Right0(0−0) 0(0−0) 0(0−0) Reach Top Left 309(4−615) 317(5−629) 307(0−614) Reach Bottom Right0(0−0) 0(0−0) 0(0−0) Reach Bottom Left0(0−0) 0(0−0) 0(0−0) Jaco Reach Top Right1(1−1) 2(0−4) 5(0−10) Reach Top Left 50(0−100) 9(0−18) 16(0−31) Reach Bottom Right0(0−0) 15(5−25) 21(0−42) Reach Bottom Left3(1−6) 18(2−34) 1(0−3) DIAYN Walker Walk 459(278−652) 536(305−766) 519(315−722) Stand 478(463−494) 447(422−472) 517(433−602) Run 87(81−93) 84(78−89) 87(65−110) Flip 235(151−319) 251(151−352) 301(213−388) Quadruped Stand 763(725−814) 432(176−688) 804(756−851) Roll Fast 497(480−514) 293(179−407) 495(491−498) Roll 767(726−808) 350(152−650) 761(736−786) Jump 628(587−669) 234(89−379) 608(594−622) Escape 65(62−69) 21(14−27) 67(55−79) Maze Reach Top Right0(0−0) 0(0−0) 0(0−0) Reach Top Left 654(565−742) 928(907−950) 814(725−903) Reach Bottom Right0(0−0) 0(0−0) 8(0−16) Reach Bottom Left169(0−506) 7(0−14) 49(0−98) Jaco Reach Top Right4(2−7) 10(5−15) 4(1−8) Reach Top Left 5(1−10) 1(0−2) 2(0−3) Reach Bottom Right9(4−13) 6(3−8) 7(0−14) Reach Bottom Left25(2−47) 12(6−18) 25(3−47) 27Table 8: Aggregate zero-shot performance on ExORL for all evaluation statistics recommended by [1]. VC-FB outperforms all methods across all evaluation statistics. ↑ means a higher score is better; ↓ means a lower score is better. Note that the optimality gap is large because we set γ = 1000and for many dataset-domain-tasks the maximum achievable score is far from 1000. Statistic SF-LAP GC-IQL FB CQL MC-FB (ours) VC-FB (ours) IQM ↑ 92 95 99 128 136 148 Mean ↑ 87 126 118 138 142 154 Median ↑ 108 104 104 132 133 144 Optimality Gap ↓ 0.92 0 .88 0 .89 0 .87 0 .86 0.84 Table 9: D4RL experimental results. For each dataset-domain pair, we report the score at the step for which the IQM is maximised when averaging across 3 seeds. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap.. Domain Dataset CQL SF-LAP FB VC-FB (ours) MC-FB (ours) HalfCheetah medium 36 (36-36) 4 (1-7) 3 (-1-8) 39 (38-40) 32 (28-36) medium-expert 43 (37-51) 26 (20-32) 3 (0-6) 27 (24-28) 24 (21-30) medium-replay 42 (42-42) 29 (29-30) 7 (2-11) 31 (26-36) 20 (18-22) Walker2d medium 70 (68-73) 7 (1-13) 7 (0-14) 42 (37-45) 36 (33-40) medium-expert 102 (97-107) 15 (0-31) 3 (1-5) 82 (72-92) 74 (66-77) medium-replay 13 (11-14) 11 (8-15) 12 (7-17) 20 (19-21) 22 (19-24) All All 48 15 5 34 28 D Value Conservative Universal Successor Features In this section, we develop value conservative regularisation for use by Universal Successor Features (USF) [5, 7], the primary alternative to FB for zero-shot RL. Recall from Section 2 that successor features require a state-feature mapping φ : S →Rd which is usually obtained by some representation learning method [5]. Universal successor features are the expected discounted sum of these features, starting in state s0, taking action a0 and following the task-dependent policy πz thereafter ψ(s0, a0, z) := E  X t≥0 γtφ(st+1)|s0, a0, πz  . (21) USFs satisfy a Bellman equation [7] and so can be trained using TD-learning on the Bellman residuals: LSF = E(st,at,st+1)∼D,z∼Z \u0000 ψ(st, at, z)⊤z − φ(st+1)⊤z − γ ¯ψ(st+1, πz(st+1), z)⊤z \u00012 , (22) where ¯ψ is a lagging target network updated via Polyak averaging, and Z is identical to that used for FB training (Appendix B.1.2). As with FB representations, the policy maximises the Q function defined by ψ: πz(s) := argmaxaψ(s, a, z)⊤z, (23) 28and for continuous state and action spaces is trained in an actor critic formulation. Like FB, USF training requires next action samples at+1 ∼ πz(st+1) for the TD targets. We therefore expect SFs to suffer the same failure mode discussed in Section 3 (OOD state-action value overestimation) and to benefit from the same remedial measures (value conservatism). Training value-conservative successor features (VC-SF) amounts to substituting the USF Q function definition and loss for FB’s in Equation 11: LVC-SF = α · \u0000 Es∼D,a∼µ(a|s),z∼Z[ψ(s, a, z)⊤z] − E(s,a)∼D,z∼Z[ψ(s, a, z)⊤z] \u0001 + LSF. (24) Both the maximum value approximator µ(a|s) (Equation 17, Section B.1.3) and α-tuning (Equation 18, Section B.1.4) can be extracted identically to the FB case with any occurrence of F(s, a, z)⊤z substituted with ψ(s, a, z)⊤z. As USFs do not predict successor measures we cannot formulate measure-conservative USFs. E Negative Results In this section we provide detail on experiments we attempted, but which did not provide results significant enough to be included in the main body. E.1 Downstream Finetuning If we relax the zero-shot requirement, could pre-trained conservative FB representations be finetuned on new tasks or domains? Base CQL models have been finetuned effectively on unseen tasks using both online and offline data [41], and we had hoped to replicate similar results with VC-FB and MC-FB. We ran offline and online finetuning experiments and provide details on their setups and results below. All experiments were conducted on the Walker domain. Offline finetuning. We considered a setting where models are trained on a low quality dataset initially, before a high quality dataset becomes available downstream. We used models trained on the RANDOM -100k dataset and finetuned them on both the full RND and RND-100k datasets, with models trained from scratch used as our baseline. Finetuning involved the usual training protocol as described in Algorithm 1, but we limited the number of learning steps to 250k. We found that though performance improved during finetuning, it improved no quicker than the models trained from scratch. This held for both the full RND and RND-100k datasets. We conclude that the parameter initialisation delivered after training on a low quality dataset does not obviously expedite learning when high quality data becomes available. Online finetuning. We considered the online finetuning setup where a trained representation is deployed in the target environment, required to complete a specified task, and allowed to collect a replay buffer of reward-labelled online experience. We followed a standard online RL protocol where a batch of transitions was sampled from the online replay buffer after each environment step for use in updating the model’s parameters. We experimented with fixing z to the target task during in the actor updates (Line 16, Algorithm 1), but found it caused a quick, irrecoverable collapse in actor performance. This suggested uniform samples from Z provide a form of regularisation. We granted the agents 500k steps of interaction for online finetuning. We found that performance never improved beyond the pre-trained (init) performance during finetuning. We speculated that this was similar to the well-documented failure mode of online finetuning of CQL [59], namely taking sub-optimal actions in the real env, observing unexpectedly high reward, and updating their policy toward these sub-optimal actions. But we note that FB representations do not update w.r.t observed rewards, and so conclude this cannot be the failure mode. Instead it seems likely that FB algorithms cannot 29Figure 9: Learning curves for methods finetuned on the full RND dataset. Solid lines represent base models trained on RANDOM -100k, then finetuned; dashed lines represent models trained from scratch. The finetuned models perform no better than models trained from scratch after 250k learning steps, suggesting model re-training is currently a better strategy than offline finetuning. use the narrow, unexploratory experience obtained from attempting to perform a specific task to improve model performance. We believe resolving issues associated with finetuning conservative FB algorithms once the zero-shot requirement is relaxed is an important future direction and hope that details of our negative attempts to this end help facilitate future research. 30Figure 10: Learning curves for online finetuning. The performance at the end of pre-training (init performance) is plotted as a dashed line for each method. None of the methods consistently outperform their init performance after 250k online transitions. F Learning Curves & Hyperparameter Sensitivity ExORL Learning Curves. We report the learning curves for all zero-shot RL methods and CQL in Figures 11, 12, and 13. For all domains except Jaco, the y-axis limit is fixed at 1000 as that is the maximum score achievable in the DeepMind Control Suite. For Jaco-related figures, the y-axis limits is fixed at 100 as no method achieves a score higher than this. Hyperparameter Sensitivity. We report the sensitivity of VC-FB and MC-FB to the choice of two new hyperparameters: conservative budget τ and action samples per policy N on the ExORL benchmark. Figure 14 plots the sensitivity of VC-FB to the choice of τ on Walker and Maze domains across RND and RANDOM datasets. Figure 15 plots the sensitivity of MC-FB to the choice of τ on Walker and Maze domains across RND and RANDOM datasets. Figure 16 plots the sensitivity of MC-FB to the choice of N on Walker and Maze domains across RND and RANDOM datasets. We further explore the sensitivity of VC-FB performance on Walker2D from the D4RL benchmark w.r.t. the choice of conservative budget τ. Figure 17 plots this relationship when trained on the “medium-expert\" dataset from D4RL. 31Figure 11: Learning Curves (1/3). Models are evaluated every 20,000 timesteps where we perform 10 rollouts and record the IQM. Curves are the IQM of this value across 5 seeds; shaded areas are one standard deviation. 32Figure 12: Learning Curves (2/3). Models are evaluated every 20,000 timesteps where we perform 10 rollouts and record the IQM. Curves are the IQM of this value across 5 seeds; shaded areas are one standard deviation. 33Figure 13: Learning Curves (3/3). Models are evaluated every 20,000 timesteps where we perform 10 rollouts and record the IQM. Curves are the IQM of this value across 5 seeds; shaded areas are one standard deviation. 34Figure 14: VC-FB sensitivity to conservative budget τ on Walker and Maze. Top: RND dataset; bottom: RANDOM dataset. Maximum IQM return across the training run averaged over 3 random seeds Figure 15: MC-FB sensitivity to conservative budget τ on Walker and Maze. Top: RND dataset; bottom: RANDOM dataset. Maximum IQM return across the training run averaged over 3 random seeds 35Figure 16: MC-FB sensitivity to action samples per policy N on Walker and Maze. Top: RND dataset; bottom: RANDOM dataset. Maximum IQM return across the training run averaged over 3 random seeds. Figure 17: VC-FB sensitivity to choice of conservative budget τ on Walker2D from the D4RL benchmark. 36G Code Snippets G.1 Update Step 1 def update_fb ( 2 self , 3 observations : torch . Tensor , 4 actions : torch . Tensor , 5 next_observations : torch . Tensor , 6 discounts : torch . Tensor , 7 zs: torch . Tensor , 8 step : int , 9 ) -> Dict [str , float ]: 10 \"\"\" 11 Calculates the loss for the forward - backward representation network . 12 Loss contains two components : 13 1. Forward - backward representation ( core ) loss : a Bellman update 14 on the successor measure ( equation 24 , Appendix B) 15 2. Conservative loss : penalises out -of - distribution actions 16 Args : 17 observations : observation tensor of shape [ batch_size , observation_length ] 18 actions : action tensor of shape [ batch_size , action_length ] 19 next_observations : next observation tensor of 20 shape [ batch_size , observation_length ] 21 discounts : discount tensor of shape [ batch_size , 1] 22 zs: policy tensor of shape [ batch_size , z_dimension ] 23 step : current training step 24 Returns : 25 metrics : dictionary of metrics for logging 26 \"\"\" 27 28 # update step common to all FB models 29 ( 30 core_loss , 31 core_metrics , 32 F1 , 33 F2 , 34 B_next , 35 M1_next , 36 M2_next , 37 _, 38 _, 39 actor_std_dev , 40 ) = self . _update_fb_inner ( 41 observations = observations , 42 actions = actions , 43 next_observations = next_observations , 44 discounts = discounts , 45 zs=zs , 46 step =step , 47 ) 48 49 # calculate MC or VC penalty 50 if self . mcfb : 51 ( 52 conservative_penalty , 53 conservative_metrics , 54 ) = self . _measure_conservative_penalty ( 55 observations = observations , 56 next_observations = next_observations , 57 zs=zs , 58 actor_std_dev = actor_std_dev , 59 F1=F1 , 60 F2=F2 , 61 B_next = B_next , 62 M1_next = M1_next , 63 M2_next = M2_next , 64 ) 65 # VCFB 66 else : 67 ( 68 conservative_penalty , 69 conservative_metrics , 70 ) = self . _value_conservative_penalty ( 71 observations = observations , 72 next_observations = next_observations , 73 zs=zs , 3774 actor_std_dev = actor_std_dev , 75 F1=F1 , 76 F2=F2 , 77 ) 78 79 # tune alpha from conservative penalty 80 alpha , alpha_metrics = self . _tune_alpha ( 81 conservative_penalty = conservative_penalty 82 ) 83 conservative_loss = alpha * conservative_penalty 84 85 total_loss = core_loss + conservative_loss 86 87 # step optimiser 88 self . FB_optimiser . zero_grad ( set_to_none = True ) 89 total_loss . backward () 90 for param in self .FB. parameters (): 91 if param . grad is not None : 92 param . grad . data . clamp_ (-1, 1) 93 self . FB_optimiser . step () 94 95 return metrics G.2 Value-Conservative Penalty 1 def _value_conservative_penalty ( 2 self , 3 observations : torch . Tensor , 4 next_observations : torch . Tensor , 5 zs: torch . Tensor , 6 actor_std_dev : torch . Tensor , 7 F1: torch . Tensor , 8 F2: torch . Tensor , 9 ) -> torch . Tensor : 10 \"\"\" 11 Calculates the value conservative penalty for FB. 12 Args : 13 observations : observation tensor of shape [ batch_size , observation_length ] 14 next_observations : next observation tensor of shape 15 [ batch_size , observation_length ] 16 zs: task tensor of shape [ batch_size , z_dimension ] 17 actor_std_dev : standard deviation of the actor 18 F1: forward embedding no. 1 19 F2: forward embedding no. 2 20 Returns : 21 conservative_penalty : the value conservative penalty 22 \"\"\" 23 24 with torch . no_grad (): 25 # repeat observations , next_observations , zs , and Bs 26 # we fold the action sample dimension into the batch dimension 27 # to allow the tensors to be passed through F and B; we then 28 # reshape the output back to maintain the action sample dimension 29 repeated_observations_ood = observations . repeat ( 30 self . ood_action_samples , 1, 1 31 ). reshape ( self . ood_action_samples * self . batch_size , -1) 32 repeated_zs_ood = zs. repeat ( self . ood_action_samples , 1, 1). reshape ( 33 self . ood_action_samples * self . batch_size , -1 34 ) 35 ood_actions = torch . empty ( 36 size =( self . ood_action_samples * self . batch_size , self . action_length ), 37 device = self . _device , 38 ). uniform_ (-1, 1) 39 40 repeated_observations_actor = observations . repeat ( 41 self . actor_action_samples , 1, 1 42 ). reshape ( self . actor_action_samples * self . batch_size , -1) 43 repeated_next_observations_actor = next_observations . repeat ( 44 self . actor_action_samples , 1, 1 45 ). reshape ( self . actor_action_samples * self . batch_size , -1) 46 repeated_zs_actor = zs. repeat ( self . actor_action_samples , 1, 1). reshape ( 47 self . actor_action_samples * self . batch_size , -1 48 ) 49 actor_current_actions , _ = self . actor ( 50 repeated_observations_actor , 51 repeated_zs_actor , 3852 std = actor_std_dev , 53 sample =True , 54 ) # [ actor_action_samples * batch_size , action_length ] 55 56 actor_next_actions , _ = self . actor ( 57 repeated_next_observations_actor , 58 z= repeated_zs_actor , 59 std = actor_std_dev , 60 sample =True , 61 ) # [ actor_action_samples * batch_size , action_length ] 62 63 # get Fs 64 ood_F1 , ood_F2 = self .FB. forward_representation ( 65 repeated_observations_ood , ood_actions , repeated_zs_ood 66 ) # [ ood_action_samples * batch_size , latent_dim ] 67 68 actor_current_F1 , actor_current_F2 = self .FB. forward_representation ( 69 repeated_observations_actor , actor_current_actions , repeated_zs_actor 70 ) # [ actor_action_samples * batch_size , latent_dim ] 71 actor_next_F1 , actor_next_F2 = self .FB. forward_representation ( 72 repeated_next_observations_actor , actor_next_actions , repeated_zs_actor 73 ) # [ actor_action_samples * batch_size , latent_dim ] 74 repeated_F1 , repeated_F2 = F1. repeat ( 75 self . actor_action_samples , 1, 1 76 ). reshape ( self . actor_action_samples * self . batch_size , -1) , F2. repeat ( 77 self . actor_action_samples , 1, 1 78 ). reshape ( 79 self . actor_action_samples * self . batch_size , -1 80 ) 81 cat_F1 = torch . cat ( 82 [ 83 ood_F1 , 84 actor_current_F1 , 85 actor_next_F1 , 86 repeated_F1 , 87 ], 88 dim =0 , 89 ) 90 cat_F2 = torch . cat ( 91 [ 92 ood_F2 , 93 actor_current_F2 , 94 actor_next_F2 , 95 repeated_F2 , 96 ], 97 dim =0 , 98 ) 99 100 repeated_zs = zs. repeat ( self . total_action_samples , 1, 1). reshape ( 101 self . total_action_samples * self . batch_size , -1 102 ) 103 104 # convert to Qs 105 cql_cat_Q1 = torch . einsum (\"sd , sd -> s\", cat_F1 , repeated_zs ). reshape ( 106 self . total_action_samples , self . batch_size , -1 107 ) 108 cql_cat_Q2 = torch . einsum (\"sd , sd -> s\", cat_F2 , repeated_zs ). reshape ( 109 self . total_action_samples , self . batch_size , -1 110 ) 111 112 cql_logsumexp = ( 113 torch . logsumexp ( cql_cat_Q1 , dim =0) . mean () 114 + torch . logsumexp ( cql_cat_Q2 , dim =0) . mean () 115 ) 116 117 # get existing Qs 118 Q1 , Q2 = [ torch . einsum (\"sd , sd -> s\", F, zs) for F in [F1 , F2 ]] 119 120 conservative_penalty = cql_logsumexp - (Q1 + Q2). mean () 121 122 return conservative_penalty G.3 Measure-Conservative Penalty 1 def _measure_conservative_penalty ( 2 self , 393 observations : torch . Tensor , 4 next_observations : torch . Tensor , 5 zs: torch . Tensor , 6 actor_std_dev : torch . Tensor , 7 F1: torch . Tensor , 8 F2: torch . Tensor , 9 B_next : torch . Tensor , 10 M1_next : torch . Tensor , 11 M2_next : torch . Tensor , 12 ) -> torch . Tensor : 13 \"\"\" 14 Calculates the measure conservative penalty . 15 Args : 16 observations : observation tensor of shape [ batch_size , observation_length ] 17 next_observations : next observation tensor of shape 18 [ batch_size , observation_length ] 19 zs: task tensor of shape [ batch_size , z_dimension ] 20 actor_std_dev : standard deviation of the actor 21 F1: forward embedding no. 1 22 F2: forward embedding no. 2 23 B_next : backward embedding 24 M1_next : successor measure no. 1 25 M2_next : successor measure no. 2 26 Returns : 27 conservative_penalty : the measure conservative penalty 28 \"\"\" 29 30 with torch . no_grad (): 31 # repeat observations , next_observations , zs , and Bs 32 # we fold the action sample dimension into the batch dimension 33 # to allow the tensors to be passed through F and B; we then 34 # reshape the output back to maintain the action sample dimension 35 repeated_observations_ood = observations . repeat ( 36 self . ood_action_samples , 1, 1 37 ). reshape ( self . ood_action_samples * self . batch_size , -1) 38 repeated_zs_ood = zs. repeat ( self . ood_action_samples , 1, 1). reshape ( 39 self . ood_action_samples * self . batch_size , -1 40 ) 41 ood_actions = torch . empty ( 42 size =( self . ood_action_samples * self . batch_size , self . action_length ), 43 device = self . _device , 44 ). uniform_ (-1, 1) 45 46 repeated_observations_actor = observations . repeat ( 47 self . actor_action_samples , 1, 1 48 ). reshape ( self . actor_action_samples * self . batch_size , -1) 49 repeated_next_observations_actor = next_observations . repeat ( 50 self . actor_action_samples , 1, 1 51 ). reshape ( self . actor_action_samples * self . batch_size , -1) 52 repeated_zs_actor = zs. repeat ( self . actor_action_samples , 1, 1). reshape ( 53 self . actor_action_samples * self . batch_size , -1 54 ) 55 actor_current_actions , _ = self . actor ( 56 repeated_observations_actor , 57 repeated_zs_actor , 58 std = actor_std_dev , 59 sample =True , 60 ) # [ actor_action_samples * batch_size , action_length ] 61 62 actor_next_actions , _ = self . actor ( 63 repeated_next_observations_actor , 64 z= repeated_zs_actor , 65 std = actor_std_dev , 66 sample =True , 67 ) # [ actor_action_samples * batch_size , action_length ] 68 69 # get Fs 70 ood_F1 , ood_F2 = self .FB. forward_representation ( 71 repeated_observations_ood , ood_actions , repeated_zs_ood 72 ) # [ ood_action_samples * batch_size , latent_dim ] 73 74 actor_current_F1 , actor_current_F2 = self .FB. forward_representation ( 75 repeated_observations_actor , actor_current_actions , repeated_zs_actor 76 ) # [ actor_action_samples * batch_size , latent_dim ] 77 actor_next_F1 , actor_next_F2 = self .FB. forward_representation ( 78 repeated_next_observations_actor , actor_next_actions , repeated_zs_actor 79 ) # [ actor_action_samples * batch_size , latent_dim ] 80 repeated_F1 , repeated_F2 = F1. repeat ( 81 self . actor_action_samples , 1, 1 4082 ). reshape ( self . actor_action_samples * self . batch_size , -1) , F2. repeat ( 83 self . actor_action_samples , 1, 1 84 ). reshape ( 85 self . actor_action_samples * self . batch_size , -1 86 ) 87 cat_F1 = torch . cat ( 88 [ 89 ood_F1 , 90 actor_current_F1 , 91 actor_next_F1 , 92 repeated_F1 , 93 ], 94 dim =0 , 95 ) 96 cat_F2 = torch . cat ( 97 [ 98 ood_F2 , 99 actor_current_F2 , 100 actor_next_F2 , 101 repeated_F2 , 102 ], 103 dim =0 , 104 ) 105 106 cml_cat_M1 = torch . einsum (\"sd , td -> st\", cat_F1 , B_next ). reshape ( 107 self . total_action_samples , self . batch_size , -1 108 ) 109 cml_cat_M2 = torch . einsum (\"sd , td -> st\", cat_F2 , B_next ). reshape ( 110 self . total_action_samples , self . batch_size , -1 111 ) 112 113 cml_logsumexp = ( 114 torch . logsumexp ( cml_cat_M1 , dim =0) . mean () 115 + torch . logsumexp ( cml_cat_M2 , dim =0) . mean () 116 ) 117 118 conservative_penalty = cml_logsumexp - ( M1_next + M2_next ). mean () 119 120 return conservative_penalty 41G.4 α Tuning 1 def _tune_alpha ( 2 self , 3 conservative_penalty : torch . Tensor , 4 ) -> torch . Tensor : 5 \"\"\" 6 Tunes the conservative penalty weight ( alpha ) w.r.t. target penalty . 7 Discussed in Appendix B .1.4 8 Args : 9 conservative_penalty : the current conservative penalty 10 Returns : 11 alpha : the updated alpha 12 \"\"\" 13 14 # alpha auto - tuning 15 alpha = torch . clamp ( self . critic_log_alpha . exp () , min =0.0 , max =1 e6) 16 alpha_loss = ( 17 -0.5 * alpha * ( conservative_penalty - self . target_conservative_penalty ) 18 ) 19 20 self . critic_alpha_optimiser . zero_grad () 21 alpha_loss . backward ( retain_graph = True ) 22 self . critic_alpha_optimiser . step () 23 alpha = torch . clamp ( self . critic_log_alpha . exp () , min =0.0 , max =1 e6). detach () 24 25 return alpha 42H NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 433. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical proofs. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimen- tal results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Code and hyperparameters are provided; datasets and environments are open-source. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 44(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See Appendix A and B, and https://github.com/enjeeneer/zero-shot-rl. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to repro- duce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Appendix A and B. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] 45Justification: See Appendix C. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer re- sources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix A.3. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimen- tal runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 46• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper discusses methods for improving the performance of zero-shot RL methods when trained on low quality offline datasets. We do not expect this work to directly impact society positively or negatively. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy- ment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to par- ticular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strate- gies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. 47• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: https://github.com/enjeeneer/zero-shot-rl. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their sub- missions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: 48• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applica- ble), such as the institution conducting the review. 49",
      "references": [
        "Deep reinforcement learning at the edge of the statistical precipice.",
        "Model-based offline planning.",
        "Layer normalization.",
        "Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning.",
        "Successor features for transfer in reinforcement learning.",
        "Learning successor states and goal-dependent values: A mathematical viewpoint.",
        "Universal successor features approximators.",
        "Openai gym.",
        "Language models are few-shot learners.",
        "Exploration by random network distillation.",
        "Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.",
        "Decision transformer: Reinforcement learning via sequence modeling.",
        "A simple framework for contrastive learning of visual representations.",
        "Bail: Best-action imitation learning for batch deep reinforcement learning.",
        "Improving generalization for temporal difference learning: The successor representation.",
        "Challenges of real-world reinforcement learning.",
        "Bootstrap methods: another look at the jackknife.",
        "Diversity is all you need: Learning skills without a reward function.",
        "Contrastive learning as goal-conditioned reinforcement learning.",
        "Continuous doubly constrained batch reinforcement learning.",
        "D4rl: Datasets for deep data-driven reinforcement learning.",
        "For sale: State-action representation learning for deep reinforcement learning.",
        "Off-policy deep reinforcement learning without exploration.",
        "Generalized decision transformer for offline hindsight information matching.",
        "Extreme q-learning: Maxent rl without entropy.",
        "Off-policy deep reinforcement learning by bootstrapping the covariate shift.",
        "Emaq: Expected-max q-learning operator for simple yet effective offline and online rl.",
        "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
        "Fast task inference with variational intrinsic successor features.",
        "Array programming with numpy.",
        "Matplotlib: A 2d graphics environment.",
        "Reinforcement learning with unsupervised auxiliary tasks.",
        "Offline reinforcement learning as one big sequence modeling problem.",
        "Low emission building control with zero-shot reinforcement learning.",
        "Reward-free exploration for reinforcement learning.",
        "Is pessimism provably efficient for offline rl?",
        "Morel: Model-based offline reinforcement learning.",
        "Adam: A method for stochastic optimization.",
        "On spectral graph drawing.",
        "Offline reinforcement learning with fisher divergence critic regularization.",
        "Offline q-learning on diverse multi-task data both scales and generalizes.",
        "Stabilizing off-policy q-learning via bootstrapping error reduction.",
        "Conservative q-learning for offline reinforcement learning.",
        "Multi-game decision transformers.",
        "Offline reinforcement learning: Tutorial, review, and perspectives on open problems.",
        "Continuous control with deep reinforcement learning.",
        "Aps: Active pretraining with successor features.",
        "Off-policy policy gradient with state distribution correction.",
        "Mildly conservative q-learning for offline reinforcement learning.",
        "Offline reinforcement learning with value-based episodic memory.",
        "Conservative offline distributional reinforcement learning.",
        "How far I’ll go: Offline goal-conditioned reinforcement learning via f-advantage regression.",
        "Deployment-efficient reinforcement learning via model-based offline optimization.",
        "pandas: a foundational python library for data analysis and statistics.",
        "DualDice: Behavior-agnostic estimation of discounted stationary distribution corrections.",
        "AWAC: Accelerating online reinforcement learning with offline datasets.",
        "Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning.",
        "HiQL: Offline goal-conditioned RL with latent states as actions.",
        "Automatic differentiation in PyTorch.",
        "Curiosity-driven exploration by self-supervised prediction.",
        "Self-supervised exploration via disagreement.",
        "Weighted policy constraints for offline reinforcement learning.",
        "Fast imitation via behavior foundation models.",
        "Off-policy temporal-difference learning with function approximation.",
        "RAMBO-RL: Robust adversarial model-based offline reinforcement learning.",
        "A generalist agent.",
        "High-resolution image synthesis with latent diffusion models.",
        "Learning to predict by the methods of temporal differences.",
        "Learning from demonstration.",
        "Is imitation learning the route to humanoid robots?",
        "Universal value function approximators.",
        "A dataset perspective on offline reinforcement learning.",
        "How crucial is transformer in decision transformer?",
        "Reinforcement learning: An introduction.",
        "An emphatic approach to the problem of off-policy temporal-difference learning.",
        "DeepMind control suite.",
        "MuJoCo: A physics engine for model-based control.",
        "Learning one representation to optimize all rewards.",
        "Does zero-shot reinforcement learning exist?",
        "Exponentially weighted imitation learning for batched historical data.",
        "Optimal goal-reaching reinforcement learning via quasimetric learning.",
        "Supported policy optimization for offline reinforcement learning.",
        "The Laplacian in RL: Learning representations with efficient approximations.",
        "Behavior regularized offline reinforcement learning.",
        "Uncertainty weighted actor-critic for offline reinforcement learning.",
        "Prompting decision transformer for few-shot policy generalization.",
        "Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline RL.",
        "RoRL: Robust offline reinforcement learning via conservative smoothing.",
        "What is essential for unseen goal generalization of offline goal-conditioned RL?",
        "A behavior regularized implicit policy for offline reinforcement learning.",
        "Don’t change the algorithm, change the data: Exploratory data for offline reinforcement learning.",
        "COMBO: Conservative offline model-based policy optimization.",
        "MOPO: Model-based offline policy optimization.",
        "Provable benefits of actor-critic methods for offline RL.",
        "Online decision transformer."
      ],
      "meta_data": {
        "arxiv_id": "2309.15178v3",
        "authors": [
          "Scott Jeen",
          "Tom Bewley",
          "Jonathan M. Cullen"
        ],
        "published_date": "2023-09-26T18:20:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies that existing zero-shot RL methods (successor-feature/measure approaches) break down when pre-trained on small, homogeneous offline datasets due to out-of-distribution (OOD) action value overestimation. Introduces simple, plug-in conservative regularizers—Value-Conservative Forward-Backward (VC-FB) and Measure-Conservative Forward-Backward (MC-FB) representations—that suppress OOD action estimates during training. Demonstrates across 17 tasks in four ExORL domains and two D4RL domains that conservative zero-shot RL outperforms prior zero-shot, goal-conditioned and even task-specific offline RL baselines without hurting performance on large heterogeneous data.",
        "methodology": "Extend Conservative Q-Learning ideas to zero-shot RL. Using Forward-Backward (FB) representations: 1) Value conservatism—add a penalty proportional to log-sum-exp of Q estimates for actions sampled from a maximising distribution µ, minus in-dataset Q; 2) Measure conservatism—apply same penalty to predicted successor measures F(s,a,z)^T B(s+). Penalty weight α is tuned online via Lagrangian dual ascent to maintain a budget τ. µ is approximated with mixed uniform and current-policy action samples (N=3). Techniques are architecture-agnostic and also derivable for Universal Successor Features. Additional variant DVC-FB directs penalty using B(sg).",
        "experimental_setup": "Pre-train on ExORL offline datasets collected with RND, DIAYN and RANDOM exploration; both full (≈5–10M transitions) and 100k subsamples. Domains: Walker, Quadruped, Maze, Jaco (total 17 tasks). Baselines: vanilla FB, SF-Laplacian, goal-conditioned IQL, CQL and offline TD3. Train 1M gradient steps, batch 512, evaluate every 20k steps with 10 rollouts; report inter-quartile mean (IQM) over 5 seeds. Additional tests on D4RL HalfCheetah and Walker2D with medium, medium-replay, medium-expert datasets. Compute: A100 GPUs (~110 GPU-days). Hyperparameter sweeps and ablations (dataset size, τ, action samples).",
        "limitations": "Performance sensitive to conservative budget τ and action-sample count; α auto-tuning mitigates but narrow task distributions (e.g., D4RL) still require manual tuning. Gain shrinks on very large diverse datasets; random datasets may remain uninformative. Increases training time (~3× FB). No formal guarantees; analysis empirical. Methods rely on FB/USF architectures, limiting applicability where such representations are hard to learn. Finetuning experiments showed minimal benefit, indicating limited adaptability after deployment.",
        "future_research_directions": "1) Replace CQL-style penalty with expectile/IQL or advantage-weighted schemes to remove α tuning. 2) Develop theory on task-space coverage and dataset quality to choose between VC-FB and MC-FB and guide data collection. 3) Improve finetuning/online adaptation mechanisms for conservative zero-shot models. 4) Scale to high-dimensional real-world settings (e.g., robotics, building control) and study safety constraints. 5) Explore automated hyperparameter selection and efficient computation to reduce training cost.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Current feature-space augmentation techniques for data-efficient image classification fall into two extremes: (1) hand-crafted or adversarial perturbations that create only a limited number of views and can easily step off the class manifold, and (2) input-space generative methods that require re-rendering pixels and often break the frozen-backbone constraint. There is no existing way to *learn a rich, label-preserving distribution of features directly inside a frozen representation space* so that we can *grow the effective few-shot training set by orders of magnitude* while still keeping the backbone fixed and the augmentation fully differentiable. This gap limits both in-distribution accuracy and robustness to real distribution shifts.",
    "method": "Class-Conditional Feature Diffusion Augmentation (CCFD-Aug)\n1. Frozen backbone: ViT-B/16 pretrained on ImageNet-21k; last-block patch features (N×C, C=768) cached for the few-shot support set.\n2. Lightweight score network S_θ: two-layer MLP (C+embed(y)+4 → 1024 → C) with sinusoidal timestep embedding; ≈1.2 M parameters shared by all classes.\n3. Denoising score matching in feature space: add Gaussian noise to cached features at one of T=20 logarithmically spaced noise levels σ_t. Optimise\n     L_DSM = E_{f,y,t,ϵ}[‖S_θ(f+σ_tϵ,y,t) − ϵ‖²].\n   Training uses only the few labelled examples (no unlabelled data) and completes in ≤3 min on a single GPU because features are low-dimensional.\n4. Sampling: for each class, generate K synthetic features by 20-step DDPM ancestral sampling starting from N(0,σ_max²I) and conditioning on the class token. K is chosen so that synthetic:real = 10:1.\n5. Classifier training: concatenate real and synthetic features and train the standard MAP head g_ϕ for 2 000 steps. A small *manifold consistency* penalty encourages the classifier to agree on a real feature and its nearest synthetic neighbour:\n     L = CE(g_ϕ(f_real),y)+CE(g_ϕ(f_syn),y)+λ·KL(softmax g_ϕ(f_real) ‖ softmax g_ϕ(f_syn)).\n   λ=0.5 by default.\n6. Inference: only g_ϕ is used; S_θ is discarded, so runtime cost equals the FroFA baseline.\nNovel aspects:\n• First use of *diffusion modelling inside a frozen feature manifold* for data augmentation.\n• Generates unlimited, diversity-controlled, label-conditioned samples without touching pixels.\n• Trains with <2 M extra parameters and no backbone updates—compatible with on-device constraints.\n• Combines generative (diversity) and consistency (invariance) regularisation in a single, lightweight pipeline.",
    "experimental_setup": "Datasets\n• ID: ImageNet-1k 1/5/10-shot; CIFAR-100 5-shot.\n• OOD: ImageNet-R and ImageNet-Sketch (same classifier, no tuning).\nBaselines\n a) Linear probe b) FroFA c) Consistency-Regularised FroFA (CR-FroFA) d) Adversarial-Consistency FroFA (AC-FroFA).\nModels & Hyper-params\n• Backbone: ViT-B/16 frozen.\n• Score net: MLP 768→1024→768, SiLU, LayerNorm, T=20, σ_max=1.0 σ_feat.\n• Optimiser θ: Adam, lr 1e-3, 1 000 steps (1-shot) or 2 000 (≥5-shot), batch 64.\n• Classifier ϕ: MAP head 512→K, SGD lr 0.05, cosine decay, 2 000 steps, batch 256.\nEvaluation\n• Primary metric: top1_accuracy on ImageNet validation.\n• Secondary: top1_accuracy on OOD sets, robustness gap Δ=ID−OOD, calibration ECE.\nAblations\n(1) K ∈ {1×, 5×, 10×} synthetic ratio (2) remove consistency loss (3) vary σ_max and T (4) compare diffusion vs Gaussian / adversarial noise of equal norm.",
    "primary_metric": "top1_accuracy",
    "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\n\n# ---------- score network (feature diffusion) ----------\nclass ScoreNet(nn.Module):\n    def __init__(self, dim, n_classes, dim_embed=128):\n        super().__init__()\n        self.t_embed = nn.Sequential(\n            nn.Linear(4, dim_embed), nn.SiLU(), nn.Linear(dim_embed, dim_embed))\n        self.y_embed = nn.Embedding(n_classes, dim_embed)\n        self.net = nn.Sequential(\n            nn.Linear(dim+dim_embed, 1024), nn.SiLU(), nn.LayerNorm(1024),\n            nn.Linear(1024, dim))\n    def forward(self, f, y, t):            # f [B,C], y [B], t scalar in (0,1]\n        h = torch.cat([f, self.t_embed(t) + self.y_embed(y)], dim=-1)\n        return self.net(h)\n\n# ---------- single DSM training step ----------\n@torch.no_grad()\ndef sigma(t, sigma_max=1.0, sigma_min=0.01):\n    return sigma_max * (sigma_min/sigma_max)**t  # log-linear schedule\n\ndef dsm_step(scorenet, opt, f, y):\n    B = f.size(0)\n    t = torch.rand(B,1, device=f.device)\n    sig = sigma(t)\n    noise = torch.randn_like(f)\n    f_noisy = f + sig*noise\n    pred = scorenet(f_noisy, y, t)\n    loss = (pred - noise).pow(2).mean()\n    opt.zero_grad(); loss.backward(); opt.step();\n    return loss.item()\n\n# ---------- ancestral sampling (simplified) ----------\n@torch.no_grad()\ndef sample_features(scorenet, y, K=10, T=20, sigma_max=1.0, sigma_min=0.01):\n    f = torch.randn(len(y)*K, 768, device=y.device) * sigma_max\n    y_rep = y.repeat_interleave(K)\n    for i in range(T):\n        t = torch.full((f.size(0),1), 1 - i/(T-1), device=f.device)\n        sig = sigma(t, sigma_max, sigma_min)\n        grad = scorenet(f, y_rep, t)\n        f = f + (sig**2)*grad               # Euler–Maruyama step\n        if i < T-1:\n            sig_next = sigma(t - 1/(T-1), sigma_max, sigma_min)\n            f += torch.randn_like(f) * (sig_next * (sig_next>0)).sqrt()\n    return f\n\n# ---------- classifier + consistency loss ----------\nclass MAPHead(nn.Module):\n    def __init__(self, dim_in, dim_h, n_cls):\n        super().__init__(); self.attn = nn.MultiheadAttention(dim_in, 4, batch_first=True)\n        self.fc = nn.Linear(dim_in, n_cls)\n    def forward(self, x): v,_ = self.attn(x,x,x); return self.fc(v.mean(1))\n\ndef cls_step(model, opt, f_real, f_syn, y, lam=0.5):\n    logits_r = model(f_real)\n    logits_s = model(f_syn)\n    ce = F.cross_entropy(logits_r, y) + F.cross_entropy(logits_s, y)\n    kl = F.kl_div(F.log_softmax(logits_r,1), F.softmax(logits_s.detach(),1), 'batchmean')\n    loss = ce + lam*kl\n    opt.zero_grad(); loss.backward(); opt.step();\n    return loss.item()\n",
    "expected_result": "ImageNet-1k, ViT-B/16 frozen, mean of 5 seeds:\n               1-shot   5-shot  10-shot\nLinear probe   50.2     64.5     70.3\nFroFA          54.0     66.8     72.1\nCR-FroFA       55.5     68.2     73.3\nAC-FroFA       57.0     70.0     74.6\nCCFD-Aug       58.2±0.4 71.4±0.3 75.8±0.2\n\nOOD (ImageNet-R, 5-shot):\nLinear 40.1  FroFA 43.5  CR 44.2  AC 47.8  CCFD-Aug 49.9  (≈+2.1 pp vs AC-FroFA)\nID–OOD gap reduced by ≈30 % relative to FroFA. Calibration ECE falls from 8.7 % (AC) to 6.3 % (CCFD-Aug).\nComputation: score-net training <3 min (1-shot) to 8 min (10-shot) on RTX 3090; classifier training unchanged.",
    "expected_conclusion": "Learning a *class-conditional diffusion model inside the frozen feature manifold* allows us to synthesise unlimited, label-faithful features from just a handful of examples. Unlike adversarial or hand-crafted perturbations, CCFD-Aug explores the true class geometry, yielding both higher few-shot accuracy (+1.2 pp over the best prior method) and markedly better robustness to real distribution shifts, all without touching the backbone or incurring test-time cost. Academically, this is the first work to bridge diffusion modelling with parameter-efficient transfer learning, opening a new axis—*generative feature augmentation*—alongside adversarial and consistency approaches. Socially, the ability to train reliable vision systems from extremely scarce data and deploy them unchanged at inference time benefits privacy-sensitive and resource-limited applications such as conservation drones, rural health diagnostics, and assistive devices."
  }
}